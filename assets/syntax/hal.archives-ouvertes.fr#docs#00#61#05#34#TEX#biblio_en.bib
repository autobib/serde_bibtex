@article{ Ler:2010:iid,
	title = {Optimal model selection in density estimation},
	author = {Lerasle, Matthieu},
	JOURNAL = {Ann. Inst. H. Poincar{\'e} Probab. Statist.},
	FJOURNAL = {Annales de l'Institut Henri Poincar{\'e}. Probabilit{\'e}s et Statistiques},
	note = {Accepted. arXiv:0910.1654},
	YEAR = {2011},
	ISSN = {0246-0203},
	VOLUME = {},
	NUMBER = {},
	PAGES = {},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Arl_Mas:2009:pente,
    AUTHOR = {Arlot, Sylvain and Massart, Pascal},
     TITLE = {Data-driven calibration of penalties for least-squares regression},
   JOURNAL = {Journal of Machine Learning Research},
  FJOURNAL = {Journal of Machine Learning Research (JMLR)},
    VOLUME = {10},
      YEAR = {2009},
     PAGES = {245--279 (electronic)},
  SUBJECTS = {modsel},
       PDF = {http://www.di.ens.fr/willow/pdfs/2009\\_Arlot\\_Massart\\_JMLR.pdf},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@inproceedings{Arl_Bac:2009:minikernel_nips,
 title = {Data-driven calibration of linear estimators with minimal penalties},
 author = {Arlot, Sylvain and Bach, Francis},
 booktitle = {Advances in Neural Information Processing Systems 22},
 editor = {Y. Bengio and D. Schuurmans and J. Lafferty and C. K. I. Williams and A. Culotta},
 pages = {46--54},
 year = {2009},
  SUBJECTS = {modsel},
   PDF = {http://books.nips.cc/papers/files/nips22/NIPS2009\\_0291.pdf},
}


@misc{Arl_Bac:2009:minikernel_long,
    AUTHOR = {Arlot, Sylvain and Bach, Francis},
     TITLE = {Data-driven calibration of linear estimators with minimal penalties},
      NOTE = {arXiv:0909.1884v2},
     MONTH = jul,
      YEAR = {2011},
    EPRINT = {arXiv:0909.1884v2},
       PDF = {http://arxiv.org/pdf/0909.1884v2},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@Article{ Evgeniou,
	author = {Evgeniou, Theodoros and Micchelli, Charles A. and Pontil, Massimiliano},
	title = {Learning Multiple Tasks with Kernel Methods},
	journal = {Journal of Machine Learning Research},
	volume = {6},
	pages = {615--637},
	year = {2005}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@inproceedings{liang10regularization,
  title = {Asymptotically Optimal Regularization in Smooth Parametric Models},
  author = {Liang, Percy and Bach, Francis and Bouchard, Guillaume and Jordan, Michael I.},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2010},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@Article{ Zhang,
	author = {Zhang, Tong},
	title = {Learning Bounds for Kernel Regression using Effective Data Dimensionality},
	journal = {Neural Computation},
	year = {2005},
	volume = {17(9)},
	pages = {2077--2098}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@Book{ Massart,
	author = {Massart, Pascal},
	title = {Concentration Inequalities and Model Selection},
	publisher = {Springer},
	year = {2007},
	series = {{\'E}cole d'{\'E}t{\'e} de Probabilit{\'e}s de Saint Flour XXXIII - 2003}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@Article{ BirgeMassart_min_pen,
	author = {Birg{\'e}, Lucien and Massart, Pascal},
	title = {Minimal Penalties for {G}aussian Model Selection},
	journal = {Probability Theory and Related Fields},
	volume = {138},
	pages = {33--73},
	year = {2007}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@Article{ Aronszajn,
	author = {Aronszajn, Nachman},
	title = {Theory of Reproducing Kernels},
	journal = {Transactions of the American Mathematical Society},
	volume = {68},
	number = {3},
	year = {1950},
	month = {May},
	pages = {337--404}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


					@unpublished{
					BLEAKLEY:2009:HAL-00422430:1
					,
    HAL_ID = {hal-00422430},
    title = { {J}oint segmentation of many a{CGH} profiles using fast group {LARS}},
    author = {{B}leakley, {K}evin and {V}ert, {J}ean-{P}hilippe},
    abstract = {{A}rray-{B}ased {C}omparative {G}enomic {H}ybridization (a{CGH}) is a method used to search for genomic regions with copy numbers variations. {F}or a given a{CGH} profile, one challenge is to accurately segment it into regions of constant copy number. {S}ubjects sharing the same disease status, for example a type of cancer, often have a{CGH} profiles with similar copy number variations, due to duplications and deletions relevant to that particular disease. {W}e introduce a constrained optimization algorithm that jointly segments a{CGH} profiles of many subjects. {I}t simultaneously penalizes the amount of freedom the set of profiles have to jump from one level of constant copy number to another, at genomic locations known as breakpoints. {W}e show that breakpoints shared by many different profiles tend to be found first by the algorithm, even in the presence of significant amounts of noise. {T}he algorithm can be formulated as a group {LARS} problem. {W}e propose an extremely fast way to find the solution path, i.e., a sequence of shared breakpoints in order of importance. {F}or no extra cost the algorithm smoothes all of the a{CGH} profiles into piecewise-constant regions of equal copy number, giving low-dimensional versions of the original data. {T}hese can be shown for all profiles on a single graph, allowing for intuitive visual interpretation. {S}imulations and an implementation of the algorithm on bladder cancer a{CGH} profiles are provided.},
    language = {{A}nglais},
    affiliation = {{C}entre de {B}ioinformatique - {CBIO} - {M}ines {P}aris{T}ech - {C}ancer et g{\'e}n{\^o}me: {B}ioinformatique, biostatistiques et {\'e}pid{\'e}miologie d'un syst{\`e}me complexe - {INSERM} : {U}900 - {I}nstitut {C}urie - {M}ines {P}aris{T}ech },

				}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@Article{ Maurer2006,
	author = {Maurer, Andreas},
	title = {Bounds for Linear Multi-Task Learning},
	journal = {Journal of Machine Learning Research},
	year = {2006},
	volume = {7},
	pages = {117--139"}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



@book{ab-nnltf-99,
  author = {Anthony, Martin and Bartlett, Peter L.},
  title = {Neural Network Learning: Theoretical Foundations},
  publisher = {Cambridge University Press},
  year = {1999},
  isbn = {0 521 57353 X}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@Article{ bm-rgcrbsr-02,
	author = {Bartlett, Peter L.  and Mendelson, Shahar},
	title = {Rademacher and {G}aussian complexities: Risk bounds and structural results},
	journal = {Journal of Machine Learning Research},
	volume = {3},
	pages = {463--482},
	pdf = {http://www.jmlr.org/papers/volume3/bartlett02a/bartlett02a.pdf},
	year = {2002}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@mastersthesis{Solnon_master,
  author = {Solnon, Matthieu},
  title = {R{\'e}gression Multi-t{\^a}ches et P{\'e}nalit{\'e} Minimale},
  school = {Universit{\'e} Paris 11, {\'E}cole Normale Sup{\'e}rieure},
  year ={2010}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% hal-00461639, version 1
%% http://hal.archives-ouvertes.fr/hal-00461639/en/


@unpublished{BAUDRY:2010:HAL-00461639:1,
    HAL_ID = {hal-00461639},
    title = { {S}lope {H}euristics: {O}verview and {I}mplementation},
    author = {{B}audry, {J}ean-{P}atrick and {M}augis, {C}athy and {M}ichel, {B}ertrand},
    abstract = {{M}odel selection is a general paradigm which includes many statistical problems. {O}ne of the most fruitful and popular approaches to carry it out is the minimization of a penalized criterion. {B}irg{\'e} and {M}assart (2006) have proposed a promising data-driven method to calibrate such criteria whose penalties are known up to a multiplicative factor: the ``slope heuristics''. {T}heoretical works validate this heuristic method in some situations and several papers report a promising practical behavior in various frameworks. {T}he purpose of this work is twofold. {F}irst, an introduction to the slope heuristics and an overview of the theoretical and practical results about it are presented. {S}econd, we focus on the practical difficulties occurring for applying the slope heuristics. {A} new practical approach is carried out and compared to the standard dimension jump method. {A}ll the practical solutions discussed in this paper in different frameworks are implemented and brought together in a {M}atlab graphical user interface called capushe.},
    keywords = {{D}ata-driven slope estimation; {D}imension jump; {S}lope {H}euristics,},
    language = {{A}nglais},
    affiliation = {{U}niversit{\'e} {P}aris {S}ud - {U}niversit{\'e} {P}aris {S}ud - {P}aris {XI} - {M}ath{\'e}matiques appliqu{\'e}es {P}aris 5 - {MAP}5 - {CNRS} : {UMR}8145 - {U}niversit{\'e} {P}aris {D}escartes - {SELECT} - {INRIA} {S}aclay - {I}le de {F}rance - {INRIA} - {U}niversit{\'e} {P}aris {S}ud - {P}aris {XI} - {CNRS} : {UMR} - {I}nstitut de {M}ath{\'e}matiques de {T}oulouse - {IMT} - {U}niversit{\'e} {P}aul {S}abatier - {T}oulouse {III} - {U}niversit{\'e} {T}oulouse le {M}irail - {T}oulouse {II} - {U}niversit{\'e} des {S}ciences {S}ociales - {T}oulouse {I} - {I}nstitut {N}ational des {S}ciences {A}ppliqu{\'e}es de {T}oulouse - {CNRS} : {UMR}5219 - {L}aboratoire de {S}tatistique {T}h{\'e}orique et {A}ppliqu{\'e}e - {LSTA} - {U}niversit{\'e} {P}ierre et {M}arie {C}urie - {P}aris {VI} },
    note = {{RR}-7223 {RR} {INRIA}-7223, {V}ersion 1 },
    day = {05},
    month = {03},
    year = {2010},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@PhDThesis{ VerzelenPhd,
	author = {N. Verzelen},
	title = {Gaussian Graphical Models and Model Selection},
	school = {Universit{\'e} Paris 11},
	year = {2008}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@PhDThesis{ VillersPhd,
	author = {Villers, Fanny},
	title = {Tests et selection de mod{\`e}les pour l'analyse de donn{\'e}es prot{\'e}omiques et transcriptomiques},
	school = {Universit{\'e} Paris 11},
	year = {2007}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@PhDThesis{ SaumardPhd,
	author = {Saumard, Adrien},
	title = {Estimation par minimum de contraste r{\'e}gulier et heuristique de pente en s{\'e}lection de mod{\`e}les},
	school = {Universit{\'e} de Rennes I},
	year = {2010}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@Article{ Lounici_Pontil_Tsybakov_VDGeer,
	author = {Lounici, Karim and Pontil, Massimiliano and Tsybakov, Alexandre and van de Geer, Sarah A.},
	title = {Taking advantage of sparsity in multi-task learning},
	journal = {Conference On Learning Theory},
	year = {2009}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Bakker:2003:TCG:945365.945370,
 author = {Bakker, Bart and Heskes, Tom},
 title = {Task clustering and gating for bayesian multitask learning},
 journal = {Journal of Machine Learning Research},
 volume = {4},
 month = {December},
 year = {2003},
 issn = {1532-4435},
 pages = {83--99},
 numpages = {17},
 doi = {http://dx.doi.org/10.1162/153244304322765658},
 acmid = {945370},
 publisher = {JMLR.org},
} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Thrun96g,
  AUTHOR         = {Thrun, Sebastian and O'Sullivan, Joseph},
  YEAR           = {1996},
  TITLE          = {Discovering Structure in Multiple Learning Tasks: 
                    The {TC} Algorithm},
  journal      = {Proceedings of the 13th International Conference on 
                    Machine Learning},
  EDITOR         = {L. Saitta},
  PUBLISHER      = {Morgen Kaufmann},
  ADDRESS        = {San Mateo, CA}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@comment{{ Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use http://www.jstor.org/page/info/about/policies/terms.jsp. If you have questions or need assistance using JSTOR, please contact JSTOR Support ($acsServer/action/showContactSupportForm) and let us know how we can help you. }}
@comment{{NUMBER OF CITATIONS : 1}}

    @article{Brown_Zidek_1980,
     jstor_articletype = {research-article},
     title = {Adaptive Multivariate Ridge Regression},
     author = {Brown, Philip J. and Zidek, James V.},
     journal = {The Annals of Statistics},
     jstor_issuetitle = {},
     volume = {8},
     number = {1},
     jstor_formatteddate = {Jan., 1980},
     pages = {pp. 64-74},
     ISSN = {00905364},
     abstract = {A multivariate version of the Hoerl-Kennard ridge regression rule is introduced. The choice from among a large class of possible generalizations is guided by Bayesian considerations; the result is implicitly in the work of Lindley and Smith although not actually derived there. The proposed rule, in a variety of equivalent forms is discussed and the choice of its ridge matrix considered. As well, adaptive multivariate ridge rules and closely related empirical Bayes procedures are presented, these being for the most part formal extensions of certain univariate rules. Included is the Efron-Morris multivariate version of the James-Stein estimator. By means of an appropriate generalization of a result of Morris (see Thisted) the mean square error of these adaptive and empirical Bayes rules are compared.},
     language = {English},
     year = {1980},
     publisher = {Institute of Mathematical Statistics},
     copyright = {Copyright © 1980 Institute of Mathematical Statistics},
    }


@comment{{ These records have been provided through JSTOR. http://www.jstor.org }}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Caruana:1997:ML:262868.262872,
 author = {Caruana, Rich},
 title = {Multitask Learning},
 journal = {Machine Learning},
 volume = {28},
 issue = {1},
 month = {July},
 year = {1997},
 issn = {0885-6125},
 pages = {41--75},
 numpages = {35},
 doi = {10.1023/A:1007379606734},
 acmid = {262872},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {backpropagation, generalization, inductive transfer, k-nearest neighbor, kernel regression, multitask learning, parallel transfer, supervised learning},
} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@book{Schölkopf_Smola_learning,
   author = {Sch\"olkopf, Bernhard  and Smola, Alexander J.},
   title = {Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond},
   year = {2002},
   series = {Adaptive Computation and Machine Learning},
   publisher = {MIT Press},
   pages = {644},
   month = {12},
   address = {Cambridge, MA, USA},
   abstract = {In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs-kernels—for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics.
Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@ARTICLE{Aka70,
  AUTHOR = {Akaike , Hirotogu},
  TITLE = {Statistical Predictor Identification},
  JOURNAL = {Annals of the Institute of Statistical Mathematics},
  YEAR = {1970},
  VOLUME = {22},
  PAGES = {203-217}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@Book{ Horn_Johnson_Matrix_analysis,
	author = {Horn, Roger A. and Johnson, Charles R.},
	title = {Topics in Matrix Analysis},
        isbn={9780521467131},
	publisher = {Cambridge University Press},
	year = {1991},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


@article{Arl:2009:RP,
    AUTHOR = {Arlot, Sylvain},
     TITLE = {Model selection by resampling penalization},
   JOURNAL = {Electron. J. Stat.},
  FJOURNAL = {Electronic Journal of Statistics},
    VOLUME = {3},
      YEAR = {2009},
     PAGES = {557--624 (electronic)},
      ISSN = {1935-7524},
       DOI = {10.1214/08-EJS196},
      SICI = {1935-7524(2009)3:0<557:MSBRP>2.0.CO;2-3},
       PDF = {http://www.di.ens.fr/willow/pdfs/2009_Arlot_EJS-2008-196.pdf},
  SUBJECTS = {reech modsel},
   MRCLASS = {62Gxx},
  MRNUMBER = {MR2519533},
}


%% hal-00262478, version 2
%% http://hal.archives-ouvertes.fr/hal-00262478/en/
@article{ARLOT:2009:HAL-00262478:2,
    HAL_ID = {hal-00262478},
    title = { {M}odel selection by resampling penalization},
    author = {{A}rlot, {S}ylvain},
    abstract = {{I}n this paper, a new family of resampling-based penalization procedures for model selection is defined in a general framework. {I}t generalizes several methods, including {E}fron's bootstrap penalization and the leave-one-out penalization recently proposed by {A}rlot (2008), to any exchangeable weighted bootstrap resampling scheme. {I}n the heteroscedastic regression framework, assuming the models to have a particular structure, these resampling penalties are proved to satisfy a non-asymptotic oracle inequality with leading constant close to 1. {I}n particular, they are asympotically optimal. {R}esampling penalties are used for defining an estimator adapting simultaneously to the smoothness of the regression function and to the heteroscedasticity of the noise. {T}his is remarkable because resampling penalties are general-purpose devices, which have not been built specifically to handle heteroscedastic data. {H}ence, resampling penalties naturally adapt to heteroscedasticity. {A} simulation study shows that resampling penalties improve on {V}-fold cross-validation in terms of final prediction error, in particular when the signal-to-noise ratio is not large.},
    keywords = {non-parametric statistics ; resampling ; exchangeable weighted bootstrap ; model selection ; penalization ; non-parametric regression ; adaptivity ; heteroscedastic data ; regressogram ; histogram selection},
    language = {{E}nglish},
    affiliation = {{L}aboratoire d'informatique de l'{\'e}cole normale sup{\'e}rieure - {LIENS} - {CNRS} : {UMR}8548 - {E}cole {N}ormale {S}up{\'e}rieure de {P}aris - {ENS} {P}aris - {WILLOW} - {INRIA} {R}ocquencourt - {INRIA} - {E}cole {N}ormale {S}up{\'e}rieure de {P}aris - {ENS} {P}aris - {E}cole des {P}onts {P}aris{T}ech - {CNRS} : {UMR}8548 },
    pages = {557--624 },
    journal = {{E}lectronic {J}ournal of {S}tatistics },
    volume = {3 },
    note = {extended version of http://hal.archives-ouvertes.fr/hal-00125455, with a technical appendix {AMS} 62{G}09 ; 62{G}08 ; 62{M}20 },
    audience = {international },
    doi = {10.1214/08-EJS196 },
    month = {06},
    year = {2009},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Obozinski_Wainwright_Jordan_2011,
title = {Support Union Recovery in High-Dimensional Multivariate Regression},
author = {Obozinski, Guillaume and Wainwright, Martin J. and Jordan, Michael I. },
journal = {The Annals of Statistics},
year = {2011},
volume = {39},
number = {1},
pages = {1-17},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{DBLP:journals/ml/ArgyriouEP08,
  author    = {Argyriou, Andreas  and
               Evgeniou, Theodoros  and
                Pontil, Massimiliano},
  title     = {Convex multi-task feature learning},
  journal   = {Machine Learning},
  volume    = {73},
  number    = {3},
  year      = {2008},
  pages     = {243-272},
  ee        = {http://dx.doi.org/10.1007/s10994-007-5040-8},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{Ando:2005:FLP:1046920.1194905,
 author = {Ando, Rie Kubota and Zhang, Tong},
 title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
 journal = {Journal of  Machine Learning Research},
 volume = {6},
 month = {December},
 year = {2005},
 issn = {1532-4435},
 pages = {1817-1853},
 numpages = {37},
 acmid = {1194905},
 publisher = {JMLR.org},
} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@techreport{Lounici:1277345,
      author       = "Lounici, Karim and Pontil, Massimiliano and Tsybakov,
                      Alexandre B. and van de Geer, Sara",
      title        = "Oracle Inequalities and Optimal Inference under Group
                      Sparsity",
      number       = "arXiv:1007.1771",
      month        = "Jul",
      year         = "2010",
      note         = "Comments: 37 pages",
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{gasso2009,
   author={Gasso, Gilles and Rakotomamonjy, Alain and Canu, St\'ephane},
   title={Recovering sparse signals with non-convex penalties and DC programming},
   journal={IEEE Trans. Signal Processing},
   volume={57},
   number={12},
   year={2009},
   pages={4686--4698}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@book{ GP,
title={Gaussian Processes for Machine Learning},
author={Rasmussen, Carl E. and Williams, Christopher K.I.},
publisher={MIT Press},
year={2006}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@book {Wah:1990,
    AUTHOR = {Wahba, Grace},
     TITLE = {Spline Models for Observational Data},
    SERIES = {CBMS-NSF Regional Conference Series in Applied Mathematics},
    VOLUME = {59},
 PUBLISHER = {Society for Industrial and Applied Mathematics (SIAM)},
   ADDRESS = {Philadelphia, PA},
      YEAR = {1990},
     PAGES = {xii+169},
      ISBN = {0-89871-244-0},
   MRCLASS = {62G05 (62J02 65D10 65U05)},
  MRNUMBER = {MR1045442 (91g:62028)},
MRREVIEWER = {Girdhar G. Agarwal},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@ARTICLE{ jacob:clustered,
AUTHOR = {Laurent Jacob and Francis Bach and Jean-Philippe Vert},
TITLE = {Clustered Multi-Task Learning: A Convex Formulation},
JOURNAL = {Computing Research Repository},
PAGES = {-1--1},
YEAR = {2008}
  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@book{hastie1990generalized,
  title={Generalized Additive Models},
  author={Hastie, Trevor J. and Tibshirani, Robert J.},
  isbn={9780412343902},
  year={1990},
  publisher={Taylor and Francis}
}
