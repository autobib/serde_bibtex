%%% ----------------------------------------------------------------------
%%% BibTeX-file {
%%%    author	 = "{Electronic Visualization Library Service}",
%%%    filename  = "EVL-1998.bib",
%%%    address   = "Konrad-Zuse-Zentrum f{\"u}r
%%%                 Informationstechnik Berlin (ZIB)
%%%                 Scientific Visualization Department
%%%                 Takustr. 7
%%%                 14195 Berlin
%%%                 Germany",
%%%    URL       = "http://visinfo.zib.de/EVlib/",
%%%    email     = "davis@zib.de",
%%%    supported = "yes",
%%%    docstring = "This file contains the complete bibliography of
%%%                 references submitted to the 
%%%                 Electronic Visualization Library for the year 1998.",
%%% }


@InProceedings{EVL-1998-1,
  year =         "1998",
  title =        "An integrated problem solving environment: the
                 {SCIR}un computational steering system",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-1",
  author =       "S. G. Parker and M. Miller and C. D. Hansen and C. R.
                 Johnson",
  abstract =     "The usefulness of computational steering tools and
                 systems need not be argued. However, the mechanisms for
                 implementing these tools differ tremendously as we
                 illustrate in a new steering taxonomy. Many times,
                 steering mechanisms limit the steering to either
                 modifications during the algorithm development phase or
                 during the modeling and computational cycle but inhibit
                 steering for all phases. Problem Solving Environments
                 (PSEs) extend steering capabilities by allowing similar
                 steering mechanisms to be exploited during all phases
                 of development, application, and performance steering,
                 while also allowing the same visualization and analysis
                 tools to be used during all phases. An integrated PSE
                 provides a complete set of tools for a scientist to
                 solve a class of problems. In this context,
                 computational steering becomes a versatile tool for
                 making changes in models, for developing new algorithms
                 and for tuning the performance of an application. The
                 intent is for the PSE to focus on helping the scientist
                 accurately solve a problem in a minimum amount of time.
                 SCIRun, our model PSE, is a scientific programming
                 environment that allows the interactive construction,
                 debugging, and steering of large-scale scientific
                 computations. Using this {"}computational workbench,{"}
                 a scientist can design, modify, and visualize
                 simulations interactively via a dataflow programming
                 model. We describe SCIRun in the context of other PSEs
                 and steering tools and discuss why a tightly integrated
                 problem solving environment such as SCIRun, helps limit
                 and simplify the design and debugging phases of
                 computational science applications and how such an
                 environment aids in the scientific discovery process.",
  language =     "en",
  note =         "submitted",
  booktitle =    "31st Hawaii International Conference on System
                 Sciences (HICSS-31)",
}

@InProceedings{EVL-1998-10,
  year =         "1998",
  title =        "Ein {FEM}-basierter Mimikgenerator f{\"{u}}r animierte
                 anthropomorphe Avatare",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-10",
  author =       "Rolf M. Koch and Markus H. Gross and Albert A.
                 Bosshard",
  language =     "de",
  abstract =     "Diese Arbeit beschreibt den Prototypen eines Editors
                 f{\"{u}}r menschliche Gesichtsausdr{\"{u}}cke.
                 Besondere Merkmale des vorgestellten Systems sind dabei
                 der enge Bezug zu anatomischen Daten f{\"{u}}r die
                 Simulation sowie die Ber{\"{u}}cksichtigung der
                 Gesichtsanatomie bei der Definition der Muskelgruppen.
                 Sowohl die hohe Qualit{\"{a}}t der C1-stetigen
                 Ausgabegeometrie als auch der Abstraktionsgrad dieses
                 Editors, der die emotionsbasierte Modellierung
                 unterst{\"{u}}tzt, sind weitere auszeichnende
                 Eigenschaften. Durch die Verwendung der Methode der
                 Finiten Elemente wird eine bessere Approximation des
                 Ergebnisses erreicht als dies z.B. bei
                 Partikel-Systemen der Fall ist. Das pr{\"{a}}sentierte
                 Verfahren kann zur Animation von anthropomorphen
                 Avataren in virtuellen Umgebungen dienen.",
  organization = "ETH Z{\"u}rich, Institut for Sientific Computing",
  month =        jan,
  note =         "Darmstadt, 28.-29.10.1997",
  booktitle =    "Proceedings of AAA 97",
}

@InProceedings{EVL-1998-100,
  pages =        "137--143",
  year =         "1998",
  title =        "Multi-faceted Insight Through Interoperable Visual
                 Information Analysis Paradigms",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-100",
  author =       "Beth Hetzler and Paul Whitney and Lou Martucci and Jim
                 Thomas",
  language =     "en",
  abstract =     "To gain insight and understanding of complex
                 information collections, users must be able to
                 visualize and explore many facets of the information.
                 This paper presents several novel visual methods from
                 an information analyst perspective. We present a sample
                 scenario, using the various tools to gain a variety of
                 insights from a large information collection. We
                 conclude that no single paradigm or visual method is
                 sufficient for many analytical tasks. Often a suite of
                 integrated methods offers a better analytic environment
                 in today's emerging culture of information overload and
                 rapidly changing issues. We also conclude that the
                 interactions among these visual paradigms are equally
                 as important as, if not more important than, the
                 paradigms themselves.",
  organization = "IEEE",
  booktitle =    "Proceedings IEEE Symposium on Information
                 Visualization 1998",
}

@InProceedings{EVL-1998-101,
  pages =        "95--104",
  year =         "1998",
  title =        "{MAPS}: Multiresolution Adaptive Parametrization of
                 Surfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-101",
  author =       "Aaron W. F. Lee and Wim Sweldens and Peter
                 Schr{\"o}der and Lawrence Cowsar and David Dobkin",
  language =     "en",
  abstract =     "We construct smooth parametrizations of irregular
                 connectivity triangulations of arbitrary genus
                 2-manifolds. Our algorithm uses hierarchical
                 simplification to efficiently induce a parametrization
                 of the original mesh over a base domain consisting of a
                 small number of triangles. This initial parametrization
                 is further improved through a hierarchical smoothing
                 procedure based on Loop subdivision applied in the
                 parameter domain. Our method supports both fully
                 automatic and user constrained operations. In the
                 latter, we accomodate point and edge constraints to
                 force the alignment of iso-parameter lines with desired
                 features. We show how to use the parametrization for
                 fast, hierarchical subdivision connectivity remeshing
                 with guaranteed error bounds. The remeshing algorithm
                 constructs an adaptively subdivided mesh directly
                 without first resorting to uniform subdivision followed
                 by subsequent sparsification. It thus avoids the
                 exponential cost of the latter. Our parametrizations
                 are also useful for texture mapping and morphing
                 applications, among others.",
  organization = "ACM SIGGRAPH",
  keywords =     "meshes, surface parametrization, mesh simplification,
                 remeshing, texture mapping, multiresolution,
                 subdivision surfaces, Loop scheme",
  booktitle =    "SIGGRAPH'98 Conference Proceedings",
}

@Book{EVL-1998-102,
  year =         "1998",
  title =        "Computational Visualization. Graphics, Abstraction,
                 and Interactivity",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-102",
  author =       "Thomas Strothotte",
  language =     "en",
  abstract =     "A unified and coherent introduction to the notion of
                 abstraction in interactive computer graphics is
                 provided by this book. Abstraction entails refinement
                 of images based on geometric models so as to reflect
                 the importance of the features of the model for the
                 dialog context and the visualization goal. This may
                 require leaving out irrelevant details or accentuating
                 significant features by adding details or enlarging or
                 deforming parts. Such modifications are routine by hand
                 but are at the leading edge of research in 2D and 3D
                 computer graphics. The authors see the abstraction
                 process as an interactive exploration of complex
                 information spaces, and report especially on zooming
                 and rendering techniques. Benefits are discussed for
                 applications in medical illustration and technical
                 documentation.",
  keywords =     "Visualization, Navigation in information spaces,
                 Graphical abstraction, Medical education, Technical
                 documentation",
  copyright =    "Springer",
  publisher =    "Springer, Heidelberg",
}

@InProceedings{EVL-1998-103,
  pages =        "7--14",
  year =         "1998",
  title =        "3{D} Scan Conversion of {CSG} Models into Distance
                 Volumes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-103",
  author =       "David E. Breen and Sean Mauch and Ross T. Whitaker",
  language =     "en",
  abstract =     "A distance volume is a volume dataset where the value
                 stored at each voxel is the shortest distance to the
                 surface of the object being represented by the volume.
                 Distance volumes are a useful representation in a
                 number of computer graphics applications. In this paper
                 we present a technique for generating a distance volume
                 with sub-voxel accuracy from one type of geometric
                 model, a Constructive Solid Geometry (CSG) model
                 consisting of superellipsoid primitives. The distance
                 volume is generated in a two step process. The first
                 step calculates the shortest distance to the CSG model
                 at a set of points within a narrow band around the
                 evaluated surface. Additionally, a second set of
                 points, labeled the zero set, which lies on the CSG
                 model's surface are computed. A point in the zero set
                 is associated with each point in the narrow band. Once
                 the narrow band and zero set are calculated, a Fast
                 Marching Method is employed to propagate the shortest
                 distance and closest point information out to the
                 remaining voxels in the volume. Our technique has been
                 used to scan convert a number of CSG models, producing
                 distance volumes which have been utilized in a variety
                 of computer graphics applications, e.g. CSG surface
                 evaluation, offset surface generation, and 3-D model
                 morphing.",
  organization = "IEEE, ACM SIGGRAPH",
  copyright =    "IEEE, ACM SIGGRAPH",
  booktitle =    "IEEE Symposium on Volume Visualization",
}

@InProceedings{EVL-1998-104,
  pages =        "15--22",
  year =         "1998",
  title =        "Coloring Voxel-Based Objects For Virtual Endoscopy",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-104",
  author =       "Omer Shibolet and Daniel Cohen-Or",
  language =     "en",
  abstract =     "This paper describes a method for coloring voxel-based
                 models. The method generalizes the two-part texture
                 mapping technique to color non-convex objects in a more
                 natural way. The method was developed for coloring
                 internal cavities for the application of virtual
                 endoscopy, where the surfaces are shaped like a general
                 cylinder in the macro-level, but with folds and bumps
                 in the more detailed levels. Given a flat texture, the
                 coloring method defines a mapping between the 3D
                 surface and the texture which reflects the tensions of
                 the points on the surface. The core of the method is a
                 technique for mapping such non-convex surfaces to
                 convex ones. The new technique is based on a discrete
                 dilation process that is fast and robust, and bypasses
                 many of the numerical problems common to previous
                 methods.",
  organization = "IEEE, ACM SIGGRAPH",
  keywords =     "texture mapping, voxel-based coloring, virtual
                 endoscopy, 3D morphology",
  copyright =    "IEEE, ACM SIGGRAPH",
  booktitle =    "IEEE Symposium on Volume Visualization",
}

@InProceedings{EVL-1998-105,
  pages =        "23--30",
  year =         "1998",
  title =        "Using Distance Maps for Accurate Surface
                 Representation in Sampled Volumes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-105",
  author =       "Sarah F. F. Gibson",
  language =     "en",
  abstract =     "High quality rendering and physics-based modeling in
                 volume graphics have been limited because
                 intensity-based volumetric data do not represent
                 surfaces well. High spatial frequencies due to abrupt
                 intensity changes at object surfaces result in jagged
                 or terraced surfaces in rendered images. The use of a
                 distance-to-closest-surface function to encode object
                 surfaces is proposed. This function varies smoothly
                 across surfaces and hence can be accurately
                 reconstructed from sampled data. The zero-value
                 iso-surface of the distance map yields the object
                 surface and the derivative of the distance map yields
                 the surface normal. Examples of rendered images are
                 presented along with a new method for calculating
                 distance maps from sampled binary data.",
  organization = "IEEE, ACM SIGGRAPH",
  keywords =     "Volume Rendering, Volume Graphics, Surgical
                 Simulation, Medical Applications",
  copyright =    "IEEE, ACM SIGGRAPH",
  booktitle =    "IEEE Symposium on Volume Visualization",
}

@InProceedings{EVL-1998-106,
  pages =        "31--38",
  year =         "1998",
  title =        "A Real-Time Volume Rendering Architecture Using an
                 Adaptive Resampling Scheme for Parallel and Perspective
                 Projections",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-106",
  author =       "Masato Ogata and TakaHide Ohkami and Hugh C. Lauer and
                 Hanspeter Pfister",
  language =     "en",
  abstract =     "This paper describes an object-order real-time volume
                 rendering architecture using an adaptive resampling
                 scheme to perform re-sampling operations in a unified
                 parallel-pipeline manner for both parallel and
                 perspective projections. Unlike parallel projections,
                 perspective projections require a variable resampling
                 structure due to diverging perspective rays. In order
                 to address this issue, we propose an adaptive pipelined
                 convolution block for resampling operations using the
                 level of resolution to keep the parallel-pipeline
                 structure regular. We also propose to use
                 multi-resolution datasets prepared for different levels
                 of grid resolution to bound the convolution operations.
                 The proposed convolution block is organized using a
                 systolic array structure, which works well with a
                 distributed skewed memory for conflict-free accesses of
                 voxels. We present the results of some experiments with
                 our software simulators of the proposed architecture
                 and discuss about important technical issues.",
  organization = "IEEE, ACM SIGGRAPH",
  keywords =     "Volume Graphics, Volume Rendering, Raycasting,
                 Raytracing, Parallel Projection, Perspective
                 Projection, Scientific Visualization, Real-Time,
                 Systolic Array",
  copyright =    "IEEE, ACM SIGGRAPH",
  booktitle =    "IEEE Symposium on Volume Visualization",
}

@InProceedings{EVL-1998-107,
  pages =        "39--46",
  year =         "1998",
  title =        "Adding Shadows to a Texture-Based Volume Renderer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-107",
  author =       "Uwe Behrens and Ralf Ratering",
  language =     "en",
  abstract =     "Texture-based volume rendering is a technique to
                 efficiently visualize volumetric data using texture
                 mapping hardware. In this paper we present an
                 algorithm, that extends this approach to render shadows
                 for the volume. The algorithm takes advantage of fast
                 frame-buffer operations modern graphics hardware
                 offers, but does not depend on any special purpose
                 hardware. The visual impression of the final image is
                 significantly improved by bringing more structure and
                 three-dimensional information into the often foggyish
                 appearance of texture-based volume renderings. Although
                 the algorithm does not perform lighting calculations,
                 the resulting image has a shaded appearance, which is a
                 further visual cue to spatial understanding of the data
                 and lets the images appear more realistic. As
                 calculating the shadows is independent of the
                 visualization process it can be applied to any form of
                 volume visualization, though volume rendering based on
                 two- or three-dimensional texture mapping hardware
                 makes the most sense. Compared to unshadowed
                 texture-based volume rendering, performance decreases
                 by less than 50%, which is still sufficient to
                 guarantee interactive manipulation of the volume data.
                 In the special case where only the camera is moving
                 with the light position fixed to the scene there is no
                 performance decrease at all, because recalculation has
                 only to be done if the position of the light source
                 with respect to the volume changes.",
  organization = "IEEE, ACM SIGGRAPH",
  keywords =     "volume rendering, shadow algorithms, texture-based
                 volume rendering, image compositing",
  copyright =    "IEEE, ACM SIGGRAPH",
  booktitle =    "IEEE Symposium on Volume Visualization",
}

@InProceedings{EVL-1998-108,
  pages =        "47--53",
  year =         "1998",
  title =        "Volume Animation using the Skeleton Tree",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-108",
  author =       "Nikhil Gagvani and D. Kenchammana-Hosekote and D.
                 Silver",
  language =     "en",
  abstract =     "In this paper, we describe a technique to animate
                 volumes using a volumetric skeleton. The skeleton is
                 computed from the actual volume, based on a reversible
                 thinning procedure using the distance transform.
                 Polygons are never computed, and the entire process
                 remains in the volume domain. The skeletal points are
                 connected and arranged in a {"}skeleton-tree{"}, which
                 can be used for articulation in an animation program.
                 The full volume object is regrown from the transformed
                 skeletal points. Since the skeleton is an intuitive
                 mechanism for animation, the animator deforms the
                 skeleton and causes corresponding deformations in the
                 volume object. The volumetric skeleton can also be used
                 for volume morphing, automatic path navigation, volume
                 smoothing and compression/decimation.",
  organization = "IEEE, ACM SIGGRAPH",
  keywords =     "Volume Graphics, Skeleton, Animation, Volume
                 Deformation",
  copyright =    "IEEE, ACM SIGGRAPH",
  booktitle =    "IEEE Symposium on Volume Visualization",
}

@InProceedings{EVL-1998-109,
  pages =        "55--62",
  year =         "1998",
  title =        "Adaptive Perspective Ray Casting",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-109",
  author =       "Kevin Kreeger and Ingmar Bitter and Frank Dachille and
                 Baoquan Chen and Arie Kaufman",
  language =     "en",
  abstract =     "We present a method to accurately and efficiently
                 perform perspective volumetric ray casting of uniform
                 regular datasets, called Exponential-Region (ER)
                 Perspective. Unlike previous methods which undersample,
                 oversample, or approximate the data, our method near
                 uniformly samples the data throughout the viewing
                 volume. In addition, it gains algorithmic advantages
                 from a regular sampling pattern and cache-coherent read
                 access, making it an algorithm well suited for
                 implementation on hardware architectures for volume
                 rendering. We qualify the algorithm by its filtering
                 characteristics and demonstrate its effectiveness by
                 contrasting its antialiasing quality and timing with
                 other perspective ray casting methods.",
  organization = "IEEE, ACM SIGGRAPH",
  keywords =     "Volume rendering, Perspective ray casting, Adaptive
                 supersampling, Volume rendering hardware",
  copyright =    "IEEE, ACM SIGGRAPH",
  booktitle =    "IEEE Symposium on Volume Visualization",
}

@TechReport{EVL-1998-11,
  year =         "1998",
  title =        "A Bernstein-{B}{\'{e}}zier Based Approach to Soft
                 Tissue Simulation",
  author =       "S. H. Martin Roth and Markus Hans Gross and Silvio
                 Turello and Friedrich Robert Carls",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-11",
  language =     "en",
  abstract =     "This paper discusses a Finite Element approach for
                 volumetric soft tissue modeling in the context of
                 facial surgery simulation. We elaborate on the
                 underlying physics and address some computational
                 aspects of the finite element discretization. In
                 contrast to existing approaches speed is not our first
                 concern, but we strive for the highest possible
                 accuracy of simulation. We therefore propose an
                 extension of linear elasticity towards
                 incompressibility and nonlinear material behavior, in
                 order to describe the complex properties of human soft
                 tissue more accurately. Furthermore, we incorporate
                 higher order interpolation functions using a
                 Bernstein-B{\'{e}}zier formulation, which has various
                 advantageous properties, such as its integral
                 polynomial form of arbitrary degree, efficient
                 subdivision schemes, and suitability for geometric
                 modeling and rendering. In addition, the use of
                 tetrahedral Finite Elements does not put any
                 restriction on the geometry of the simulated volumes.
                 Experimental results obtained from a synthetic block of
                 soft tissue and from the Visible Human Data Set
                 illustrate the performance of the envisioned model.",
  month =        jan,
  keywords =     "finite element, facial surgery simulation",
  number =       "282",
  institution =  "Computer Science Department, ETH Z{\"{u}}rich",
}

@InProceedings{EVL-1998-110,
  pages =        "63--69",
  year =         "1998",
  title =        "Edge Preservation in Volume Rendering Using
                 Splatting",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-110",
  author =       "Jian Huang and Roger Crawfis",
  language =     "en",
  abstract =     "This paper presents a method to preserve sharp edge
                 details in splatting for volume rendering. Conventional
                 splatting algorithms produce fuzzy images for views
                 close to the volume model. The lack of details in such
                 views greatly hinders study and manipulation of data
                 sets using virtual navigation. Our method applies a
                 non-linear warping to the footprints of conventional
                 splat and builds a table of footprints for different
                 possible edge positions and edge strengths. When
                 rendering, we pick a footprint from the table for each
                 splat, based on the relative position of the voxel to
                 the closest edge. Encouraging results have been
                 achieved both for synthetic data and medical data.",
  organization = "IEEE, ACM SIGGRAPH",
  copyright =    "IEEE, ACM SIGGRAPH",
  booktitle =    "IEEE Symposium on Volume Visualization",
}

@InProceedings{EVL-1998-111,
  pages =        "71--78",
  year =         "1998",
  title =        "Probabilistic Segmentation of Volume Data for
                 Visualization Using {SOM}-{PNN} Classifier",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-111",
  author =       "Feng Ma and Wenping Wang and Wai Wan Tsang and Zesheng
                 Tang and Shaowei Xia and Xin Tong",
  language =     "en",
  abstract =     "We present a new probabilistic classifier, called
                 SOM-PNN classifier, for volume data classification and
                 visualization. The new classifier produces
                 probabilistic classification with Bayesian confidence
                 measure which is highly desirable in volume rendering.
                 Based on the SOM map trained with a large training data
                 set, our SOM-PNN classifier performs the probabilistic
                 classification using the PNN algorithm. This combined
                 use of SOM and PNN overcomes the shortcomings of the
                 parametric methods, the non-parametric methods, and the
                 SOM method. The proposed SOM-PNN classifier has been
                 used to segment the CT sloth data and the 20 human MRI
                 brain volumes resulting in much more informative 3D
                 rendering with more details and less artifacts than
                 other methods. Numerical comparisons demonstrate that
                 the SOM-PNN classifier is a fast, accurate and
                 probabilistic classifier for volume rendering.",
  organization = "IEEE, ACM SIGGRAPH",
  keywords =     "medical image segmentation, multiscale, wavelet
                 transform, SOM, PNN, SOM-PNN classifier, 3D volume
                 rendering",
  copyright =    "IEEE, ACM SIGGRAPH",
  booktitle =    "IEEE Symposium on Volume Visualization",
}

@InProceedings{EVL-1998-112,
  pages =        "79--86",
  year =         "1998",
  title =        "Semi-Automatic Generation of Transfer Functions for
                 Direct Volume Rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-112",
  author =       "Gordon Kindlmann and James W. Durkin",
  language =     "en",
  abstract =     "Although direct volume rendering is a powerful tool
                 for visualizing complex structures within volume data,
                 the size and complexity of the parameter space
                 controlling the rendering process makes generating an
                 informative rendering challenging. In particular, the
                 specification of the transfer function the mapping from
                 data values to renderable optical properties is
                 frequently a time-consuming and unintuitive task.
                 Ideally, the data being visualized should itself
                 suggest an appropriate transfer function that brings
                 out the features of interest without obscuring them
                 with elements of little importance. We demonstrate that
                 this is possible for a large class of scalar volume
                 data, namely that where the regions of interest are the
                 boundaries between different materials. A transfer
                 function which makes boundaries readily visible can be
                 generated from the relationship between three
                 quantities: the data value and its first and second
                 directional derivatives along the gradient direction. A
                 data structure we term the histogram volume captures
                 the relationship between these quantities throughout
                 the volume in a position independent, computationally
                 efficient fashion. We describe the theoretical
                 importance of the quantities measured by the histogram
                 volume, the implementation issues in its calculation,
                 and a method for semi-automatic transfer function
                 generation through its analysis. We conclude with
                 results of the method on both idealized synthetic data
                 as well as real world datasets.",
  organization = "IEEE, ACM SIGGRAPH",
  copyright =    "IEEE, ACM SIGGRAPH",
  booktitle =    "IEEE Symposium on Volume Visualization",
}

@InProceedings{EVL-1998-113,
  pages =        "87--94",
  year =         "1998",
  title =        "An Exact Interactive Time Visibility Ordering
                 Algorithm for Polyhedral Cell Complexes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-113",
  author =       "Cl{\´a}udio T. Silva and Joseph S. B. Mitchell and
                 Peter L. Williams",
  language =     "en",
  abstract =     "A visibility ordering of a set of objects, from a
                 given viewpoint, is a total order on the objects such
                 that if object a obstructs object b,then b precedes a
                 in the ordering. Such orderings are extremely useful
                 for rendering volumetric data. We present an algorithm
                 that generates a visibility ordering of the cells of an
                 unstructured mesh, provided that the cells are convex
                 polyhedra and nonintersecting, and that the visibility
                 ordering graph does not contain cycles. The overall
                 mesh may be nonconvex and it may have disconnected
                 components. Our technique employs the sweep paradigm to
                 determine an ordering between pairs of exterior (mesh
                 boundary) cells which can obstruct one another. It then
                 builds on Williams' MPVO algorithm [33] which exploits
                 the ordering implied by adjacencies within the mesh.
                 The partial ordering of the exterior cells found by
                 sweeping is used to augment the DAG created in Phase II
                 of the MPVO algorithm. Our method thus removes the
                 assumption of the MPVO algorithm that the mesh be
                 convex and connected, and thereby allows us to extend
                 MPVO algorithm, without using the heuristics that were
                 originally suggested by Williams (and are sometimes
                 problematic). The resulting XMPVO algorithm has been
                 analyzed, and a variation of it has been implemented
                 for unstructured tetrahedral meshes; we provide
                 experimental evidence that it performs very well in
                 practice.",
  organization = "IEEE, ACM SIGGRAPH",
  keywords =     "Volume rendering, scientific visualization, finite
                 element methods, depth ordering, volume visualization,
                 visibility ordering",
  copyright =    "IEEE, ACM SIGGRAPH",
  booktitle =    "IEEE Symposium on Volume Visualization",
}

@InProceedings{EVL-1998-114,
  pages =        "95--102",
  year =         "1998",
  title =        "Hypervolume Visualization: {A} Challenge in
                 Simplicity",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-114",
  author =       "C. L. Bajaj and V. Pascucci and G. Rabbiolo and D. R.
                 Schikore",
  language =     "en",
  abstract =     "Hyper-volume visualization is designed to provide
                 simple and fully explanatory images that give
                 comprehensive insights into the global structure of
                 scalar fields of any dimension. The basic idea is to
                 have a dimension independent viewing system that scales
                 nicely with the geometric dimension of the dataset and
                 that can be combined with classical approaches like
                 isocontouring and animation of slices of nD data. We
                 completely abandon (for core simplicity) rendering
                 techniques, such as hidden surface removal or lighting
                 or radiosity, that enhance three dimensional realism
                 and concentrate on the real-time display of images that
                 highlight structural (topological) features of the nD
                 dataset (holes, tunnels, cavities, depressions,
                 extrema, etc). Hyper-volume visualization on the one
                 hand is a generalization of direct parallel projection
                 methods in volume rendering. To achieve efficiency (and
                 real-time performance on a graphics workstation) we
                 combine the advantages of (i) a hierarchical
                 representations of the hyper-volume data for
                 multiresolution display and (ii) generalized object
                 space splatting combined with texture-mapped graphics
                 hardware acceleration. The development of a system that
                 implements display techniques for multidimensional
                 datasets requires careful design of both algorithms and
                 user interfaces that scale linearly with the dimension
                 n of the input geometric space. This is a major
                 challenge since straightforward generalizations of
                 standard techniques that are suitable for display of 3D
                 data yield exceedingly intricate interfaces. For
                 example, a view manipulation graphical user interface
                 is usually based on a rotation of the object about
                 Cartesian rotation axes, with possibly unit quaternions
                 internal representations for the rotation group.
                 Unfortunately the number of independent rotation axes
                 grows quadratically with dimension (three in 3D to six
                 in 4D to ten in 5D to fifteen in 6D space). Going back
                 to the basics of parallel projections, we develop an
                 alternative scheme that is very simple to implement and
                 immediately gives a view manipulation graphical user
                 interface that scales linearly with the dimension. One
                 can still utilize matrix or quaternion or higher
                 dimensional rotational group representations,
                 internally for calculations. The main results of our
                 paper are thus both a multi-resolution direct rendering
                 algorithm and scalable graphical user interface that
                 provides insightfull global views of scalar fields in
                 any dimension, while maintaining the fundamental
                 characteristics of ease of use, and quick exploratory
                 user interaction.",
  organization = "IEEE, ACM SIGGRAPH",
  copyright =    "IEEE, ACM SIGGRAPH",
  booktitle =    "IEEE Symposium on Volume Visualization",
}

@InProceedings{EVL-1998-115,
  pages =        "103--110",
  year =         "1998",
  title =        "Extracting Iso-valued Features in 4-dimensional Scalar
                 Fields",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-115",
  author =       "Chris Weigle and David C. Banks",
  language =     "en",
  abstract =     "Isosurfaces are an important tool for finding features
                 in 3D scalar data. This paper describes how recursive
                 contour meshing is applied to extract similar features
                 in 4-dimensional space. In the case of time-varying
                 isosurfaces f(x, y, ,z, t) = c, the technique
                 constructs a solid mesh for the isosurface that sweeps
                 a volume in space-time. An instance of an isosurface at
                 a particular time results from applying a second
                 constraint against this volume. The envelope defined by
                 the time-varying isosurface can be captured in a
                 similar way: when a time-varying isosurface f=c reaches
                 is maximum extent, the function's partial derivative
                 with respect to time must be zero. This second
                 constraint and produces a surface containing the
                 extrema of the isosurfaces. Multi-resolution models and
                 inter-penetrating blobby objects and can also be
                 extracted from 4-dimensional representations.",
  organization = "IEEE, ACM SIGGRAPH",
  copyright =    "IEEE, ACM SIGGRAPH",
  booktitle =    "IEEE Symposium on Volume Visualization",
}

@InProceedings{EVL-1998-118,
  pages =        "111--118",
  year =         "1998",
  title =        "Object Voxelization by Filtering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-118",
  author =       "Milos Sr{\'{a}}mek and Arie Kaufman",
  language =     "en",
  abstract =     "We analyze different filters used for the voxelization
                 of analytically described objects. We show that, when
                 the voxel model is used for visualization, the filter
                 design is related to the subsequent rendering phase,
                 namely the gradient estimation technique. Our
                 theoretical and experimental analyses show that, in
                 order to avoid a systematic error in normal estimation,
                 the density profile near the surface should be linearly
                 proportional to the distance from the surface. Optimal
                 thickness of this transient region has been estimated
                 to be about 3 voxel units. Based on these results, we
                 propose a technique for voxelization of arbitrary
                 parametric surfaces, using a hierarchical subdivision
                 of the 2D surface domain.",
  organization = "IEEE, ACM SIGGRAPH",
  copyright =    "IEEE, ACM SIGGRAPH",
  booktitle =    "IEEE Symposium on Volume Visualization",
}

@InProceedings{EVL-1998-119,
  pages =        "119--126",
  year =         "1998",
  title =        "An Accurate Method for Voxelizing Polygon Meshes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-119",
  author =       "Jian Huang and Roni Yagel and Vassily Filippov and
                 Yair Kurzion",
  language =     "en",
  abstract =     "The process of generating discrete surfaces in a
                 volumetric representation, termed voxelization, is
                 confronted with topological considerations as well as
                 accuracy and efficiency requirements. We introduce a
                 new method for voxelizing planar objects which, unlike
                 existing methods, provides topological conformity
                 through geometric measures. We extend our approach to
                 provide, for the first time, an accurate and coherent
                 method for voxelizing polygon meshes. This method
                 eliminates common voxelization artifacts at edges and
                 vertices. We prove the method's topological attributes
                 and report performance of our implementation. Finally,
                 we demonstrate that this approach forms a basis for a
                 new set of voxelization algorithms by voxelizing an
                 example cubic object.",
  organization = "IEEE, ACM SIGGRAPH",
  copyright =    "IEEE, ACM SIGGRAPH",
  booktitle =    "IEEE Symposium on Volume Visualization",
}

@TechReport{EVL-1998-12,
  year =         "1998",
  title =        "Emotion Editing using Finite Elements",
  author =       "Rolf M. Koch and Markus H. Gross and Albert A.
                 Bosshard",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-12",
  language =     "en",
  abstract =     "This paper describes the prototype of a facial
                 expression editor. In contrast to existing systems the
                 presented editor takes advantage of both medical data
                 for the simulation and the consideration of facial
                 anatomy during the definition of muscle groups. The
                 C1-continuous geometry and the high degree of
                 abstraction for the expression editing sets this system
                 apart from others. Using finite elements we achieve a
                 better precision in comparison to particle systems.
                 Furthermore, a precomputing of facial action units
                 enables us to compose facial expressions by a
                 superposition of facial action geometries in real-time.
                 The presented model is based on a generic facial model
                 using a thin plate and membrane approach for the
                 surface and elastic springs for facial tissue modeling.
                 It has been used successfully for performing facial
                 surgery simulation. We illustrate features of our
                 system with examples from the Visible Human Dataset.",
  month =        jan,
  keywords =     "Facial Modeling, Emotion Editing",
  number =       "281",
  institution =  "Computer Science Department, ETH Z{\"{u}}rich",
}

@InProceedings{EVL-1998-120,
  pages =        "127--134",
  year =         "1998",
  title =        "Wavelet Based Adaptive Interpolation For Volume
                 Rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-120",
  author =       "Ricardo S{\'a}nchez and Marcelo Carvajal",
  language =     "en",
  abstract =     "We present a method for adaptive interpolation, to be
                 used with any volume rendering algorithm, which
                 provides high image quality while diminishing
                 considerably the computation time. The method uses the
                 projection details of the wavelet transform of the
                 sampled data to obtain a local estimate of the spectral
                 content of the data. Whenever the function (sampled
                 data) approximates a polynomial of order less than or
                 equal to the moments of the wavelet, the details are
                 zero or near zero. This property is used to identify
                 constant, linear and non-linear zones within the data
                 in order to choose the appropriate interpolation filter
                 during rendering time.",
  organization = "IEEE, ACM SIGGRAPH",
  keywords =     "Wavelets, interpolation filters, volume rendering,
                 isosurface rendering",
  copyright =    "IEEE, ACM SIGGRAPH",
  booktitle =    "IEEE Symposium on Volume Visualization",
}

@InProceedings{EVL-1998-121,
  pages =        "135--142",
  year =         "1998",
  title =        "Opacity-Weighted Color Interpolation For Volume
                 Sampling",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-121",
  author =       "Craig M. Wittenbrink and Thomas Malzbender and Michael
                 E. Goss",
  language =     "en",
  abstract =     "Volume rendering creates images from sampled
                 volumetric data. The compute intensive nature of volume
                 rendering has driven research in algorithm
                 optimization. An important speed optimization is the
                 use of preclassification and preshading. We demonstrate
                 an artifact that results when interpolating from
                 preclassified or preshaded colors and opacity values
                 separately. This method is flawed, leading to visible
                 artifacts. We present an improved technique,
                 opacity-weighted color interpolation, evaluate the RMS
                 error improvement, hardware and algorithm efficiency,
                 and demonstrated improvements. We show analytically
                 that opacity-weighted color interpolation exactly
                 reproduces material based interpolation results for
                 certain volume classifiers, with the efficiencies of
                 preclassification. Our proposed technique may also have
                 broad impact on opacity-texture-mapped polygon
                 rendering.",
  organization = "IEEE, ACM SIGGRAPH",
  keywords =     "volume rendering, compositing, ray tracing",
  copyright =    "IEEE, ACM SIGGRAPH",
  booktitle =    "IEEE Symposium on Volume Visualization",
}

@InProceedings{EVL-1998-122,
  pages =        "143--151",
  year =         "1998",
  title =        "Design Of Accurate And Smooth Filters For Function And
                 Derivative Reconstruction",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-122",
  author =       "Torsten M{\"o}ller and Klaus Mueller and Yair Kurzion
                 and Raghu Machiraju and Roni Yagel",
  language =     "en",
  abstract =     "The correct choice of function and derivative
                 reconstruction filters is paramount to obtaining highly
                 accurate renderings. Most filter choices are limited to
                 a set of commonly used functions, and the visualization
                 practitioner has so far no way to state his preferences
                 in a convenient fashion. Much work has been done
                 towards the design and specification of filters using
                 frequency based methods. However, for visualization
                 algorithms it is more natural to specify a filter in
                 terms of the smoothness of the resulting reconstructed
                 function and the spatial reconstruction error. Hence,
                 in this paper, we present a methodology for designing
                 filters based on spatial smoothness and accuracy
                 criteria. We first state our design criteria and then
                 provide an example of a filter design exercise. We also
                 use the filters so designed for volume rendering of
                 sampled data sets and a synthetic test function. We
                 demonstrate that our results compare favorably with
                 existing methods.",
  organization = "IEEE, ACM SIGGRAPH",
  keywords =     "Interpolation, Approximation, Quadrature, Numerical
                 Differentiation, Picture Generation, Image Generation,
                 Reconstruction, Volume Rendering, Filter Design,
                 interpolation, derivatives",
  copyright =    "IEEE, ACM SIGGRAPH",
  booktitle =    "IEEE Symposium on Volume Visualization",
}

@InProceedings{EVL-1998-123,
  pages =        "19--26",
  year =         "1998",
  title =        "Large Scale Terrain Visualization Using The Restricted
                 Quadtree Triangulation",
  author =       "Renato Pajarola",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-123",
  language =     "en",
  abstract =     "Real-time rendering of triangulated surfaces has
                 attracted growing interest in the last few years.
                 However, interactive visualization of very large scale
                 grid digital elevation models is still a hard problem.
                 The graphics load must be controlled by an adaptive
                 surface triangulation and by taking advantage of
                 different levels of detail. Furthermore, the management
                 of the visible scene requires efficient access to the
                 terrain database. We describe a all-in-one
                 visualization system which integrates adaptive
                 triangulation, dynamic scene management and spatial
                 data handling. The triangulation model is based on the
                 restricted quadtree triangulation. Furthermore, we
                 present new algorithms of the restricted quadtree
                 triangulation. These include among others exact error
                 approximation, progressive meshing, performance
                 enhancements and spatial access.",
  organization = "IEEE",
  copyright =    "IEEE",
  booktitle =    "Proceedings IEEE Visualization'98",
}

@InProceedings{EVL-1998-124,
  pages =        "35--42",
  year =         "1998",
  title =        "Smooth View-Dependent Level-Of-Detail Control and its
                 Application to Terrain Rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-124",
  author =       "Hugues Hoppe",
  language =     "en",
  abstract =     "The key to real-time rendering of large-scale surfaces
                 is to locally adapt surface geometric complexity to
                 changing view parameters. Several schemes have been
                 developed to address this problem of view-dependent
                 level-of-detail control. Among these, the
                 view-dependent progressive mesh (VDPM) framework
                 represents an arbitrary triangle mesh as a hierarchy of
                 geometrically optimized refinement transformations,
                 from which accurate approximating meshes can be
                 efficiently retrieved. In this paper we extend the
                 general VDPM framework to provide temporal coherence
                 through the runtime creation of geomorphs. These
                 geomorphs eliminate {"}popping{"} artifacts by smoothly
                 interpolating geometry. Their implementation requires
                 new output-sensitive data structures, which have the
                 added benefit of reducing memory use. We specialize the
                 VDPM framework to the important case of terrain
                 rendering. To handle huge terrain grids, we introduce a
                 block-based simplification scheme that constructs a
                 progressive mesh as a hierarchy of block refinements.
                 We demonstrate the need for an accurate approximation
                 metric during simplification. Our contributions are
                 highlighted in a real-time flyover of a large, rugged
                 terrain. Notably, the use of geomorphs results in
                 visually smooth rendering even at 72 frames/sec on a
                 graphics workstation.",
  organization = "IEEE",
  copyright =    "IEEE",
  booktitle =    "Proceedings IEEE Visualization'98",
}

@InProceedings{EVL-1998-125,
  pages =        "43--50",
  year =         "1998",
  title =        "Efficient Implementation of Multi-Triangulations",
  author =       "Leila De Floriani and Paola Magillo and Enrico Puppo",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-125",
  language =     "en",
  abstract =     "The Multi-Triangulation (MT) is a general framework
                 for managing the Level-of-Detail in large triangle
                 meshes, which we have introduced in our previous work.
                 In this paper, we describe an efficient implementation
                 of an MT based on vertex decimation. We present general
                 techniques for querying an MT, which are independent of
                 a specific application, and which can be applied for
                 solving problems, such as selective refinement,
                 windowing, point location, and other spatial
                 interference queries. We describe alternative data
                 structures for encoding an MT, which achieve different
                 trade-offs between space and performance. Experimental
                 results are discussed.",
  organization = "IEEE",
  copyright =    "IEEE",
  booktitle =    "Proceedings IEEE Visualization'98",
}

@InProceedings{EVL-1998-126,
  pages =        "59--66",
  year =         "1998",
  title =        "A general method for preserving attribute values on
                 simplified meshes",
  author =       "P. Cignoni and C. Montani and C. Rocchini and R.
                 Scopigno",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-126",
  language =     "en",
  abstract =     "Many sophisticated solutions have been proposed to
                 reduce the geometric complexity of 3D meshes. A less
                 studied problem is how to preserve on a simplified mesh
                 the detail (e.g. color, high frequency shape detail,
                 scalar fields, etc.) which is encoded in the original
                 mesh. We present a general approach for preserving
                 detail on simplified meshes. The detail (or high
                 frequency information) lost after simplification is
                 encoded through texture or bump maps. The original
                 contribution is that preservation is performed after
                 simplification, by building set of triangular texture
                 patches that are then packed in a single texture map.
                 Each simplified mesh face is sampled to build the
                 associated triangular texture patch; a new method for
                 storing this set of texture patches into a standard
                 rectangular texture is presented and discussed. Our
                 detail preserving approach makes no assumptions about
                 the simplification process adopted to reduce mesh
                 complexity and allows highly efficient rendering. The
                 solution is very general, allowing preservation of any
                 attribute value defined on the high resolution mesh. We
                 also describe an alternative application: the
                 conversion of 3D models with 3D static procedural
                 textures into standard 3D models with 2D textures.",
  organization = "IEEE",
  copyright =    "IEEE",
  booktitle =    "Proceedings IEEE Visualization'98",
}

@InProceedings{EVL-1998-127,
  pages =        "119--126",
  year =         "1998",
  title =        "Efficient Co-Triangulation of Large Data Sets",
  author =       "Henrik Weimer and Joe Warren and Jane Troutner and
                 Wendell Wiggins and John Shrout",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-127",
  language =     "en",
  abstract =     "This paper presents an efficient algorithm for the
                 reconstruction of a multivariate function from multiple
                 sets of scattered data. Given N sets of scattered data
                 representing N distinct dependent variables that have
                 been sampled independently over a common domain and N
                 error tolerance values the algorithm constructs a
                 triangulation of the domain of the data and associates
                 multivariate values with the vertices of the
                 triangulation. The resulting linear interpolation of
                 these multivariate values yields a multivariate
                 function, called a co-triangulation, that represents
                 all of the dependent data up to the given error
                 tolerance. A simple iterative algorithm for the
                 construction of a co-triangulation from any number of
                 data sets is presented and analyzed. The main
                 contribution of this paper lies in the description of a
                 highly efficient framework for the realization of this
                 approximation algorithm. While the asymptotic time
                 complexity of the algorithm certainly remains within
                 the theoretical bounds, we demonstrate that it is
                 possible to achieve running times that depend only
                 linearly on the number of data even for very large
                 problems with above two million samples. This efficient
                 realization of the algorithm uses adapted dynamic
                 data-structures and careful caching in an integrated
                 framework.",
  organization = "IEEE",
  copyright =    "IEEE",
  booktitle =    "Proceedings IEEE Visualization'98",
}

@InProceedings{EVL-1998-128,
  pages =        "181--188",
  year =         "1998",
  title =        "The Gridfit Algorithm: An Efficient and Effective
                 Approach to Visualizing Large Amounts of Spatial Data",
  author =       "Daniel A. Keim and Annemarie Herrmann",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-128",
  language =     "en",
  abstract =     "In a large number of applications, data is collected
                 and referenced by their spatial locations. Visualizing
                 large amounts of spatially referenced data on a
                 limited-size screen display often results in poor
                 visualizations due to the high degree of overplotting
                 of neighboring data points. In this paper, we introduce
                 a new approach to visualizing large amounts of
                 spatially referenced data. The basic idea is to
                 intelligently use the unoccupied pixels of the display
                 instead of overplotting data points. After formally
                 describing the problem, we present two solutions which
                 are based on (1) placing overlapping data points on the
                 nearest unoccupied pixel and (2) shifting data points
                 along a screen-filling curve (e.g., Hilbertcurve). We
                 then develop a more sophisticated approach called
                 Gridfit, which is based on a hierarchical partitioning
                 of the data space. We evaluate all three approaches
                 with respect to their efficiency and effectiveness and
                 show the superiority of the Gridfit approach. For
                 measuring the effectiveness, we not only present the
                 resulting visualizations but also introduce
                 mathematical effectiveness criteria measuring
                 properties of the generated visualizations with respect
                 to the original data such as distance- and
                 position-preservation.",
  organization = "IEEE",
  copyright =    "IEEE",
  booktitle =    "Proceedings IEEE Visualization'98",
}

@InProceedings{EVL-1998-129,
  pages =        "189--196",
  year =         "1998",
  title =        "{TOPIC} {ISLANDS} - {A} Wavelet-Based Text
                 Visualization System",
  author =       "Nancy E. Miller and Pak Chung Wong and Mary Brewster
                 and Harlan Foote",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-129",
  language =     "en",
  abstract =     "We present a novel approach to visualize and explore
                 unstructured text. The underlying technology, called
                 TOPIC-O-GRAPHY (TM), applies wavelet transforms to a
                 custom digital signal constructed from words within a
                 document. The resultant multiresolution wavelet energy
                 is used to analyze the characteristics of the narrative
                 flow in the frequency domain, such as theme changes,
                 which is then related to the overall thematic content
                 of the text document using statistical methods. The
                 thematic characteristics of a document can be analyzed
                 at varying degrees of detail, ranging from
                 section-sized text partitions to partitions consisting
                 of a few words. Using this technology, we are
                 developing a visualization system prototype known as
                 TOPIC ISLANDS (TM) to browse a document, generate fuzzy
                 document outlines, summarize text by levels of detail
                 and according to user interests, define meaningful
                 subdocuments, query text content, and provide summaries
                 of topic evolution.",
  organization = "IEEE",
  copyright =    "IEEE",
  booktitle =    "Proceedings IEEE Visualization'98",
}

@Article{EVL-1998-13,
  pages =        "6--7",
  year =         "1998",
  title =        "VisInfo - Fachinformationsdienst Visualisierung",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-13",
  author =       "Ingmar Decker and Peter M{\"u}ller and Daniel
                 Limbach",
  abstract =     "Was in anderen Fachrichtungen schon l{\"{a}}ngst
                 etabliert ist, fehlte bisher f{\"{u}}r die
                 Wissenschaftliche Visualisierung: ein
                 Fachinformationsdienst im WWW, mit dem Wissenschaftler,
                 die auf diesem Gebiet t{\"{a}}tig sind, nach Literatur,
                 Software und aktuellen Neuigkeiten suchen k{\"{o}}nnen.
                 Bisher war man auf die Suche in Zeitschriften und
                 Proceedings angewiesen, was angesichts gek{\"{u}}rzter
                 Mittel f{\"{u}}r Literatur immer schwieriger wird. Oder
                 man sucht mittels bekannter Suchmaschinen im Web nach
                 Fachbegriffen und Autoren, was bei der rasch wachsenden
                 Informationsflut im WWW immer schwieriger und
                 un{\"{u}}bersichtlicher wird. Mit dem
                 Fachinformations-dienst Visualisierung im WWW, kurz
                 VisInfo, soll diese L{\"{u}}cke geschlossen und
                 Wissenschaftlern weltweit ein kostenloser Zugang rund
                 um die Uhr zu aktuellen Dokumenten der
                 Wissenschaftlichen Visualisierung geboten werden. Neue,
                 bild-bezogene Suchmethoden unterst{\"{u}}tzen dabei den
                 Nutzer, der das Angebot auch aktiv erweitern kann.
                 VisInfo ist ein Projekt des DFN-Vereins,
                 gef{\"{o}}rdert mit Mitteln des BMBF.",
  language =     "de",
  month =        mar,
  volume =       "46",
  copyright =    "DFN-Verein",
  journal =      "DFN-Mitteilungen",
}

@InProceedings{EVL-1998-130,
  pages =        "197--204",
  year =         "1998",
  title =        "Continuous Cartogram Construction",
  author =       "Donald H. House and Christopher J. Kocmoud and",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-130",
  language =     "en",
  abstract =     "Area cartograms are used for visualizing
                 geographically distributed data by attaching
                 measurements to regions of a map and scaling the
                 regions such that their areas are proportional to the
                 measured quantities. A continuous area cartogram is a
                 cartogram that is constructed without changing the
                 underlying map topology. We present a new algorithm for
                 the construction of continuous area cartograms that was
                 developed by viewing their construction as a
                 constrained optimization problem. The algorithm uses a
                 relaxation method that exploits hierarchical
                 resolution, constrained dynamics, and a scheme that
                 alternates goals of achieving correct region areas and
                 adjusting region shapes. It is compared favorably to
                 existing methods in its ability to preserve region
                 shape recognition cues, while still achieving high
                 accuracy.",
  organization = "IEEE",
  copyright =    "IEEE",
  booktitle =    "Proceedings IEEE Visualization'98",
}

@InProceedings{EVL-1998-131,
  pages =        "217--224",
  year =         "1998",
  title =        "Visualizing Differences in Movies of Cortical
                 Activity",
  author =       "Kay A. Robbins and David M. Senseman",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-131",
  language =     "en",
  abstract =     "This paper discusses techniques for visualizing
                 structure in video data and other data sets that
                 represent time snapshots of physical phenomena.
                 Individual frames of a movie are treated as vectors and
                 projected onto a low-dimensional subspace spanned by
                 the principal components. Movies can be compared and
                 their differences visualized by analyzing the nature of
                 the subspace and the projections of multiple movies
                 onto the same subspace. The approach is demonstrated on
                 an application in neurobiology in which the electrical
                 response of a visual cortex to optical stimulation is
                 imaged onto a high-speed photodiode array to produce a
                 cortical movie. Techniques for sampling movies over a
                 single trial and multiple trials are discussed. The
                 approach provides the traditional benefits of principal
                 component analysis (compression, noise reduction and
                 classification) and also allows visual separation of
                 spatial and temporal behavior.",
  organization = "IEEE",
  copyright =    "IEEE",
  booktitle =    "Proceedings IEEE Visualization'98",
}

@Article{EVL-1998-132,
  pages =        "31--37",
  year =         "1998",
  title =        "Fast algorithms for clipping lines and line segments
                 in \({E}^{2}\)",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-132",
  author =       "Duc Huy Bui and Vaclav Skala",
  language =     "en",
  abstract =     "New modifications of the Cohen-Sutherland algorithm
                 for clipping lines and line segments in \(E^{2}\) are
                 presented. The suggested algorithms are based on a
                 technique of coding the line direction together with
                 the end points of the clipped line segment. They solve
                 all cases more effectively. The algorithms are
                 convenient for clippings lines or line segments by
                 rectangle. Theoretical considerations and experimental
                 results are also presented.",
  keywords =     "Line clipping, Computer graphics, Algorithm
                 complexity, Geometric algorithms, Algorithm complexity
                 analysis",
  volume =       "14",
  number =       "1",
  journal =      "The Visual Computer",
}

@Article{EVL-1998-133,
  pages =        "109--125",
  year =         "1998",
  title =        "Nested radiosity for plant canopies",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-133",
  author =       "Micha{\"{e}}l Chelle and Bruno Andrieu and Kadi
                 Bouatouch",
  language =     "en",
  abstract =     "We present nested radiosity, a new approach for
                 simulating the distribution of natural light within
                 plant canopies. The geometric description of a canopy
                 requires many polygons, and its spatial coherence is
                 lower than that of an architectural scene. We compute
                 the total irradiance of polygons by summing the
                 irradiance due to close polygons (calculated with the
                 radiosity equation) and the irradiance brought by
                 distant polygons (approximated by the radiative
                 transfer equation). The reduced number of form factors
                 can be stored in a sparse matrix. Then several
                 radiative conditions can be quickly simulated. The
                 simulation results for this model are compared with
                 those produced by a Monte Carlo ray tracing.",
  keywords =     "Global illumination, Radiosity, Nested model,
                 Vegetation, Validation",
  volume =       "14",
  number =       "3",
  journal =      "The Visual Computer",
}

@Article{EVL-1998-134,
  pages =        "95--108",
  year =         "1998",
  title =        "A marching method for the triangulation of surfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-134",
  author =       "Erich Hartmann",
  language =     "en",
  abstract =     "All surfaces that can be described by collections of
                 equations, especially the parametric ones, can be
                 treated uniformly as implicit surfaces. The idea of
                 numerical implicitization makes this possible. We
                 introduce a marching method for the triangulation of
                 implicit surfaces. The method produces coherent nets of
                 triangles, even for sets of intersecting surface
                 patches.",
  keywords =     "Triangulation, Marching method, Numerical
                 implicitization, Parametric surface, Implicit surface,
                 Offset surface, Blend surface",
  volume =       "14",
  number =       "3",
  journal =      "The Visual Computer",
}

@Article{EVL-1998-135,
  pages =        "126--137",
  year =         "1998",
  title =        "Generation of crack patterns with a physical model",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-135",
  author =       "Koichi Hirota and Yasuyuki Tanoue and Toyohisa
                 Kaneko",
  language =     "en",
  abstract =     "We describe our approach to generating realistic crack
                 patterns on the basis of a physical model. Our
                 simulation model consists of nodes that represent
                 infinitesimal volume elements on the surface of an
                 object and springs that represent the effects of the
                 physical connection between these elements. The
                 fragmentation of the material is expressed by the
                 cutting of springs. We applied this model to the drying
                 process of mud, and we confirmed that it is capable of
                 generating cracks similar to actual ones and that the
                 generated pattern can be varied by changing the model
                 parameters.",
  keywords =     "Crack pattern, Physical model, Simulation, Scientific
                 visualization, Animation techniques",
  volume =       "14",
  number =       "3",
  journal =      "The Visual Computer",
}

@Article{EVL-1998-136,
  pages =        "166--176",
  year =         "1998",
  title =        "Three-dimensional geometric metamorphosis based on
                 harmonic maps",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-136",
  author =       "Takashi Kanai and Hiromasa Suzuki and Fumihiko
                 Kimura",
  language =     "en",
  abstract =     "Animations with deforming objects are frequently used
                 in computer graphics applications. Metamorphosis (or
                 morphing) of 3D objects is one of the techniques that
                 realize a shape transformation between two or more
                 existing objects. We present a new algorithm for 3D
                 geometric metamorphosis between two objects based on
                 harmonic mapping. Our algorithm is applicable to
                 arbitrary polyhedra that are homeomorphic to a 3D
                 sphere or a 2D disk. In our algorithm, each of the two
                 3D objects is first embedded in the circular disk on
                 the plane. This embedded model has the same graph
                 structure as its 3D objects. We can overlap two
                 embedded models and use the correspondence established
                 between the two objects to generate intermediate
                 objects. The user only specifies a boundary loop on an
                 object and a vertex on that boundary to control
                 interpolation.",
  keywords =     "Geometric modeling, Metamorphosis, Harmonic mapping,
                 Correspondence problem",
  volume =       "14",
  number =       "4",
  journal =      "The Visual Computer",
}

@Article{EVL-1998-137,
  pages =        "83--94",
  year =         "1998",
  title =        "Shadow generation for volumetric data sets",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-137",
  author =       "Rolf H. {van Lengen} and J{\"{o}}rg Meyer and Mathias
                 Matzat and Hans Hagen",
  language =     "en",
  abstract =     "Shadow information is essential for visual perception
                 of complex scenes. Both shadow location and orientation
                 clarify spatial relationships between objects. Shadows
                 improve depth perception and the impression of realism.
                 Our algorithm, integrated into a hybrid rendering
                 system for medical data sets, is based on a shadow
                 Z-buffer technique for fast and efficient shadow
                 generation. Volumetric data contain information about
                 the grid points only. Such data do not provide surface
                 information that could be projected immediately onto
                 the shadow map. To solve this problem, we have
                 implemented two techniques. The first uses a modified
                 adaptive version of the well-known marching cubes
                 algorithm for the special characteristics of medical
                 data sets. The algorithm uses material properties for a
                 precise representation of object boundaries, generating
                 volumetric objects quickly and effectively. There are
                 two representations of the same data set: we use a
                 view-independent approximation to display shadows and
                 the original representation of the volume for object
                 visualization in full precision. The second algorithm
                 uses a ray-tracing approach to create shadow maps. The
                 same routine is used for object rendering, but is
                 restricted to depth-value generation. Semitransparent
                 objects are handled by storing an intensity profile in
                 addition to the depth value.",
  keywords =     "Volume visualization, Scientific visualization,
                 Medical imaging, Shadow Z-buffer, Shadow map",
  volume =       "14",
  number =       "2",
  journal =      "The Visual Computer",
}

@Article{EVL-1998-138,
  pages =        "39--51",
  year =         "1998",
  title =        "Virtual sculpting and virtual woodcut printing",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-138",
  author =       "Shinji Mizuno and Minoru Okada and {Jun-ichiro}
                 Toriwaki",
  language =     "en",
  abstract =     "We propose an interactive modeling technique to form
                 solid objects with curved surfaces as if we were
                 sculpting them. Users can remove or attach arbitrary
                 ellipsoid shapes from or to a solid object by operating
                 virtual chisels, and the surface changes in every
                 carving operation in real time. The shape of a carved
                 solid object is described with a CSG model. The carved
                 solid objects generated by our virtual sculpting system
                 look like real wooden sculptures. Thus we propose a
                 method to generate a woodcut print from the virtual
                 printing block generated by our system.",
  keywords =     "Virtual reality, Interactive solid modeling, CSG
                 model, Nonphotorealistic image",
  volume =       "14",
  number =       "2",
  journal =      "The Visual Computer",
}

@Article{EVL-1998-139,
  pages =        "1--17",
  year =         "1998",
  title =        "Surface subdivision for generating superquadrics",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-139",
  author =       "M. Eugenia Montiel and Alberto S. Aguado and Ed
                 Zaluska",
  language =     "en",
  abstract =     "Superquadrics have been considered as important models
                 for part-level description in computer graphics and
                 computer vision. The description power of these models
                 resides in their parameterised nature, which allows the
                 definition of a wide variety of shapes, yet maintains a
                 compact characterisation. We present a subdivision
                 technique for modelling and displaying superquadrics.
                 The method defines the surface of a superquadric as a
                 deformation of an ellipsoid. The linear arc-length
                 parameterisation obtained provides a regular
                 distribution of the parameters along the surface.
                 Furthermore, the definition simplifies computations in
                 scanning by avoiding the evaluation of rational
                 exponents. We exploit the geometric properties of an
                 ellipsoid during subdivision for additional
                 simplification.",
  keywords =     "Modelling of surfaces, Surface subdivision, Parametric
                 surfaces, Superquadrics, Superellipses, Rendering
                 techniques, Display algorithms, Curve tracing,
                 Tessellation, Reparameterisation, Deformations",
  volume =       "14",
  number =       "1",
  journal =      "The Visual Computer",
}

@TechReport{EVL-1998-14,
  year =         "1998",
  title =        "Real Time Simulation and Visualization of {NC} Milling
                 Processes for Inhomogeneous Materials on Low-End
                 Graphics Hardware",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-14",
  author =       "Andreas K{\"o}nig and Eduard Gr{\"o}ller",
  abstract =     "Simulation and visualization of NC milling processes
                 has become an important step in computer aided
                 manufacturing. The usage of stock materials with
                 specific locally varying properties (like density,
                 accuracy, color, ...) becomes more and more important
                 with new technologies emerging in the material
                 industry. Our new approach, using volumetric
                 representation, has been adapted to this needs and
                 copes with inhomogeneous material properties. Taking
                 color as one possible material property, our approach
                 enables the visualization of milled wood or compound
                 materials. Furthermore, our approach has been developed
                 with the usage of low-end graphics hardware in mind.
                 The algorithms have been optimized to ensure
                 interactive update rates even on standard personal
                 computers without hardware graphics acceleration.",
  language =     "en",
  month =        jan,
  keywords =     "visualization, NC milling",
  number =       "TR-186-2-98-04",
  institution =  "Vienna University of Technology, Computer Graphics,
                 Visualisation and Animation Group",
}

@Article{EVL-1998-140,
  pages =        "140--152",
  year =         "1998",
  title =        "Collaborative environments of IntelligentBox for
                 distributed 3{D} graphics applications",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-140",
  author =       "Yoshihiro Okada and Yuzuru Tanaka",
  language =     "en",
  abstract =     "An IntelligentBox is a user-friendly rapid-prototyping
                 software development system for interactive 3D graphics
                 applications. It represents objects as reactive 3D
                 visual objects (called boxes) that can manually be
                 combined with other boxes. A RoomBox is a particular
                 box provided for constructing distributed 3D graphics
                 applications. Multiple RoomBoxes on different computers
                 share specific user-operation events with each other
                 and virtually provide some users with a shared 3D
                 space. Already developed boxes work in a shared 3D
                 space without any modifications. Therefore, it is
                 possible to construct distributed 3D graphics
                 applications without creating any programs.",
  keywords =     "IntelligentBox, 3D Toolkit, Construction system,
                 Distributed graphics applications",
  volume =       "14",
  number =       "4",
  journal =      "The Visual Computer",
}

@Article{EVL-1998-141,
  pages =        "153--165",
  year =         "1998",
  title =        "Resolving occlusion in image sequence made easy",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-141",
  author =       "Kiem Ching Ong and Hung Chuan Teh and Tiow Seng Tan",
  language =     "en",
  abstract =     "While the task of seamlessly merging
                 computer-generated 3D objects into an image sequence
                 can be done manually, such effort often lacks
                 consistency across the images. It is also time
                 consuming and prone to error. This paper proposes a
                 framework that solves the occlusion problem without
                 assuming a priori computer models from the input scene.
                 It includes a new algorithm to derive approximate 3D
                 models about the real scene based on recovered geometry
                 information and user-supplied segmentation results. The
                 framework has been implemented, and it works for
                 amateur home videos. The result is an easy-to-use
                 system for applications like the visualization of new
                 architectures in a real environment.",
  keywords =     "Augmented reality, Occlusion, Structure-from-motion,
                 Segmentation",
  volume =       "14",
  number =       "4",
  journal =      "The Visual Computer",
}

@Article{EVL-1998-142,
  pages =        "52--68",
  year =         "1998",
  title =        "Computer-aided illusions: ambiguity, perspective and
                 motion",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-142",
  author =       "Lillian F. Schwartz",
  language =     "en",
  abstract =     "The use of distorted perspective and an understanding
                 of optics by Renaissance artists resulted in the
                 creation of unique illusions in their work. Today, the
                 need to translate satellite pictures into a rectilinear
                 format and to project, record and adapt images to the
                 different aspect ratios of film and video, is causing
                 new algorithms to be written into software. These
                 programs simplify and automate the task of converting
                 distorted perspective into a proper linear perspective
                 and the reverse. While investigating the use of these
                 technologies to create new illusions in art, I
                 rediscovered unique phenomenona in two different
                 Renaissance paintings.",
  keywords =     "Illusions, Perspective, Renaissance art,
                 Anamorphosis",
  volume =       "14",
  number =       "2",
  journal =      "The Visual Computer",
}

@Article{EVL-1998-143,
  pages =        "18--30",
  year =         "1998",
  title =        "A system subdivision approach for large radiosity
                 computation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-143",
  author =       "Mikio Shinya and Takeaki Mori and Noriyoshi Osumi",
  language =     "en",
  abstract =     "A new algorithm for the radiosity computation of
                 higher scene complexity is presented. The algorithm
                 regards a large radiosity system as a collection of
                 small subsystems. The subsystems interact with each
                 other only through input and output functions, so the
                 large initial equation system can be divided into a
                 collection of small equation systems. This reduces the
                 computational cost to O(n) for n subsystems. Moreover,
                 the data necessary for each subsystem computation is
                 completely localized, which allows the database to be
                 stored on disk. The algorithm can easily be implemented
                 with a slight modification of the hierarchical
                 radiosity algorithm. Experiments demonstrate the
                 efficiency of the algorithm.",
  keywords =     "Global illumination, Hierarchical radiosity, System
                 subdivision",
  volume =       "14",
  number =       "1",
  journal =      "The Visual Computer",
}

@Article{EVL-1998-144,
  pages =        "177--192",
  year =         "1998",
  title =        "Continuous-resolution-level constraints in variational
                 design of multiresolution shapes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-144",
  author =       "Shigeo Takahashi and Yoshihisa Shinagawa and Tosiyasu
                 L. Kunii",
  language =     "en",
  abstract =     "This paper introduces continuous-resolution-level
                 constraints to hierarchical editing of curves and
                 surfaces based on B-spline wavelets. The constraints
                 specify the shape at a continuous-resolution level by
                 interpolating those at integer-resolution levels.
                 Energy functions subject to the shape deformations are
                 used to control the smoothness of the curves and
                 surfaces. This paper proposes two interpolation schemes
                 for the continuous-level shapes: linear interpolation
                 and cardinal-spline interpolation. The continuous-level
                 shape is obtained as a transformation of that at an
                 integer-resolution level, and the continuous-level
                 constraints are reduced to those at integer-resolution
                 levels. Experimental results are presented to show that
                 the continuous-level constraints effectively control
                 the multiresolution curves and surfaces.",
  keywords =     "Continuous-resolution levels, Geometric constraints,
                 Variational modeling, Curves and surfaces, Wavelets",
  volume =       "14",
  number =       "4",
  journal =      "The Visual Computer",
}

@Article{EVL-1998-145,
  pages =        "69--82",
  year =         "1998",
  title =        "Recursive algebraic curve fitting and rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-145",
  author =       "Shouqing Zhang and Ling Li and Hock Soon Seah",
  language =     "en",
  abstract =     "We describe a recursive algorithm that uses quadratic
                 algebraic curve segments to vectorize digital images.
                 The closeness of fitting and the smoothness of
                 connection between curve segments are ensured by a
                 recursive algebraic curve fitting and a subsequent
                 fine-tuning procedure. The idea is to provide an
                 alternative way to vectorize outside parametric
                 schemes, while maintaining the precision of parametric
                 vectorization. We can also have all the new features of
                 algebraic representation; for instance, the implicit
                 forms and unique insights into curve shapes and control
                 point weights. We also present a triangular quadtree
                 rendering scheme for displaying algebraic curves. These
                 algorithms combine features from both parametric and
                 algebraic schemes to meet different requirements for
                 curve fitting.",
  keywords =     "Vectorization, Curve fitting, Recursive, Algebraic
                 curve",
  volume =       "14",
  number =       "2",
  journal =      "The Visual Computer",
}

@InProceedings{EVL-1998-146,
  pages =        "279--286",
  year =         "1998",
  title =        "Fast and Memory Efficient Polygonal Simplification",
  author =       "Peter Lindstrom and Greg Turk",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-146",
  language =     "en",
  abstract =     "Conventional wisdom says that in order to produce
                 high-quality simplified polygonal models, one must
                 retain and use information about the original model
                 during the simplification process. We demonstrate that
                 excellent simplified models can be produced without the
                 need to compare against information from the original
                 geometry while performing local changes to the model.
                 We use edge collapses to perform simplification, as do
                 a number of other methods. We select the position of
                 the new vertex so that the original volume of the model
                 is maintained and we minimize the per-triangle change
                 in volume of the tetrahedra swept out by those
                 triangles that are moved. We also maintain surface area
                 near boundaries and minimize the per-triangle area
                 changes. Calculating the edge collapse priorities and
                 the positions of the new vertices requires only the
                 face connectivity and the the vertex locations in the
                 intermediate model. This approach is memory efficient,
                 allowing the simplification of very large polygonal
                 models, and it is also fast. Moreover, simplified
                 models created using this technique compare favorably
                 to a number of other published simplification methods
                 in terms of mean geometric error.",
  organization = "IEEE",
  copyright =    "IEEE",
  booktitle =    "Proceedings IEEE Visualization'98",
}

@InProceedings{EVL-1998-147,
  pages =        "313--317",
  year =         "1998",
  title =        "Wavelets over Curvilinear Grids",
  author =       "Gregory M. Nielson and Il-Hong Jung and Junwon Sung",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-147",
  language =     "en",
  abstract =     "We develop multiresolution models for analyzing and
                 visualizing two-dimensional flows over curvilinear
                 grids. Our models are based upon nested spaces of
                 piecewise defined functions defined over nested
                 curvilinear grid domains. The nested domains are
                 selected so as to maintain the original geometry of the
                 inner boundary. We first give the refinement and
                 decomposition equations for haar wavelets over these
                 domains. Next, using lifting techniques we develop and
                 show examples of piecewise linear wavelets over
                 curvilinear grids.",
  organization = "IEEE",
  copyright =    "IEEE",
  booktitle =    "Proceedings IEEE Visualization'98",
}

@InProceedings{EVL-1998-148,
  pages =        "319--326",
  year =         "1998",
  title =        "Image-Based Transfer Function Design for Data
                 Exploration in Volume Visualization",
  author =       "Shiaofen Fang and Tom Biddlecom and Mihran Tuceryan",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-148",
  language =     "en",
  abstract =     "Transfer function design is an integrated component in
                 volume visualization and data exploration. The common
                 trial-and-error approach for transfer function
                 searching is a very difficult and time-consuming
                 process. A goal-oriented and parameterized transfer
                 function model is, therefore, crucial in guiding the
                 transfer function searching process for better and more
                 meaningful visualization results. This paper presents
                 an image-based transfer function model that integrates
                 3D image processing tools into the volume visualization
                 pipeline to facilitate the search for an image-based
                 transfer function in volume data visualization and
                 exploration. The model defines a transfer function as a
                 sequence of 3D image processing procedures, and allows
                 the users to adjust a set of qualitative and
                 descriptive parameters to achieve their subjective
                 visualization goals. 3D image enhancement and boundary
                 detection tools, and their integration methods with
                 volume visualization algorithms are described in this
                 paper. The application of this approach for 3D
                 microscopy data exploration and analysis is also
                 discussed.",
  organization = "IEEE",
  copyright =    "IEEE",
  booktitle =    "Proceedings IEEE Visualization'98",
}

@InProceedings{EVL-1998-150,
  pages =        "391--396",
  year =         "1998",
  title =        "Interpolation of Triangle Hierarchies",
  author =       "Axel Friedrich and Konrad Polthier and Markus
                 Schmies",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-150",
  language =     "en",
  abstract =     "We consider interpolation between keyframe
                 hierarchies. We impose a set of weak constraints that
                 allows smooth interpolation between two keyframe
                 hierarchies in an animation or, more generally, allows
                 the interpolation in an n-parameter family of
                 hierarchies. We use hierarchical triangulations
                 obtained by the Rivara element bisection algorithm and
                 impose a weak compatibility constraint on the set of
                 root elements of all keyframe hierarchies. We show that
                 the introduced constraints are rather weak. The
                 strength of our approach is that the interpolation
                 works in the class of conforming triangulations and
                 simplifies the task of finding the intermediate
                 hierarchy, which is the union of the two (, or more,)
                 keyframe hierarchies involved in the interpolation
                 process. This allows for an efficient generation of the
                 intermediate connectivity and additionally ensures that
                 the intermediate hierarchy is again a conforming
                 hierarchy satisfying the same constraints.",
  organization = "IEEE",
  copyright =    "IEEE",
  booktitle =    "Proceedings IEEE Visualization'98",
}

@Article{EVL-1998-151,
  pages =        "1--31",
  year =         "1998",
  title =        "Rational Parametrizations of Nonsingular Real Cubic
                 Surfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-151",
  author =       "Chandrajit L. Bajaj and Robert J. Holt and Arun N.
                 Netravali",
  language =     "en",
  abstract =     "Real cubic algebraic surfaces may be described by
                 either implicit or parametric equations. One
                 particularly useful representation is the rational
                 parametrization, where the three spatial coordinates
                 are given by rational functions of two parameters.
                 These parametrizations take on different forms for
                 different classes of cubic surfaces. Classification of
                 real cubic algebraic surfaces into five families for
                 the nonsingular case is based on the configuration of
                 27 lines on them. We provide a method of extracting all
                 these lines by constructing and solving a polynomial of
                 degree 27. Simple roots of this polynomial correspond
                 to real lines on the surface, and real skew lines are
                 used to form rational parametrizations for three of
                 these families. Complex conjugate skew lines are used
                 to parametrize surfaces from the fourth family. The
                 parametrizations for these four families involve
                 quotients of polynomials of degree no higher than four.
                 Each of these parametrizations covers the whole surface
                 except for a few points, lines, or conic sections. The
                 parametrization for the fifth family, as noted
                 previously in the literature, requires a square root.
                 We also analyze the image of the derived rational
                 parametrization for both real and complex parameter
                 values, together with ``base'' points where the
                 parametrizations are ill-defined.",
  keywords =     "cubic surface modeling, dual form representations,
                 graphics display, numeric and symbolic computations,
                 rational parametrization",
  volume =       "17",
  number =       "1",
  journal =      "ACM Transactions on Graphics",
}

@Article{EVL-1998-152,
  pages =        "116--141",
  year =         "1998",
  title =        "Three-Dimensional Distance Field Metamorphosis",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-152",
  author =       "Daniel {Cohen-Or} and Amira Solomovici and David
                 Levin",
  language =     "en",
  abstract =     "Given two or more objects of general topology,
                 intermediate objects are constructed by a distance
                 field metamorphosis. In the presented method the
                 interpolation of the distance field is guided by a warp
                 function controlled by a set of corresponding anchor
                 points. Some rules for defining a smooth
                 least-distorting warp function are given. To reduce the
                 distortion of the intermediate shapes, the warp
                 function is decomposed into a rigid rotational part and
                 an elastic part. The distance field interpolation
                 method is modified so that the interpolation is done in
                 correlation with the warp function. The method provides
                 the animator with a technique that can be used to
                 create a set of models forming a smooth transition
                 between pairs of a given sequence of keyframe models.
                 The advantage of the new approach is that it is capable
                 of morphing between objects having a different
                 topological genus where no correspondence between the
                 geometric primitives of the models needs to be
                 established. The desired correspondence is defined by
                 an animator in terms of a relatively small number of
                 anchor points.",
  keywords =     "computer animation, interpolation, morphing, radial
                 basis function, shape-blending, warping",
  volume =       "17",
  number =       "2",
  journal =      "ACM Transactions on Graphics",
}

@Article{EVL-1998-153,
  pages =        "32--49",
  year =         "1998",
  title =        "The Bisector Surface of Rational Space Curves",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-153",
  author =       "Gershon Elber and {Myung-Soo} Kim",
  language =     "en",
  abstract =     "Given a point and a rational curve in the plane, their
                 bisector curve is rational [Farouki and Johnston
                 1994a]. However, in general, the bisector of two
                 rational curves in the plane is not rational [Farouki
                 and Johnstone 1994b]. Given a point and a rational
                 space curve, this article shows that the bisector
                 surface is a rational ruled surface. Moreover, given
                 two rational space curves, we show that the bisector
                 surface is rational (except for the degenerate case in
                 which the two curves are coplanar).",
  keywords =     "bisector, medial surface, skeleton, Voronoi surface",
  volume =       "17",
  number =       "1",
  journal =      "ACM Transactions on Graphics",
}

@Article{EVL-1998-154,
  pages =        "143--157",
  year =         "1998",
  title =        "Computing Moments of Objects Enclosed by Piecewise
                 Polynomial Surfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-154",
  author =       "Carlos {Gonzalez-Ochoa} and Scott McCammon and
                 J{\"{o}}rg Peters",
  language =     "en",
  abstract =     "Combining a polynomial free-form surface
                 representation with Gauss' divergence theorem allows
                 efficient and exact calculation of the moments of the
                 enclosed objects. For example, for an cubic
                 representation, volume, center of mass, and the inertia
                 tensor can be computed in seconds even for complex
                 objects with several thousand patches while change due
                 to local modification of the surface geometry can be
                 computed in real-time as feedback for animation or
                 design. Speed and simplicity of the approach allow
                 solving the inverse problem of modeling to match
                 prescribed moments.",
  volume =       "17",
  number =       "3",
  journal =      "ACM Transactions on Graphics",
}

@Article{EVL-1998-155,
  pages =        "71--83",
  year =         "1998",
  title =        "Efficient Clipping of Arbitrary Polygons",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-155",
  author =       "G{\"{u}}nther Greiner and Kai Hormann",
  language =     "en",
  abstract =     "Clipping 2D polygons is one of the basic routines in
                 computer graphics. In rendering complex 3D images it
                 has to be done several thousand times. Efficient
                 algorithms are therefore very important. We present
                 such an efficient algorithm for clipping arbitrary
                 2D-polygons. The algorithm can handle arbitrary closed
                 polygons, specifically where the clip and subject
                 polygons may self-intersect. The algorithm is simple
                 and faster than Vatti's [1992] algorithm, which was
                 designed for the general case as well. Simple
                 modifications allow determination of union and
                 set-theoretic differences of two arbitrary polygons.",
  keywords =     "clipping, polygon comparison",
  volume =       "17",
  number =       "2",
  journal =      "ACM Transactions on Graphics",
}

@Article{EVL-1998-156,
  pages =        "158--176",
  year =         "1998",
  title =        "Sampling Procedural Shaders Using Affine Arithmetic",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-156",
  author =       "Wolfgang Heidrich and Philipp Slusallek and
                 {Hans-Peter} Seidel",
  language =     "en",
  abstract =     "Procedural shaders have become popular tools for
                 describing surface reflectance functions and other
                 material properties. In comparison to fixed resolution
                 textures, they have the advantage of being
                 resolution-independent and storage-efficient. While
                 procedural shaders provide an interface for evaluating
                 the shader at a single point, it is not easily possible
                 to obtain an average value of the shader together with
                 accurate error bounds over a finite area. Yet the
                 ability to compute such error bounds is crucial for
                 several interesting applications, most notably
                 hierarchical area sampling for global illumination,
                 using the finite element approach, and for generation
                 of textures used in interactive computer graphics.
                 Using affine arithmetic for evaluating the shader over
                 a finite area yields a tight, conservative error
                 interval for the shader function. Compilers can
                 automatically generate code for utilizing affine
                 arithmetic from within shaders implemented in a
                 dedicated language such as the RenderMan shading
                 language.",
  volume =       "17",
  number =       "3",
  journal =      "ACM Transactions on Graphics",
}

@Article{EVL-1998-157,
  pages =        "177--208",
  year =         "1998",
  title =        "{V-Clip}: Fast and Robust Polyhedral Collision
                 Detection",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-157",
  author =       "Brian Mirtich",
  language =     "en",
  abstract =     "This article presents the Voronoi-clip, or V-Clip,
                 collision detection algorithm for polyhedral objects
                 specified by a boundary representation. V-Clip tracks
                 the closest pair of features between convex polyhedra,
                 using an approach reminiscent of the Lin-Canny closest
                 features algorithm. V-Clip is an improvement over the
                 latter in several respects. Coding complexity is
                 reduced, and robustness is significantly improved; the
                 implementation has no numerical tolerances and does not
                 exhibit cycling problems. The algorithm also handles
                 penetrating polyhedra, and can therefore be used to
                 detect collisions between nonvconvex polyhedra
                 described as hierarchies of convex pieces. The article
                 presents the theoretical principles of V-Clip, and
                 gives a pseudocode description of the algorithm. It
                 also documents various test that compare V-Clip,
                 Lin-Canny, and the Enhanced GJK algorithm, a
                 simplex-based algorithm that is widely used for the
                 same application. The results show V-Clip to be a
                 strong contender in this field, comparing favorably
                 with the other algorithms in most of the tests, in term
                 of both performance and robustness.",
  volume =       "17",
  number =       "3",
  journal =      "ACM Transactions on Graphics",
}

@Article{EVL-1998-158,
  pages =        "50--70",
  year =         "1998",
  title =        "The Directional Parameter Plane Transform of a Height
                 Field",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-158",
  author =       "David W. Paglieroni",
  language =     "en",
  abstract =     "The linear Parameter Plane Transform (PPT) of a height
                 field attributes an inverted cone of empty space to
                 each height field cell. In is known that height field
                 ray-tracing efficiency can be improved by traversing
                 rays in steps across inverted cones of empty space.
                 However, steps across inverted cones of empty space
                 along rays close to the base of a steep ridge will be
                 short, even if there are no obstructions along the line
                 of sight, because the cones will be narrow. This
                 weakness can be virtually eliminated by allowing the
                 opening angles of the inverted cones of empty space to
                 vary between sectors, i.e., by directionalizing the
                 linear PPT. An efficient algorithm for computing the
                 linear directional PPT of a height field is given and
                 its properties are investigated.",
  keywords =     "directional distance transform, directional parameter
                 plane transform, height field, ray tracing, terrain
                 visualization",
  volume =       "17",
  number =       "1",
  journal =      "ACM Transactions on Graphics",
}

@Article{EVL-1998-159,
  pages =        "84--115",
  year =         "1998",
  title =        "Geometric Compression Through Topological Surgery",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-159",
  author =       "Gabriel Taubin and Jarek Rossignac",
  language =     "en",
  abstract =     "The abundance and importance of complex 3-D data bases
                 in major industry segments, the affordability of
                 interactive 3-D rendering for office and consumer use,
                 and the exploitation of the Internet to distribute and
                 share 3-D data have intensified the need for an
                 effective 3-D geometric compression technique that
                 would significantly reduce the time required to
                 transmit 3-D models over digital communication
                 channels, and the amount of memory or disk space
                 required to store the models. Because the prevalent
                 representation of 3-D models for graphics purposes is
                 polyhedral and because polyhedral models are in general
                 triangulated for rendering, this article introduces a
                 new compressed representation for complex triangulated
                 models and simple, yet efficient, compression and
                 decompression algorithms. In this scheme, vertex
                 positions are quantized within the desired accuracy, a
                 vertex spanning tree is used to predict the position of
                 each vertex from 2,3, or 4 of its ancestors in the
                 tree, and the correction vectors are entropy encoded.
                 Properties, such as normals, colors, and texture
                 coordinates, are compressed in a similar manner. The
                 connectivity is encoded with no loss of information to
                 an average of less than two bits per triangle. The
                 vertex spanning tree and a small set of jump edges are
                 used to split the model into a simple polygon. A
                 triangle spanning tree and a sequence of marching bits
                 are used to encode the triangulation of the polygon.
                 Our approach improves on Michael Deering's pioneering
                 results by exploiting the geometric coherence of
                 several ancestors in the vertex spanning tree,
                 preserving the connectivity with no loss of
                 information, avoiding vertex repetitions, and using
                 about three fewer bits for the connectivity. However,
                 since decompression requires random access to all
                 vertices, this method must be modified for hardware
                 rendering with limited onboard memory. Finally, we
                 demonstrate implementation results for a variety of
                 {VRML} models with up to two orders of magnitude
                 compression.",
  keywords =     "geometry compression, 3D mesh compression, {VRML}",
  volume =       "17",
  number =       "2",
  journal =      "ACM Transactions on Graphics",
}

@InProceedings{EVL-1998-16,
  pages =        "275--286",
  year =         "1998",
  title =        "Realistic Modeling and Rendering of Plant Ecosystems",
  author =       "O. Deussen and P. Hanrahan and B. Lintermann and R.
                 Mech and M. Pharr and P. Prusinkiewicz",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-16",
  language =     "en",
  abstract =     "Modeling and rendering of natural scenes with
                 thousands of plants poses a number of problems. The
                 terrain must be modeled and plants must be distributed
                 throughout it in a realistic manner, reflecting the
                 interactions of plants with each other and with their
                 environment. Geometric models of individual plants,
                 consistent with their positions within the ecosystem,
                 must be synthesized to populate the scene. The scene,
                 which may consist of billions of primitives, must be
                 rendered efficiently while incorporating the subtleties
                 of lighting in a natural environment. We have developed
                 a system built around a pipeline of tools that address
                 these tasks. The terrain is designed using an
                 interactive graphical editor. Plant distribution is
                 determined by hand (as one would do when designing a
                 garden), by ecosystem simulation, or by a combination
                 of both techniques. Given parametrized procedural
                 models of individual plants, the geometric complexity
                 of the scene is reduced by approximate instancing, in
                 which similar plants, groups of plants, or plant organs
                 are replaced by instances of representative objects
                 before the scene is rendered. The paper includes
                 examples of visually rich scenes synthesized using the
                 system.",
  organization = "ACM SIGGRAPH",
  month =        jul,
  keywords =     "realistic image synthesis, modeling of natural
                 phenomena, ecosystem simulation, self-thinning, plant
                 model, vector quantization, approximate instancing",
  booktitle =    "SIGGRAPH '98 Conference Proceedings",
}

@Article{EVL-1998-160,
  pages =        "66--68",
  year =         "1998",
  title =        "Interactive 3{D} Visualization of Optical Phenomena",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-160",
  author =       "David C. Banks and Jay M. Brown and John T. Foley and
                 Kiril N. Vidince and {Ming-Hoe} Kiu",
  language =     "en",
  abstract =     "We created interactive 3D graphical modules for
                 visualizing optical phenomena by applying 3D
                 visualization techniques to instructional courseware.",
  volume =       "18",
  number =       "4",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-161,
  pages =        "40--42",
  year =         "1998",
  title =        "Visualizing Rich, Structured Hypermedia",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-161",
  author =       "Keith Andrews",
  language =     "en",
  abstract =     "The visualization techniques described here help users
                 understand and manage the complexity of large,
                 structured hypermedia collections.",
  keywords =     "information visualization, hierarchies, link maps,
                 metadata, Hyperwave",
  volume =       "18",
  number =       "4",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-162,
  pages =        "38--41",
  year =         "1998",
  title =        "Autoscan: {A} Flexible and Portable {3D} Scanner",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-162",
  author =       "Nunzio Alberto Borghese and Giancarlo Ferrigno and
                 Guido Baroni and Antonio Pedotti and Stefano Ferrari
                 and Riccardo Savar{\`{e}}",
  language =     "en",
  abstract =     "A portable 3D scanning system called {Autoscan}
                 providing flexibility, reliability, and accuracy for
                 scanning 3D surfaces.",
  volume =       "18",
  number =       "3",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-163,
  pages =        "30--38",
  year =         "1998",
  title =        "Principles for Information Visualization
                 Spreadsheets",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-163",
  author =       "Ed {Huai-hsin} Chi and John Riedl and Phillip Barry
                 and Joseph Konstan",
  language =     "en",
  abstract =     "The visualization spreadsheet provides a framework for
                 exploring large and complex data sets. Structuring user
                 interactions using a spreadsheet paradigm creates a
                 powerful tool for information visualization.",
  volume =       "18",
  number =       "4",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-164,
  pages =        "24--29",
  year =         "1998",
  title =        "Information Rich Glyphs for Software Management Data",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-164",
  author =       "Mei C. Chuah and Stephen G. Eick",
  language =     "en",
  abstract =     "Three high-dimensional glyphs for viewing software
                 project management data assist in perhaps the most
                 challenging engineering task in modern times ---
                 managing large software projects.",
  volume =       "18",
  number =       "4",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-165,
  pages =        "66--77",
  year =         "1998",
  title =        "Engineering a Human Factor-Based Geographic User
                 Interface",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-165",
  author =       "T. Todd Elvins and Ramesh Jain",
  language =     "en",
  abstract =     "An alternative GIS (geographic information system)
                 user interface designed using a formal mathematical
                 model and human-factor engineering relies on direct
                 manipulation of icons representing spatial and temporal
                 data layers.",
  volume =       "18",
  number =       "3",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-166,
  pages =        "54--59",
  year =         "1998",
  title =        "Collaborative Visualization in Augmented Reality",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-166",
  author =       "Anton Fuhrmann and Helwig L{\"{o}}ffelmann and Dieter
                 Schmalstieg and Michael Gervautz",
  language =     "en",
  abstract =     "Studierstube is an augmented reality system with
                 several advantages over conventional desktop and other
                 virtual reality environments. It includes true
                 stereoscopy, 3D interaction, individual viewpoints and
                 customized views for multiple users, unhindered natural
                 collaboration, and low cost. We demonstrate its
                 application by integrating our virtual environment into
                 a commercial visualization system. Several
                 visualizations of dynamical systems constructed with
                 our toolkit DynSys3D illustrate Studierstube's
                 benefits. An evaluation of the collaboration techniques
                 completes the presentation.",
  volume =       "18",
  number =       "4",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-167,
  year =         "1998",
  title =        "Visualization of an Imperfect World",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-167",
  author =       "Nahum Gershon",
  language =     "en",
  abstract =     "Our brains are wired to accept what we see as the
                 truth. This holds true in nature most of the time. In
                 synthetic imagery, however, it doesn't always. To begin
                 with, the visualized data and information can be
                 inaccurate or even wrong. Moreover, in the synthetic
                 digital world anybody can visualize anything in any
                 shape or form, disregarding how users might perceive or
                 get the information. Worse, as technology develops, it
                 becomes easier to do so. Understanding the data and
                 information and reaching sound decisions requires
                 knowing what pieces of information or data are
                 accurate, complete, consistent, and certain,
                 identifying which are not and by how much, and making
                 the presentation accurate. Certain techniques help us
                 do this, as explained here.",
  volume =       "18",
  number =       "4",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-168,
  pages =        "32--43",
  year =         "1998",
  title =        "The Irradiance Volume",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-168",
  author =       "Gene Greger and Peter Shirley and Philip M. Hubbard
                 and Donald P. Greenberg",
  language =     "en",
  abstract =     "The irradiance volume supports the reconstruction of
                 believable approximations to illumination in situations
                 that overwhelm traditional global illumination
                 algorithms.",
  volume =       "18",
  number =       "2",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-169,
  year =         "1998",
  title =        "Feature Recognition from {CAD} Models",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-169",
  author =       "{Jung-Hyun} Han and Aristides A. G. Requicha",
  language =     "en",
  abstract =     "{IF$^2$} (Integrated Incremental Feature Finder)
                 handles arbitrary spatial intersections between
                 features such as holes and slots. As a design evolves,
                 {IF$^2$} incrementally updates features.",
  volume =       "18",
  number =       "2",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-170,
  pages =        "59--79",
  year =         "1998",
  title =        "The {Truga001}: {A} Scalable Rendering Processor",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-170",
  author =       "Tsuneo Ikedo and Jianhua Ma",
  language =     "en",
  abstract =     "The {Truga001} is a single-chip rendering processor
                 for virtual reality and multimedia systems. It embeds
                 12 graphics processors and 7 special modules in a
                 single chip.",
  volume =       "18",
  number =       "2",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-171,
  pages =        "49--53",
  year =         "1998",
  title =        "Visualizing 3{D} Flow",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-171",
  author =       "Victoria Interrante and Chester Grosch",
  language =     "en",
  abstract =     "Line integral convolution (LIC) is an elegant and
                 versatile technique for representing directional
                 information via patterns of correlation in a texture.
                 In this article we discuss some of the factors that may
                 underlie the perceptual difficulties that we can
                 encounter with dense 3D displays and describe
                 strategies for more effectively visualizing 3D flow
                 with volume LIC. Specifically, we suggest techniques
                 for selectively emphasizing critical regions of
                 interest in a flow, for facilitating the accurate
                 perception of the 3D depth and orientation of
                 overlapping streamlines, for efficiently incorporating
                 an indication of orientation into a flow
                 representation, and for conveying additional
                 information about related scalar quantities such as
                 temperature or vorticity over a flow via subtle,
                 continuous line width and color variations.",
  volume =       "18",
  number =       "4",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-172,
  pages =        "95--103",
  year =         "1998",
  title =        "Triangle Rendering Using Adaptive Subdivision",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-172",
  author =       "Yakov Kamen and Leon Shirman",
  language =     "en",
  abstract =     "A new approach to piecewise linear approximation for
                 perspectively correct rendering operates by triangle
                 subdivision. The method derives error bounds between
                 perspective mapping and its linear approximation on
                 triangles. The major idea uses triangle subdivision
                 versus scanline subdivision, capturing the number of
                 subtriangles based on the ratio of homogeneous
                 coordinates. We show that it's possible to reduce the
                 number of dimensions by several orders of magnitude and
                 still maintain high-quality rendering.",
  volume =       "18",
  number =       "2",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-173,
  pages =        "70--74",
  year =         "1998",
  title =        "Automatic Vortex Core Detection",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-173",
  author =       "David N. Kenwright and Robert Haimes",
  language =     "en",
  abstract =     "An eigenvector method for vortex identification was
                 applied to recent numerical and experimental studies in
                 external flow aerodynamics. This article shows that it
                 is an effective way to extract and visualize features
                 such as vortex cores, spiral vortex breakdowns, and
                 vortex bursts. The algorithm has also been incorporated
                 in a finite element flow solver to guide an automatic
                 mesh refinement program. Results show that this
                 approach can resolve small-scale vortical structures in
                 helicopter rotor simulations that are not captured on
                 coarse meshes.",
  volume =       "18",
  number =       "4",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-174,
  pages =        "60--65",
  year =         "1998",
  title =        "Efficient Visualization of Crash-Worthiness
                 Simulations",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-174",
  author =       "Sven Kuschfeldt and Michael Holzner and Ove Sommer and
                 Thomas Ertl",
  language =     "en",
  abstract =     "Finite element postprocessing has been dominated by
                 software tightly integrated with simulation packages.
                 Many of these packages have not kept up with
                 state-of-the-art developments in graphics technology
                 and visualization techniques. In particular, the large
                 and time-dependent data sets resulting from
                 crash-worthiness simulations in the automotive
                 development process demand new visualization tools.
                 This article demonstrates that careful design of scene
                 graph structures and extensive use of texture mapping
                 can improve rendering performance and visual appearance
                 for postprocessing tasks. Furthermore, a new iconic
                 visualization method improves the understanding of
                 cross-section forces and bending moments in
                 longitudinal structures of the car body.",
  volume =       "18",
  number =       "4",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-175,
  pages =        "44--58",
  year =         "1998",
  title =        "On-the-fly Texture Computation for Real-Time Surface
                 Shading",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-175",
  author =       "Gavin Miller and Mark Halstead and Michael Clifton",
  language =     "en",
  abstract =     "Standard texture-mapping hardware enables rapid
                 rendering of color-mapped surfaces with interpolated
                 surface shading. New algorithms extend this to bump
                 mapping, Phong shading, and reflection mapping.",
  volume =       "18",
  number =       "2",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-176,
  pages =        "18--23",
  year =         "1998",
  title =        "Exploring Large Graphs in 3{D} Hyperbolic Space",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-176",
  author =       "Tamara Munzner",
  language =     "en",
  abstract =     "Graphs have a very natural visual representation as
                 nodes and connecting links arranged in space. Although
                 many examples show the computational manipulation of
                 large data sets that can be expressed in graph form,
                 visual representations of them as graphs are not
                 common. Current tools are inadequate for the job:
                 conventional systems are often challenged by hundreds
                 of edges, and none can handle more than a few thousand
                 edges. However, nonvisual manipulation of graphs with
                 50,000 edges is commonplace, and much larger instances
                 exist. A graph drawing system that focuses on the
                 interactive browsing of large graphs can be targeted
                 for the quite different tasks of browsing and
                 exploration. Many researchers in scientific
                 visualization have recognized the split between
                 explanatory and exploratory goals. This distinction is
                 equally relevant for graph drawing.",
  volume =       "18",
  number =       "4",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-177,
  pages =        "28--37",
  year =         "1998",
  title =        "Optical {3D} Digitizers: Bringing Life to the Virtual
                 World",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-177",
  author =       "Michael Petrov and Andrey Talapov and Timothy
                 Robertson and Alexei Lebedev and Alexander Zhilyaev and
                 Leonid Polonsky",
  language =     "en",
  abstract =     "The authors took a 3D scanner developer's viewpoint to
                 review different 3D digitizing techniques. They briefly
                 compare existing systems and give special
                 considerations to active optical triangulation-based
                 scanners.",
  volume =       "18",
  number =       "3",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-178,
  pages =        "22--31",
  year =         "1998",
  title =        "Composite Lighting Simulations with Lighting
                 Networks",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-178",
  author =       "Philipp Slusalek and Marc Stamminger and Wolfgang
                 Heidrich and {Jan-Christian} Popp and {Hans-Peter}
                 Seidel",
  language =     "en",
  abstract =     "Lighting Networks combine different global
                 illumination algorithms in a composite lighting
                 simulation and allow for restricting costly lighting
                 effects to important parts of the scene.",
  volume =       "18",
  number =       "2",
  journal =      "IEEE Computer Graphics \& Applications",
}

@Article{EVL-1998-179,
  pages =        "54--60",
  year =         "1998",
  title =        "Electric Field Sensing For Graphical Interfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-179",
  author =       "Joshua Smith and Tom White and Christopher Dodge and
                 Joseph Paradiso and Neil Gershenfeld and David
                 Allport",
  language =     "en",
  abstract =     "The authors outline the theory and implementation of
                 low-frequency electric fields as sensing techniques,
                 then present a range of applications developed for
                 interacting with computer graphics.",
  volume =       "18",
  number =       "3",
  journal =      "IEEE Computer Graphics \& Applications",
}

@InProceedings{EVL-1998-18,
  pages =        "79--86",
  year =         "1998",
  title =        "{IVORY} - An Object-Oriented Framework for
                 Physics-Based Information Visualization in Java",
  author =       "T. C. Sprenger and M. H. Gross and D. Bielser and T.
                 Strasser",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-18",
  language =     "en",
  abstract =     "We present IVORY, a newly developed,
                 platform-independent framework for physics-based
                 visualization. IVORY is especially designed for
                 information visualization applications and
                 multidimensional graph layout. It is fully implemented
                 in Java 1.1 and its architecture features client-server
                 setup, which allows to run the visualization even on
                 thin clients. In addition, VRML 2.0 exports can be
                 viewed by any VRML plugged-in WWW-browser. Individual
                 visual metaphors are invoked into IVORY via an advanced
                 plug-in mechanism, where plug-ins can be implemented by
                 any experienced user. The configuration of IVORY is
                 accomplished using a script language, called IVML. Some
                 interactive visualization examples, such as the
                 integration of an haptic interface illustrate the
                 performance and versatility of our system. Our current
                 implementation supports NT 4.0.",
  keywords =     "three-dimensional information visualization,
                 physics-based graph layout, object-oriented
                 visualization toolkit, multidimensional information
                 modeling, time varying data",
  booktitle =    "Proceedings IEEE Symposium on Information
                 Visualization 1998",
}

@Article{EVL-1998-181,
  title =        "The rectangle of influence drawability problem",
  language =     "en",
  month =        apr,
  number =       "1",
  pages =        "1--22",
  year =         "1998",
  author =       "G. Liotta and A. Lubiw and H. Meijer and S. H.
                 Whitesides",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-181",
  abstract =     "Motivated by rectangular visibility and graph drawing
                 applications, we study the problem of characterizing
                 classes of graphs that admit rectangle of influence
                 drawings. We consider several classes of graphs and
                 show, for each class, that testing whether a graph G
                 has a rectangle of influence drawing can be done in
                 O(n) time, where n is the number of vertices of G . If
                 the test for G is affirmative, we show how to construct
                 a rectangle of influence drawing of G . All the drawing
                 algorithms can be implemented so that they (1) produce
                 drawings with all vertices placed at intersection
                 points of an integer grid of size O(n2) , (2) perform
                 arithmetic operations on integers only, and (3) run in
                 O(n) time, where n is the number of vertices of the
                 input graph.",
  keywords =     "Graph drawing, Proximity, Rectangle of influence",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-182,
  title =        "Optimal slope selection via cuttings",
  language =     "en",
  month =        apr,
  number =       "1",
  pages =        "23--29",
  year =         "1998",
  author =       "Herv{\'{e}} Br{\"o}nnimann and Bernard Chazelle",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-182",
  abstract =     "We give an optimal deterministic O(n log n)-time
                 algorithm for slope selection. The algorithm borrows
                 from the optimal solution given in (Cole et al., 1989)
                 but avoids the complicated machinery of the AKS sorting
                 network and parametric searching. This is achieved by
                 redesigning and refining the O(n log 2 n)-time
                 algorithm of Chazelle et al. (1993) with the help of
                 additional approximation tools.",
  keywords =     "Deterministic optimal algorithm, Arrangements,
                 Sorting",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-183,
  title =        "Practical methods for approximating shortest paths on
                 a convex polytope in ${R}^3$",
  language =     "en",
  month =        apr,
  number =       "1",
  pages =        "31--46",
  year =         "1998",
  author =       "John Hershberger and Subhash Suri",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-183",
  abstract =     "We propose an extremely simple approximation scheme
                 for computing shortest paths on the surface of a convex
                 polytope in three dimensions. Given a convex polytope P
                 with n vertices and two points p,q on its surface, let
                 $d_P (p,q)$ denote the shortest path distance between p
                 and q on the surface of P . Our algorithm produces a
                 path of length at most $2 d_P (p,q)$ in time O(n) .
                 Extending this result, we can also compute an
                 approximation of the shortest path tree rooted at an
                 arbitrary point $x \in P$ in time O(n log n) . In the
                 approximate tree, the distance between a vertex $v \in
                 P$ and x is at most $c d_P(x,v)$ , where $c = 2.38
                 (1+\varepsilon)$ for any fixed $\varepsilon > 0$ . The
                 best algorithms for computing an exact shortest path on
                 a convex polytope take $\Omega (n^2)$ time in the worst
                 case; in addition, they are too complicated to be
                 suitable in practice. We can also get a weak
                 approximation result in the general case of k disjoint
                 convex polyhedra: in O(n) time our algorithm gives a
                 path of length at most 2k times the optimal.",
  keywords =     "Shortest path, Approximation, Convex polyhedron,
                 Shortest path tree",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-184,
  title =        "Note on an art gallery problem",
  language =     "en",
  month =        apr,
  number =       "1",
  pages =        "47--55",
  year =         "1998",
  author =       "Gy{\"o}rgy Csizmadia and G{\'{e}}za T{\'{o}}th",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-184",
  abstract =     "It is proved that for $n>3$, $\lceil (2)/(5)(n-3)
                 \rceil$ guards are enough to monitor any simply
                 connected art gallery room of n sides if they are
                 stationed at fixed points and their range of vision is
                 $180^o$. Furthermore, the position of the guards can be
                 determined by an O(n)-time algorithm.",
  keywords =     "Visibility, Art gallery, Graph",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-185,
  title =        "Illumination by floodlights",
  language =     "en",
  month =        apr,
  number =       "1",
  pages =        "57--70",
  year =         "1998",
  author =       "William Steiger and Ileana Streinu",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-185",
  abstract =     "We consider three problems about the illumination of
                 planar regions with floodlights of prescribed angles.
                 Problem 1 is the decision problem: given a wedge W of
                 angle $\phi \le \pi$, n points $p_1,...,p_n$ in the
                 plane and n angles $\alpha_1,...,\alpha n$ such that
                 $\sum^n_{i=1}\alpha_i \le \theta$, decide whether W can
                 be illuminated by floodlights of angles
                 $\alpha_1,...,\alpha_n$ placed in some order at the
                 points $p_1,...,p_n$ and then rotated appropriately. We
                 show that this problem is the exponential time and a
                 specialized version of it (when $\phi=\theta$) is in
                 NP. The second problem arises when the n points are in
                 the complementary wedge of W and $\theta \ge \phi$.
                 Bose et al. have shown that a solution exists and gave
                 an O(n log n) algorithm to place the floodlights. Here
                 we give a matching lower bound. Problem 3 involves the
                 illumination of the whole plane. The algorithm of Bose
                 et al. uses an O(n log n) tripartitioning algorithm to
                 reduce problem 3 to problem 2. We give a linear time
                 tripartitioning algorithm of independent interest.",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-186,
  title =        "Approximation of convex figures by pairs of
                 rectangles",
  language =     "en",
  month =        may,
  number =       "2",
  pages =        "77--87",
  year =         "1998",
  author =       "Otfried Schwarzkopf and Ulrich Fuchs and G{\"u}nter
                 Rote and Emo Welzl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-186",
  abstract =     "We consider the problem of approximating a convex
                 figure in the plane by a pair (r,R) of homothetic (that
                 is, similar and parallel) rectangles with $r \subseteq
                 C \subseteq R$. We show the existence of such a pair
                 where the sides of the outer rectangle are at most
                 twice as long as the sides of the inner rectangle,
                 thereby solving a problem posed by P{\'{o}}lya and
                 Szeg{\H{o}}. If the n vertices of a convex polygon C
                 are given as a sorted array, such an approximating pair
                 of rectangles can be computed in time $O(log^2 n)$.",
  keywords =     "Computational geometry, Planar, Convex polygon
                 Approximation, Inner--outer-approximation, Rectangle",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-187,
  title =        "Finding constrained and weighted Voronoi diagrams in
                 the plane",
  language =     "en",
  month =        may,
  number =       "2",
  pages =        "89--104",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-187",
  author =       "Cao An Wang and Yung H. Tsin",
  abstract =     "The Voronoi diagram of a set of weighted points
                 (sites) whose visibilities are constrained by a set of
                 line segments (obstacles) on the plane is studied. The
                 diagram is called constrained and weighted Voronoi
                 diagram. When all the sites are of the same weight, it
                 becomes the constrained Voronoi diagram in which the
                 endpoints of the obstacles need not be sites. An
                 $\Omega (m^2 n^2)$ lower bound on the combinatorial
                 complexity of both constrained Voronoi diagram and
                 constrained and weighted Voronoi diagram is
                 established, where n is the number of sites and m is
                 the number of obstacles. For constrained Voronoi
                 diagram, an $O(m^2 n^2 + n^4)$ time and space algorithm
                 is presented. The algorithm is optimal when $m \ge cn$
                 , for any positive constant c . For constrained and
                 weighted Voronoi diagram, an $O(m^2 n^2 + n^4 2^{\alpha
                 (n)})$ time and $O(m^2 n^2 + n^4)$ space algorithm
                 (where $\alpha(n)$ is the functional inverse of the
                 Ackermann's function) is presented. The algorithm is
                 near-optimal when $m \ge cn$ , for any positive
                 constant c.",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-188,
  title =        "An output sensitive algorithm for discrete convex
                 hulls",
  language =     "en",
  month =        may,
  number =       "2",
  pages =        "125--138",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-188",
  author =       "Sariel Har-Peled",
  abstract =     "Given a convex body C in the plane, its discrete hull
                 is $C^0 = ConvexHull( C {\ca}p L)$ , where $L = Z
                 \times Z$ is the integer lattice. We present an $O(
                 |C0| \log \delta(C))$-time algorithm for calculating
                 the discrete hull of C , where $|C^0|$ denotes the
                 number of vertices of $C^0$ , and $\delta(C)$ is the
                 diameter of $C$ . Actually, using known combinatorial
                 bounds, the running time of the algorithm is $O(\delta
                 (C)^{2/3} log \delta (C))i$ . In particular, this bound
                 applies when C is a disk.",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-189,
  title =        "On circumscribing polygons for line segments",
  language =     "en",
  month =        may,
  number =       "2",
  pages =        "121--124",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-189",
  author =       "J{\'{a}}nos Pach and Eduardo Rivera-Campo",
  abstract =     "Any family of $k^3+1$ pairwise disjoint line segments
                 in the Euclidean plane $E^2$, such that no three of
                 their endpoints are collinear, has $k+1$ members
                 admitting a circumscribing polygon. That is, one can
                 find a simple polygon P with $2k+2$ vertices such that
                 each of these segments is either an edge or an internal
                 diagonal of P.",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@InProceedings{EVL-1998-19,
  pages =        "397--402",
  year =         "1998",
  title =        "Progressive Tetrahedralizations",
  author =       "Oliver G. Staadt and Markus H. Gross",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-19",
  language =     "en",
  abstract =     "This paper describes some fundamental issues for
                 robust implementations of progressively refined
                 tetrahedralizations generated through sequences of edge
                 collapses. We address the definition of appropriate
                 cost functions and explain on various tests which are
                 necessary to preserve the consistency of the mesh when
                 collapsing edges. Although being considered a special
                 case of progressive simplicial complexes the results of
                 our method are of high practical importance and can be
                 used in many different applications, such as finite
                 element meshing, scattered data interpolation or
                 rendering of unstructured volume data.",
  organization = "IEEE",
  keywords =     "mesh simplification, multiresolution, FEM meshing",
  booktitle =    "Proceedings IEEE Visualization '98",
}

@Article{EVL-1998-190,
  title =        "The vertex--edge visibility graph of a polygon",
  language =     "en",
  month =        may,
  number =       "2",
  pages =        "105--120",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-190",
  author =       "Joseph {O'Rourke} and Ileana Streinu",
  abstract =     "We introduce a new polygon visibility graph, the
                 vertex--edge visibility graph $G_{V E}$, and
                 demonstrate that it encodes more geometric information
                 about the polygon than does the vertex visibility graph
                 $G_V$.",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-191,
  title =        "Intersections with random geometric objects",
  language =     "en",
  month =        jun,
  number =       "3",
  pages =        "139--154",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-191",
  author =       "Prosenjit Bose and Luc Devroye",
  abstract =     "We present a systematic study of the expected
                 complexity of the intersection of geometric objects. We
                 first study the expected size of the intersection
                 between a random Voronoi diagram and a generic
                 geometric object that consists of a finite collection
                 of line segments in the plane. Using this result, we
                 explore the intersection complexity of a random Voronoi
                 diagram with the following target objects which may or
                 may not be random: a line segment, a Voronoi diagram, a
                 minimum spanning tree, a Gabriel graph, a relative
                 neighborhood graph, a Hamiltonian circuit, a furthest
                 point Voronoi diagram, a convex hull, a k-dimensional
                 tree, and a rectangular grid.",
  keywords =     "Voronoi diagram, Range searching, Intersection
                 complexity, Probabilistic analysis, Computational
                 geometry",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-192,
  title =        "On triangulating three-dimensional polygons",
  language =     "en",
  month =        jun,
  number =       "3",
  pages =        "155--170",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-192",
  author =       "Gill Barequet and Matthew Dickerson and David
                 Eppstein",
  abstract =     "A three-dimensional polygon is triangulable if it has
                 a non-self-intersecting triangulation which defines a
                 simply-connected 2-manifold. We show that the problem
                 of deciding whether a 3-dimensional polygon is
                 triangulable is NP-complete. We then establish some
                 necessary conditions and some sufficient conditions for
                 a polygon to be triangulable, providing special cases
                 when the decision problem may be answered in polynomial
                 time.",
  keywords =     "Three-dimensions, Triangulation",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-193,
  title =        "Recognizing weakly convex visible polygons",
  language =     "en",
  month =        jun,
  number =       "3",
  pages =        "171--186",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-193",
  author =       "Supriya Biswas and D. Chithra Prasad and Sudebkumar
                 Prasant Pal",
  abstract =     "Two points inside a simple polygon are said to be
                 convex visible if the Euclidean shortest path between
                 them makes either only right turns or only left turns.
                 A simple polygon P is said to be weakly convex visible
                 from a line segment inside P, if every point in the
                 polygon is convex visible from some point of the line
                 segment. In this paper we propose an algorithm for
                 recognizing weakly convex visible polygons. Our
                 algorithm computes a line segment inside the given
                 polygon from which the polygon is weakly convex
                 visible. For an n -sided polygon, the algorithm runs in
                 $O(n^2 log n)$ time.",
  keywords =     "Convex visibility, Convex visibility segment, Eave,
                 Hourglass",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-194,
  title =        "Visibility with multiple diffuse reflections",
  language =     "en",
  month =        jun,
  number =       "3",
  pages =        "187--196",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-194",
  author =       "D. Chithra Prasad and Sudebkumar P. Pal and Tamal K.
                 Dey",
  abstract =     "The combinatorial complexity of the region lit up from
                 a point light source inside a simple $n$-gon with
                 perfectly reflecting edges, after at most k specular
                 reflections was established as $O(n^{2k})$ (Aronov et
                 al., 1996). A lower bound of $\Omega((n/k)^{2k})$ was
                 also established, matching the upper bound for any
                 fixed k. In a real situation, surfaces may not be
                 perfect mirrors; indeed most surfaces may be non-shiny
                 or rough, causing diffuse reflection, rather than
                 specular reflection. In contrast to the result of
                 (Aronov et al., 1996), the combinatorial complexity of
                 the region lit up from a point inside a simple $n$-gon
                 after k diffuse reflections is established here to be
                 $O(n^{2 \lceil (k+1)/2 \rceil +1})$.",
  keywords =     "Visibility, Diffuse reflection, Combinatorial
                 complexity",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-195,
  title =        "Getting around a lower bound for the minimum Hausdorff
                 distance",
  language =     "en",
  month =        jun,
  number =       "3",
  pages =        "197--202",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-195",
  author =       "L. Paul Chew and Klara Kedem",
  abstract =     "We consider the following geometric pattern matching
                 problem: find the minimum Hausdorff distance between
                 two point sets under translation with $L_1$ or
                 $L_\infty$ as the underlying metric. Huttenlocher,
                 Kedem and Sharir have shown that this minimum distance
                 can be found by constructing the upper envelope of
                 certain Voronoi surfaces. Further, they show that if
                 the two sets are each of cardinality n then the
                 complexity of the upper envelope of such surfaces is
                 $\Omega (n^3)$. We examine the question of whether one
                 can get around this cubic lower bound, and show that
                 under the $L_1$ and $L_\infty$ metrics, the time to
                 compute the minimum Hausdorff distance between two
                 point sets is $O(n^2 \log^2 n).",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-196,
  title =        "Rectangular grid drawings of plane graphs",
  language =     "en",
  month =        jun,
  number =       "3",
  pages =        "203--220",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-196",
  author =       "Md. Saidur Rahman and Shin-ichi Nakano and Takao
                 Nishizeki",
  abstract =     "The rectangular grid drawing of a plane graph G is a
                 drawing of G such that each vertex is located on a grid
                 point, each edge is drawn as a horizontal or vertical
                 line segment, and the contour of each face is drawn as
                 a rectangle. In this paper we give a simple linear-time
                 algorithm to find a rectangular grid drawing of G if it
                 exists. We also give an upper bound $W+H \le n/2$ on
                 the sum of required width W and height H and a bound $W
                 H \le n^2/16$ on the area of a rectangular grid drawing
                 of G, where n is the number of vertices in G. These
                 bounds are best possible, and hold for any compact
                 rectangular grid drawing.",
  keywords =     "Algorithm, Rectangular drawing, Grid drawing, Grid
                 area, Floorplanning",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-197,
  title =        "Wall thickness control in layered manufacturing for
                 surfaces with closed slices",
  language =     "en",
  month =        jul,
  number =       "4",
  pages =        "223--238",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-197",
  author =       "Seth Allen and Debasish Dutta",
  abstract =     "The layered manufacture of sheet (surface) parts is
                 considered for processes that use support structure. In
                 particular, for surfaces with closed slices and solids
                 whose interiors are unimportant, a new method for
                 construction that reduces the need for
                 {"}traditional{"} support structure, hence reducing
                 wasted material and decreasing build time, is proposed.
                 This method selectively thickens the surface's walls
                 and is a natural compliment to adaptive slicing.
                 Instructive examples are provided.",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-198,
  title =        "r-regular shape reconstruction from unorganized
                 points",
  language =     "en",
  month =        jul,
  number =       "4",
  pages =        "239--247",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-198",
  author =       "D. Attali",
  abstract =     "In this paper, the problem of reconstructing a
                 surface, given a set of scattered data points is
                 addressed. First, a precise formulation of the
                 reconstruction problem is proposed. The solution is
                 mathematically defined as a particular mesh of the
                 surface called the normalized mesh. This solution has
                 the property to be included inside the Delaunay graph.
                 A criterion to detect faces of the normalized mesh
                 inside the Delaunay graph is proposed. This criterion
                 is proved to provide the exact solution in 2D for
                 points sampling a r -regular shapes with a sampling
                 path $\epsilon < \sin (\pi / 8)r$. In 3D, this result
                 cannot be extended and the criterion cannot retrieve
                 every face. A heuristic is proposed in order to
                 complete the surface.",
  keywords =     "Shape reconstruction, Interpolation, Delaunay graph,
                 Voronoi graph, Normalized mesh, r-regular shape, Sample
                 points",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-199,
  title =        "Computing fence designs for orienting parts",
  language =     "en",
  month =        jul,
  number =       "4",
  pages =        "249--262",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-199",
  author =       "Robert-Paul Berretty and Ken Goldberg and Mark H.
                 Overmars and A. Frank van der Stappen",
  abstract =     "A common task in automated manufacturing processes is
                 to orient parts prior to assembly. We consider
                 sensorless orientation of a polygonal part by a
                 sequence of fences. We show that any polygonal part can
                 be oriented by a sequence of fences placed along a
                 conveyor belt, thereby settling a conjecture by Wiegley
                 et al. (1997), and present the first polynomial-time
                 algorithm to compute the shortest such sequence. The
                 algorithm is easy to implement and runs in time $O(n^3
                 \log n)$, where n is the number of vertices of the
                 part.",
  keywords =     "Robotics, Parts feeding, Fence design, Planning",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Unpublished{EVL-1998-2,
  year =         "1998",
  title =        "Approaches to the Successful Design and Implementation
                 of {VR} Applications",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-2",
  author =       "Steve Bryson",
  abstract =     "Virtual reality is the use of various computer
                 graphics systems in combination with various display
                 and interface devices to provide the effect of
                 immersion in an interactive three-dimensional
                 computer-generated environment in which the virtual
                 objects have spatial presence. We call this interactive
                 three-dimensional computer-generated environment a
                 virtual environment. By immersion I mean the sense that
                 either the user's point of view or some part of the
                 user's body (e.g. hand) is contained within the
                 computer-generated space. By presence I mean that the
                 computer-generated objects in the virtual environment
                 have an apparent position in three-dimensional space
                 relative to the user.",
  language =     "en",
  note =         "http://science.nas.nasa.gov/~bryson/papers.html",
}

@Article{EVL-1998-200,
  title =        "{RAPID}: Randomized pharmacophore identification for
                 drug design",
  language =     "en",
  month =        jul,
  number =       "4",
  pages =        "263--272",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-200",
  author =       "P. W. Finn and L. E. Kavraki and J.-C. Latombe and R.
                 Motwani and C. Shelton and S. Venkatasubramanian and A.
                 Yao",
  abstract =     "This paper describes a randomized approach for finding
                 invariants in a set of flexible and chemically distinct
                 ligands (drug molecules) that underlies an integrated
                 software system called RAPID currently under
                 development. An invariant is a collection of features
                 embedded in $R^3$ which is present in one or more of
                 the possible low-energy conformations of each ligand.
                 Such invariants are called pharmacophores and contain
                 the parts of the ligand that are primarily responsible
                 for its binding with a receptor. The identification of
                 pharmacophores is crucial in drug design since
                 frequently the structure of targeted receptor is
                 unknown but a number of molecules that interact with it
                 have been discovered by experiments. In these cases the
                 pharmacophore is used as a template for building more
                 effective drugs. It is expected that our techniques and
                 results will prove useful in other applications such as
                 molecular database screening and comparative molecular
                 field analysis.",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-201,
  title =        "A perturbation scheme for spherical arrangements with
                 application to molecular modeling",
  language =     "en",
  month =        jul,
  number =       "4",
  pages =        "273--287",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-201",
  author =       "Dan Halperin and Christian R. Shelton",
  abstract =     "We describe a software package for computing and
                 manipulating the subdivision of a sphere by a
                 collection of (not necessarily great) circles and for
                 computing the boundary surface of the union of spheres.
                 We present problems that arise in the implementation of
                 the software and the solutions that we have found for
                 them. At the core of the paper is a novel perturbation
                 scheme to overcome degeneracies and precision problems
                 in computing spherical arrangements while using
                 floating point arithmetic. The scheme is relatively
                 simple, it balances between the efficiency of
                 computation and the magnitude of the perturbation, and
                 it performs well in practice. In one $O(n)$ time pass
                 through the data, it perturbs the inputs necessary to
                 insure no potential degeneracies and then passes the
                 perturbed inputs on to the geometric algorithm. We
                 report and discuss experimental results. Our package is
                 a major component in a larger package aimed to support
                 geometric queries on molecular models; it is currently
                 employed by chemists working in {"}rational drug
                 design{"}. The spherical subdivisions are used to
                 construct a geometric model of a molecule where each
                 sphere represents an atom.",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-202,
  title =        "A computational basis for higher-dimensional
                 computational geometry and applications",
  language =     "en",
  month =        jul,
  number =       "4",
  pages =        "289--303",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-202",
  author =       "K. Mehlhorn and M. M{\"u}ller and S. N{\"a}her and S.
                 Schirra and M. Seel and C. Uhrig and J. Ziegler",
  abstract =     "In this paper we describe and discuss a kernel for
                 higher-dimensional computational geometry and we
                 present its application in the calculation of convex
                 hulls and Delaunay triangulations. The kernel is
                 available in form of a software library module
                 programmed in C++ extending LEDA. We introduce the
                 basic data types like points, vectors, directions,
                 hyperplanes, segments, rays, lines, spheres, affine
                 transformations, and operations connecting these types.
                 The description consists of a motivation for the basic
                 class layout as well as topics like layered software
                 design, runtime correctness via checking routines and
                 documentation issues. Finally we shortly describe the
                 usage of the kernel in the application domain.",
  keywords =     "Software library, Implementation, Convex hull,
                 Delaunay triangulation",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-203,
  title =        "Rotational polygon overlap minimization and
                 compaction",
  language =     "en",
  month =        jul,
  number =       "4",
  pages =        "305--318",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-203",
  author =       "Victor J. Milenkovic",
  abstract =     "An effective and fast algorithm is given for
                 rotational overlap minimization: given an overlapping
                 layout of polygons $P_1, P_2, P_3,...,P_k$ in a
                 container polygon Q, translate and rotate the polygons
                 to diminish their overlap to a local minimum. A (local)
                 overlap minimum has the property that any perturbation
                 of the polygons increases the overlap. Overlap
                 minimization is modified to create a practical
                 algorithm for compaction: starting with a
                 non-overlapping layout in a rectangular container, plan
                 a non-overlapping motion that diminishes the length or
                 area of the container to a local minimum. Experiments
                 show that both overlap minimization and compaction work
                 well in practice and are likely to be useful in
                 industrial applications.",
  keywords =     "Layout, Packing or nesting of irregular polygons,
                 Containment, Minimum enclosure, Compaction, Linear
                 programming",
  volume =       "10",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-204,
  title =        "Optimal placement of convex polygons to maximize point
                 containment",
  language =     "en",
  month =        jun,
  number =       "1",
  pages =        "1--16",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-204",
  author =       "Matthew Dickerson and Daniel Scharstein",
  abstract =     "Given a convex polygon P with m vertices and a set S
                 of n points in the plane, we consider the problem of
                 finding a placement of P (allowing both translation and
                 rotation) that contains the maximum number of points in
                 S . We present first an algorithm requiring $O(n^2 k
                 m^2 \log (m n))$ time and $O(n+m)$ space, where k is
                 the maximum number of points contained. We then give a
                 refinement that makes use of bucketing to improve the
                 running time to $O(n k^2 c^2 m^2 \log (m k))$, where c
                 is the ratio of length to width of the polygon. This
                 provides an improvement over the best previously known
                 algorithm linear in n when k is large ($\Theta (n)$)
                 and a cubic when k is small. We also show that the
                 algorithm can be extended to solve bichromatic and
                 general weighted variants of the problem. The algorithm
                 is self-contained and utilizes the geometric properties
                 of the containing regions in the parameter space of
                 transformations.",
  keywords =     "Maximizing placement, Rotation diagram",
  volume =       "11",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-205,
  title =        "On some geometric selection and optimization problems
                 via sorted matrices",
  language =     "en",
  month =        jun,
  number =       "1",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-205",
  author =       "Alex Glozman and Klara Kedem and Gregory Shpitalnik",
  abstract =     "In this paper we apply the selection and optimization
                 technique of Frederickson and Johnson to a number of
                 geometric selection and optimization problems, some of
                 which have previously been solved by parametric search,
                 and provide efficient and simple algorithms. Our
                 technique improves the solutions obtained by parametric
                 search by a log n factor. For example, we apply the
                 technique to the two-line center problem, where we want
                 to find two strips that cover a given set S of n points
                 in the plane, so as to minimize the width of the
                 largest of the two strips.",
  keywords =     "Computational geometry, Algorithm, Selection,
                 Optimization, Two-line center",
  volume =       "11",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-206,
  pages =        "29--54",
  year =         "1998",
  title =        "Minimum-width grid drawings of plane graphs",
  author =       "Marek Chrobak and Shin-ichi Nakano",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-206",
  language =     "en",
  abstract =     "Given a plane graph G , we wish to draw it in the
                 plane in such a way that the vertices of G are
                 represented as grid points, and the edges are
                 represented as straight-line segments between their
                 endpoints. An additional objective is to minimize the
                 size of the resulting grid. It is known that each plane
                 graph can be drawn in such a way in an
                 $(n-2)\times(n-2)$ grid (for $n\ge 3$), and that no
                 grid smaller than $(2n/3-1)\times(2n/3-1)$ can be used
                 for this purpose, if n is a multiple of 3. In fact, for
                 all $n\ge 3$ , each dimension of the resulting grid
                 needs to be at least $\lfloor 2(n-1)/3 \rfloor$, even
                 if the other one is allowed to be unbounded. In this
                 paper we show that this bound is tight by presenting a
                 grid drawing algorithm that produces drawings of width
                 $\lfloor 2(n-1)/3\rfloor$. The height of the produced
                 drawings is bounded by $4 \lfloor 2(n-1)/3 \rfloor -1$.
                 Our algorithm runs in linear time and is easy to
                 implement.",
  volume =       "11",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  number =       "1",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-207,
  title =        "The largest k-ball in a d-dimensional box",
  language =     "en",
  month =        jun,
  number =       "2",
  pages =        "59--67",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-207",
  author =       "Hazel Everett and Ivan Stojmenovic and Pavel Valtr and
                 Sue Whitesides",
  abstract =     "This paper considers d-dimensional boxes of the form
                 ${x | -a_i \le x_i \le a_i}$ in $E^d$. It establishes a
                 formula for the radius of the largest k -dimensional
                 ball contained in such a d-dimensional box.",
  volume =       "11",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-208,
  title =        "The union of moving polygonal pseudodiscs --
                 Combinatorial bounds and applications",
  language =     "en",
  month =        jun,
  number =       "2",
  pages =        "69--81",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-208",
  author =       "Mark de Berg and Hazel Everett and Leonidas J.
                 Guibas",
  abstract =     "Let P be a set of polygonal pseudodiscs in the plane
                 with n edges in total translating with fixed velocities
                 in fixed directions. We prove that the maximum number
                 of combinatorial changes in the union of the
                 pseudodiscs in P is $\Theta(n^2 \alpha(n))$. In
                 general, if the pseudodiscs move along curved
                 trajectories, then the maximum number of changes in the
                 union is $\Theta(n \lambda_{s+2}(n))$, where s is the
                 maximum number of times any triple of polygon edges
                 meet in a common point. We apply this result to prove
                 that the complexity of the space of lines missing a set
                 of n convex homothetic polytopes of constant complexity
                 in 3-space is $O(n^2 \lambda_4 (n))$. This bound is
                 almost tight in the worst case.",
  volume =       "11",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-209,
  title =        "Spheres, molecules, and hidden surface removal",
  language =     "en",
  month =        jun,
  number =       "2",
  pages =        "83--102",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-209",
  author =       "Dan Halperin and Mark H. Overmars",
  abstract =     "We devise techniques to manipulate a collection of
                 loosely interpenetrating spheres in three-dimensional
                 space. Our study is motivated by the representation and
                 manipulation of molecular configurations, modeled by a
                 collection of spheres. We analyze the sphere model and
                 point to properties that make it more easy to
                 manipulate than an arbitrary collection of spheres. For
                 this special sphere model we present efficient
                 algorithms for computing its union boundary and for
                 hidden surface removal. The efficiency and practicality
                 of our approach are demonstrated by experiments on
                 actual molecule data.",
  keywords =     "Computational geometry, Molecular modeling,
                 Arrangements, Hidden surface removal",
  volume =       "11",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-21,
  title =        "A New Line Integral Convolution Algorithm for
                 Visualizing Time-Varying Flow Fields",
  language =     "en",
  month =        apr,
  number =       "2",
  pages =        "98--108",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-21",
  author =       "Han-Wei Shen and David L. Kao",
  abstract =     "New challenges on vector field visualization emerge as
                 time-dependent numerical simulations become ubiquitous
                 in the field of computational fluid dynamics (CFD). To
                 visualize data generated from these simulations,
                 traditional techniques, such as displaying particle
                 traces, can only reveal flow phenomena in preselected
                 local regions and, thus, are unable to track the
                 evolution of global flow features over time. This paper
                 presents a new algorithm, called UFLIC (Unsteady Flow
                 LIC), to visualize vector data in unsteady flow fields.
                 Our algorithm extends a texture synthesis technique,
                 called Line Integral Convolution (LIC), by devising a
                 new convolution algorithm that uses a time-accurate
                 value scattering scheme to model the texture advection.
                 In addition, our algorithm maintains the coherence of
                 the flow animation by successively updating the
                 convolution results over time. Furthermore, we propose
                 a parallel UFLIC algorithm that can achieve high
                 load-balancing for multiprocessor computers with shared
                 memory architecture. We demonstrate the effectiveness
                 of our new algorithm by presenting image snapshots from
                 several CFD case studies.",
  note =         "Abstract:
                 http://www.computer.org/tvcg/tg1998/v0098abs.htm",
  keywords =     "Flow visualization, vector field visualization, image
                 convolution, line integral convolution, flow animation,
                 unsteady flows, texture synthesis, parallel algorithm",
  volume =       "4",
  copyright =    "IEEE",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1998-210,
  title =        "The vertex set of a 0/1-polytope is strongly
                 {P}-enumerable",
  language =     "en",
  month =        jun,
  number =       "2",
  pages =        "103--109",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-210",
  author =       "Michael R. Bussieck and Marco E. L{\"u}bbecke",
  abstract =     "In this paper, we discuss the computational complexity
                 of the following enumeration problem: given a rational
                 convex polyhedron P defined by a system of linear
                 inequalities, output each vertex of P . It is still an
                 open question whether there exists an algorithm for
                 listing all vertices in running time polynomial in the
                 input size and the output size. Informally speaking, a
                 linear running time in the output size leads to the
                 notion of P -enumerability introduced by Valiant
                 (1979). The concept of strong P -enumerability
                 additionally requires an output independent space
                 complexity of the respective algorithm. We give such an
                 algorithm for polytopes all of whose vertices are among
                 the vertices of a polytope combinatorially equivalent
                 to the hypercube. As a very important special case,
                 this class of polytopes contains all 0/1-polytopes. Our
                 implementation based on the commercial LP solver CPLEX
                 (CPLEX is a registered trademark of ILOG, Inc.) is
                 superior to general vertex enumeration algorithms. We
                 give an example how simplifications of our algorithm
                 lead to efficient enumeration of combinatorial
                 objects.",
  keywords =     "Vertex enumeration, Convex polyhedra, 0/1-polytopes,
                 Strong P-enumerability, Computational complexity",
  volume =       "11",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-211,
  title =        "Edge guards in rectilinear polygons",
  language =     "en",
  month =        jun,
  number =       "2",
  pages =        "111--123",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-211",
  author =       "Iliana Bjorling-Sachs",
  abstract =     "In this paper we consider the problem of placing edge
                 guards to supervise a rectilinear art gallery. We show
                 that no gallery with n vertices requires more than
                 $\lfloor(3n+4)/16\rfloor$ guards. Since this number of
                 edge guards is necessary for some galleries the bound
                 is tight.",
  keywords =     "Art galleries, Edge guards, Rectilinear polygons",
  volume =       "11",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-212,
  title =        "Erased arrangements of lines and convex decompositions
                 of polyhedra",
  language =     "en",
  month =        feb,
  number =       "3",
  pages =        "129--143",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-212",
  author =       "J. E. Hershberger and J. S. Snoeyink",
  abstract =     "In 1984, B. Chazelle proposed a notch-cutting
                 procedure for decomposing a non-convex polyhedron into
                 convex pieces. This paper shows that notch-cutting,
                 when applied to a polyhedron with n faces and r reflex
                 dihedral angles, gives a convex decomposition with
                 $\Theta(nr+r^{7/3})$ worst-case complexity. The upper
                 and lower bounds are obtained by studying the
                 complexity of the horizon of a segment in an
                 incrementally-constructed erased arrangement of n
                 lines. Efficient deterministic algorithms to compute
                 this decomposition are also described.",
  keywords =     "Computational geometry, Arrangements, Polyhedra,
                 Convex decomposition, Horizon theorems",
  volume =       "9",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-213,
  title =        "A note on parallel algorithms for optimal h-v drawings
                 of binary trees",
  language =     "en",
  month =        feb,
  number =       "3",
  pages =        "145--158",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-213",
  author =       "Panagiotis T. Metaxas and Grammati E. Pantziou and
                 Antonis Symvonis",
  abstract =     "In this paper we present a method to obtain optimal
                 h-v drawings in parallel. Based on parallel tree
                 contraction, our method computes optimal (with respect
                 to a class of cost functions of the enclosing
                 rectangle) drawings in $O(log^2 n)$ parallel time by
                 using a polynomial number of EREW processors. The
                 number of processors reduces substantially when we
                 study minimum area drawings. Our work places the
                 problem of obtaining optimal size h-v drawings in NC,
                 presenting the first algorithm with polylogarithmic
                 time complexity.",
  keywords =     "h-v drawing, Parallel algorithm, Parallel tree
                 contraction, Tree layout",
  volume =       "9",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-214,
  title =        "Unit disk graph recognition is {NP}-hard",
  language =     "en",
  month =        jan,
  number =       "1-2",
  pages =        "3--24",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-214",
  author =       "Heinz Breu and David G. Kirkpatrick",
  abstract =     "Unit disk graphs are the intersection graphs of unit
                 diameter closed disks in the plane. This paper gives a
                 polynomial-time reduction from SATISFIABILITY to the
                 problem of recognizing unit disk graphs. Equivalently,
                 it shows that determining if a graph has sphericity 2
                 or less, even if the graph is planar or is known to
                 have sphericity at most 3, is NP-hard. We show how this
                 reduction can be extended to 3 dimensions, thereby
                 showing that unit sphere graph recognition, or
                 determining if a graph has sphericity 3 or less, is
                 also NP-hard. We conjecture that K-sphericity is
                 NP-hard for all fixed K greater than 1.",
  keywords =     "Disk intersection graphs, Disk touching graphs,
                 NP-hard, Sphericity",
  volume =       "9",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-215,
  title =        "A better heuristic for orthogonal graph drawings",
  language =     "en",
  month =        feb,
  number =       "3",
  pages =        "159--180",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-215",
  author =       "Therese Biedl and Goos Kant",
  abstract =     "An orthogonal drawing of a graph is an embedding in
                 the plane such that all edges are drawn as sequences of
                 horizontal and vertical segments. Linear time and space
                 algorithms for drawing biconnected planar graphs
                 orthogonally with at most $2n+4$ bends on a grid of
                 size $n \times n$ are known in the literature. In this
                 paper we generalize this result to connected and
                 non-planar graphs. Moreover, we show that in almost all
                 cases each edge is bent at most twice",
  volume =       "9",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-216,
  title =        "Combinatorial complexity of translating a box in
                 polyhedral 3-space",
  language =     "en",
  month =        feb,
  number =       "3",
  pages =        "181--196",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-216",
  author =       "Dan Halperin and Chee-Keng Yap",
  abstract =     "We study the space of free translations of a box
                 amidst polyhedral obstacles with n vertices. We show
                 that the combinatorial complexity of this space is
                 $O(n^2 \alpha(n))$ , where $\alpha(n)$ is the inverse
                 Ackermann function. Our bound is within an $\alpha(n)$
                 factor off the lower bound, and it constitutes an
                 improvement of almost an order of magnitude over the
                 best previously known (and naive) bound for this
                 problem, $O(n^3)$. For the case of a convex polygon of
                 fixed (constant) size translating in the same setting
                 (namely, a two-dimensional polygon translating in
                 three-dimensional space), we show a tight bound $O(n^2
                 \alpha(n))$ on the complexity of the free space.",
  keywords =     "Computational geometry, Motion planning, Minkowski
                 sums",
  volume =       "9",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-217,
  title =        "Linear area upward drawings of {AVL} trees",
  language =     "en",
  month =        jan,
  number =       "1-2",
  pages =        "25--42",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-217",
  author =       "P. Crescenzi and P. Penna and A. Piperno",
  abstract =     "We prove that any AVL tree admits a linear-area
                 straight-line strictly-upward planar grid drawing, that
                 is, a drawing in which (a) each edge is mapped into a
                 single straight-line segment, (b) each node is placed
                 below its parent, (c) no two edges intersect, and (d)
                 each node is mapped into a point with integer
                 coordinates.",
  keywords =     "Graph drawing, Upward drawing, Area requirement",
  volume =       "9",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-218,
  title =        "On fat partitioning, fat covering and the union size
                 of polygons",
  language =     "en",
  month =        mar,
  number =       "4",
  pages =        "197--210",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-218",
  author =       "Marc van Kreveld",
  abstract =     "The complexity of the contour of the union of simple
                 polygons with n vertices in total can be $O(n^)$ in
                 general. A notion of fatness for simple polygons is
                 introduced that extends most of the existing fatness
                 definitions. It is proved that a set of fat polygons
                 with n vertices in total has union complexity $O(n \log
                 \log n)$, which is a generalization of a similar result
                 for fat triangles (Matou{\v{s}}ek et al., 1994).
                 Applications to several basic problems in computational
                 geometry are given, such as efficient hidden surface
                 removal, motion planning, injection molding, and more.
                 The result is based on a new method to partition a fat
                 simple polygon P with n vertices into $O(n)$ fat convex
                 quadrilaterals, and a method to cover (but not
                 partition) a fat convex quadrilateral with $O(1)$ fat
                 triangles. The maximum overlap of the triangles at any
                 point is two, which is optimal for any exact cover of a
                 fat simple polygon by a linear number of fat
                 triangles.",
  keywords =     "Fat polygons, Partitioning, Covering, Union of
                 polygons, Complexity of contour",
  volume =       "9",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-219,
  title =        "Experiments on the practical {I}/{O} efficiency of
                 geometric algorithms: Distribution sweep versus plane
                 sweep",
  language =     "en",
  month =        mar,
  number =       "4",
  pages =        "211--236",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-219",
  author =       "Yi-Jen Chiang",
  abstract =     "We present an extensive experimental study comparing
                 the performance of four algorithms for the following
                 orthogonal segment intersection problem: given a set of
                 horizontal and vertical line segments in the plane,
                 report all intersecting horizontal--vertical pairs. The
                 problem has important applications in VLSI layout and
                 graphics, which are large-scale in nature. The
                 algorithms under evaluation are our implementations of
                 distribution sweep and three variations of plane sweep.
                 Distribution sweep is specifically designed for the
                 situations in which the problem is too large to be
                 solved in internal memory, and theoretically has
                 optimal I/O cost. Plane sweep is a well-known and
                 powerful technique in computational geometry, and is
                 optimal for this particular problem in terms of
                 internal computation. The three variations of plane
                 sweep differ by the sorting methods (external versus
                 internal sorting) used in the preprocessing phase and
                 the dynamic data structures (B-tree versus
                 2--3--4-tree) used in the sweeping phase. We generate
                 the test data by three programs that use a random
                 number generator while producing some interesting
                 properties that are predicted by our theoretical
                 analysis. The sizes of the test data range from 250
                 thousand segments to 2.5 million segments. The
                 experiments provide detailed quantitative evaluation of
                 the performance of the four algorithms, and the
                 observed behavior of the algorithms is consistent with
                 their theoretical properties. This is, to the best of
                 our knowledge, the first experimental algorithmic study
                 comparing the practical performance between
                 external-memory algorithms and conventional algorithms
                 with large-scale test data.",
  keywords =     "Segment intersection, Plane sweep, Distribution sweep,
                 B-tree, External-memory algorithm, Implementation,
                 Experimentation, Probabilistic analysis",
  volume =       "9",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-22,
  title =        "Visualizing Nonlinear Vector Field Topology",
  language =     "en",
  month =        apr,
  number =       "2",
  pages =        "109--116",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-22",
  author =       "Gerik Scheuermann and Heinz Kr{\"u}ger and Martin
                 Menzel and Alyn P. Rockwood",
  abstract =     "We present our results on the visualization of
                 nonlinear vector field topology. The underlying
                 mathematics is done in Clifford algebra, a system
                 describing geometry by extending the usual vector space
                 by a multiplication of vectors. We started with the
                 observation that all known algorithms for vector field
                 topology are based on piecewise linear or bilinear
                 approximation, and that these methods destroy the local
                 topology if nonlinear behavior is present. Our
                 algorithm looks for such situations, chooses an
                 appropriate polynomial approximation in these areas,
                 and, finally, visualizes the topology. This overcomes
                 the problem, and the algorithm is still very fast
                 because we are using linear approximation outside these
                 small but important areas. The paper contains a
                 detailed description of the algorithm and a basic
                 introduction to Clifford algebra.",
  keywords =     "Vector field topology, Clifford algebra,
                 visualization",
  volume =       "4",
  copyright =    "IEEE",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1998-220,
  title =        "New results on drawing angle graphs",
  language =     "en",
  month =        jan,
  number =       "1-2",
  pages =        "43--82",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-220",
  author =       "Ashim Garg",
  abstract =     "An angle graph is a graph with a fixed cyclic order of
                 the edges around each vertex and an angle specified for
                 every pair of consecutive edges incident on a vertex.
                 We study the problem of constructing a drawing of an
                 angle graph that preserves its angles, and present
                 several new results. We disprove the conjectures of
                 Vijayan (1986) about the relationship between the
                 planarity of an angle graph and the planarity of its
                 biconnected components. We show that testing an angle
                 graph for planarity is NP-hard. Using our NP-hardness
                 result, we show that given a triconnected planar graph
                 and an angle $\alpha$, the problem of determining
                 whether it admits a planar straight-line drawing in
                 which the angle between any two consecutive edges
                 incident on the same vertex is at least $\alpha$, is
                 NP-hard. A multilayered angle graph is one whose edges
                 are assigned to a given set of layers. A multilayered
                 angle graph is multiplanar if it admits a drawing in
                 which edges assigned to the same layer do not cross. We
                 prove that testing a multilayered angle graph for
                 multiplanarity is NP-hard even if we restrict the
                 angles to be multiples of $90^o$ and the number of
                 layers to two. We give a simple linear time algorithm
                 for testing whether a series--parallel angle graph
                 admits a (nonplanar) drawing that preserves its
                 angles.",
  keywords =     "Graph drawing, Layout, Angle, Constraints, Resolution
                 area, Series--parallel graphs",
  volume =       "9",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-221,
  title =        "Recognizing polygonal parts from width measurements",
  language =     "en",
  month =        mar,
  number =       "4",
  pages =        "237--246",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-221",
  author =       "Esther M. Arkin and Martin Held and Joseph S. B.
                 Mitchell and Steven S. Skiena",
  abstract =     "Automatic recognition of parts is an important problem
                 in many industrial applications. One model of the
                 problem is: given a finite set of polygonal parts, use
                 a set of {"}width{"} measurements taken by a
                 parallel-jaw gripper to determine which part is
                 present. We study the problem of computing efficient
                 strategies ({"}grasp plans{"}), with the goal to
                 minimize the number of measurements necessary in the
                 worst case. We show that finding a minimum length grasp
                 plan is NP -hard, and give a polynomial time
                 approximation algorithm that is simple and produces a
                 solution that is within a log factor from optimal.",
  keywords =     "Robotics, Grasping, Object recognition, Decision tree,
                 Approximation algorithm",
  volume =       "9",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-222,
  title =        "Algorithms for area-efficient orthogonal drawings",
  language =     "en",
  month =        jan,
  number =       "1-2",
  pages =        "83--110",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-222",
  author =       "Achilleas Papakostas and Ioannis G. Tollis",
  abstract =     "An orthogonal drawing of a graph is a drawing such
                 that vertices are placed on grid points and edges are
                 drawn as sequences of vertical and horizontal segments.
                 In this paper we present linear time algorithms that
                 produce orthogonal drawings of graphs with n vertices.
                 If the maximum degree is four, then the drawing
                 produced by our first algorithm needs area at most
                 (roughly) $0.76^n2$, and introduces at most $2n+2$
                 bends. Also, each edge of such a drawing has at most
                 two bends. Our algorithm is based on forming and
                 placing pairs of vertices of the graph. If the maximum
                 degree is three, then the drawing produced by our
                 second algorithm needs at most (roughly) $(1/4)n^2$
                 area and, if the graph is biconnected, at most $\lfloor
                 n/2 \rfloor +3$ bends. These upper bounds match the
                 upper bounds known for planar graphs of maximum degree
                 3. This algorithm produces optimal drawings (within a
                 constant of 2) with respect to the number of bends,
                 since there is a lower bound of $n/2+1$ in the number
                 of bends for orthogonal drawings of maximum degree 3
                 graphs.",
  volume =       "9",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-223,
  title =        "On determining the congruence of point sets in d
                 dimensions",
  language =     "en",
  month =        mar,
  number =       "4",
  pages =        "247--256",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-223",
  author =       "Tatsuya Akutsu",
  abstract =     "This paper considers the following problem: given two
                 point sets A and B ($|A| = |B| = n$) in d dimensional
                 Euclidean space, determine whether or not A is
                 congruent to B . This paper presents an $O(n^{(d-1)/2}
                 \log n$) time randomized algorithm. The birthday
                 paradox, which is well-known in combinatorics, is used
                 effectively in this algorithm. Although this algorithm
                 is Monte-Carlo type (i.e., it may give a wrong result),
                 this improves a previous $O(n^{d-2} \log n)$ time
                 deterministic algorithm considerably. This paper also
                 shows that if d is not bounded, the problem is at least
                 as hard as the graph isomorphism problem in the sense
                 of the polynomiality. Several related results are
                 described too.",
  keywords =     "Pattern matching, Congruence, Birthday paradox,
                 Randomized algorithm",
  volume =       "9",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-224,
  title =        "Universal 3-dimensional visibility representation for
                 graphs",
  language =     "en",
  month =        jan,
  number =       "1-2",
  pages =        "111--125",
  year =         "1998",
  author =       "Helmut Alt and Michael Godau and Sue Whitesides",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-224",
  abstract =     "This paper studies 3-dimensional visibility
                 representations of graphs in which objects in 3-D
                 correspond to vertices and vertical visibilities
                 between these objects correspond to edges. We ask which
                 classes of simple objects are universal, i.e., powerful
                 enough to represent all graphs. In particular, we show
                 that there is no constant k for which the class of all
                 polygons having k or fewer sides is universal. However,
                 we show by construction that every graph on n vertices
                 can be represented by polygons each having at most $2n$
                 sides. The construction can be carried out by an
                 $O(n^2)$ algorithm. We also study the universality of
                 classes of simple objects (translates of a single, not
                 necessarily polygonal object) relative to cliques $K_n$
                 and similarly relative to complete bipartite graphs
                 $_{Kn,m}$.",
  keywords =     "Graph drawing, Visibility, Graph representation,
                 Geometric graph theory",
  volume =       "9",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-225,
  title =        "Converting triangulations to quadrangulations",
  language =     "en",
  month =        mar,
  number =       "4",
  pages =        "257--276",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-225",
  author =       "Suneeta Ramaswami and Pedro Ramos and Godfried
                 Toussaint",
  abstract =     "We study the problem of converting triangulated
                 domains to quadrangulations, under a variety of
                 constraints. We obtain a variety of characterizations
                 for when a triangulation (of some structure such as a
                 polygon, set of points, line segments or planar
                 subdivision) admits a quadrangulation without the use
                 of Steiner points, or with a bounded number of Steiner
                 points. We also investigate the effect of demanding
                 that the Steiner points be added in the interior or
                 exterior of a triangulated simple polygon and propose
                 efficient algorithms for accomplishing these tasks. For
                 example, we give a linear-time method that
                 quadrangulates a triangulated simple polygon with the
                 minimum number of outer Steiner points required for
                 that triangulation. We show that this minimum can be at
                 most $lfloor n/3 \rfloor$, and that there exist
                 polygons that require this many such Steiner points. We
                 also show that a triangulated simple n -gon may be
                 quadrangulated with at most $\lfloor n/4 \rfloor$
                 Steiner points inside the polygon and at most one
                 outside. This algorithm also allows us to obtain, in
                 linear time, quadrangulations from general triangulated
                 domains (such as triangulations of polygons with holes,
                 a set of points or line segments) with a bounded number
                 of Steiner points.",
  keywords =     "Matchings, Triangulations, Quadrangulations, Simple
                 polygons, Mesh-generation, Finite element methods,
                 Scattered data interpolation",
  volume =       "9",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@Article{EVL-1998-226,
  title =        "Combinatorial aspects of geometric graphs",
  language =     "en",
  month =        mar,
  number =       "4",
  pages =        "277--287",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-226",
  author =       "Shang-Hua Teng",
  abstract =     "As a special case of our main result, we show that for
                 all $L>0$, each k-nearest neighbor graph in d
                 dimensions excludes $K_h$ as a depth L minor if
                 $h=\Omega(L^d)$. More generally, we prove that the
                 overlap graphs defined by Miller, Teng, Thurston and
                 Vavasis (1993) have this combinatorial property. By a
                 construction of Plotkin, Rao and Smith (1994), our
                 result implies that overlap graphs have {"}good{"}
                 cut-covers, answering an open question of Kaklamanis,
                 Krizanc and Rao (1993). Consequently, overlap graphs
                 can be emulated on hypercube graphs with a constant
                 factor of slow-down and on butterfly graphs with a
                 factor of $O(log^{\*}n)$ slow-down. Therefore,
                 computations on overlap graphs, such as finite element
                 and finite difference methods on {"}well-conditioned{"}
                 meshes and image processing on k -nearest neighbor
                 graphs, can be performed on hypercubic parallel
                 machines with a linear speed-up. Our result, in
                 conjunction with a result of Plotkin, Rao and Smith,
                 also yields a combinatorial proof that overlap graphs
                 have separators of sublinear size. We also show that
                 with high probability, the Delaunay diagram, the
                 relative neighborhood graph, and the k -nearest
                 neighbor graph of a random point set exclude $K_h$ as a
                 depth L minor if $h=\Omega(L^{d/2}\log n)$.",
  volume =       "9",
  copyright =    "Copyright 1998, Elsevier Science, All rights
                 reserved.",
  journal =      "Computational Geometry. Theory and Applications",
}

@InProceedings{EVL-1998-227,
  pages =        "233--242",
  year =         "1998",
  title =        "Rendering Generalized Cylinders with Paintstrokes",
  author =       "Ivan Neulander and Michiel van de Panne",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-227",
  language =     "en",
  abstract =     "We present an efficient technique for dynamically
                 tessellating generalized cylinders. We make direct use
                 of the generalized cylinder's screen-space projection
                 in order to minimize the number of polygons required to
                 construct its image. Used in conjunction with our
                 A-Buffer polygon renderer, our technique strikes a good
                 balance between speed and image quality when used at
                 small to medium scales, generally surpassing other
                 methods for rendering generalized cylinders.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-228,
  pages =        "223--232",
  year =         "1998",
  title =        "Virtual navigation of complex scenes using clusters of
                 cylindrical panoramic images",
  author =       "Sing Bing Kang and Pavan Kumar Desikan",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-228",
  language =     "en",
  abstract =     "The traditional approach of generating novel virtual
                 views of an object or a scene is to render the
                 appropriate 3-D model. The alternative is to render
                 directly from the original images; this approach, which
                 is based on pixel interpolation or reprojection, is
                 called image-based rendering . In this paper, we
                 describe a technique that enables virtual navigation
                 within a complex environment using an image-based
                 rendering technique. In particular, we make use of
                 <em>clusters of cylindrical panoramic images. Each
                 cluster of panoramic images allows the user to smoothly
                 navigate within a particular area, say within a single
                 room. Having a collection of such interconnected
                 clusters would enable the user to seamlessly navigate
                 within a complex environment, such as an entire floor
                 of a building, with each cluster representing a room.
                 To achieve this goal, we examine a few techniques for
                 image-based rendering using a cluster of cylindrical
                 panoramic images to synthesize views from virtual
                 viewpoints. We also describe our method for enabling
                 smooth transition between clusters.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-229,
  pages =        "217--222",
  year =         "1998",
  title =        "Globally Optimal Image Mosaics",
  author =       "Kirk L. Duffin and William A. Barrett",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-229",
  language =     "en",
  abstract =     "In this paper we examine the simultaneous solution of
                 a set of image transformations with the goal of
                 creating a globally optimal image mosaic. We examine an
                 alternative parameterization of the full projective
                 matrix transformation that leads to elimination of
                 independent skew and aspect ratio parameters for each
                 image. We also create a scale-free distance error
                 metric which prevents the tendency of simultaneously
                 solved systems to tend toward the zero solution.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@Article{EVL-1998-23,
  title =        "Structure-Significant Representation of Structured
                 Datasets",
  language =     "en",
  month =        apr,
  number =       "2",
  pages =        "117--132",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-23",
  author =       "Raghu Machiraju and Zhifan Zhu and Bryan Fry and
                 Robert Moorhead",
  abstract =     "Numerical simulation of physical phenomena is now an
                 accepted way of scientific inquiry. However, the field
                 is still evolving, with a profusion of new solution and
                 grid-generation techniques being continuously proposed.
                 Concurrent and retrospective visualization are being
                 used to validate the results, compare them among
                 themselves and with experimental data, and browse
                 through large scientific databases. There exists a need
                 for representation schemes which allow access of
                 structures in an increasing order of smoothness (or
                 decreasing order of significance). We describe our
                 methods on datasets obtained from curvilinear grids.
                 Our target application required visualization of a
                 computational simulation performed on a very remote
                 supercomputer. Since no grid adaptation was performed,
                 it was not deemed necessary to simplify or compress the
                 grid. In essence, we treat the solution as if it were
                 in the computational domain. Inherent to the
                 identification of significant structures is determining
                 the location of the scale coherent structures and
                 assigning saliency values to them. Scale coherent
                 structures are obtained as a result of combining the
                 coefficients of a wavelet transform across scales. The
                 result of this operation is a correlation mask that
                 delineates regions containing significant structures. A
                 spatial subdivision (e.g., octree) is used to delineate
                 regions of interest. The mask values in these
                 subdivided regions are used as a measure of information
                 content. Later, another wavelet transform is conducted
                 within each subdivided region and the coefficients are
                 sorted based on a perceptual function with bandpass
                 characteristics. This allows for ranking of structures
                 based on the order of significance, giving rise to an
                 adaptive and embedded representation scheme. We
                 demonstrate our methods on two datasets from
                 computational field simulations. Essentially, we show
                 how our methods allow the ranked access of significant
                 structures. We also compare our adaptive representation
                 scheme with a fixed blocksize scheme.",
  keywords =     "wavelet transform, structure detection, human visual
                 system, progressive transmission",
  volume =       "4",
  copyright =    "IEEE",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@InProceedings{EVL-1998-230,
  pages =        "209--216",
  year =         "1998",
  title =        "Edge Enhancement Issues in Half-Toning",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-230",
  author =       "John. W. Buchanan and Lisa M. Streit and Oleg
                 Veryovka",
  language =     "en",
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-231,
  pages =        "201--208",
  year =         "1998",
  title =        "Layered Deformable Models with Implicit Surfaces",
  author =       "Marie-Paule Cani-Gascuel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-231",
  language =     "en",
  abstract =     "Several challenging problems in the animation of
                 deformable objects can be solved by combining existing
                 models with implicit surfaces. The latter are used as
                 an extra layer which coats any kind of structure that
                 moves and deforms over time. Implicit surfaces detect
                 collisions, model local deformations, and transmit
                 response forces to the inner structures. They also
                 control the variations of the object's volume. We
                 present several applications from the animation of
                 organic shapes to the simulation of soft inelastic
                 substances that can separate into smaller pieces or
                 fuse into larger ones.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-232,
  pages =        "193--200",
  year =         "1998",
  title =        "Multi-Granularity Noise for Curvilinear Grid {LIC}",
  author =       "Xiaoyang Mao and Lichan Hong and Arie Kaufman and
                 Noboru Fujita and Makoto Kikukawa",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-232",
  language =     "en",
  abstract =     "A major problem of the existing curvilinear grid Line
                 Integral Convolution (LIC) algorithm is that the
                 resulting LIC textures may be distorted after being
                 mapped onto the parametric surfaces, since a
                 curvilinear grid usually consists of cells of different
                 sizes. This paper proposes a way for solving the
                 problem through using multi-granularity noise as the
                 input image for LIC. A stochastic sampling technique
                 called Poisson ellipse sampling is employed to resample
                 the computational space of a curvilinear grid into a
                 set of randomly distributed points. From this set of
                 points, we are able to reconstruct a noise image with
                 its local noise granularity being adapted to the
                 physical space cell size of the grid.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-233,
  pages =        "185--192",
  year =         "1998",
  title =        "Pain and Fatigue in Desktop {VR}: Initial Results",
  author =       "Christopher D. Shaw",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-233",
  language =     "en",
  abstract =     "This paper describes a comprehensive experimental
                 evaluation of a two-handed free-form surface editor
                 called THRED, which uses a pair of Polhemus 3D trackers
                 with added buttons in a complementary two-handed style.
                 On top of the underlying free-form surface editor
                 application was built two other user interfaces that
                 provide reasonable competition for the two-handed
                 style. The second interface uses one button-enhanced 3D
                 tracker in the dominant hand, with the non-dominant
                 hand selecting commands from the keyboard. The third
                 style is a mouse-based interface that is a simplified
                 clone of the Alias modeler. This user study evaluates
                 these interfaces in terms of pain and fatigue. The
                 results show that experienced minimal pain and fatigue
                 with THRED, an a par with that experienced in the
                 mouse-based interface, but there was statistically
                 significant fatigue in the use of the One-Handed
                 interface. The pain and fatigue surveys clearly
                 indicate that THRED and the Mouse-Based interface yield
                 low discomfort, which contradicts the established
                 wisdom that bat-based interfaces are likely to be
                 painful or fatiguing to use.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-234,
  pages =        "177--184",
  year =         "1998",
  title =        "On the Use of Perceptual Cues and Data Mining for
                 Effective Visualization of Scientific Datasets",
  author =       "Christopher G. Healey",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-234",
  language =     "en",
  abstract =     "Scientific datasets are often difficult to analyse or
                 visualize, due to their large size and high
                 dimensionality. We propose a two-step approach to
                 address this problem. We begin by using data mining
                 algorithms to identify areas of interest within the
                 dataset. This allows us to reduce a dataset's size and
                 dimensionality, and to estimate missing values or
                 correct erroneous entries. We display the results of
                 the data mining step using visualization techniques
                 based on perceptual cues. Our visualization tools are
                 designed to exploit the power of the low-level human
                 visual system. The result is a set of displays that
                 allow users to perform rapid and accurate exploratory
                 data analysis. In order to demonstrate our techniques,
                 we visualized an environmental dataset being used to
                 model salmon growth and migration patterns. Data mining
                 was used to identify significant attributes and to
                 provide accurate estimates of plankton density. We used
                 colour and texture to visualize the significant
                 attributes and estimated plankton densities for each
                 month for the years 1956 to 1964. Experiments run in
                 our laboratory showed that the colours and textures we
                 chose support rapid and accurate element
                 identification, boundary detection, region tracking,
                 and estimation. The result is a visualization tool that
                 allows users to quickly locate specific plankton
                 densities and the boundaries they form. Users can
                 compare plankton densities to other environmental
                 conditions like sea surface temperature and current
                 strength. Finally, users can track changes in any of
                 the dataset's attributes on a monthly or yearly
                 basis.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-235,
  pages =        "168--176",
  year =         "1998",
  title =        "A Probabilistic Character Layout Strategy for Mobile
                 Text Entry",
  author =       "Tom Bellman and I. Scott MacKenzie",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-235",
  language =     "en",
  abstract =     "We present a text entry strategy designed for small,
                 input-limited devices in mobile situations. In an
                 existing text entry interaction style, arrow keys move
                 a cursor around the character set, displayed on a 3 or
                 4 line liquid crystal display, and a select key outputs
                 the highlighted character to the display. The
                 Fluctuating Optimal Character Layout (FOCL) strategy
                 aims to improve entry rates with this interaction
                 technique. After each character c entered, the layout
                 is rearranged so that the most likely next characters
                 are closer to the cursor. Each new layout is optimal
                 with respect to c, given digram (letter-pair)
                 probabilities in common English. FOCL significantly
                 reduces kspc--the average number of keystrokes per
                 character--, a non-empirical measure that partly
                 accounts for text entry speed. However, it also
                 requires the user to locate each character in an
                 unfamiliar layout, thus adding visual search time to
                 the task. In a longitudinal experiment comparing the
                 fluctuating layout approach to a fixed QWERTY layout,
                 we found no significant difference in entry speeds. We
                 discuss our design rationale and various modifications
                 to the design that may yield a performance
                 improvement.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-236,
  pages =        "161--167",
  year =         "1998",
  title =        "Cognitive Modeling in Human-Computer Interaction",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-236",
  author =       "Bonnie John",
  language =     "en",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-237,
  pages =        "151--160",
  year =         "1998",
  title =        "Footprint--based Quadruped Motion Synthesis",
  author =       "Nick Torkos and Michiel van de Panne",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-237",
  language =     "en",
  abstract =     "This paper applies trajectory-based optimization
                 techniques to the synthesis of quadruped motions. The
                 animator specifies hard constraints, consisting of
                 footprint locations and their timings, and soft
                 constraints that encode both physically-plausible
                 behavior and the notion of comfortable positions. By
                 dealing first and foremost with the spline trajectories
                 representing the gross motion of the quadruped, the
                 resulting optimization problem can be solved
                 efficiently and robustly. Results include walking,
                 jumping, and galloping quadrupeds.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-238,
  pages =        "143--150",
  year =         "1998",
  title =        "Active Implicit Surface for Animation",
  author =       "Mathieu Desbrun and Marie-Paule Cani-Gascuel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-238",
  language =     "en",
  abstract =     "This paper introduces a new model of deformable
                 surfaces designed for animation, which we call active
                 implicit surfaces. The underlying idea is to animate a
                 potential field defined by discrete values stored in a
                 grid, rather than directly animating a surface. This
                 surface, defined as an iso-potential of the field, has
                 the ability to follow a given object using a snake-like
                 strategy. Surface tension and other characteristics
                 such as constant surface area or constant volume may be
                 added to this model. The implicit formulation allows
                 the surface to easily experience topology changes
                 during simulation. We present an optimized
                 implementation: computations are restricted to a close
                 neighborhood around the surface. Applications range
                 from the coating of deformable materials simulated by
                 particle systems (the surface hides the granularity of
                 the underlying model) to the generation of
                 metamorphosis between shapes that may not have the same
                 topology.",
  booktitle =    "Graphics Interfac",
}

@InProceedings{EVL-1998-239,
  pages =        "133--142",
  year =         "1998",
  title =        "Simulating the Flow of Liquid Droplets",
  author =       "Patrick Fournier and Arash Habibi and Pierre Poulin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-239",
  language =     "en",
  abstract =     "The ever-changing nature of liquids makes them very
                 difficult to model and animate. This paper addresses
                 the simulation of one aspect of liquids, i.e. droplets
                 running down surfaces. We present a model oriented
                 towards a visually-satisfying simulation and
                 efficiency. The efficiency results from the separation
                 between the shape and the motion of a droplet. The
                 motion accounts for all changes encountered along the
                 path followed over a mesh of triangles. It is affected
                 by various properties modeled as friction, adhesion,
                 roughness, and collisions between droplets. Streaks are
                 also added along the paths. We characterize the shape
                 of a droplet by a small set of properties, such as
                 volume conservation, surface tension, etc. We model
                 them as constraints to satisfy. The shape model is
                 mainly based on mass-springs. It is simple and
                 efficient, and it guarantees that whatever the values
                 of the unconstrained parameters, all produced shapes
                 satisfy the characteristic properties, and thus, can
                 represent different types of droplets. Rendered
                 animations of various liquids illustrate the resulting
                 simulation.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@Article{EVL-1998-24,
  title =        "Topology Simplification for Polygonal Virtual
                 Environments",
  language =     "en",
  month =        apr,
  number =       "2",
  pages =        "133--144",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-24",
  author =       "Jihad El-Sana and Amitabh Varshney",
  abstract =     "We present a topology simplifying approach that can be
                 used for genus reductions, removal of protuberances,
                 and repair of cracks in polygonal models in a unified
                 framework. Our work is complementary to the existing
                 work on geometry simplification of polygonal datasets
                 and we demonstrate that using topology and geometry
                 simplifications together yields superior
                 multiresolution hierarchies than is possible by using
                 either of them alone. Our approach can also address the
                 important issue of repair of cracks in polygonal
                 models, as well as for rapid identification and removal
                 of protuberances based on internal accessibility in
                 polygonal models. Our approach is based on identifying
                 holes and cracks by extending the concept of a-shapes
                 to polygonal meshes under the Loo distance metric. We
                 then generate valid triangulations to fill them using
                 the intuitive notion of sweeping an Loo cube over the
                 identified regions.",
  keywords =     "surface, solid, and object representations,
                 hierarchical approximation, model simplification,
                 levels-of-detail generation, shape approximation,
                 geometric modeling, topology simplification, CAD model
                 repair",
  volume =       "4",
  copyright =    "IEEE",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@InProceedings{EVL-1998-240,
  pages =        "125--132",
  year =         "1998",
  title =        "Animating Sand, Mud, and Snow",
  author =       "Robert Sumner and James O'Brien and Jessica Hodgins",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-240",
  language =     "en",
  abstract =     "Computer animations often lack the subtle
                 environmental changes that should occur due to the
                 actions of the characters. Squealing car tires usually
                 leave no skid marks, airplanes rarely leave jet trails
                 in the sky, and most runners leave no footprints. In
                 this paper, we describe a simulation model of ground
                 surfaces that can be deformed by the impact of rigid
                 body models of animated characters. To demonstrate the
                 algorithms, we show footprints made by a runner in
                 sand, mud, and snow as well as bicycle tire tracks, a
                 bicycle crash, and a falling runner. The shapes of the
                 footprints in the three surfaces are quite different,
                 but the effects were controlled through only five
                 essentially independent parameters. To assess the
                 realism of the resulting motion, we compare the
                 simulated footprints to video footage of human
                 footprints in sand.",
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-241,
  pages =        "117--124",
  year =         "1998",
  title =        "A Graph Based Interface for Representing Volume
                 Visualization Results",
  author =       "James Patten and Kwan-Liu Ma",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-241",
  language =     "en",
  abstract =     "This paper discusses a graph based user interface for
                 representing the results of the volume visualization
                 process. As images are rendered, they are connected to
                 other images in a graph based on their rendering
                 parameters. The user can take advantage of the
                 information in this graph to understand how certain
                 rendering parameter changes affect a dataset, making
                 the visualization process more efficient. Because the
                 graph contains more information than is contained in an
                 unstructured history of images, the image graph is also
                 helpful for collaborative visualization and
                 animation.",
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-242,
  pages =        "107--116",
  year =         "1998",
  title =        "Wavelet-Based 3{D} Compression Scheme for Very Large
                 Volume Data",
  author =       "Insung Ihm and Sanghun Park",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-242",
  language =     "en",
  abstract =     "Visualizing very large volume data has been recognized
                 as a task requiring great effort in a variety of
                 science and engineering fields. In particular, such
                 data usually places considerable demands on run-time
                 memory space. This paper describes an effective 3D
                 compression scheme for very large volume data that
                 exploits the power of wavelet theory. In designing our
                 compression method, we have compromised between two
                 important factors: high compression ratio and fast
                 run-time random access. Our experimental results on the
                 Visual Human data sets show that our method achieves
                 fairly good compression ratios. In addition, it
                 minimizes the overhead caused during run-time
                 reconstruction of voxel values. This 3D compression
                 scheme will be useful in developing many interactive
                 visualization systems for huge volume data, and will
                 make visualization technology accessible to a much
                 wider range of users, as it can be based on personal
                 computers or low-end workstations with limited
                 memory.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-243,
  pages =        "99--106",
  year =         "1998",
  title =        "Interactive Volume Cutting",
  author =       "Kevin Chun-Ho Wong and Yu-Hang Siu Siu and Pheng-Ann
                 Heng and Hanqiu Sun",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-243",
  language =     "en",
  abstract =     "In 2D, Intelligent Scissors is an efficient
                 interactive tool for image segmentation. By interactive
                 use of a dynamic-programming graph-searching algorithm,
                 a region of interest in the image can be accurately
                 obtained. In this paper, we introduce the use of
                 Intelligent Scissors for contour detection on a
                 volumetric data surface. It is fast enough to be used
                 in an interactive virtual environment, in which the
                 user can intuitively select the contours in volumetric
                 data in an accurate and robust manner. Moreover, we
                 extend our work to the volume data manipulation,
                 cutting off the interesting part of the volume by
                 providing a contour on its surface. The cutting surface
                 is computed by a fast dynamic programming algorithm. By
                 using this tool, many new volumetric data models can be
                 created from an existing one in an effective way.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-244,
  pages =        "92--98",
  year =         "1998",
  title =        "Perception and Data Visualization: The Foundations of
                 Experimental Semiotics",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-244",
  author =       "Colin Ware",
  language =     "en",
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-245,
  pages =        "82--91",
  year =         "1998",
  title =        "Clonal Mosaic Model for the Synthesis of Mammalian
                 Coat Patterns",
  author =       "Marcelo Walter and Alain Fournier and Mark Reimers",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-245",
  language =     "en",
  abstract =     "We introduce the Clonal Mosaic model for the synthesis
                 of mammalian coat patterns, present its implementation
                 for modeling and display purposes, and give a few
                 examples of generated patterns. The model is based on
                 cell division and cell-to-cell interactions, and it can
                 generate repeating spotted and striped patterns
                 occurring in several species of mammals, especially the
                 big cats and giraffes. From a biological perspective,
                 the model has a strong appeal in light of recent
                 experimental evidence on pigment cells and other
                 pigment related mechanisms; from a computer graphics
                 perspective, the model can not only deliver many
                 patterns which are visually similar to real patterns
                 and can be used as textures, but it is also amenable to
                 simulation on arbitrary surfaces.",
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-246,
  pages =        "73--81",
  year =         "1998",
  title =        "Software Tools for Craniofacial Growth and
                 Reconstruction",
  author =       "Katrina Archer and Kevin Coughlan and David Forsey and
                 Sonja Struben",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-246",
  language =     "en",
  abstract =     "Forensic artists use tissue thicknesses at well known
                 landmarks on the craniofacial skeleton to generate a
                 potential likeness of an individual from a skull. This
                 is a very subjective and time consuming process. We
                 present two prototype software packages: one for
                 simulating the growth of a craniofacial skeleton either
                 forwards or backwards in time, and the other for
                 generating a facial reconstruction over a skeleton. The
                 craniofacial growth model uses three-dimensional data
                 from specific landmarks through time to drivethe growth
                 process. Tissue depth markers on the craniofacial model
                 determine the interpolated shape of the facial
                 reconstruction and can be altered to cover the range of
                 potential likenesses. Examples of a facial
                 reconstruction and a grown craniofacial skeleton are
                 shown.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-247,
  pages =        "65--72",
  year =         "1998",
  title =        "On Approximating Rough Curves with Fractal Functions",
  author =       "Wayne O. Cochran and John C. Hart and Patrick J.
                 Flynn",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-247",
  language =     "en",
  abstract =     "Fractal functions are explored as a representation for
                 rough data in computer graphics. Two new techniques for
                 using fractal interpolation functions to approximate
                 rough functions and curves are introduced. The first is
                 based on a Hough transform of fractal function
                 transformation parameters. The second is based on
                 previous techniques in fractal image compression. These
                 techniques are then demonstrated on the task of
                 recovering the parameters of a fractal function,
                 approximating a rough function and approximating the
                 boundary of a leaf.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-248,
  pages =        "57--64",
  year =         "1998",
  title =        "Multiresolution Surface Reconstruction for
                 Hierarchical {B}-splines",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-248",
  author =       "David Forsey and David Wong",
  language =     "en",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-249,
  pages =        "51--56",
  year =         "1998",
  title =        "The \alpha--\gamma--\delta's of Digital Media
                 Convergence",
  author =       "Alvy Ray Smith",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-249",
  language =     "en",
  abstract =     "There is no theoretical roadblock obstructing the
                 integration of different media types into a single
                 digital medium - after all, bits are bits - but there
                 are several real problems hindering the so-called
                 digital convergence. The alpha problem is that between
                 premultiplied and non-premultiplied alpha. The gamma
                 problem concerns the nonlinearity that many of today's
                 applications insist on burning into their image data.
                 The delta problem is about the integration of the
                 discrete and the continuous - eg, samples (pixels) and
                 geometry. The subtleties of these are explored - eg,
                 {"}square pixels{"} and non-rectangular images - and a
                 current example of how wrong things can get - the US
                 digital television transmission formats battle - is
                 elaborated.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@Article{EVL-1998-25,
  title =        "Constructing Hierarchies for Triangle Meshes",
  language =     "en",
  month =        apr,
  number =       "2",
  pages =        "145--161",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-25",
  author =       "Tran S. Gieng and Bernd Hamann and Kenneth I. Joy and
                 Gregory L. Schussman and Issac J. Trotts",
  abstract =     "We present a method to produce a hierarchy of triangle
                 meshes that can be used to blend different levels of
                 detail in a smooth fashion. The algorithm produces a
                 sequence of meshes $M_0$, $M_1$, $M_2$, $\ldots$,
                 $M_{\mbox{n}}$, where each mesh $M_{\mbox{i}}$ can be
                 transformed to mesh $M_{\mbox{i}+1}$ through a set of
                 triangle-collapse operations. For each triangle, a
                 function is generated that approximates the underlying
                 surface in the area of the triangle, and this function
                 serves as a basis for assigning a weight to the
                 triangle in the ordering operation and for supplying
                 the points to which the triangles are collapsed. The
                 algorithm produces a limited number of intermediate
                 meshes by selecting, at each step, a number of
                 triangles that can be collapsed simultaneously. This
                 technique allows us to view a triangulated surface
                 model at varying levels of detail while insuring that
                 the simplified mesh approximates the original surface
                 well.",
  keywords =     "Mesh simplification, triangle meshes, level-of-detail
                 representation, shape approximation, multiresolution",
  volume =       "4",
  copyright =    "IEEE",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@InProceedings{EVL-1998-250,
  pages =        "43--50",
  year =         "1998",
  title =        "A General Framework for Mesh Decimation",
  author =       "Leif Kobbelt and Swen Campagna and Hans-Peter Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-250",
  language =     "en",
  abstract =     "The decimation of highly detailed meshes has emerged
                 as an important issue in many computer graphics related
                 fields. A whole library of different algorithms has
                 been proposed in the literature. By carefully
                 investigating such algorithms, we can derive a generic
                 structure for mesh reduction schemes which is analogous
                 to a class of greedy-algorithms for heuristic
                 optimization. Particular instances of this algorithmic
                 template allow to adapt to specific target
                 applications. We present a new mesh reduction algorithm
                 which clearly reflects this meta scheme and efficiently
                 generates decimated high quality meshes while observing
                 global error bounds.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-251,
  pages =        "35--42",
  year =         "1998",
  title =        "An Improved Parametric Side--Vertex Triangle Mesh
                 Interpolant",
  author =       "Stephen Mann",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-251",
  language =     "en",
  abstract =     "There are many schemes for fitting triangular surface
                 patches to a triangular net of data. In general, local
                 schemes produce surfaces with poor surface quality.
                 Although variational techniques construct surfaces of
                 higher quality, such techniques tend to be
                 computationally expensive. In this paper, I will
                 present modifications to Nielson's side-vertex method
                 that improve its surface quality without resorting to
                 variational techniques.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-252,
  pages =        "26--34",
  year =         "1998",
  title =        "Triangle Mesh Compression",
  author =       "Costa Touma and Craig Gotsman",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-252",
  language =     "en",
  abstract =     "A novel algorithm for the encoding of orientable
                 manifold triangle mesh geometry is presented. Mesh
                 connectivity is encoded in a lossless manner. Vertex
                 coordinate data is uniformly quantized and then
                 losslessly encoded. The compression ratios achieved by
                 the algorithm are shown to be significantly better than
                 those of currently available algorithms, for both
                 connectivity and coordinate data. Use of our algorithm
                 may lead to significant reduction of bandwidth required
                 for the transmission of VRML files over the Internet.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-253,
  pages =        "17--25",
  year =         "1998",
  title =        "Lighting Networks --- {A} New Approach for Designing
                 Lighting Algorithms",
  author =       "Philipp Slusallek and Marc Stamminger and Hans-Peter
                 Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-253",
  language =     "en",
  abstract =     "In the past, new global illumination algorithms have
                 usually been designed as a single module that was
                 responsible for the simulation of all aspects of
                 illumination in a scene. A recently developed
                 alternative is the design of small and specialized
                 algorithms (lighting operators) together with an
                 infrastructure for creating more complex algorithms by
                 connecting these building blocksi - the Lighting
                 Network. In this paper, we discuss the benefits of the
                 Lighting Network approach for designing new and
                 improved global illumination algorithms. Lighting
                 Networks not only provide a flexible infrastructure for
                 new algorithms, they also support a better theoretic
                 understanding of the lighting simulation process. We
                 show that a small number of global light propagation
                 operators already provides the basis for creating many
                 of todays illumination algorithms. Their illumination
                 results are converted into more suitable
                 representations by purely local conversion operators
                 that are specific to an illumination algorithm. Varying
                 the composition of these operators and introducing new
                 elements allows us to create and explore the benefits
                 of new simulation algorithms. We demonstrate the
                 potential of Lighting Networks with several examples,
                 implementing a diverse set of algorithms, such as
                 density estimation, irradiance gradients, and a
                 composite lighting simulation.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-254,
  pages =        "1--7",
  year =         "1998",
  title =        "Visibility Streaming for Network-based Walkthroughs",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-254",
  author =       "Daniel Cohen-Or and Eyal Zadicario",
  language =     "en",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-255,
  pages =        "8--16",
  year =         "1998",
  title =        "Ray-Tracing Procedural Displacement Shaders",
  author =       "Wolfgang Heidrich and Hans-Peter Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-255",
  language =     "en",
  abstract =     "Displacement maps and procedural displacement shaders
                 are a widely used approach of specifying geometric
                 detail and increasing the visual complexity of a scene.
                 While it is relatively straightforward to handle
                 displacement shaders in pipeline based rendering
                 systems such as the Reyes-architecture, it is much
                 harder to efficiently integrate displacement-mapped
                 surfaces in ray-tracers. Many commercial ray-tracers
                 tessellate the surface into a multitude of small
                 triangles. This introduces a series of problems such as
                 excessive memory consumption and possibly undetected
                 surface detail. In this paper we describe a novel way
                 of ray-tracing procedural displacement shaders
                 directly, that is, without introducing intermediate
                 geometry. Affine arithmetic is used to compute bounding
                 boxes for the shader over any range in the parameter
                 domain. The method is comparable to the direct
                 ray-tracing of B{\'e}zier surfaces and implicit
                 surfaces using B{\'e}zier clipping and interval
                 methods, respectively.",
  month =        jun,
  booktitle =    "Graphics Interface",
}

@InProceedings{EVL-1998-256,
  pages =        "200--205",
  year =         "1998",
  title =        "Point-sets with few $k$-sets",
  author =       "Helmut Alt and Stefan Felsner and Ferran Hurtado and
                 Marc Noy",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-256",
  language =     "en",
  abstract =     "A k-set of a finite set S of points in the plane is a
                 subset of cardinality k that can be separated from the
                 rest by a straight line. The question of how many
                 k-sets a set of n points can contain is a longstanding
                 open problem where a lower bound of $\Omega (n log k)$
                 and an upper bound of $O(n k^{1/3}) are k nown today.
                 Under certain restrictions on the set S, for example,
                 if all points lie on a c onvex curve, a linear upper
                 bound can be shown. Here, we will generalize this
                 observation by showing that if the points of S lie on a
                 constant number of con vex curves, the number of k-sets
                 remains linear in n.",
  keywords =     "convex curve, k-set, Lov{\'a}sz' procedure",
  booktitle =    "Proceedings ACM Symposium on Computational Geometry
                 1998",
}

@Article{EVL-1998-257,
  pages =        "89--103",
  year =         "1998",
  title =        "Matching convex shapes with respect to the symmetric
                 difference",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-257",
  author =       "Helmut Alt and Ulrich Fuchs and G{\"u}nter Rote and
                 Gerald Weber",
  language =     "en",
  volume =       "21",
  journal =      "Algorithmica",
}

@Article{EVL-1998-258,
  pages =        "33--38",
  year =         "1998",
  title =        "Exact Primitives for Smallest Enclosing Ellipses",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-258",
  author =       "Bernd G{\"a}rtner and Sven Sch{\"o}nherr",
  language =     "en",
  abstract =     "The problem of finding the unique closed ellipsoid of
                 smallest volume enclosing an $n$-point set $P$ in
                 $d$-space (known as the L{\"o}wner-John ellipsoid of
                 $P$) is an instance of convex programming and can be
                 solved by general methods in time $O(n)$ if the
                 dimension is fixe. The problem-specific parts of these
                 methods are encapsulated in primitive operations that
                 deal with subproblems of constant size. We derive
                 explicit formulae for the primitive operations of
                 Welzl's randomized method in dimension $d=2$. Compared
                 to previous ones, these formulae are simpler and faster
                 to evaluate, and they only contain rational
                 expressions, allowing for an exact solution.",
  volume =       "68",
  number =       "1",
  journal =      "Information Processing Letters",
}

@InProceedings{EVL-1998-259,
  pages =        "337--346",
  year =         "1998",
  title =        "Point Set Labeling with Sliding Labels",
  author =       "Marc van Kreveld and Tycho Strijk and Alexander
                 Wolff",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-259",
  language =     "en",
  abstract =     "This paper discusses algorithms for labeling sets of
                 points in the plane, where labels are not restricted to
                 some finite number of positions. We show that
                 continuously sliding labels allows more points to be
                 labeled both in theory and in practice. We define six
                 different models of labeling, and analyze how much
                 better---more points get a label---one model can be
                 than another. Maximizing the number of labeled points
                 is NP-hard, but we show that all models have a
                 polynomial-time approximation scheme, and all models
                 have a simple and efficient factor-$\frac{1}{2}$
                 approximation algorithm. Finally, we give experimental
                 results based on the factor-$\frac{1}{2}$ approximation
                 algorithm to compare the models in practice.",
  month =        jun,
  booktitle =    "Proceedings 14th Annual ACM Symposium Computational
                 Geometry",
}

@Article{EVL-1998-26,
  title =        "{RSVP}: {A} Geometric Toolkit for Controlled Repair of
                 Solid Models",
  language =     "en",
  month =        apr,
  number =       "2",
  pages =        "162--177",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-26",
  author =       "Gill Barequet and Christian A. Duncan and Subodh
                 Kumar",
  abstract =     "This paper presents a system and the associated
                 algorithms for repairing the boundary representation of
                 CAD models. Two types of errors are considered:
                 topological errors, i.e., aggregate errors, like
                 zero-volume parts, duplicate or missing parts,
                 inconsistent surface orientation, etc., and geometric
                 errors, i.e., numerical imprecision errors, like cracks
                 or overlaps of geometry. The output of our system
                 describes a set of clean and consistent two-manifolds
                 (possibly with boundaries) with derived adjacencies.
                 Such solid representation enables the application of a
                 variety of rendering and analysis algorithms, e.g.,
                 finite-element analysis, radiosity computation, model
                 simplification, and solid free-form fabrication. The
                 algorithms described here were originally designed to
                 correct errors in polygonal B-Reps. We also present an
                 extension for spline surfaces. Central to our system is
                 a procedure for inferring local adjacencies of edges.
                 The geometric representation of topologically-adjacent
                 edges are merged to evolve a set of two-manifolds.
                 Aggregate errors are discovered during the merging
                 step. Unfortunately, there are many ambiguous
                 situations where errors admit more than one valid
                 solution. Our system proposes an object-repairing
                 process based on a set of user-tunable heuristics. The
                 system also allows the user to override the algorithm's
                 decisions in a repair-visualization step. In essence,
                 this visualization step presents an organized and
                 intuitive way for the user to explore the space of
                 valid solutions and to select the correct one.",
  keywords =     "Model repair, edge matching",
  volume =       "4",
  copyright =    "IEEE",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@TechReport{EVL-1998-261,
  year =         "1998",
  title =        "A Picture-based Document Retrieval Service for the
                 Electronic Visualization Library",
  author =       "Ingmar Decker",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-261",
  language =     "en",
  abstract =     "This report describes the development of an
                 experimental service for image-based document retrieval
                 for the Electronic Visualization Library (EVlib). The
                 EVlib is a digital library for scientific
                 visualization, established at the Konrad-Zuse-Zentrum
                 f{\"u}r Informationstechnik Berlin (ZIB). The
                 image-based retrieval service allows users to look for
                 documents by describing the images they contain. This
                 query method was developed based on the assumption that
                 (1) images often represent relevant parts of the
                 contents of a document, and (2) images are often
                 remembered well. An image-based approach provides a new
                 quality of accessing and exploring scientific
                 literature. Motivation, concepts and realization of our
                 service are outlined. Results of a user test are
                 presented, too. The results indicate that this service
                 can be used for searching and browsing the document
                 collection in principle. On the other hand, problems
                 were detected which can give fruitful hints for future
                 work concerning document and image retrieval.",
  month =        dec,
  keywords =     "visual information retrieval, document retrieval,
                 content-based image retrieval, image indexing, image
                 database, digital library, picture memory,
                 visualization, user interface",
  number =       "TR 98-07",
  institution =  "Konrad-Zuse-Zentrum f{\"u}r Informationstechnik
                 Berlin",
}

@InProceedings{EVL-1998-262,
  pages =        "25--28",
  year =         "1998",
  title =        "Boundary Surface Shrinking - a Continuous Approach to
                 3{D} Center Line Extraction",
  author =       "Hartmut Schirmacher and Malte Z{\"o}ckler and Detlev
                 Stalling and Hans-Christian Hege",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-262",
  language =     "en",
  abstract =     "We present a new algorithm for efficient and robust
                 approximation of skeletons and construction of
                 connected center line graphs from 3D image data. The
                 algorithm is based on the idea of shrinking the
                 boundary surface along the gradients of the object's
                 distance map. After transforming the surface in that
                 way, duplicate vertices and line segments are
                 eliminated. If the skeleton contains no medial surface,
                 a graph representation from the centerline can directly
                 be extracted from the remaining edges. The algorithm
                 has proved to perform well on several different
                 datasets.",
  month =        jul,
  editor =       "Bernd Girod and Heinrich Niemann and Hans-Peter
                 Seidel",
  booktitle =    "Proceedings of IMDSP'98",
}

@InProceedings{EVL-1998-263,
  pages =        "137--142",
  year =         "1998",
  title =        "Segmentation of 3{D} Medical Images with Subvoxel
                 Accuracy",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-263",
  author =       "D. Stalling and M. Z{\"o}ckler and H.-C. Hege",
  language =     "en",
  abstract =     "Segmentation tools in medical imaging are either based
                 on editing geometric curves or on the assignment of
                 region labels to image voxels. While the first approach
                 is well suited to describe smooth contours at subvoxel
                 accuracy, the second approach is conceptually more
                 simple and guarantees a unique classification of image
                 areas. However, contours extracted from labeled images
                 typically exhibit strong staircase artifacts and are
                 not well suited to represent smooth tissue boundaries.
                 In this paper we describe how this drawback can be
                 circumvented by supplementing region labels with
                 additional probability information. We integrated our
                 approach into an interactive segmentation system
                 providing a well-defined set of manual and
                 semi-automatic editing tools. All tools update both
                 region labels and probabilities simultaneously, thus
                 allowing one to define segmentation results at high
                 resolution. We applied our techniques to generate 3D
                 polygonal models of anatomical structure.",
  month =        jun,
  editor =       "H. U. Lemke and K. Inamura and M. W. Vannier and A. G.
                 Farman",
  booktitle =    "Proceedings of CAR'98. Computer Assisted Radiology and
                 Surgery",
}

@InProceedings{EVL-1998-264,
  pages =        "343--352",
  year =         "1998",
  title =        "Non-Distorted Texture Mapping for Sheared Triangulated
                 Meshes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-264",
  author =       "Bruno L{\'{e}}vy and {Jean-Laurent} Mallet",
  language =     "en",
  abstract =     "This article introduces new techniques for
                 non-distorted texture mapping on complex triangulated
                 meshes. Texture coordinates are assigned to the
                 vertices of the triangulation by using an iterative
                 optimization algorithm, honoring a set of constraints
                 minimizing the distortions. As compared to other global
                 optimization techniques, our method allows the user to
                 specify the surface zones where distortions should be
                 minimized in order of preference. The modular approach
                 described in this paper results in a highly flexible
                 method, facilitating a customized mapping construction.
                 For instance, it is easy to align the texture on the
                 surface with a set of user defined isoparametric
                 curves. Moreover, the mapping can be made continuous
                 through cuts, allowing to parametrize in one go complex
                 cut surfaces. It is easy to specify other constraints
                 to be honored by the so-constructed mappings, as soon
                 as they can be expressed by linear (or linearizable)
                 relations. This method has been integrated successfully
                 within a widely used {CAD} software dedicated to
                 geosciences. In this context, applications of the
                 method comprise numerical computations of physical
                 properties stored in fine grids within texture space,
                 unfolding geological layers and generating grids that
                 are suitable for finite element analysis. The impact of
                 the method could be also important for 3D paint
                 systems.",
  organization = "ACM SIGGRAPH",
  month =        jul,
  editor =       "Michael Cohen",
  keywords =     "Non Distorted Texture Mapping, Parametrization,
                 Discrete Smooth Interpolation, Optimization",
  booktitle =    "SIGGRAPH 98 Conference Proceedings",
}

@Article{EVL-1998-265,
  pages =        "373--389",
  year =         "1998",
  title =        "Three-dimensional metamorphosis: a survey",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-265",
  author =       "Francis Lazarus and Anne Verroust",
  language =     "en",
  keywords =     "Metamorphosis, Shape Transformation, Interpolation,
                 Computer animation, Geometric modelling",
  volume =       "14",
  journal =      "The Visual Computer",
}

@Article{EVL-1998-266,
  year =         "1998",
  title =        "Multiresolution Analysis on Irregular Surface Meshes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-266",
  author =       "Georges-Pierre Bonneau",
  language =     "en",
  abstract =     "Wavelet-based methods have proven their efficiency for
                 the visualization at different levels of detail,
                 progressive transmission, and compression of large data
                 sets. The required core of all wavelet-based methods is
                 a hierarchy of meshes that satisfies
                 subdivision-connectivity: This hierarchy has to be the
                 result of a subdivision process starting from a base
                 mesh. Examples include quadtree uniform 2D meshes,
                 octree uniform 3D meshes, or 4-to-1 split triangular
                 meshes. In particular, the necessity of
                 subdivision-connectivity prevents the application of
                 wavelet-based methods on irregular triangular meshes.
                 In this paper, a {"}wavelet-like{"} decomposition is
                 introduced that works on piecewise constant data sets
                 over irregular triangular surface meshes. The
                 decomposition/reconstruction algorithms are based on an
                 extension of wavelet-theory allowing hierarchical
                 meshes without subdivision-connectivity property. Among
                 others, this approach has the following features: It
                 allows exact reconstruction of the data set, even for
                 nonregular triangulations, and it extends previous
                 results on Haar-wavelets over 4-to-1 split
                 triangulations.",
  month =        oct,
  volume =       "4",
  keywords =     "Wavelets, nonregular triangulations, compression,
                 visualization",
  number =       "4",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1998-267,
  year =         "1998",
  title =        "Adaptive Projection Operators in Multiresolution
                 Scientific Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-267",
  author =       "Mario Ohlberger and Martin Rumpf",
  language =     "en",
  abstract =     "Recently, multiresolution visualization methods have
                 become an indispensable ingredient of real-time
                 interactive postprocessing. The enormous databases,
                 typically coming along with some hierarchical
                 structure, are locally resolved on different levels of
                 detail to achieve a significant savings of CPU and
                 rendering time. Here, the method of adaptive projection
                 and the corresponding operators on data functions,
                 respectively, are introduced. They are defined and
                 discussed as mathematically rigorous foundations for
                 multiresolution data analysis. Keeping in mind data
                 from efficient numerical multigrid methods, this
                 approach applies to hierarchical nested grids
                 consisting of elements which are any tensor product of
                 simplices, generated recursively by an arbitrary,
                 finite set of refinement rules from some coarse grid.
                 The corresponding visualization algorithms, e.g., color
                 shading on slices or isosurface rendering, are confined
                 to an appropriate depth-first traversal of the grid
                 hierarchy. A continuous projection of the data onto an
                 adaptive, extracted subgrid is thereby calculated
                 recursively. The presented concept covers different
                 methods of local error measurement, time-dependent data
                 which have to be interpolated from a sequence of key
                 frames, and a tool for local data focusing.
                 Furthermore, it allows for a continuous level of
                 detail.",
  month =        oct,
  volume =       "4",
  keywords =     "Adaptive projection operators, multiresolution,
                 efficient data analysis, error indicators, hierarchical
                 grids, visualization of large data sets",
  number =       "4",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1998-268,
  year =         "1998",
  title =        "Visualization of Scientific Video Data Using {KL}
                 Decomposition",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-268",
  author =       "Kay A. Robbins",
  language =     "en",
  abstract =     "Fast methods are developed for visualizing and
                 classifying certain types of scientific video data.
                 These techniques, which are based on KL decomposition,
                 find a best coordinate system for a data set. When the
                 data set represents a temporally ordered collection of
                 images, the best coordinate system leads to
                 approximations that are separable in time and space.
                 Practical methods for computing this best coordinate
                 system are discussed and physically significant
                 visualizations for experimental video data are
                 developed. The visualization techniques are applied to
                 two experimental systems-- one from combustion and the
                 other from neurobiology-- to show how relevant
                 information can be quickly extracted from video data.
                 These techniques can be integrated into the video
                 acquisition process to provide real-time feedback to
                 the experimentalist during the operation of an
                 experiment.",
  month =        oct,
  volume =       "4",
  keywords =     "Scientific visualization, real-time visualization,
                 video analysis",
  number =       "4",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1998-269,
  year =         "1998",
  title =        "{MIP-Map} Level Selection for Texture Mapping",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-269",
  author =       "Jon P. Ewins and Marcus D. Waller and Martin White and
                 Paul F. Lister",
  language =     "en",
  abstract =     "Texture mapping is a fundamental feature of computer
                 graphics image generation. In current PC-based
                 acceleration hardware, MIP-mapping with bilinear and
                 trilinear filtering is a commonly used filtering
                 technique for reducing spatial aliasing artifacts. The
                 effectiveness of this technique in reducing image
                 aliasing at the expense of blurring is dependent upon
                 the MIP-map level selection and the associated
                 calculation of screen-space to texture-space pixel
                 scaling. This paper describes an investigation of
                 practical methods for per-pixel and per-primitive level
                 of detail calculation. This investigation was carried
                 out as part of the design work for a screen-space
                 rasterization ASIC. The implementations of several
                 algorithms of comparable visual quality are discussed
                 and a comparison is provided in terms of per-primitive
                 and per-pixel computational costs.",
  month =        oct,
  volume =       "4",
  keywords =     "Texture mapping, filtering, MIP-map, minification,
                 level of detail, rasterization, interpolation",
  number =       "4",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1998-27,
  title =        "Splatting Errors and Antialiasing",
  language =     "en",
  month =        apr,
  number =       "2",
  pages =        "178--?",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-27",
  author =       "Klaus Mueller and Torsten M{\"o}ller and J. Edward
                 {Swan II} and Roger Crawfis and Naeem Shareef and Roni
                 Yagel",
  abstract =     "This paper describes three new results for volume
                 rendering algorithms utilizing splatting. First, an
                 antialiasing extension to the basic splatting algorithm
                 is introduced that mitigates the spatial aliasing for
                 high-resolution volumes. Aliasing can be severe for
                 high-resolution volumes or volumes where a high depth
                 of field leads to converging samples along the
                 perspective axis. Next, an analysis of the common
                 approximation errors in the splatting process for
                 perspective viewing is presented. In this context, we
                 give different implementations, distinguished by
                 efficiency and accuracy, for adding the splat
                 contributions to the image plane. We then present new
                 results in controlling the splatting errors and also
                 show their behavior in the framework of our new
                 antialiasing technique. Finally, current work in
                 progress on extensions to splatting for temporal
                 antialiasing is demonstrated. Here, we present a simple
                 but highly effective scheme for adding motion blur to
                 fast moving volumes.",
  keywords =     "Volume rendering, splatting, direct volume rendering,
                 resampling, reconstruction, antialiasing, perspective
                 projection, motion blur",
  volume =       "4",
  copyright =    "IEEE",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1998-270,
  year =         "1998",
  title =        "Perception of Human Motion With Different Geometric
                 Models",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-270",
  author =       "Jessica K. Hodgins and James F. O'Brien and Jack
                 Tumblin",
  language =     "en",
  abstract =     "Human figures have been animated using a variety of
                 geometric models, including stick figures, polygonal
                 models, and NURBS-based models with muscles, flexible
                 skin, or clothing. This paper reports on experimental
                 results indicating that a viewer's perception of motion
                 characteristics is affected by the geometric model used
                 for rendering. Subjects were shown a series of paired
                 motion sequences and asked if the two motions in each
                 pair were {"}the same or different{"} The motion
                 sequences in each pair were rendered using the same
                 geometric model. For the three types of motion
                 variation tested, sensitivity scores indicate that
                 subjects were better able to observe changes with the
                 polygonal model than they were with the stick figure
                 model.",
  month =        oct,
  volume =       "4",
  keywords =     "Motion perception, motion sensitivity, computer
                 animation, geometric model, perceptual study,
                 biological motion stimuli, light-dot display",
  number =       "4",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1998-271,
  year =         "1998",
  title =        "Novel View Synthesis by Cascading Trilinear Tensors",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-271",
  author =       "Shai Avidan and Amnon Shashua",
  language =     "en",
  abstract =     "We present a new method for synthesizing novel views
                 of a 3D scene from two or three reference images in
                 full correspondence. The core of this work is the use
                 and manipulation of an algebraic entity termed the
                 trilinear tensor that links point correspondences
                 across three images. For a given virtual camera
                 position and orientation, a new trilinear tensor can be
                 computed based on the original tensor of the reference
                 images. The desired view can then be created using this
                 new trilinear tensor and point correspondences across
                 two of the reference images.",
  month =        oct,
  volume =       "4",
  keywords =     "Image-based rendering, trilinear tensor, virtual
                 reality, image manipulation",
  number =       "4",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1998-272,
  pages =        "54--66",
  year =         "1998",
  title =        "A system for constructing private digital libraries
                 through information space exploration",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-272",
  author =       "Masanori Sugimoto and Norio Katayama and Atsuhiro
                 Takasu",
  language =     "en",
  abstract =     "Digital libraries on the Internet are constructed
                 separately and managed autonomously, and their
                 semantics are different from each other. On the other
                 hand, users of digital libraries have their own
                 information needs. In order for users to fully utilize
                 digital libraries, therefore, a new method for
                 promoting mediation between the semantics of digital
                 libraries and users' information needs is vital. COSPEX
                 (COnceptual SPace EXplorer), proposed in this paper, is
                 a system that can enhance such mediation. Through the
                 exploration of information spaces encompassing multiple
                 digital libraries, users of COSPEX can construct their
                 own private digital libraries.",
  keywords =     "information gathering, visual interface, universal
                 query interface, information space exploration, query
                 articulation and formulation, extraction of information
                 needs",
  number =       "2",
  journal =      "International Journal on Digital Libraries",
}

@InProceedings{EVL-1998-273,
  year =         "1998",
  title =        "Visualizing 2{D} flows: Integrate and Draw",
  URL =          "http://www.informatik.uni-rostock.de/~carlos/FlowVisualization/IntegrateDraw.html",
  author =       "Carlos P{\'{e}}rez Risquet",
  abstract =     "Integrate and Draw is an effective technique for
                 visualizing 2D vector fields. This technique computes
                 flow images without the needing of any input texture
                 and is faster than other usual techniques for
                 visualizing 2D flows, because, as the name of the
                 algorithm suggests, this method results in simply
                 drawing streamlines without any convolution taking
                 place. Integrate and Draw improves the delineation of
                 the flow lines with respect to LIC images, by drawing
                 more solid lines that do not ondulate as frequently in
                 value. Thus, the image contrast increases and the
                 vector field structure becomes clearer.",
  language =     "en",
  keywords =     "visualization, flow visualization, lic",
  booktitle =    "Proceedings of the 9th Eurographics Workshop on
                 Visualization in Scientific Computing, Germany 1998.",
}

@TechReport{EVL-1998-274,
  year =         "1998",
  title =        "Hyperbolic and Parabolic Quadric Surface Fitting
                 Algorithms - Comparison Between the Least Squares
                 Approach and the Parameter Optimization Approach",
  author =       "Min Dai and Timothy S. Newman",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-274",
  abstract =     "Locating and classifying quadric surfaces is a
                 significant step in the recognition of 3D manufactured
                 objects because quadric surfaces are commonly occuring
                 shapes in man-made products. Surface fitting based on
                 the input sample data point set is an effective
                 strategy for quadric surface recognition. Two
                 algorithms of quadric surface fitting that are
                 especially useful for hyperboloid and paraboloid
                 fitting are described in this report. One is the Least
                 Squares Approach and the other is the Parameter
                 Optimization Approach. A comparison is made between the
                 performances of these methods.",
  number =       "TR-UAH-CS-1998-02",
  institution =  "Computer Science, Dept., Univ. Alabama in Huntsville",
}

@TechReport{EVL-1998-275,
  year =         "1998",
  title =        "Practical and efficient computation of additively
                 weighted Voronoi cells for applications in molecular
                 biology",
  author =       "Hans-Martin Will",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-275",
  abstract =     "This paper is concerned with the efficient computation
                 of additively weighted Voronoi cells for applications
                 in molecular biology. We propose a projection map for
                 the representation of these cells leading to a
                 surprising insight into their geometry. We present a
                 randomized algorithm computing one such cell amidst n
                 other spheres in expected time O(n^2 log n). Since the
                 best known upper bound on the complexity such a cell is
                 O(n^2), this is optimal up to a logarithmic factor.
                 However, the experimentally observed behavior of the
                 complexity of these cells is linear in n. In this case
                 our algorithm performs the task in expected time O(n
                 log^2 n). A variant of this algorithm was implemented
                 and performs well on problem instances from molecular
                 biology.",
  month =        may,
  address =      "Institute of Theoretical Computer Science",
  number =       "300",
  institution =  "ETH Z{\"u}rich",
}

@TechReport{EVL-1998-276,
  year =         "1998",
  title =        "Geometric properties of spatial additively weighted
                 Voronoi cells",
  author =       "Hans-Martin Will",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-276",
  abstract =     "In this report, we discuss the geometric properties of
                 spatial additively weighted Voronoi (AWV-) cells.
                 First, we give a short survey of previously known
                 properties of the geometry of these cells. Then, we
                 give a detailed account on the geometry of the edges of
                 AWV-cells. Finally, and this is our main contribution,
                 we prove a new and tight lower bound of $\Theta(n^2)$
                 on the worst-case combinatorial complexity of a single
                 AWV-cell defined by $n$ spheres.",
  month =        jul,
  address =      "Institute of Theoretical Computer Science",
  number =       "302",
  institution =  "ETH Z{\"u}rich",
}

@TechReport{EVL-1998-277,
  year =         "1998",
  title =        "Best-Fit of Sculptured Surfaces",
  author =       "Walter Gander and David Sourlier",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-277",
  abstract =     "The parametric representation of surfaces is often
                 avoided in best-fit computations because of the large
                 number of unknowns. By exploiting the matrix structure,
                 D. Sourlier has developed an effective best-fit
                 software 'FUNKE', which is based on parametric
                 representation. FUNKE is perfectly applicable for
                 sculptured surfaces. We report on the best-fit of a
                 turbine blade. In the second part we propose a new
                 shape distance measure to compare two CAD-described
                 surfaces. We give algorithms in Matlab to compute this
                 shape distance and the corresponding transformation",
  month =        oct,
  address =      "Institute of Scientific Computing",
  number =       "307",
  institution =  "ETH Z{\"u}rich",
}

@TechReport{EVL-1998-278,
  year =         "1998",
  title =        "Applications of the Generic Programming Paradigm in
                 the Design of {CGAL}",
  author =       "Herv{\'{e}} Br{\"o}nnimann and Lutz Kettner and Stefan
                 Schirra and Remco Veltkamp",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-278",
  abstract =     "We report on the use of the generic programming
                 paradigm in the computational geometry algorithms
                 library CGAL. The parameterization of the geometric
                 algorithms in CGAL enhances flexibility and
                 adaptability and opens an easy way for abolishing
                 precision and robustness problems by exact but
                 nevertheless efficient computation. Furthermore we
                 discuss circulators, which are an extension of the
                 iterator concept to circular structures. Such
                 structures arise frequently in geometric computing.",
  month =        nov,
  address =      "Institute for Theoretical Computer Science",
  institution =  "ETH Z{\"u}rich",
}

@TechReport{EVL-1998-279,
  year =         "1998",
  title =        "Interactive Cuts through 3-Dimensional Soft Tissue",
  author =       "Daniel Bielser and Volker A. Maiwald and Markus H.
                 Gross",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-279",
  abstract =     "We describe a physically based framework for real-time
                 modeling and interactive cutting of 3-dimensional soft
                 tissue that can be used for surgery simulation. Unlike
                 existing approaches which are mostly designed for
                 tensorproduct grids our methods operate on tetrahedral
                 decompositions giving more topological and geometric
                 flexibility for the efficient modeling of complex
                 anatomical structures. We start from an initial
                 tetrahedralization such as being provided by any
                 conventional meshing method. In order to track
                 topological changes tetrahedra intersected by the
                 virtual scalpel are split into substructures whose
                 connectivity follows the trajectory of the cut, which
                 can be arbitrary. For the efficient computation of
                 collisions between the scalpel and individual
                 tetrahedra we devised a local collision detection
                 algorithm. The underlying physics is approximated
                 through masses and springs attached to each tetrahedral
                 vertex and edge. A hierarchical Runge-Kutta iteration
                 computes the relaxation of the system by traversing the
                 designed data structures in a breadth-first order. The
                 framework includes a force-feedback interface and uses
                 real-time texture mapping to enhance the visual
                 realism.",
  month =        nov,
  address =      "Institute of Scientific Computing",
  number =       "309",
  institution =  "ETH Z{\"u}rich",
}

@InProceedings{EVL-1998-28,
  pages =        "287--290",
  year =         "1998",
  title =        "Parameterizing Meshes with Arbitrary Topology",
  author =       "S. Campagna and H.-P. Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-28",
  language =     "en",
  abstract =     "Parameterizing meshes is a basic requirement for many
                 applications, including, e.g., reverse engineering,
                 texture mapping, and re-meshing. We present a new fast
                 algorithm that uses the hierarchical representation of
                 a polygonal mesh with arbitrary topology for generating
                 a geometry driven parameterization.",
  editor =       "H. Niemann and H.-P. Seidel and B. Girod",
  booktitle =    "Image and Multidimensional Digital Signal Processing
                 '98",
  publisher =    "infix",
}

@TechReport{EVL-1998-280,
  year =         "1998",
  title =        "Line Simplification with Restricted Orientations",
  author =       "Gabriele Neyer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-280",
  abstract =     "Line Simplification with Restricted Orientations We
                 study the C-oriented line simplification problem: Given
                 a polygonal chain P represented by an ordered set of
                 vertices p1,...,pn in the plane, a set of orientations
                 C, and a constant e, we search for a ``C-oriented''
                 polygonal chain Q consisting of the minimum number of
                 line segments that has distance at most e to P in the
                 Frechet metric. A polygonal chain is C-oriented if the
                 line segments are parallel to orientations in C. We
                 restrict our attention to the version of the problem
                 where two circles of radius e formed around adjacent
                 vertices of the polygonal chain do not intersect. We
                 solve the C-oriented line simplification problem
                 constructively by using dynamic programming together
                 with a nice data structure. For usual cases of C our
                 algorithm solves the problem in time O(k n^2 log(n))
                 where k is the minimum number of line segments of Q and
                 uses O(k n^2) space.",
  month =        dec,
  address =      "Institute of Theoretical Computer Science",
  keywords =     "line, simplification, c-oriented, subway, map",
  number =       "311",
  institution =  "Institute for Theoretical Computer Science, ETH
                 Z{\"u}rich",
}

@TechReport{EVL-1998-281,
  year =         "1998",
  title =        "Improving Behavior Efficiency in Virtual Worlds",
  author =       "Jeff White",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-281",
  abstract =     "As the complexity of computer games grows, it becomes
                 increasingly necessary to manage the computational cost
                 of both geometry and behavior. Although there have been
                 many advances in software and hardware for efficiently
                 processing geometry, there are relatively few
                 techniques for managing behaviors. In anticipation of
                 the rising cost of behavior execution, we identify
                 techniques for improving behavior efficiency and
                 discuss how these techniques are applicable to specific
                 properties of behavior. We present examples that we
                 have implemented that demonstrate the use and
                 effectiveness of these techniques. We also describe a
                 framework that allows applications to reuse these
                 techniques. Although we focus on game environments, we
                 believe this work is applicable to other virtual
                 environments, such as information visualization,
                 animated films, and virtual walkthroughs.",
  month =        oct,
  number =       "CS-98-10",
  institution =  "Brown University",
}

@TechReport{EVL-1998-282,
  year =         "1998",
  title =        "Lightning: {A} System for Reusing Ray Trace Data for
                 Fast Lighting of 3{DS}cenes",
  author =       "Jennifer K. Stewart",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-282",
  abstract =     "The most time-consuming part of lighting 3D scenes in
                 computer graphics is the rendering and re-rendering
                 artists have to do as they edit the lights and their
                 parameters. Most lighting editors have some features to
                 expedite this process, such as low resolution or fast
                 shading render previews, but these typically fail to
                 generate exactly the same shadows, highlights, and
                 reflections that appear when the scene is rendered
                 normally. We present a method for ray tracing 3D scenes
                 where these characteristics are rendered just as
                 accurately as with traditional ray tracing, but in up
                 to 98% less time. By storing information from an
                 initial, expensive, ray trace of the scene, we can
                 re-render subsequent changes to the lights in a small
                 fraction of the time it takes to do a regular ray
                 trace, and far more accurately than using a low
                 resolution or fast shading render preview.",
  month =        aug,
  number =       "CS-98-09",
  institution =  "Brown University",
}

@Article{EVL-1998-283,
  pages =        "186--201",
  year =         "1998",
  title =        "Effects of Perceiving and Imaging Scenes on Memory for
                 Pictures",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-283",
  author =       "Helene Intraub and Carmela V. Gottesman and Amy J.
                 Bills",
  abstract =     "This article presents results of experiments, which
                 show that people tend to remember central objects of a
                 scene bigger than they were physically presented.",
  keywords =     "picture memory, image processing, recall,
                 recognition",
  volume =       "24",
  number =       "1",
  journal =      "Journal of Experimental Psychology",
}

@Article{EVL-1998-284,
  pages =        "261--282",
  year =         "1998",
  title =        "Phenomenology of Attention: 1. Color, Location,
                 Orientation, and Spatial Frequency",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-284",
  author =       "William Prinzmetal and Hedy Amiri and Kristin Allen
                 and Tami Edwards",
  abstract =     "This articles presents results of studies on how
                 attentions influences the perception of certain visual
                 features. How does attention affect the perceived
                 color, location, or orientation of objects? Results
                 show that higher attention may lead to a reduction of
                 uncertainty in the evaluation of the above mentioned
                 visual features.",
  volume =       "24",
  keywords =     "picture memory, recall, attention,",
  number =       "1",
  journal =      "Journal of Experimental Psychology: Human Perception
                 and Performance",
}

@Book{EVL-1998-285,
  year =         "1998",
  title =        "Virtuelle Informationsr{\"a}ume mit {VRML}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-285",
  author =       "Rolf D{\"a}{\ss}ler and Hartmut Palm",
  language =     "de",
  abstract =     "This book describes how to create virtual information
                 spaces using VRML. These information spaces are based
                 on clustering document collections. An implementation
                 is presented using Perl.",
  keywords =     "Information retrieval, information visualization,
                 document retrieval, clustering, VRML",
  publisher =    "dpunkt Verlag, Heidelberg",
}

@Article{EVL-1998-286,
  pages =        "53--65",
  year =         "1998",
  title =        "Enhanced Integrate and Draw with Feature
                 Visualization",
  author =       "Carlos P{\'{e}}rez Risquet and Holger Theisel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-286",
  abstract =     "EIDraw is a combined technique to visualize not only
                 the flow, but also some other scalar quantities
                 measured on a 2D vector field. Some features of a
                 vector field such as orientation, magnitude, curvature,
                 etc. are often difficult to visualize at the same time
                 as the flow. Only animations or sophisticated methods
                 may succeed in this case. The technique we present here
                 is very fast and makes possible the visualization of
                 the flow together with any other of these features in a
                 still image. This can be very useful by visualizations
                 over the Internet for example, where the time is a
                 precious resource.",
  month =        dec,
  volume =       "22",
  keywords =     "visualization; flow visualization; vector field
                 visualization",
  journal =      "Rostocker Informatik-Berichte",
}

@InProceedings{EVL-1998-287,
  year =         "1998",
  title =        "Supporting 3{D} Warping Visual Feedback for Virtual
                 Reality",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-287",
  author =       "B. Thomas",
  month =        dec,
  address =      "Adelaide, Australia 14-21",
  booktitle =    "Proceedings OzCHI'98 The Australian Conference on
                 Computer-Human Interaction",
}

@InProceedings{EVL-1998-288,
  year =         "1998",
  title =        "Experiments with Animating Direct Manipulation in a
                 Drawing Editor",
  author =       "B. H. Thomas and P. Calder and V. Demczuk",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-288",
  month =        feb,
  address =      "Perth, Australia 157-168",
  keywords =     "animation, cartooning, direct manipulation, graphical
                 interfaces, drawing editors, InterViews",
  booktitle =    "ACSC'98 - The 21st Australasian Computer Science
                 Conference",
}

@InProceedings{EVL-1998-289,
  year =         "1998",
  title =        "Warping distributed system configurations",
  author =       "B. H. Thomas and D. Stotts",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-289",
  month =        may,
  address =      "Annapolis, Maryland, USA",
  keywords =     "warping, collaborative 3D virtual environments, user
                 interaction, interface animation, MUVEE",
  booktitle =    "4th International Conference on Configurable
                 Distributed Systems",
}

@InProceedings{EVL-1998-29,
  pages =        "149--168",
  year =         "1998",
  title =        "Effiziente Visualisierungsverfahren zur interaktiven
                 Qualit{\"{a}}tskontrolle von
                 Au{\ss{}}enhautkonstruktionen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-29",
  author =       "G. Greiner and S. Campagna",
  language =     "de",
  note =         "Arbeitskreis Virtuelle Produktientwicklung",
  editor =       "S. Kuschfeldt and T. Ertl",
  booktitle =    "Synthese von Berechnung, Simulation und
                 Visualisierung",
  publisher =    "Techniik + Kommunikation",
}

@Book{EVL-1998-290,
  year =         "1998",
  title =        "Color for Science, Art and Technolog",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-290",
  abstract =     "The aim of this book is to assemble a series of
                 chapters, written by experts in their fields, covering
                 the basics of color - and then some more. In this way,
                 readers are supplied with almost anything they want to
                 know about color outside their own area of expertise.
                 Thus, the color measurement expert, as well as the
                 general reader, can find here information on the
                 perception, causes, and uses of color. For the artist
                 there are details on the causes, measurement,
                 perception, and reproduction of color. Within each
                 chapter, authors were requested to indicate directions
                 of future efforts, where applicable. One might
                 reasonably expect that all would have been learned
                 about color in the more than three hundred years since
                 Newton established the fundamentals of color science.
                 This is not true because: (a) the measurement of color
                 still has unresolved complexities (Chapter 2); (b) many
                 of the fine details of color vision remain unknown
                 (Chapter 3); (c) every few decades a new movement in
                 art discovers original ways to use new pigments, and
                 dyes continue to be discovered (Chapter 5); (d) the
                 philosophical approach to color has not yet
                 crystallized (Chapter 7); (e) new pigments and dyes
                 continue to be discovered (Chapters 10 and 11); (f) the
                 study of the biological and therapeutic effects of
                 color is still in its infancy (Chapter 2). Color
                 continues to develop towards maturity and the editor
                 believes that there is much common ground between the
                 sciences and the arts and that color is a major
                 connecting bridge.",
  editor =       "K. Nassau",
  copyright =    "Elsevier Science",
  publisher =    "Elsevier Science",
}

@Article{EVL-1998-291,
  pages =        "1--20",
  year =         "1998",
  title =        "{Calibration-Free Augmented Reality}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-291",
  author =       "Kiriakos N. Kutulakos and James R. Vallino",
  abstract =     "Camera calibration and the acquisition of Euclidean 3D
                 measurements have so far been considered necessary
                 requirements for overlaying three-dimensional graphical
                 objects with live video. In this article, we describe a
                 new approach to video-based augmented reality that
                 avoids both requirements: It does not use any metric
                 information about the calibration parameters of the
                 camera or the 3D locations and dimensions of the
                 environment's objects. The only requirement is the
                 ability to track across frames at least four fiducial
                 points that are specified by the user during system
                 initialization and whose world coordinates are unknown.
                 Our approach is based on the following observation:
                 Given a set of four or more noncoplanar 3D points, the
                 projection of all points in the set can be computed as
                 a linear combination of the projections of just four of
                 the points. We exploit this observation by 1) tracking
                 regions and color fiducial points at frame rate, and 2)
                 representing virtual objects in a non-Euclidean, affine
                 frame of reference that allows their projection to be
                 computed as a linear combination of the projection of
                 the fiducial points. Experimental results on two
                 augmented reality systems, one monitor-based and one
                 head-mounted, demonstrate that the approach is readily
                 implementable, imposes minimal computational and
                 hardware requirements, and generates real-time and
                 accurate video overlays even when the camera parameters
                 vary dynamically.",
  month =        jan,
  keywords =     "augmented reality, real-time computer vision,
                 calibration, registration, affine representations,
                 feature tracking, 3D interaction techniques",
  volume =       "4",
  number =       "1",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1998-292,
  pages =        "21--36",
  year =         "1998",
  title =        "{Efficient Collision Detection Using Bounding Volume
                 Hierarchies of k-DOPs}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-292",
  author =       "James T. Klosowski and Joseph S. B. Mitchell and Henry
                 Sowizral and Karel Zikan",
  abstract =     "Collision detection is of paramount importance for
                 many applications in computer graphics and
                 visualization. Typically, the input to a collision
                 detection algorithm is a large number of geometric
                 objects comprising an environment, together with a set
                 of objects moving within the environment. In addition
                 to determining accurately the contacts that occur
                 between pairs of objects, one needs also to do so at
                 real-time rates. Applications such as haptic
                 force-feedback can require over 1,000 collision queries
                 per second. In this paper, we develop and analyze a
                 method, based on bounding-volume hierarchies, for
                 efficient collision detection for objects moving within
                 highly complex environments. Our choice of bounding
                 volume is to use a ``discrete orientation polytope''
                 (``k-dop''), a convex polytope whose facets are
                 determined by halfspaces whose outward normals come
                 from a small fixed set of k orientations. We compare a
                 variety of methods for constructing hierarchies
                 (``BV-trees'') of bounding k-dops. Further, we propose
                 algorithms for maintaining an effective BV-tree of
                 k-dops for moving objects, as they rotate, and for
                 performing fast collision detection using BV-trees of
                 the moving objects and of the environment. Our
                 algorithms have been implemented and tested. We provide
                 experimental evidence showing that our approach yields
                 substantially faster collision detection than previous
                 methods.",
  month =        jan,
  keywords =     "collision detection, intersection searching, bounding
                 volume hierarchies, discrete orientation polytopes,
                 bounding boxes, virtual reality, virtual environments",
  volume =       "4",
  number =       "1",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1998-293,
  pages =        "37--54",
  year =         "1998",
  title =        "{A High Accuracy Volume Renderer for Unstructured
                 Data}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-293",
  author =       "Peter L. Williams and Nelson L. Max and Clifford M.
                 Stein",
  abstract =     "This paper describes a volume rendering system for
                 unstructured data, especially finite element data, that
                 creates images with very high accuracy. The system will
                 currently handle meshes whose cells are either linear
                 or quadratic tetrahedra. Compromises or approximations
                 are not introduced for the sake of efficiency. Whenever
                 possible, exact mathematical solutions for the radiance
                 integrals involved and for interpolation are used. The
                 system will also handle meshes with mixed cell types:
                 tetrahedra, bricks, prisms, wedges, and pyramids, but
                 not with high accuracy. Accurate semitransparent shaded
                 isosurfaces may be embedded in the volume rendering.
                 For very small cells, subpixel accumulation by
                 splatting is used to avoid sampling error. A revision
                 to an existing accurate visibility ordering algorithm
                 is described, which includes a correction and a method
                 for dramatically increasing its efficiency. Finally,
                 hardware assisted projection and compositing are
                 extended from tetrahedra to arbitrary convex
                 polyhedra.",
  month =        jan,
  keywords =     "volume rendering, unstructured meshes, high accuracy,
                 finite element method, isosurfaces, splatting, cell
                 projection, visibility ordering, depth sorting",
  volume =       "4",
  number =       "1",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1998-294,
  pages =        "55--70",
  year =         "1998",
  title =        "{Modeling, Animating, and Rendering Complex Scenes
                 Using Volumetric Textures}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-294",
  author =       "Fabrice Neyret",
  abstract =     "Complex repetitive scenes containing forests, foliage,
                 grass, hair, or fur, are challenging for common
                 modeling and rendering tools. The amount of data, the
                 tediousness of modeling and animation tasks, and the
                 cost of realistic rendering have caused such kind of
                 scene to see only limited use even in high-end
                 productions. We describe here how the use of volumetric
                 textures is well suited to such scenes. These
                 primitives can greatly simplify modeling and animation
                 tasks. More importantly, they can be very efficiently
                 rendered using ray tracing with few aliasing artifacts.
                 The main idea, initially introduced by Kajiya and Kay
                 [9], is to represent a pattern of 3D geometry in a
                 reference volume, that is tiled over an underlying
                 surface much like a regular 2D texture. In our
                 contribution, the mapping is independent of the mesh
                 subdivision, the pattern can contain any kind of shape,
                 and it is prefiltered at different scales as for
                 MIP-mapping. Although the model encoding is volumetric,
                 the rendering method differs greatly from traditional
                 volume rendering: A volumetric texture only exists in
                 the neighborhood of a surface, and the repeated
                 instances (called texels) of the reference volume are
                 spatially deformed. Furthermore, each voxel of the
                 reference volume contains a key feature which controls
                 the reflectance function that represents aggregate
                 intravoxel geometry. This allows for ray-tracing of
                 highly complex scenes with very few aliasing artifacts,
                 using a single ray per pixel (for the part of the scene
                 using the volumetric texture representation). The major
                 technical considerations of our method lie in the
                 ray-path determination and in the specification of the
                 reflectance function.",
  month =        jan,
  keywords =     "volumetric textures, complex geometry, levels of
                 detail",
  volume =       "4",
  number =       "1",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1998-295,
  pages =        "71--81",
  year =         "1998",
  title =        "{Line Art Illustrations of Parametric and Implicit
                 Forms}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-295",
  author =       "Gershon Elber",
  abstract =     "A technique is presented for line art rendering of
                 scenes composed of freeform surfaces. The line art that
                 is created for parametric surfaces is practically
                 intrinsic and is globally invariant to changes in the
                 surface parameterization. This method is equally
                 applicable for line art rendering of implicit forms,
                 creating a unified line art rendering method for both
                 parametric and implicit forms. This added flexibility
                 exposes a new horizon of special, parameterization
                 independent, line art effects. Moreover, the production
                 of the line art illustrations can be combined with
                 traditional rendering techniques such as transparency
                 and texture mapping. Examples that demonstrate the
                 capabilities of the proposed approach are presented for
                 both the parametric and implicit forms.",
  month =        jan,
  keywords =     "sketches, illustrations, line drawings, freeform
                 surfaces, NURBs, implicit forms, surface coverage,
                 printing",
  volume =       "4",
  number =       "1",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1998-296,
  pages =        "82--?",
  year =         "1998",
  title =        "{Fast Horizon Computation at All Points of a Terrain
                 With Visibility and Shading Applications}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-296",
  author =       "A. James Stewart",
  abstract =     "A terrain is most often represented with a digital
                 elevation map consisting of a set of sample points from
                 the terrain surface. This paper presents a fast and
                 practical algorithm to compute the horizon, or skyline,
                 at all sample points of a terrain. The horizons are
                 useful in a number of applications, including the
                 rendering of self-shadowing displacement maps,
                 visibility culling for faster flight simulation, and
                 rendering of cartographic data. Experimental and
                 theoretical results are presented which show that the
                 algorithm is more accurate that previous algorithms and
                 is faster than previous algorithms in terrains of more
                 than 100,000 sample points.",
  month =        jan,
  keywords =     "terrain, digital elevation map, horizon, skyline,
                 visibility, shadows, rendering, GIS",
  volume =       "4",
  number =       "1",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@TechReport{EVL-1998-297,
  year =         "1998",
  title =        "Volume Visualization",
  author =       "Irene Gargantini",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-297",
  keywords =     "volume rendering introduction, overview, data
                 structures, techniques, fundamental principles",
  number =       "TR 523",
  institution =  "Department of computer science, University of Western
                 Ontario",
}

@InProceedings{EVL-1998-298,
  year =         "1998",
  title =        "3-{D} Catheter Path Reconstruction from Biplane
                 Angiography using 3{D} Snakes",
  author =       "C. Molina and G. P. Prause and P. Radeva and M.
                 Sonka",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-298",
  address =      "San Diego, California",
  month =        feb,
  keywords =     "3-D snakes, biplane angiography, 3-D reconstruction,
                 segmentation, 3D B-splines, cardiac imaging, coronary
                 tree reconstruction",
  booktitle =    "SPIE - Medical Imaging",
}

@Article{EVL-1998-299,
  year =         "1998",
  title =        "Coupled {B}-Snake Grids and Constrained Thin-Plate
                 Splines for Analysis of 2{D} Tissue Deformations from
                 Tagged {MRI}",
  author =       "Amir A. Amini and Yasheng Chen and Rupert Curwen and
                 Vaidy Mani and Jean Sun",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-299",
  month =        jun,
  volume =       "17",
  number =       "3",
  journal =      "IEEE Transactions on Medical Imaging",
}

@Unpublished{EVL-1998-3,
  year =         "1998",
  title =        "Effects of lag and frame rate on various tracking
                 tasks",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-3",
  author =       "Steve Bryson",
  abstract =     "Virtual environments involve the user in an
                 interactive three-dimensional computer generated
                 environment. The methods of interaction typically
                 involve direct manipulation of virtual objects via
                 three-dimensional trackers. The tracking signal may be
                 degraded in various ways, impacting the ability of the
                 user to perform various tasks. This presentation will
                 address the impact of two types of degradation in the
                 tracking signal, lag (transport delay) and low frame
                 rate. These degradations are common in existing virtual
                 reality systems. While the impact of lag on human
                 performance is comparatively well studied, the impact
                 of low frame rate has not been widely studied. The
                 impact of lag and low frame rate on two tasks will be
                 compared and studied: Pursuit tracking and placing. The
                 tasks will be studied in a two-dimensional context,
                 eliminating ambiguities due to three-dimensional
                 perception and display. Simple conclusions will be
                 drawn that can serve as guidelines for developers
                 designing interactive virtual environments. The
                 relationship between these conclusions and theories of
                 human performance will be briefly addressed.",
  language =     "en",
  note =         "http://science.nas.nasa.gov/~bryson/papers.html",
}

@InProceedings{EVL-1998-30,
  year =         "1998",
  title =        "Fast and Interactive 3{D}--Segmentation of Medical
                 Volume Data",
  author =       "P. Hastreiter and T. Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-30",
  language =     "en",
  abstract =     "The segmentation of tomographic image data is an
                 important prerequisite for a meaningful visualization
                 in medicine. To circumvent difficulties related to
                 automatic approaches, we suggest interactive
                 segmentation which includes the knowledge of a user
                 more efficiently. Using intelligent scissors, as
                 presented in [1] for the 2D case, we suggest 3D filters
                 for the calculation of the cost matrix, an automatic
                 procedure which propagates contours to adjacent slices
                 and three communicating 2D displays for convenient
                 delineation of contours in volume data. Additionally,
                 we provide volume growing based on a statistical
                 approach, as presented in [2, 3], allowing to select
                 coherent subvolumes interactively. For the immediate
                 evaluation we introduce additional 3D displays for the
                 visualization of polygonal and volumetric
                 representations of the segmentation results.",
  organization = "IEEE Signal Processing Society",
  editor =       "H. Niemann and H.-P. Seidel and B. Girod",
  booktitle =    "Proceedings of Workshop on Image and Multi-dimensional
                 Digital Signal Processing (IMDSP)",
}

@Article{EVL-1998-300,
  year =         "1998",
  title =        "An Enhanced Spring Model for Information
                 Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-300",
  author =       "Holger Theisel and Matthias Kreuseler",
  month =        sep,
  note =         "Proceedings Eurographics 98",
  volume =       "17",
  number =       "3",
  journal =      "Computer Graphics Forum",
}

@InProceedings{EVL-1998-301,
  year =         "1998",
  title =        "Flexible Embedded Image Communication using Levels of
                 Detail and Regions of Interest",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-301",
  author =       "U. Rauschenbach and H. Schumann",
  month =        nov,
  booktitle =    "Proceedings of IMC '98 - Interactive Applications of
                 Mobile Computing, Rostock, Germany",
}

@Article{EVL-1998-302,
  pages =        "38--42",
  year =         "1998",
  title =        "Visualisierung multimedialer Informationen mit mobilen
                 Computersystemen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-302",
  author =       "H. Schumann and T. Kirste",
  volume =       "13",
  journal =      "Informatik, Forschung&Entwicklung",
}

@InProceedings{EVL-1998-303,
  pages =        "171--174",
  year =         "1998",
  title =        "A real time 3{D} visualization prototype for
                 interventional magnetic resonance imaging",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-303",
  author =       "J. Fischer; S. Weiss and H. Schumann",
  booktitle =    "Proceedings Computer Graphics and Imaging (CGIM'98),
                 Halifax, Kanada",
}

@InProceedings{EVL-1998-304,
  year =         "1998",
  title =        "Progressive Image Transmission using Levels of Detail
                 and Regions of Interest",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-304",
  author =       "U. Rauschenbach",
  month =        jun,
  booktitle =    "Proceedings of IASTED Conference on Computer Graphics
                 and Imaging - CGIM'98, Halifax, Nova Scotia, Canada",
}

@InProceedings{EVL-1998-305,
  year =         "1998",
  title =        "Color- and texture-based image segmentation using {EM}
                 and its application to content-based image retrieval",
  author =       "Serge Belongie and Chad Carson and Hayit Greenspan and
                 Jitendra Malik",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-305",
  abstract =     "Retrieving images from large and varied collections
                 using image content as a key is a challenging and
                 important problem. In this paper we present a new image
                 representation which provides a transformation from the
                 raw pixel data to a small set of image regions which
                 are coherent in color and texture space. This so-called
                 {"}Blobworld{"} representation is based on segmentation
                 using the Expectation-Maximization algorithm on
                 combined color and texture features. The texture
                 features we use for the segmentation arise from a new
                 approach to texture description and scale selection. We
                 describe a system that uses the Blobworld
                 representation to retrieve images. An important and
                 unique aspect of the system is that, in the context of
                 similarity-based querying, the user is allowed to view
                 the internal representation of the submitted image and
                 the query results. Similar systems do not offer the
                 user this view into the workings of the system;
                 consequently, the outcome of many queries on these
                 systems can be quite inexplicable, despite the
                 availability of knobs for adjusting the similarity
                 metric.",
  month =        jan,
  booktitle =    "Proceedings of the Sixth International Conference on
                 Computer Vision",
}

@Article{EVL-1998-306,
  pages =        "12--14",
  year =         "1998",
  title =        "Gesundheitsdaten hautnah - Das Telekonsultationssystem
                 TeCoMed in Mecklenburg-Vorpommern",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-306",
  author =       "H. Schumann and N. L{\'{o}}pez de Ch{\'{a}}vez and L.
                 Gierl and M. Bull",
  volume =       "4",
  journal =      "GeoBIT {"}Das Magazin f{\"{u}}r raumbezogene
                 Informationstechnologie{"}",
}

@InProceedings{EVL-1998-307,
  pages =        "317--326",
  year =         "1998",
  title =        "Generalized self-approaching curves",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-307",
  author =       "Oswin Aichholzer and Franz Aurenhammer and Christian
                 Icking and Rolf Klein and Elmar Langetepe and
                 G{\"u}nter Rote",
  month =        nov,
  editor =       "Kyung-Yong Chwa and Oscar H. Ibarra",
  volume =       "1533",
  series =       "Lecture Notes in Computer Science",
  booktitle =    "Algorithms and Computation --- Ninth Annual
                 International Symposium on Algorithms and Computation.
                 Taejon, Korea",
  publisher =    "Springer-Verlag",
}

@Article{EVL-1998-308,
  pages =        "1--16",
  year =         "1998",
  title =        "A visibility representation for graphs in three
                 dimensions",
  author =       "Prosenjit Bose and Hazel Everett and S{\'a}ndor Fekete
                 and Michael E. Houle and Anna Lubiw and Henk Meijer and
                 Kathleen Romanik and G{\"u}nter Rote and Tom Shermer
                 and Sue Whitesides and Christian Zelle",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-308",
  volume =       "2",
  journal =      "Journal of Graph Algorithms and Applications",
}

@InProceedings{EVL-1998-309,
  year =         "1998",
  title =        "Virtual Reality as Communication Aid for Persons With
                 Aphasia",
  author =       "E. Ahlsen and V. Geroimenko",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-309",
  month =        oct,
  keywords =     "virtual reality, visual communication, picture
                 communication, rehabilitation, disability, imgage
                 database, visuo-spatial orientation, Virtual
                 Communication for Aphasia (VCA) prototype",
  booktitle =    "Second Swedish Symposium on Multimodal Communication,
                 Lund Universitz, Sweden",
}

@InProceedings{EVL-1998-31,
  pages =        "78--85",
  year =         "1998",
  title =        "Integrated Registration and Visualization of Medical
                 Image Data",
  author =       "P. Hastreiter and T. Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-31",
  language =     "en",
  abstract =     "Different imaging modalities give insight to vascular,
                 anatomical and functional information which assist
                 diagnosis and therapy planning in medicine.
                 Registration and consecutive visualization allow to
                 combine the image data and thereby convey more
                 meaningful images to the clinician. Applying a voxel
                 based approach based on mutual information, accurate
                 and retrospective registration is provided. However,
                 optimization and consecutive visualization procedures
                 require a huge amount of trilinear interpolation
                 operations to re-sample the data. Ensuring fast
                 performance which is fundamental for medical routine,
                 we suggest an integrated approach which takes advantage
                 of the imaging and texture mapping subsystem of
                 graphics computers. All trilinear interpolation is
                 completely performed with hardware assisted 3D texture
                 mapping. The 1D and 2D histograms of the datasets which
                 are necessary for the calculation of mutual information
                 are obtained with different hardware accelerated
                 imaging operations. For the simultaneous and
                 interactive visualization of the registered datasets a
                 new approach was developed which allows for versatile
                 fusion operations. Using similar procedures supported
                 by hardware, contributes considerably to accelerate
                 registration and visualization. Implementing our
                 approach within a previously presented framework [7,
                 16] based on OpenInventor and OpenGL provides intuitive
                 manipulation. Clinical examples of show the value of
                 our approach in practice.",
  organization = "IEEE Computer Society Press",
  booktitle =    "Proceedings of Computer Graphics International (CGI)",
}

@InProceedings{EVL-1998-310,
  pages =        "16",
  year =         "1998",
  title =        "New Features of HyperPlan, a Hyperthermia Planning
                 System",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-310",
  author =       "Martin Seeba{\ss} and Detlev Stalling and
                 Hans-Christian Hege and Peter Wust and Roland Felix and
                 Peter Deuflhard",
  abstract =     "NEW FEATURES OF HYPERPLAN, A HYPERTHERMIA PLANNING
                 SYSTEM HyperPlan is a software system for treatment
                 planning in regional hyperthermia developed at the
                 Sonderforschungsbereich 273 in Berlin. Recent
                 developments encompass generation of tetrahedral
                 patient models and calculation of electromagnetic
                 fields using the finite element (FE) method. Methods.
                 An essential step in grid generation is the
                 simplification of surfaces which result from
                 segmentation of tomographic images. We developed a
                 simplification method based on an algorithm from
                 computer graphics. The algorithm was extended to avoid
                 surface intersections and to guarantee well-shaped
                 triangles. For field calculation HyperPlan now offers
                 both the finite difference (FDTD) and the FE method.
                 The latter is based an edge elements for field
                 representation and is able to perform an adaptive grid
                 refinement in regions with large field variations.
                 Results. HyperPlan now allows fully automatic
                 generation of tetrahedral patient models. Typically
                 such a model consists of 50.000 - 70.000 elements, and
                 10 - 20 min. CPU time is needed for creation. For field
                 calculation the FE method without grid refinement
                 achieves comparable accuracy to the FDTD method within
                 about 50% of the CPU time. The FE method was used to
                 simulate the new BSD SigmaEye applicator. The results
                 show that the SigmaEye can increase T90 by about 1oC
                 compared to the Sigma60 applicator. Conclusion. The
                 current release of HyperPlan (February, 1998) covers
                 the complete planning procedure. HyperPlan has been
                 intensively tested at VirchowKlinikum, Berlin and has
                 been installed in 3 other clinics. Future work will
                 focus on semi-automatic segmentation methods and the
                 use of MR images which can be acquired in a combined
                 hyperthemia-MRI applicator.",
  booktitle =    "Proc. Hyperthermia in Clinical Oncology",
}

@TechReport{EVL-1998-311,
  year =         "1998",
  title =        "Weighted Labels for 3{D} Image Segmentation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-311",
  author =       "Detlev Stalling and Malte Z{\"o}ckler and Oliver
                 Sander and Hans-Christian Hege",
  abstract =     "Segmentation tools in medical imaging are either based
                 on editing geometric curves or on the assignment of
                 region labels to image voxels. While the first approach
                 is well suited to describe smooth contours at subvoxel
                 accuracy, the second approach is conceptually more
                 simple and guarantees a unique classification of image
                 areas. However, contours extracted from labeled images
                 typically exhibit strong staircase artifacts and are
                 not well suited to represent smooth tissue boundaries.
                 In this paper we describe how this drawback can be
                 circumvented by supplementing region labels with
                 additional weights. We integrated our approach into an
                 interactive segmentation system providing a
                 well-defined set of manual and semi-automatic editing
                 tools. All tools update both region labels as well as
                 the corresponding weights simultaneously, thus allowing
                 one to define segmentation results at high resolution.
                 We applied our techniques to generate 3D polygonal
                 models of anatomical structures",
  keywords =     "image processing, segmentation",
  institution =  "Konrad-Zuse-Zentrum f{\"ur} Informationstechnik (ZIB),
                 Preprint SC 98-39",
}

@TechReport{EVL-1998-312,
  year =         "1998",
  title =        "Visualizing Conformations in Molecular Dynamics",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-312",
  author =       "Christoph Best and Hans-Christian hege",
  abstract =     "The Monte Carlo simulation of the dynamics of complex
                 molecules produces trajectories with a large number of
                 different configurations to sample configuration space.
                 It is expected that these configurations can be
                 classified into a small number of conformations
                 representing essential changes in the shape of the
                 molecule. We present a method to visualize these
                 conformations by point sets in the plane based on a
                 geometrical distance measure between individual
                 configurations. It turns out that different
                 conformations appear as well-separated point sets. The
                 method is further improved by performing a cluster
                 analysis of the data set. The point-cluster
                 representation is used to control a three-dimensional
                 molecule viewer application to show individual
                 configurations and conformational changes. The
                 extraction of essential coordinates and visualization
                 of molecular shape is discussed.",
  keywords =     "Molecular Dynamics, Conformations, Visualization,
                 Cluster Analysis",
  institution =  "Konrad-Zuse-Zentrum f{\"ur} Informationstrechnik
                 Berlin (ZIB), Preprint SC 98-42",
}

@InProceedings{EVL-1998-313,
  pages =        "25--28",
  year =         "1998",
  title =        "Boundary Surface Shrinking - a Continuous Approach to
                 3{D} Center Line Extraction",
  author =       "Hartmut Schirmacher and Malte Z{\"o}ckler and Detlev
                 Stalling and Hans-Christian Hege",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-313",
  abstract =     "We present a new algorithm for efficient and robust
                 approximation of skeletons and construction of
                 connected center line graphs from 3D image data. The
                 algorithm is based on the idea of shrinking the
                 boundary surface along the gradients of the object's
                 distance map. After transforming the surface in that
                 way, duplicate vertices and line segments are
                 eliminated. If the skeleton contains no medial
                 surfaces, a graph representation of the centerline can
                 directly be extracted from the remaining edges. The
                 algorithm has proved to perform well on several
                 different datasets.",
  editor =       "Bernd Girod and Heinrich Niemann and Hans-Peter
                 Seidel",
  booktitle =    "Proceedings of IMDSP'98",
}

@Booklet{EVL-1998-314,
  year =         "1998",
  title =        "VideoMath-Festival at {ICM} '98",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-314",
  author =       "H.-C. Hege and Konrad Polthier",
  howpublished = "VHS Video, 76 min, Booklet, 22 pp., ISBN 3-540-92633-X
                 (NTSC), ISBN 3-540-92634-8 (PAL)",
}

@Article{EVL-1998-315,
  pages =        "3295--3507",
  year =         "1998",
  title =        "Evaluation of segmentation algorithms for generation
                 of patient models in radiofrequency hyperthermia",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-315",
  author =       "P. Wust and J. Gellermann and J. Beier and S. Wegner
                 and W. Tilly and J. Tr{\"o}ger and D. Stalling and H.
                 Oswald and H.-C. Hege and P. Deuflhard and R. Felix",
  volume =       "43",
  number =       "11",
  journal =      "Phys. Med. Biol",
}

@TechReport{EVL-1998-316,
  year =         "1998",
  title =        "A simple {C}++ library for manipulating scientic data
                 sets as structured data",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-316",
  author =       "Christoph Best",
  abstract =     "Representing scientific data sets efficiently on
                 external storage usually involves converting them to a
                 byte string representation using specialized
                 reader/writer routines. The resulting storage files are
                 frequently difficult to interpret without these
                 specialized routines as they do not contain information
                 about the logical structure of the data. Avoiding such
                 problems usually involves heavy-weight data format
                 libraries or data base systems. We present a simple C++
                 library that allows to create and access data files
                 that store structured data. The structure of the data
                 is described by a data type that can be built from
                 elementary data types (integer and floating-point
                 numbers, byte strings) and composite data types
                 (arrays, structures, unions). An abstract data access
                 class presents the data to the application. Different
                 actual data file structures can be implemented under
                 this layer. This method is particularly suited to
                 applications that require complex data structures, e.g.
                 molecular dynamics simulations. Extensions such as late
                 type binding and object persistence are discussed.",
  keywords =     "scientific file formats, structured data, object
                 persistence",
  number =       "TR-98-06",
  institution =  "Konrad-Zuse-Zentrum f{\"u}r Informationstechnik
                 (ZIB)",
}

@InProceedings{EVL-1998-317,
  pages =        "19--26",
  year =         "1998",
  title =        "Large Scale Terrain Visualization Using The Restricted
                 Quadtree Triangulation",
  author =       "Renato B. Pajarola",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-317",
  abstract =     "Real-time rendering of triangulated surfaces has
                 attracted growing interest in the last few years.
                 However, interactive visu alization of very large scale
                 grid digital elevation models is still a hard problem.
                 The graphics load must be controlled by an adaptive
                 surface triangulation and by taking advantage of dif
                 ferent levels of detail. Furthermore, the management of
                 the visi ble scene requires efficient access to the
                 terrain database. We describe a all-in-one
                 visualization system which integrates adaptive
                 triangulation, dynamic scene management and spatial
                 data handling. The triangulation model is based on the
                 restricted quadtree triangulation. Furthermore, we
                 present new algorithms of the restricted quadtree
                 triangulation. These include among others exact error
                 approximation, progressive meshing, performance
                 enhancements and spatial access.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-318,
  pages =        "27--34",
  year =         "1998",
  title =        "Contour Interpolation and Surface Reconstruction of
                 Smooth Terrain Models",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-318",
  author =       "Jianyun Chai and Takaharu Miyoshi and Eihachiro
                 Nakamae",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-319,
  pages =        "35--42",
  year =         "1998",
  title =        "Smooth View-Dependent Level-of-Detail Control and its
                 Application to Terrain Rendering",
  author =       "Hugues H. Hoppe",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-319",
  abstract =     "The key to real-time rendering of large-scale surfaces
                 is to locally adapt surface geometric complexity to
                 changing view parameters. Several schemes have been
                 developed to address this problem of view-dependent
                 level-of-detail control. Among these, the
                 view-dependent progressive mesh (VDPM) framework
                 represents an arbitrary triangle mesh as a hierarchy of
                 geometrically optimized refinement transformations,
                 from which accurate approximating meshes can be
                 efficiently retrieved. In this paper we extend the
                 general VDPM framework to provide temporal coherence
                 through the runtime creation of geomorphs. These
                 geomorphs eliminate {"}popping{"} artifacts by smoothly
                 interpolating geometry. Their implementation requires
                 new output-sensitive data structures, which have the
                 added benefit of reducing memory use. We specialize the
                 VDPM framework to the important case of terrain
                 rendering. To handle huge terrain grids, we introduce a
                 block-based simplification scheme that constructs a
                 progressive mesh as a hierarchy of block refinements.
                 We demonstrate the need for an accurate approximation
                 metric during simplification. Our contributions are
                 highlighted in a real-time flyover of a large, rugged
                 terrain. Notably, the use of geomorphs results in
                 visually smooth rendering even at 72 frames/sec on a
                 graphics workstation.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-32,
  year =         "1998",
  title =        "Fast Analysis of Intracranial Aneurysms based on
                 Interactive Direct Volume Rendering and
                 {CT}--Angiography",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-32",
  author =       "P. Hastreiter and C. Rezk--Salama and B. Tomandl and
                 K. Eberhardt and T. Ertl",
  language =     "en",
  booktitle =    "Proceedings of Conference on Medical Image Computing
                 and Compter--Assisted Intervention (MICCAI)",
}

@InProceedings{EVL-1998-320,
  pages =        "43--50",
  year =         "1998",
  title =        "Efficient Implementation of Multi-Triangulations",
  author =       "Leila De Floriani and Paola Magillo and Enrico Puppo",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-320",
  abstract =     "The Multi-Triangulation (MT) is a general framework
                 for managing the Level-of-Detail in large triangle
                 meshes, which we have introduced in our previous work.
                 In this paper, we describe an efficient implementation
                 of an MT based on vertex decimation. We present general
                 techniques for querying an MT, which are independent of
                 a specific application, and which can be applied for
                 solving problems, such as selective refinement,
                 windowing, point location, and other spatial
                 interference queries. We describe alternative data
                 structures for encoding an MT, which achieve different
                 trade-offs between space and performance. Experimental
                 results are discussed.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-321,
  pages =        "51--58",
  year =         "1998",
  title =        "Visualization of Scalar Topology for Structural
                 Enhancement",
  author =       "Chandrajit L. Bajaj and Valerio Pascucci and Daniel
                 Schikore",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-321",
  abstract =     "Scalar fields arise in every scientific application.
                 Existing scalar visualization techniques require that
                 the user infer the global scalar structure from what is
                 frequently an insufficient display of information. We
                 present a visualization technique which numerically
                 detects the structure at all scales, removing from the
                 user the responsibility of extracting information
                 implicit in the data, and presenting the structure
                 explicitly for analysis. We further demonstrate how
                 scalar topology detection proves useful for correct
                 visualization and image processing applications such as
                 image co-registration, isocontouring, and mesh
                 compression.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Scientific Visualization, Scalar Fields, Curves and
                 Surfaces, Vector Topology",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-322,
  pages =        "59--66",
  year =         "1998",
  title =        "A General Method for Recovering Attribute Values on
                 Simplified Meshes",
  author =       "Paolo Cignoni and Claudio Montani and Claudio Rocchini
                 and Roberto Scopigno",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-322",
  abstract =     "Many sophisticated solutions have been proposed to
                 reduce the geometric complexity of 3D meshes. A less
                 studied problem is how to preserve on a simplified mesh
                 the detail (e.g. color, high frequency shape detail,
                 scalar fields, etc.) which is encoded in the original
                 mesh. We present a general approach for preserving
                 detail on simplified meshes. The detail (or high
                 frequency information) lost after simplification is
                 encoded through texture or bump maps. The original
                 contribution is that preservation is performed after
                 simplification, by building set of triangular texture
                 patches that are then packed in a single texture map.
                 Each simplified mesh face is sampled to build the
                 associated triangular texture patch; a new method for
                 storing this set of texture patches into a standard
                 rectangular texture is presented and discussed. Our
                 detail preserving approach makes no assumptions about
                 the simplification process adopted to reduce mesh
                 complexity and allows highly efficient rendering. The
                 solution is very general, allowing preservation of any
                 attribute value defined on the high resolution mesh. We
                 also describe an alternative application: the
                 conversion of 3D models with 3D static procedural
                 textures into standard 3D models with 2D textures.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-323,
  pages =        "67--72",
  year =         "1998",
  title =        "Surface Reconstruction with Anisotropic Density-Scaled
                 Alpha Shapes",
  author =       "Marek Teichmann and Michael Capps",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-323",
  abstract =     "Generation of a three-dimensional model from an
                 unorganized setof points is an active area of research
                 in computer graphics. Alpha shapes can be employed to
                 construct a surface which most closelyreflects the
                 object described by the points. However, no
                 ff-shape,for any value of ff, can properly detail
                 discontinuous regions of amodel. We introduce herein
                 two methods of improving the results of reconstruction
                 using ff-shapes: density-scaling, which modulatesthe
                 value of ff depending on the density of points in a
                 region; andanisotropic shaping, which modulates the
                 form of the ff-ball basedon point normals. We give
                 experimental results that show the successes and
                 limitations of our method.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-324,
  pages =        "73--78",
  year =         "1998",
  title =        "Level of Detail Visualization of Scalar Data Sets on
                 Irregular Surface Meshes",
  author =       "Georges-Pierre Bonneau and Alexandre Gerussi",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-324",
  abstract =     "In this article, we build a multi-resolution framework
                 intended tobe used for the visualization of continuous
                 piecewise linear functions defined over triangular
                 planar or spherical meshes. In partic-ular, the dataset
                 can be viewed at different level of detail, that's to
                 say as a piecewise linear function defined over any
                 simplification ofthe base mesh. In his multi-resolution
                 form, the function requires strictly the same volume of
                 data than the original input: It is thenpossible to go
                 through consecutive levels by the use of so-called
                 detail coefficients, with exact reconstruction if
                 desired. We also showhow to choose a decimation
                 sequence that leads to a good compromise between the
                 resulting approximation error and the number ofremoved
                 vertices. The theoretical tools used here are inspired
                 from wavelet-based techniques and extended in the sense
                 that they canhandle non-nested approximation spaces.
                 The reader might also refer to [2], where a similar
                 framework is discussedfor piecewise con-stant
                 functions.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "wavelets, non-regular triangulations,compression,
                 visualization.",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-325,
  pages =        "79--86",
  year =         "1998",
  title =        "Tracking Features in Unstructured Datasets",
  author =       "Deborah Silver and Xin Wang",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-325",
  abstract =     "3D time-varying unstructured and structured datasets
                 are difficult tovisualize and analyze because of the
                 immense amount of data involved. These datasets contain
                 many evolving amorphous regions,and standard
                 visualization techniques provide no facilities to aid
                 the scientist to follow regions of interest. In this
                 paper, we present abasic framework for the
                 visualization of time-varying datasets, and a new
                 algorithm and data structure to track volume features
                 in un-structured scalar datasets. The algorithm and
                 data structure are general and can be used for
                 structured, curvilinear, adaptive and hybridgrids as
                 well. The features tracked can be any type of connected
                 regions. Examples are shown from ongoing research",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Scientific Visualization, Time-varying
                 Visualization,Feature Tracking, Computer Vision, CFD",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-326,
  pages =        "87--94",
  year =         "1998",
  title =        "Feature Detection in Linked Derived Spaces",
  author =       "Chris Henze",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-326",
  abstract =     "This paper describes by example a strategy for
                 plotting and interact-ing with data in multiple metric
                 spaces. The example system was designed for use with
                 time-varying computational fluid dynamics(CFD)
                 datasets, but the methodology is directly applicable to
                 other types of field data. The central objects embodied
                 by the tool areportraits, which show the data in
                 various coordinate systems, while preserving their
                 spatial connectivity and temporal variability.
                 Thecoordinates are derived in various ways from the
                 field data, and an important feature is that new and
                 derived portraits can be cre-ated interactively. The
                 primary operations supported by the tool are brushing
                 and linking: the user can select a subset of a given
                 por-trait, and this subset is highlighted in all
                 portraits. The user can combine highlighted subsets
                 from an arbitrary number of portraitswith the usual
                 logical operators, thereby indicating where an
                 arbitrarily complex set of conditions holds. The system
                 is useful forexploratory visualization and feature
                 detection in multivariate data.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "computational fluid dynamics, feature detection,
                 flowvisualization, multivariate visualization,
                 brushing",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-327,
  pages =        "95--102",
  year =         "1998",
  title =        "Extremal Feature Extraction from 3-{D} Vector and
                 Noisy Scalar Fields",
  author =       "Chi-Keung Tang and G{\'{e}}rard G. Medioni",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-327",
  abstract =     "We are interested in feature extraction from volume
                 data in terms ofcoherent surfaces and 3-D space curves.
                 The input can be an inaccurate scalar or vector field,
                 sampled densely or sparsely on a reg-ular 3-D grid, in
                 which poor resolution and the presence of spurious
                 noisy samples make traditional iso-surface techniques
                 inappro-priate. In this paper, we present a
                 general-purpose methodology to extract surfaces or
                 curves from a digital 3-D potential vector field
                 f(s;v)g, in which each voxel holds a scalar s
                 designating strength,and a vector v indicating
                 direction. For scalar, sparse or low resolution data,
                 we {"}vectorize{"} and {"}densify{"} the volume by
                 Tensor Votingto produce dense vector fields suitable as
                 input to our algorithms, the Extremal Surface and Curve
                 Algorithms. Both algorithms ex-tract, with sub-voxel
                 precision, coherent features representing local extrema
                 in the given vector field. These coherent features are
                 a hole-free triangulation mesh (in the surface case),
                 and a set of connected, oriented, and non-intersecting
                 polyline segments (in the curve case).We demonstrate
                 the general usefulness of both extremal algorithms on a
                 variety of real data by properly extracting their
                 inherent ex-tremal properties, such as (a) shock waves
                 induced by abrupt velocity or direction changes in a
                 flow field, (b) interacting vortex coresand vorticity
                 lines in a velocity field, (c) crestlines and ridges
                 implicit in a digital terrain map, and (d) grooves,
                 anatomical lines andcomplex surfaces from noisy dental
                 data.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Surface and curve extremality, surface fitting,
                 scalarand vector field visualization, Marching Cubes",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-328,
  pages =        "103--110",
  year =         "1998",
  title =        "Feature Comparisons of Vector Fields using Earth
                 Mover's Distance",
  author =       "Yingmei Lavin and Rajesh Kumar Batra and Lambertus
                 Hesselink",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-328",
  abstract =     "In this paper, a novel approach is introduced to
                 define a quantitative measure of closeness between
                 vector fields. The usefulness of this measurement can
                 be seen when comparing computational and experimental
                 flow fields under the same conditions. Furthermore, its
                 applicability can be extended to more cumbersome tasks
                 such as navigating through a large database searching
                 for similar topology. This new measure relies on the
                 use of critical points, which are a key feature in
                 vector field topology. In order to characterize
                 critical points, ff and fi parameters are introduced.
                 They are used to form a closed set of eight unique
                 patterns for simple critical points. These patterns are
                 also basic building blocks for higher order nonlinear
                 vector fields. In order to study and compare a given
                 set of vector fields, a measure of distance between
                 different patterns of critical points is introduced.
                 The basic patterns of critical points are mapped onto a
                 unit circle in ff - fi space. The concept of Earth
                 Mover's Distance [1] [2] is used to compute the
                 closeness between various pairs of vector fields, and a
                 nearest-neighbor query is thus produced to illustrate
                 the relationship between the given set of vector
                 fields. This approach quantitatively measures the
                 similarity and dissimilarity between vector fields. It
                 is ideal for data compression of a large flow field,
                 since only the number and types of critical points
                 along with their corresponding ff and fi parameters are
                 necessary to reconstruct the whole field. It can also
                 be used to better quantify the changes in time varying
                 data sets.",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-329,
  pages =        "111--118",
  year =         "1998",
  title =        "Building Perceptual Textures to Visualize
                 Multidimensional Datasets",
  author =       "Christopher G. Healey and James T. Enns",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-329",
  abstract =     "This paper presents a new method for using texture to
                 visualize multidimensional data elements arranged on an
                 underlying three-dimensional surface. We hope to use
                 simple texture patterns in combination with other
                 visual features like hue and intensity to increase the
                 number of attribute values we can display
                 simultaneously. Our technique builds perceptual texture
                 elements (or pexels) to represent each data element.
                 Attribute values encoded in the data element are used
                 to vary the appearance of a corresponding pexel.
                 Texture patterns that form when the pexels are
                 displayed can be used to rapidly and accurately explore
                 the dataset. Our pexels are built by controlling three
                 separate texture dimensions: height, density, and
                 regularity. Results from computer graphics, computer
                 vision, and cognitive psychology have identified these
                 dimensions as important for the formation of perceptual
                 texture patterns. We conducted a set of controlled
                 experiments to measure the effectiveness of these
                 dimensions, and to identify any visual interference
                 that may occur when all three are displayed
                 simultaneously at the same spatial location. Results
                 from our experiments show that these dimensions can be
                 used in specific combinations to form perceptual
                 textures for visualizing multidimensional datasets. We
                 demonstrate the effectiveness of our technique by
                 applying it to the problem of visualizing ocean and
                 atmospheric conditions on a topographical map of
                 eastern Asia during the summer typhoon season.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "computer graphics, experimental design, human vision,
                 multidimensional dataset, perception, preattentive
                 processing, scientific visualization, texture,
                 typhoon",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-33,
  year =         "1998",
  title =        "Efficient Representation of Cortical Convolutions for
                 the Analysis of Brain Surface Topology",
  author =       "P. Hastreiter and Ch. Rezk-Salama and G. Greiner and
                 T. Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-33",
  language =     "en",
  abstract =     "Various time efficient procedures were developed
                 allowing to calculate planar representations of the
                 brain in MR and CT clearly conveying the whole surface
                 topology. For the comparison of the provided techniques
                 we present additional complex functionality for the
                 transformation of cortical convolutions between
                 different representations after extracting and marking
                 them manually or automatically. This includes
                 re-projection to the original volume data in order to
                 compare our approach to results obtained with direct
                 volume rendering. Considering brain information
                 exclusively, and ensuring a standardized orientation
                 for the interpatient comparison different segmentation
                 and registration procedures are provided for the
                 preprocessing. All implementation was integrated in a
                 flexible and modular extensible platform allowing for
                 convenient manipulation and visualization.",
  editor =       "T. Lehman and V. Metzler and K. Spitzer and T.
                 Tolxdorff",
  keywords =     "cortical convolutions, visualization, segmentation,
                 registration",
  booktitle =    "Bildverarbeitung f{\"{u}}r die Medizin: Algorithmen,
                 Systeme, Anwendungen",
  publisher =    "Springer",
}

@InProceedings{EVL-1998-331,
  pages =        "127--134",
  year =         "1998",
  title =        "Visualizing Diffusion Tensor Images of the Mouse
                 Spinal Cord",
  author =       "David H. Laidlaw and Eric T. Ahrens and David Kremers
                 and Matthew J. Avalos and Russell E. Jacobs and Carol
                 Readhead",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-331",
  abstract =     "Within biological systems water molecules undergo
                 continuousstochastic Brownian motion. The rate of this
                 diffusion can give clues to the structure of underlying
                 tissues. In some tissues the rateis anisotropic -
                 faster in some directions than others. Diffusionrate
                 images are second-order tensor fields and can be
                 calculatedfrom diffusion-weighted magnetic resonance
                 images. A 2D diffusion tensor image (DTI) and an
                 associated anatomical scalar field,created during the
                 tensor calculation, define seven values at each spatial
                 location. Visually representing these images is a
                 challengebecause they contain so many inter-related
                 components. We present two new methods for visually
                 representing DTIs. The first methoddisplays an array of
                 ellipsoids where the shape of each ellipsoid represents
                 one tensor value. The novel aspect of this
                 representa-tion is that the ellipsoids are all
                 normalized to approximately the same size so that they
                 can be displayed simultaneously in con-text. The second
                 method uses concepts from oil painting to represent the
                 seven-valued data with multiple layers of varying
                 brushstrokes. Both methods successfully display most or
                 all of the information in DTIs and provide exploratory
                 methods for understandingthem. The ellipsoid method has
                 a simpler interpretation and explanation than the
                 painting-motivated method; the painting-motivatedmethod
                 displays more of the information and is easier to read
                 quantitatively. We demonstrate the methods on images of
                 the mousespinal cord. The visualizations show
                 significant differences between spinal cords from mice
                 suffering from Experimental AllergicEncephalomyelitis
                 (EAE) and spinal cords from wild-type mice. The
                 differences are consistent with differences shown
                 histologicallyand suggest that our new non-invasive
                 imaging methodology and visualization of the results
                 could have early diagnostic value forneurodegenerative
                 diseases.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "multi-valued visualization, tensor field
                 visualization,oil painting",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-332,
  pages =        "135--142",
  year =         "1998",
  title =        "Image-Guided Streamline Placement on Curvilinear Grid
                 Surfaces",
  author =       "Xiaoyang Mao and Yuji Hatanaka and Hidenori Higashida
                 and Atsumi Imamiya",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-332",
  abstract =     "The success of using streamline technique for
                 visualizing a vector field usually depends largely on
                 the choice of adequate seed points. Turk and Banks
                 developed an elegant technique for automatically
                 placing seed points to achieve a uniform distribution
                 of streamlines on a 2D vector field. Their method uses
                 an energy function calculated from the low-pass
                 filtered streamline image to guide the optimization
                 process of the streamline distribution. This paper
                 proposes a new technique for creating evenly
                 distributed streamlines on 3D parametric surfaces found
                 in curvilinear grids. We make use of Turk and Banks's
                 2D algorithm by first mapping the vectors on a 3D
                 surface into the computational space of the curvilinear
                 grid. To take into the consideration the mapping
                 distortion caused by the uneven grid density in a
                 curvilinear grid, a new energy function is designed and
                 used for guiding the placement of streamlines in the
                 computational space with desired local densities.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Vector field visualization, flow visualization,
                 streamline, curvilinear grid",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-333,
  pages =        "143--150",
  year =         "1998",
  title =        "A Higher-Order Method For Finding Vortex Core Lines",
  author =       "Martin Roth and Ronald Peikert",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-333",
  abstract =     "This paper presents a novel method to extract vortical
                 structures from 3D CFD vector field automatically. It
                 discusses the underlying theory and some aspects of the
                 implementation. Making use of higher-order derivatives,
                 the method is able to locate bent vortices. In order to
                 structure the recognition procedure, we distinguish
                 locating the core line from calculating attributes of
                 strength and quality. Results are presented on several
                 flow fields from the field of turbomachinery.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-334,
  pages =        "151--158",
  year =         "1998",
  title =        "Automatic Detection of Open and Closed Separation and
                 Attachment Lines",
  author =       "David N. Kenwright",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-334",
  abstract =     "A fully automatic feature detection algorithm is
                 presented that locates, and distinguishes, lines of
                 flow separation and attachment on surfaces in 3-D
                 numerical flow fields. The algorithm is based on
                 concepts from 2-D phase plane analysis of linear vector
                 fields. Unlike prior visualization techniques based on
                 particle tracing or flow topology, the phase plane
                 algorithm detects separation using local analytic
                 tests. Results show that it not only detects the
                 standard closed separation lines but also the illusive
                 open separation lines which are not captured by flow
                 topology methods.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-335,
  pages =        "159--166",
  year =         "1998",
  title =        "Isosurface Extraction in Time-Varying Fields Using a
                 Temporal Hierarchical Index Tree",
  author =       "Han-Wei Shen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-335",
  abstract =     "Many high-performance isosurface extraction algorithms
                 havebeen proposed in the past several years as a result
                 of intensive research efforts. When applying these
                 algorithms to large-scaletime-varying fields, the
                 storage overhead incurred from storing the search index
                 often becomes overwhelming. This paper proposesan
                 algorithm for locating isosurface cells in time-varying
                 fields. We devise a new data structure, called Temporal
                 HierarchicalIndex Tree, which utilizes the temporal
                 coherence that exists in a time-varying field and
                 adaptively coalesces the cells' extremevalues over
                 time; the resulting extreme values are then used to
                 create the isosurface cell search index. For a typical
                 time-varyingscalar data set, not only does this
                 temporal hierarchical index tree require much less
                 storage space, but also the amount of I/Orequired to
                 access the indices from the disk at different time
                 steps is substantially reduced. We illustrate the
                 utility and speed ofour algorithm with data from
                 several large-scale time-varying CFD simulations. Our
                 algorithm can achieve more than 80% ofdisk-space
                 savings when compared with the existing techniques,
                 while the isosurface extraction time is nearly
                 optimal.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "scalar field visualization, volume visualization,
                 isosur-face extraction, time-varying fields, marching
                 cubes, span space",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-336,
  pages =        "167--174",
  year =         "1998",
  title =        "Interactive Out-Of-Core Isosurface Extraction",
  author =       "Yi-Jen Chiang and Cl{\'{a}}udio T. Silva and William
                 J. Schroeder",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-336",
  abstract =     "In this paper, we present a novel out-of-core
                 technique for the interactive computation of
                 isosurfaces from volume data. Our algorithm minimizes
                 the main memory and disk space requirements on the
                 visualization workstation, while speeding up isosurface
                 extraction queries. Our overall approach is a two-level
                 indexing scheme. First, by our meta-cell technique, we
                 partition the original dataset into clusters of cells,
                 called meta-cells. Secondly, we produce meta-intervals
                 associated with the meta-cells, and build an indexing
                 data structure on the meta-intervals. We separate the
                 cell information, kept only in meta-cells in disk, from
                 the indexing structure, which is also in disk and only
                 contains pointers to meta-cells. Our meta-cell
                 technique is an I/O-efficient approach for computing a
                 k-d-tree-like partition of the dataset. Our indexing
                 data structure, the binary-blocked I/O interval tree,
                 is a new I/O-optimal data structure to perform stabbing
                 queries that report from a set of meta-intervals (or
                 intervals) those containing a query value q. Our tree
                 is simpler to implement, and is also more
                 space-efficient in practice than the existing
                 structures. To perform an isosurface query, we first
                 query the indexing structure, and then use the reported
                 meta-cell pointers to read from disk the active
                 meta-cells intersected by the isosurface. The
                 isosurface itself can then be generated from active
                 meta-cells. Rather than being a single-cost indexing
                 approach, our technique exhibits a smooth trade-off
                 between query time and disk space.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Isosurface Extraction, Marching Cubes,
                 Out-Of-CoreComputation, Interval Tree, Scientific
                 Visualization",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-337,
  pages =        "175--180",
  year =         "1998",
  title =        "View Dependent Isosurface Extraction",
  author =       "Yarden Livnat and Charles Hansen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-337",
  abstract =     "We propose a new approach to polygonal isosurface
                 extraction thatis based on extracting only the visible
                 portion of the isosurface. The visibility tests are
                 done in two phases. First, coarse visibility testsare
                 performed in software to determine the visible cells.
                 These tests are based on hierarchical tiles and
                 shear-warp factorization. Thesecond phase resolves the
                 visible portions of the extracted triangles and is
                 accomplished by the graphics hardware. While the latest
                 isosurface extraction methods have
                 effectivelyeliminated the search phase bottleneck, the
                 cost of constructing and rendering the isosurface
                 remains high. Many of today's largedatasets contain
                 very large and complex isosurfaces that can easily
                 overwhelm even state-of-the-art graphics hardware. The
                 proposedapproach is output sensitive and is thus well
                 suited for remote visualization applications where the
                 extraction and rendering phasesare done on a separate
                 machines.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@TechReport{EVL-1998-34,
  year =         "1998",
  title =        "Smallest Enclosing Circles -- An Exact and Generic
                 Implementation in {C++}",
  type =         "Serie B -- Informatik",
  author =       "Bernd G{\"a}rtner and Sven Sch{\"o}nherr",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-34",
  language =     "en",
  abstract =     "We present a C++ implementation of an optimisation
                 algorithm for computing the smallest (w.r.t. area)
                 enclosing circle of a finite point set in the plane.
                 The algorithm is implemented as a semi-dynamic data
                 structure, thus allowing to insert points while
                 maintaining the smallest enclosing circle. Following
                 the generic programming paradigm, we use the template
                 feature of C++ to provide generic code. The data
                 structure is parameterized with a traits class, that
                 defines the abstract interface between the optimisation
                 algorithm and the primitives it uses. The interface of
                 the data structure is compliant with the STL.",
  month =        apr,
  number =       "B 98-04",
  institution =  "Institut f{\"u}r Informatik, Freie Universit{\"a}t
                 Berlin, Germany",
}

@InProceedings{EVL-1998-341,
  pages =        "205--210",
  year =         "1998",
  title =        "A Concept for Virtual Reality Tools For Design
                 Reviews",
  author =       "Klaus Kremer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-341",
  abstract =     "The following paper discusses a concept for virtual
                 reality tools for use in design reviews of mechanical
                 products. In this discussion, the special requirements
                 of a virtual environment are given consideration. The
                 focus of this paper is on suggestions for the
                 visualization and arrangement of a product, its
                 structure, its components and their alternatives
                 together in one environment. The realization of these
                 concepts results in an 3D-interface that allows users,
                 especially engineers, to evaluate different
                 configurations of a product and gives them direct
                 access to the product structure. By applying various
                 visualization techniques, product components and their
                 attributes, e.g., their price, can be brought together
                 into one visualization. Thus, in contrast to
                 state-of-the-art software, the product structure,
                 three-dimensional, real-sized components, and attribute
                 values can be combined together in 3D-visualizations.
                 This research was done in cooperation with Christoph
                 Brandt, member of the Heinz Nixdorf's Institute's
                 virtual reality group.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Virtual Reality, virtual environments, visualization,
                 design reviews, product configuration, product
                 structures, product attributes, CAD, PDM, EDM",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-342,
  pages =        "211--216",
  year =         "1998",
  title =        "Efficient Warping for Architectural Walkthroughs using
                 Layered Depth Images",
  author =       "Voicu S. Popescu and Anselmo Lastra and Daniel G.
                 Aliaga and Manuel M. de Oliveira Neto",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-342",
  abstract =     "This paper presents efficient image-based rendering
                 techniquesused in the context of an architectural
                 walkthrough system. Portals (doors and windows) are
                 rendered by warping layered depthimages (LDIs). In a
                 preprocessing phase, for every portal, a number of
                 pre-rendered images are combined into a LDI.
                 Theresulting LDI stores, exactly once, all surfaces
                 visible in at least one of the images used in the
                 construction, so most of theexposure errors are
                 efficiently eliminated. The LDI can be warped in the
                 McMillan occlusion compatible ordering. A
                 substantialincrease in performance is obtained by
                 warping in parallel. Our parallelization scheme
                 achieves good load balancing, scales withthe number of
                 processors, and preserves the occlusion compatible
                 ordering. A fast, conservative reference-image-space
                 clippingalgorithm also reduces the warping effort",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "image-based rendering, parallel warping,occlusion
                 compatible ordering for discrete images, portal, cell,
                 exposure error, layered depth image, clipping,
                 architecturalwalkthrough",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-344,
  pages =        "225--232",
  year =         "1998",
  title =        "A Distributed Blackboard Architecture for Interactive
                 Data Visualization",
  author =       "Robert van Liere and Jan A. Harkes and Wim C. de
                 Leeuw",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-344",
  abstract =     "In this paper the motivation, design and application
                 of a distribut-ed blackboard architecture for
                 interactive data visualization is discussed. The main
                 advantages of the architecture is twofold. First,
                 itallows visualization tools to be tightly integrated
                 with simulations. Second, it allows qualitative and
                 quantitative analysis to be com-bined during the
                 visualization process.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-345,
  pages =        "233--238",
  year =         "1998",
  title =        "Interactive Ray Tracing for Isosurface Rendering",
  author =       "Steven Parker and Peter Shirley and Yarden Livnat and
                 Charles Hansen and Peter-Pike Sloan",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-345",
  abstract =     "We show that it is feasible to perform interactive
                 isosurfacing of very large rectilinear datasets with
                 brute-force ray tracing on a conventional (distributed)
                 shared-memory multiprocessor machine. Rather than
                 generate geometry representing the isosurface and
                 render with a z-buffer, for each pixel we trace a ray
                 through a volume and do an analytic isosurface
                 intersection computation. Although this method has a
                 high intrinsic computational cost, its simplicity and
                 scalability make it ideal for large datasets on current
                 high-end systems. Incorporating simple optimizations,
                 such as volume bricking and a shallow hierarchy,
                 enables interactive rendering (i.e. 10 frames per
                 second) of the 1GByte full resolution Visible Woman
                 dataset on an SGI Reality Monster. The graphics
                 capabilities of the reality monster are used only for
                 display of the final color image.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-346,
  pages =        "239--246",
  year =         "1998",
  title =        "Eliminating Popping Artifacts in Sheet Buffer-Based
                 Splatting",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-346",
  author =       "Klaus Mueller and Roger Crawfis",
  abstract =     "Splatting is a fast volume rendering algorithm which
                 achievesits speed by projecting voxels in the form of
                 pre-integrated interpolation kernels, or splats.
                 Presently, two main variants of the splat-ting
                 algorithm exist: (i) the original method, in which all
                 splats are composited back-to-front, and (ii) the
                 sheet-buffer method, in which the splats are added in
                 cache-sheets, aligned with the volume face most
                 parallel to the image plane, which are subsequently
                 composited back-to-front. The former method is prone to
                 cause bleeding artifacts from hidden objects, while the
                 latter method reduces bleeding, but causes very visible
                 color popping artifacts when the orientation of the
                 compositing sheets changes suddenly as the image screen
                 becomes more parallel to another volume face. We
                 present a new variant of the splatting algorithm in
                 which the compositing sheets are always parallel to the
                 image plane, eliminating the condition for popping,
                 while maintaining the insensitiv-ity to color bleeding.
                 This enables pleasing animated viewing of volumetric
                 objects without temporal color and lighting
                 discontinuities. The method uses a hierarchy of partial
                 splats and employs an efficient list-based volume
                 traversal scheme for fast splat access. It also offers
                 more accuracy for perspective splatting as the
                 decomposition of the individual splats facilitates a
                 better approximation to the diverging nature of the
                 rays that traverse the splatting kernels.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-347,
  pages =        "247--254",
  year =         "1998",
  title =        "Accelerated Ray-Casting for Curvilinear Volumes",
  author =       "Lichan Hong and Arie E. Kaufman",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-347",
  abstract =     "We present an efficient and robust ray-casting
                 algorithm for directly rendering a curvilinear volume
                 of arbitrarily-shaped cells. We designed the algorithm
                 to alleviate the consumption of CPU power and memory
                 space. By incorporating the essence of the projection
                 paradigm into the ray-casting process, we have
                 successfully accelerated the ray traversal through the
                 grid and data interpolations at sample points. Our
                 algorithm also overcomes the conventional limitation
                 requiring the cells to be convex. Application of this
                 algorithm to several commonly-used curvilinear data
                 sets has produced a favorable performance when compared
                 with recently reported algorithms",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Volume Visualization, Volume Rendering, Irregular
                 Grid, Curvilinear Grid, Ray-Casting, Parallel
                 Rendering, Dynamic Simulation",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-348,
  pages =        "255--262",
  year =         "1998",
  title =        "High Quality Rendering of Attributed Volume Data",
  author =       "Ulf Tiede and Thomas Schiemann and Karl Heinz
                 H{\"{o}}hne",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-348",
  abstract =     "For high quality rendering of objects segmented from
                 tomographic volume data the precise location of the
                 boundaries of adjacent objects in subvoxel resolution
                 is required. We describe a new method that determines
                 the membership of a given sample point to an object by
                 reclassifying the sample point using interpolation of
                 the original intensity values and searching for the
                 best fitting object in the neighbourhood. Using a
                 ray-casting approach we then compute the surface
                 location between successive sample points along the
                 viewing-ray by interpolation or bisection. The accurate
                 calculation of the object boundary enables a much more
                 precise computation of the gray-level-gradient yielding
                 the surface normal. Our new approach significantly
                 improves the quality of reconstructed and shaded
                 surfaces and reduces aliasing artifacts for animations
                 and magnified views. We illustrate the results on
                 different cases including the Visible-Human-Data, where
                 we achieve nearly photo-realistic images.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "partial-volume-effect, ray-casting, tomographic data,
                 Visible Human Project",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-349,
  pages =        "263--270",
  year =         "1998",
  title =        "Simplifying Surfaces with Color and Texture using
                 Quadric Error Metrics",
  author =       "Michael Garland and Paul S. Heckbert",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-349",
  abstract =     "There are a variety of application areas in which
                 there is a need for simplifying complex polygonal
                 surface models. These models often have material
                 properties such as colors, textures, and surface
                 normals. Our surface simplification algorithm, based on
                 iterative edge contraction and quadric error metrics,
                 can rapidly produce high quality approximations of such
                 models. We present a natural extension of our original
                 error metric that can account for a wide range of
                 vertex attributes.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "surface simplification, multi-resolution
                 modeling,level of detail, quadric error metric, edge
                 contraction, surface properties, discontinuity
                 preservation",
  booktitle =    "IEEE Visualization '98",
}

@TechReport{EVL-1998-35,
  year =         "1998",
  title =        "Smallest Enclosing Ellipses -- An Exact and Generic
                 Implementation in {C++}",
  type =         "Serie B -- Informatik",
  author =       "Bernd G{\"a}rtner and Sven Sch{\"o}nherr",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-35",
  language =     "en",
  abstract =     "We present a C++ implementation of an optimisation
                 algorithm for computing the smallest (w.r.t. area)
                 enclosing ellipse of a finite point set in the plane.
                 We obtain an exact solution by using Welzl's method
                 together with the primitives as described in report B
                 97-03. The algorithm is implemented as a semi-dynamic
                 data structure, thus allowing to insert points while
                 maintaining the smallest enclosing ellipse. Following
                 the generic programming paradigm, we use the template
                 feature of C++ to provide generic code. The data
                 structure is parameterized with a traits class, that
                 defines the abstract interface between the optimisation
                 algorithm and the primitives it uses. The interface of
                 the data structure is compliant with the STL.",
  month =        apr,
  number =       "B 98-05",
  institution =  "Institut f{\"u}r Informatik, Freie Universit{\"a}t
                 Berlin, Germany",
}

@InProceedings{EVL-1998-350,
  pages =        "271--278",
  year =         "1998",
  title =        "A Unified Approach for Simplifying Polygonal and
                 Spline Models",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-350",
  author =       "M. Gopi and Dinesh Manocha",
  abstract =     "We present a new approach for simplifying models
                 composed of polygons or spline patches. Given an input
                 model, the algorithm computes a new representation of
                 the model in terms of triangular Bezier patches. It
                 performs a series of geometric operations, consisting
                 of patch merging and swapping diagonals, and makes use
                 of patch connectivity information to generate C-LODs
                 (curved levels-of-detail). Each C-LOD is represented
                 using cubic triangular Bezier patches. The C-LOD's
                 provide a compact representation for storing the model.
                 The algorithm tries to minimize the surface deviation
                 error and maintains continuity at patch boundaries.
                 Given the CLOD's, the algorithm can generate their
                 polygonal approximations using static and dynamic
                 tessellation schemes. It has been implemented and we
                 highlight its performance on a number of polygonal and
                 spline models.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "model simplification, levels-of-detail, surface
                 approximation, spline patches, surface fitting, dynamic
                 tessellation",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-352,
  pages =        "287--296",
  year =         "1998",
  title =        "Simplification of Tetrahedral Meshes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-352",
  author =       "Isaac J. Trotts and Bernd Hamann and Kenneth I. Joy
                 and David F. Wiley",
  abstract =     "We present a method for the construction of multiple
                 levels of tetrahedral meshes approximating a trivariate
                 function at different levels of detail. Starting with
                 an initial, high-resolution triangulation of a
                 three-dimensional region, we construct coarser
                 representation levels by collapsing tetrahedra. Each
                 triangulation defines a linear spline function, where
                 the function values associated with the vertices are
                 the spline coefficients. Based on predicted errors, we
                 collapse tetrahedra in the grid that do not cause the
                 maximum error to exceed a user-specified threshold.
                 Bounds are stored for individual tetrahedra and are
                 updated as the mesh is simplified. We continue the
                 simplification process until a certain error is
                 reached. The result is a hierarchical data description
                 suited for the efficient visualization of large data
                 sets at varying levels of detail.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Approximation, hierarchical representation, mesh
                 generation, multi-resolution method, scattered data,
                 spline, triangulation, visualization.",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-353,
  pages =        "297--304",
  year =         "1998",
  title =        "Interactive Deformations from Tensor Fields",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-353",
  author =       "Ed Boring and Alex Pang",
  abstract =     "This paper presents techniques for interactively
                 visualizing tensor fields using deformations. The
                 conceptual idea behind this approach is to allow the
                 tensor field to manifest its influence on idealized
                 objects placed within the tensor field. This is
                 similar, though not exactly the same, to surfaces
                 deforming under load in order to relieve built up
                 stress and strain. We illustrate the effectiveness of
                 the Deviator-Isotropic tensor decomposition in
                 deformation visualizations of CFD strain rate. We also
                 investigate how directional flow techniques can be
                 extended to distinguish between regions of tensile
                 versus compressive forces.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Tensor, stress, strain, shear, normal, directional
                 flow, symmetric, antisymmetric, deviator, isotropic.",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-354,
  pages =        "305--312",
  year =         "1998",
  title =        "Real-Time Techniques for 3{D} Flow Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-354",
  author =       "Anton L. Fuhrmann and Eduard Gr{\"{o}}ller",
  abstract =     "Visualization of three-dimensional steady flow has to
                 overcome a lot of problems to be effective. Among them
                 are occlusion of distant details, lack of directional
                 and depth hints and occlusion. In this paper we present
                 methods which address these problems for real-time
                 graphic representations applicable in virtual
                 environments. We use dash tubes, i.e., animated,
                 opacity-mapped streamlines, as visualization icon for
                 3D-flow visualization. We present a texture mapping
                 technique to keep the level of texture detail along a
                 streamline nearly constant even when the velocity of
                 the flow varies considerably. An algorithm is described
                 which distributes the dash tubes evenly in space. We
                 apply magic lenses and magic boxes as interaction
                 techniques for investigating densely filled areas
                 without overwhelming the observer with visual detail.
                 Implementation details of these methods and their
                 integration in our virtual environment conclude the
                 paper.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Virtual environments, flow visualization, texturing,
                 interaction, magic lens, focussing.",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-357,
  pages =        "327--334",
  year =         "1998",
  title =        "Image-Based Rendering with Occlusions via Cubist
                 Images",
  author =       "Andrew J. Hanson and Eric A. Wernert",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-357",
  abstract =     "We attack the problem of image-based rendering with
                 occlusions and general camera motions by using
                 distorted multi-perspective images; such images provide
                 multiple-viewpoint photometry similar to the paintings
                 of cubist artists. We take scene geometry, in contrast,
                 to be embodied in mappings of viewing rays from their
                 original 3D intercepts into the warped
                 multi-perspective image space. This approach allows us
                 to render approximations of scenes with occlusions
                 using time-dense and spatially sparse sequences of
                 camera rays, which is a significant improvement over
                 the storage requirements of an equivalent animation
                 sequence. Additional data compression can be achieved
                 using sparse time keyframes as well. Interpolating the
                 paths of sparse time key-rays correctly in image space
                 requires singular interpolation functions with spatial
                 discontinuities. While there are many technical
                 questions yet to be resolved, the employment of these
                 singular interpolation functions in the
                 multi-perspective image space appears to be of
                 potential interest for generating general viewpoint
                 scene renderings with minimal data storage.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Image Based Rendering, Occlusions",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-358,
  pages =        "335--342",
  year =         "1998",
  title =        "Hierarchical Volume Analysis and Visualization Based
                 on Morphological Operators",
  author =       "Christoph L{\"{u}}rig and Thomas Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-358",
  abstract =     "One common problem in the practical application of
                 volume visualization is the proper choice of transfer
                 functions in order to color different parts of the
                 volume meaningfully. This interactive process can be
                 very complicated and time consuming. An alternative to
                 the adjustment of transfer functions is the application
                 of segmentation algorithms. These algorithms are often
                 dedicated to a limited range of data sets and tend to
                 be very compute intensive. In this paper we propose a
                 morphology based hierarchical analysis to estimate the
                 optical properties of the volume to be rendered. This
                 approach requires fewer parameters and incorporates
                 also spatial information, but it is far less compute
                 intensive than most of the segmentation methods. The
                 hierarchical analysis is constructed in analogy to the
                 wavelet analysis, except for the fact, that non-linear
                 filters are used in our case. These morphological
                 operators have a lower distortional influence on the
                 analyzed structures than the usual linear filters.",
  organization = "IEEE",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-359,
  pages =        "343--350",
  year =         "1998",
  title =        "Interactive Display of Very Large Textures",
  author =       "David Cline and Parris K. Egbert",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-359",
  abstract =     "Large textures cause bottlenecks in real-time
                 applications that often lead to a loss of
                 interactivity. These performance bottlenecks occur
                 because of disk and network transfer, texture
                 translation, and memory swapping. We present a software
                 solution that alleviates the problems associated with
                 large textures by treating texture as a bandwidth
                 limited resource rather than a finite resource. As a
                 result the display of large textures is reduced to a
                 caching problem in which texture memory serves as the
                 primary cache for texture data, main memory the
                 secondary cache, and local disk the tertiary cache. By
                 using this cache hierarchy, applications are able to
                 maintain real-time performance while displaying
                 textures hundreds of times larger than can fit into
                 texture memory.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Texture Caching, Bandwidth-Limited Resource, Texture
                 Mapping, Real-Time Display, Interactivity",
  booktitle =    "IEEE Visualization '98",
}

@TechReport{EVL-1998-36,
  year =         "1998",
  title =        "Compression Methods for Visualization",
  type =         "Technical Report",
  author =       "Markus H. Gross and Lars Lippert and Oliver G.
                 Staadt",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-36",
  language =     "en",
  abstract =     "Compression methods have become of fundamental
                 importance in almost every subfield of scientific
                 visualization. However, unlike image compression,
                 advanced visualization applications impose manifold
                 constraints on the design of appropriate algorithms,
                 where progressiveness, multiresolution or topology
                 preservation are some of the key issues. This paper
                 demonstrates the importance of multiresolution
                 compression methods for visualization using two
                 examples: The first, compression domain volume
                 rendering, enables one to visualize volume data
                 progressively and instantaneously from its compressed
                 data format and is designed for WWW and networked
                 applications. The second one is a multiresolution
                 compression and reconstruction method that allows for
                 progressive coding, transmission and geometric
                 reconstruction of surfaces and volumes. Both of the
                 presented methods are so-called transform coding
                 schemes and use wavelets for data representation.",
  month =        mar,
  keywords =     "visualization, compression, wavelets, volume
                 rendering",
  number =       "TR 293",
  institution =  "ETH Z{\"u}rich, Institute of Scientific Computing",
}

@InProceedings{EVL-1998-360,
  pages =        "351--358",
  year =         "1998",
  title =        "Pixel Masks for Screen-Door Transparency",
  author =       "Jurriaan D. Mulder and Frans C. A. Groen and Jarke J.
                 van Wijk",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-360",
  abstract =     "Rendering objects transparently gives additional
                 insight in complex and overlapping structures. However,
                 traditional techniques for the rendering of transparent
                 objects such as alpha blending are not very well suited
                 for the rendering of multiple transparent objects in
                 dynamic scenes. Screen-door transparency is a technique
                 to render transparent objects in a simple and efficient
                 way: No sorting is required and intersecting polygons
                 can be handled without further preprocessing. With this
                 technique, polygons are rendered through a mask: Only
                 where the mask is present, pixels are set. However,
                 artifacts such as incorrect opacities and distracting
                 patterns can easily occur if the masks are not
                 carefully designed. In this paper, first the
                 requirements on the masks are considered. Next, three
                 algorithms are presented for the generation of pixel
                 masks. One algorithm is designed for the creation of
                 small (e.g. 4 \Theta 4) masks. The other two algorithms
                 can be used for the creation of larger masks (e.g. 32
                 \Theta 32). For each of these algorithms results are
                 presented and discussed.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Screen-Door Transparency",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-361,
  pages =        "359--366",
  year =         "1998",
  title =        "Comparing {LIC} and Spot Noise,",
  author =       "Wim C. de Leeuw and Robert van Liere",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-361",
  abstract =     "Spot noise and line integral convolution (LIC) are two
                 texture synthesis techniques for vector field
                 visualization. In this paper the two techniques are
                 compared. Continuous directional convolution is used as
                 a common basis for comparing the techniques. It is
                 shown that the techniques are based on the same
                 mathematical concept. Comparisons of the visual
                 appearance of the output and performance of the
                 algorithms are made.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Flow visualization, texture synthesis",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-362,
  pages =        "367--374",
  year =         "1998",
  title =        "Size Preserving Pattern Mapping",
  author =       "Yair Kurzion and Torsten M{\"{o}}ller and Roni Yagel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-362",
  abstract =     "We introduce a new approach for mapping texture on
                 volumetric iso-surfaces and parametric surfaces. Our
                 approach maps 2D images on surfaces while maintaining
                 continuity and preserving the size of the mapped images
                 on the models. Our approach is fully automatic. It
                 eliminates the need for manual mapping of texture maps.
                 We use the curvature of a surface at a point in order
                 to continuously vary the scale of the mapped image.
                 This makes our approach dependent only on local
                 attributes of a point (position, normal and its
                 derivatives) and independent of the global shape and
                 topology of an object. Our method can map high
                 resolution images on low resolution volumes, hence
                 enhancing the visual appearance of rendered volume
                 data. We describe a general framework useful for all
                 surface types that have a C 1 continuous normal. We
                 demonstrate the new method for painting volume data and
                 for mapping cavities on volume data.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-363,
  pages =        "375--382",
  year =         "1998",
  title =        "Constrained Optimal Framings of Curves and Surfaces
                 Using Quaternion Gauss Maps",
  author =       "Andrew J. Hanson",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-363",
  abstract =     "We propose a general paradigm for computing optimal
                 coordinate frame fields that may be exploited to
                 visualize curves and surfaces. Parallel-transport
                 framings, which work well for open curves, generally
                 fail to have desirable properties for cyclic curves and
                 for surfaces. We suggest that minimal quaternion
                 measure provides an appropriate heuristic
                 generalization of parallel transport. Our approach
                 differs from minimal-tangential-acceleration approaches
                 due to the addition of {"}sliding ring{"} constraints
                 that fix one frame axis, but allow an axial rotational
                 freedom whose value is varied in the optimization
                 process. Our fundamental tool is the quaternion Gauss
                 map, a generalization to quaternion space of the
                 tangent map for curves and of the Gauss map for
                 surfaces. The quaternion Gauss map takes 3D coordinate
                 frame fields for curves and surfaces into corresponding
                 curves and surfaces constrained to the space of
                 possible orientations in quaternion space. Standard
                 optimization tools provide application-specific means
                 of choosing optimal, e.g., length- or area-minimizing,
                 quaternion frame fields in this constrained space.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Quaternions, frames, tubing, curves, surfaces",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-364,
  pages =        "383--390",
  year =         "1998",
  title =        "Converting Sets of Polygons to Manifold Surfaces by
                 Cutting and Stitching",
  author =       "Andr{\'{e}} P. Gueziec and Gabriel Taubin and Francis
                 Lazarus and William Horn",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-364",
  abstract =     "Many real-world polygonal surfaces contain topological
                 singularities that represent a challenge for processes
                 such as simplification, compression, smoothing, etc. We
                 present an algorithm for removing such singularities,
                 thus converting non-manifold sets of polygons to
                 manifold polygonal surfaces (orientable if necessary).
                 We identify singular vertices and edges, multiply
                 singular vertices, and cut through singular edges. In
                 an optional stitching phase, we join surface boundary
                 edges that were cut, or whose endpoints are
                 sufficiently close, while guaranteeing that the surface
                 is a manifold. We study two different stitching
                 strategies called {"}edge pinching{"} and {"}edge
                 snapping{"}; when snapping, special care is required to
                 avoid recreating singularities.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Polygonal Surface, Manifold, Cutting, Stitching.",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-365,
  pages =        "405--410",
  year =         "1998",
  title =        "Task-Specific Visualization Design: {A} Case Study in
                 Operational Weather Forecasting",
  author =       "Lloyd A. Treinish",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-365",
  abstract =     "Efforts to create highly generic visualizations often
                 fail when applied to non-research-oriented activities.
                 Instead, specialized interfaces and tools matched to
                 user goals and underlying visualization tasks are
                 developed. To avoid the cost of addressing multiple
                 requirements through independent development and
                 training, the design of different visualization
                 applications are matched to a set of tasks but built on
                 top of a common framework with a similar approach to
                 content. To promote high-level reuse of interface and
                 content elements for each application, a
                 general-purpose toolkit is employed. Such a package
                 ordinarily would lack sufficient focus to be effective
                 in operational efforts, but its direct facilities are
                 hidden from the user. This approach is tested in detail
                 by application to a demanding problem -- operational
                 weather forecasting.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-366,
  pages =        "411--414",
  year =         "1998",
  title =        "Development of a Multi-Source Visualization
                 Prototype",
  author =       "Leslie Keely and Sam Uselton",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-366",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-367,
  pages =        "415--418",
  year =         "1998",
  title =        "Data Level Comparison of Wind Tunnel and Computational
                 Fluid Data Dynamics Data",
  author =       "Qin Shen and Alex Pang and Sam Uselton",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-367",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-368,
  pages =        "419--422",
  year =         "1998",
  title =        "Selective Visualization of Vortices in Hydrodynamic
                 Flows",
  author =       "I. Ari Sadarjoen and Frits H. Post and Bing Ma and
                 David C. Banks and Hans-Georg Pagendarm",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-368",
  abstract =     "Vortices are important features in many research and
                 engineering fields. Visualization is an important step
                 in gaining more understanding and control of vortices.
                 Vortex detection criteria fall into two categories:
                 point-based scalar quantities, calculated at single
                 points, and curve-based geometric criteria, calculated
                 for e.g.streamlines. The first category is easy to
                 compute, but does not work in all cases. The second
                 category is more intuitive and should work in all
                 cases, but currently only works in 2D (or 3D projected)
                 flows. We show applications of both approaches in
                 hydrodynamic flows.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-369,
  pages =        "423--426",
  year =         "1998",
  title =        "Visual Presentation of Magnetic Resonance Images",
  author =       "J. E. van der Heyden and M. S. T. Carpendale and K.
                 Inkpen and M. S. Atkins",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-369",
  abstract =     "Medical image analysis is shifting from current
                 film-oriented light screen environments to computer
                 environments that involve viewing and analyzing large
                 sets of images on a computer screen. Magnetic Resonance
                 Imaging (MRI) studies, in particular, can involve many
                 images. This paper examines how best to meet the needs
                 of radiologists in a computational environment. To this
                 end, a field study was conducted to observe
                 radiologists' interactions during MRI analysis in the
                 traditional light screen environment. Key issues
                 uncovered involve control over focus and context,
                 dynamic grouping of images and retrieval of images and
                 image groups. To address the problem of focus and
                 context, existing layout adjustment and magnification
                 techniques are explored to provide the most appropriate
                 solution. Our interest is in combining the
                 methodologies of human computer interaction studies
                 with computational presentation possibilities to design
                 a visual environment for the crucial field of medical
                 image analysis.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Displaying, environments, distinguished, involve, ACM,
                 sets, column, aspects, alternative, light screen
                 environment, traditional light screen, distortion
                 presentation techniques, oriented light Human Factors,
                 health care, medical images, MRI image presentation,
                 user interfaces, detail and context.",
  booktitle =    "IEEE Visualization '98",
}

@TechReport{EVL-1998-37,
  year =         "1998",
  title =        "Large scale Terrain Visualization using the Restricted
                 Quadtree Triangulation",
  type =         "Technical Report",
  author =       "Renato Pajarola",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-37",
  language =     "en",
  abstract =     "Real-time rendering of triangulated surfaces has
                 attracted growing interest in the last few years.
                 However, interactive visualization of very large scale
                 terrain data is still a problem for virtual reality
                 systems. The graphics load has to be con trolled by an
                 adaptive surface triangulation and by taking advantage
                 of different levels of detail. Furthermore, the dynamic
                 management of the visible scene requires efficient
                 access to the terrain database. All components must
                 interact smoothly and efficiently to arrive at a
                 high-performance visualization system. We describe
                 triangulation, scene management and data handling that
                 overcome all problems. The triangulation model is based
                 on the restricted quadtree triangula tion and extended
                 to meet the different requirements. Moreover, we
                 present new details of the restricted quadtree
                 triangulation not mentioned previously in the lit
                 erature. These include among others exact error
                 calculation, progressive meshing, performance
                 enhancements and spatial access.",
  month =        mar,
  keywords =     "visualization, multiresolution triangulation, terrain
                 database, geoinformation system",
  number =       "TR 292",
  institution =  "ETH Z{\"u}rich, Institute of Theoretical Computer
                 Science",
}

@InProceedings{EVL-1998-370,
  pages =        "427--430",
  year =         "1998",
  title =        "Visualization in Corneal Topography",
  author =       "F. M. Vos and H. J. W. Spoelder",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-370",
  abstract =     "The anterior surface of the eye ('cornea') is
                 extremely important for good sight. Instruments
                 measuring corneal shape conventionally visualize the
                 surface characteristics by mapping the instantaneous
                 radius of curvature onto a rainbow colour scale. This
                 technique is known to have important drawbacks.
                 Firstly, not corneal shape itself is visualized, but
                 rather second order surface properties. Secondly, the
                 type of colouring produces well documented artifacts.
                 In this paper we discuss visualization techniques for a
                 more direct representation of the data. In a three-part
                 display shape deviations are presented as a height
                 surface in one window, height lines superimposed over
                 the input image in another, and a colour-mapped
                 representation of the mean normal radius of curvature
                 in a third. With the aid of some typical examples it is
                 shown that these visualizations are easy to interpret
                 by the physician and overcome the limitations of the
                 conventional techniques.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-371,
  pages =        "431--434",
  year =         "1998",
  title =        "A Case Study Using the Virtual Environment for
                 Reconstructive Surgery",
  author =       "Kevin Montgomery and Michael Stephanides and Stephen
                 Schendel and Muriel Ross",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-371",
  abstract =     "This paper details the use of a Virtual Environment
                 for Reconstructive Surgery (VERS) in the case of a 17
                 year-old boy with a severe facial defect arising from
                 the removal of a soft-tissue tumor. Computed tomography
                 (CT) scans were taken of the patient, the data were
                 segmented, a mesh was generated, and this
                 patient-specific mesh was used in a virtual environment
                 by the surgeons for preoperative visualization of the
                 defect, planning of the surgery, and production of a
                 custom surgical template to aid in repairing the
                 defect. This paper details the case of this patient,
                 provides a background on the virtual environment
                 technology used, discusses the difficulties
                 encountered, and describes the lessons learned.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Computer applications, life and medical sciences,
                 surgical planning",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-372,
  pages =        "435--438",
  year =         "1998",
  title =        "Interactive Virtual Angioscopy",
  author =       "Enrico Gobbetti and Piero Pili and Antonio Zorcolo and
                 Massimiliano Tuveri",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-372",
  abstract =     "Virtual angioscopy is a non invasive medical procedure
                 for exploring parts of the human vascular system. We
                 have developed an interactive tool that takes as input
                 data acquired with standard medical imaging modalities
                 and regards it as a virtual environment to be
                 interactively inspected. The system supports real-time
                 navigation with stereoscopic direct volume rendering
                 and dynamic endoscopic camera control, interactive
                 tissue classification, and interactive point picking
                 for morphological feature measurement. In this paper,
                 we provide an overview of the system, discuss the
                 techniques used in our prototype, and present
                 experimental results on human data sets. The
                 accompanying video-tape illustrates our approach with
                 interactive sequences showing the examination of a
                 human carotid artery.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Virtual angioscopy, endoscopy, interactive rendering,
                 volume rendering, virtual environment.",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-373,
  pages =        "439--442",
  year =         "1998",
  title =        "Volumetric Visualization of Acoustic Fields in
                 {CNMAT}¹s Sound Spatialization Theatre",
  author =       "Sami Khoury and Adrian Freed and David Wessel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-373",
  abstract =     "A new tool for real-time visualization of acoustic
                 sound fields has been developed for a new sound
                 spatialization theatre. The theatre is described and
                 several applications of the acoustic and volumetric
                 modeling software are presented.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Arts and Humanities: Performing Arts",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-374,
  pages =        "443--446",
  year =         "1998",
  title =        "Supporting Detail-in-Context for the {DNA}
                 Representation, {H}-Curves",
  author =       "M. L. Lantin and M. S. T. Carpendale",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-374",
  abstract =     "This paper presents a tool for the visual exploration
                 of DNA sequences represented as H-curves. Although very
                 long sequences can be plotted using H-curves,
                 micro-features are lost as sequences get longer. We
                 present a new three-dimensional distortion algorithm to
                 allow the magnification of a sub-segment of an H-curve
                 while preserving a global view of the curve. This is a
                 particularly appropriate for H-curves as they provide
                 useful visual information at several resolutions. Our
                 approach also extends the current possibilities of
                 detail-in-context viewing in 3D. It provides a
                 non-occluding, orthogonal technique that preserves
                 uniform scaling within regions and maintains geometric
                 continuity between regions.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-375,
  pages =        "447--450",
  year =         "1998",
  title =        "Visualizing Hilbert Curves",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-375",
  author =       "Nelson Max",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-376,
  pages =        "451--454",
  year =         "1998",
  title =        "Rear-Projecting Virtual Data onto Physical Terrain: An
                 Exercise in Two Senses Being Better Than One",
  author =       "Dru Clark and Rosemarie McKeon and Richard Marciano
                 and Michael Bailey",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-376",
  abstract =     "This paper describes a project that combined physical
                 model fabrication and virtual computer-based data
                 display to create a unique visualization presentation.
                 USGS terrain information on Prince of Wales Island,
                 Alaska was used to create a physical prototype in
                 SDSC's TeleManufacturing Facility. This model was then
                 used as a mold to create a translucent plate of the
                 terrain. Finally, deforestation data from the island
                 was color mapped and rear-projected onto the
                 translucent plate within a light box. The result is a
                 very compelling display in which both the senses of
                 sight and touch are used to make relationships between
                 terrain features and the data more readily apparent.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-377,
  pages =        "455--458",
  year =         "1998",
  title =        "Intent, Perception, and Out-of-Core Visualization
                 Applied to Terrain",
  author =       "Douglass Davis and T. Y. Jiang and William Ribarsky
                 and Nickolas Faust",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-377",
  abstract =     "GVU Technical Report Number: GIT-GVU-98-12 Title:
                 Intent, Perception, and Out-of-Core Visualization
                 Applied to Terrain Authors: Douglass Davis Tian-yue
                 Jiang WIlliam RIbarsky Nickolas Faust This paper
                 considers how out-of-core visualization applies to
                 terrain datasets, which are among the largest now
                 presented for interactive visualization and can range
                 to sizes of 20 GB and more. It is found that a
                 combination of out-of-core visualization, which tends
                 to focus on 3D data, and visual simulation, which
                 places an emphasis on visual perception and real-time
                 display of multiresolution data, results in interactive
                 terrain visualization with significantly improved data
                 access and quality of presentation. Further, the visual
                 simulation approach provides qualities that are useful
                 for general data, not just terrain.",
  organization = "IEEE",
  editor =       "by David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "GVU Technical Report Number: GIT-GVU-98-12 Title:
                 Intent, Perception, and Out-of-Core Visualization
                 Applied to Terrain Authors: Douglass Davis Tian-yue
                 Jiang WIlliam RIbarsky Nickolas Faust Abstract: This
                 paper considers how out-of-core visualization applies
                 to terrain datasets, which are among the largest now
                 presented for interactive visualization and can range
                 to sizes of 20 GB and more. It is found that a
                 combination of out-of-core visualization, which tends
                 to focus on 3D data, and visual simulation, which
                 places an emphasis on visual perception and real-time
                 display of multiresolution data, results in interactive
                 terrain visualization with significantly improved data
                 access and quality of presentation. Further, the visual
                 simulation approach provides qualities that are useful
                 for general data, not just terrain. Keywords:
                 Out-of-core, terrain visualization, quadtree, quadnode,
                 paging, caching, multiresolution",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-378,
  pages =        "459--462",
  year =         "1998",
  title =        "Production Visualization for the {ASCI} One
                 Tera{FLOPS} Machine",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-378",
  author =       "Philip D. Heermann",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-379,
  pages =        "467--470",
  year =         "1998",
  title =        "Scientific Visualization and Data Modeling of
                 Scattered Sediment Contaminant Data in New York/New
                 Jersey Estuaries",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-379",
  author =       "Hong Ma and Keith W. Jones and Eric A. Stern",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@TechReport{EVL-1998-38,
  year =         "1998",
  title =        "On the Design of {CGAL}, the Computational Geometry
                 Algorithms Library",
  type =         "Technical Report",
  author =       "Andreas Fabri and Geert-Jan Giezeman and Lutz Kettner
                 and Stefan Schirra and Sven Sch{\"o}nherr",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-38",
  language =     "en",
  abstract =     "CGAL is a Computational Geometry Algorithms Library
                 written in C++, which is developed in an ESPRIT LTR
                 project. The goal is to make the large body of
                 geometric algorithms developed in the field of
                 computational geometry available for industrial
                 application. In this report we discuss the major design
                 goals for CGAL, which are correctness, flexibility,
                 ease-of-use, efficiency, and robustness, and present
                 our approach to reach these goals. Templates and the
                 relatively new generic programming play a central role
                 in the architecture of CGAL. We give a short
                 introduction to generic programming in C++, compare it
                 to the object-oriented programming paradigm, and
                 present examples where both paradigms are used
                 effectively in CGAL. Moreover, we give an overview on
                 the current structure of the library and consider
                 software engineering aspects in the CGAL-project.",
  month =        feb,
  number =       "TR 291",
  institution =  "ETH Z{\"u}rich, Institute of Theoretical Computer
                 Science",
}

@InProceedings{EVL-1998-380,
  pages =        "471--474",
  year =         "1998",
  title =        "{POPTEX}: Interactive Ocean Model Visualization Using
                 Texture Mapping Hardware",
  author =       "Allen McPherson and Mathew Maltrud",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-380",
  abstract =     "Global circulation models are used to gain an
                 understanding of the processes that affect the Earth's
                 climate and may ultimately be used to assess the impact
                 of humanity's activities on it. The POP ocean model
                 developed at Los Alamos is an example of such a global
                 circulation model that is being used to investigate the
                 role of the ocean in the climate system. Data output
                 from POP has traditionally been visualized using video
                 technology which precludes rapid modification of
                 visualization parameters and techniques. This paper
                 describes a visualization system that leverages high
                 speed graphics hardware, specifically texture mapping
                 hardware, to accelerate data exploration to interactive
                 rates. We describe the design of the system, the
                 specific hardware features used, and provide examples
                 of its use. The system is capable of viewing ocean
                 circulation simulation results at up to 60 frames per
                 second while loading texture memory at approximately 72
                 million texels per second.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier.",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-381,
  pages =        "475--478",
  year =         "1998",
  title =        "Acoustic Imaging and Visualization of Plumes
                 Discharging from Black Smoker Vents on the Deep
                 Seafloor",
  author =       "P. Rona and K. Bemis and D. Kenchammana-Hosekote and
                 D. Silver",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-381",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-382,
  pages =        "479--482",
  year =         "1998",
  title =        "Seabed Visualization",
  author =       "Paul Chapman and Peter Stevens and Derek Wills and
                 Graham Brookes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-382",
  abstract =     "The development of a high speed multi-frequency
                 continuous scan sonar at Sonar Research & Development
                 Ltd has resulted in the acquisition of extremely
                 accurate, high resolution bathymetric data. This rich
                 underwater data provides new challenges and
                 possibilities within the field of seabed visualization.
                 This paper introduces the reader to seabed
                 visualization by describing two example case studies
                 which utilise the Seabed Visualization System developed
                 at SRD. Both case studies, harbour wall and shipwreck
                 visualization, are implemented using real survey data.
                 The high resolution of the data obtained means slight
                 changes in the seabed topography are easily
                 distinguishable. Annual survey inspections in both case
                 studies enable comparisons to be made between the data
                 sets making the visualization system an important tool
                 for management and planning.",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  keywords =     "Seabed visualization, sonar technology, harbour wall
                 visualization, shipwreck visualization.",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-383,
  pages =        "483--486",
  year =         "1998",
  title =        "Configuration Space Visualization for Mechanical
                 Design",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-383",
  author =       "Elisha Sacks and Leo Joskowicz",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-384,
  pages =        "487--490",
  year =         "1998",
  title =        "Three-Dimensional Visualization of Microstructures",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-384",
  author =       "Marco Lanzagorta and Milo V. Kral and J. Edward Swan
                 II and George Spanos and Rob Rosenberg and Eddy Kuo",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-385,
  pages =        "491--494",
  year =         "1998",
  title =        "Visualization for Multiparameter Aircraft Designs",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-385",
  author =       "Cliff A. Shaffer and Duane L. Knill and Layne T.
                 Watson",
  organization = "IEEE",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-386,
  pages =        "209--237",
  year =         "1998",
  title =        "A multiresolution framework for variational
                 subdivision",
  author =       "Leif Kobbelt and Peter Schr{\"{o}}der",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-386",
  abstract =     "Subdivision is a powerful paradigm for the generaton
                 of curves and surfaces. It is easy to implement,
                 computationally efficient, and useful in a variety of
                 applications because of its intimate connection with
                 multiresolution analysis. An important task in computer
                 graphics and geometric modeling is the construction of
                 curves that interpolate a griven set of points and
                 minimize a fairness functional (variational design). In
                 the context of subdivision, fairing leads to special
                 schemes requiring the solution of a banded linear
                 system at every subdivision step. We present several
                 examples of such schemes including one that reproduces
                 nonuniform interpolating cubic splines. Expressing the
                 construction in terms of certain elementary operations
                 we are able to embed variational subdivision in the
                 lifting framework, a powerful technique to construct
                 wavelet filter banks given a subdivision scheme. This
                 allows us to extend the traditional lifting scheme for
                 FIR filters to a certain class of IIR filters.
                 Consquently, we how how to build variationally optimal
                 curves and associated, stable wavelets in a
                 straightforward fashion. The algorithms to perform the
                 corresponding decomposition and reconstruction
                 transformations are easy to implement and efficient
                 enough for interactive applications.",
  volume =       "17 (4)",
  keywords =     "Lifting scheme, subdivision, variational modeling,
                 wavelets",
  booktitle =    "ACM Transactions on Graphics",
}

@InProceedings{EVL-1998-387,
  pages =        "238--258",
  year =         "1998",
  title =        "Jagged edges: when is filtering nedded",
  author =       "Avi C. Naiman",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-387",
  abstract =     "Depiction of oblique edges by discrete pixels usually
                 results in visible stair steps, often called jaggies. A
                 variety of filtering approaches exists to minimize this
                 visual artifact, but none has been applied selectively
                 only to those edges that would otherwise appear jagged.
                 A recent series of experiments has led to a model of
                 the visibility of jagged edges. Here, we demonstrate
                 how these data can be used efficiently to determine
                 when filtering of edges is needed to eliminate the
                 jaggies and when it is unnecessary. This work also
                 provides a template for how the results of
                 psychophysical experiments can be applied in computer
                 graphics to address image-quality questions.",
  volume =       "17 (4)",
  keywords =     "Image quality, jagged edges, jagged odges, jaggies,
                 visual sensitivity, visual sensitivty",
  booktitle =    "ACM Transactions on Graphics",
}

@InProceedings{EVL-1998-388,
  pages =        "259--286",
  year =         "1998",
  title =        "Boundary representation deformation in parametric
                 solid modeling",
  author =       "Srinivas Raghothama and Vadim Shapiro",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-388",
  abstract =     "One of the major unsolved problems in parametric solid
                 modeling is a robust update (regeneration) of the
                 solid's boundary representation, given a specified
                 change in the solid's parameter values. The fundamental
                 difficulty lies in determining the mapping between
                 boundary representations for solids in the same
                 parametric family. Several heuristic approaches have
                 been proposed for dealing with this problem, but the
                 formal properties of such mappings are not well
                 understood. We propose a formal definition for boundary
                 representation. (BR-)deformation for solids in the same
                 parametric family, based on the assumption of
                 continuity: small changes in solid parameter values
                 should result in small changes in the solid's boundary
                 reprentation, which may include local collapses of
                 cells in the boundary representation. The necessary
                 conditions that must be satisfied by any BR-deforming
                 mappings between boundary representations are powerful
                 enough to identify invalid updates in many (but not
                 all) practical situations, and the algorithms to check
                 them are simple. Our formulation provides a formal
                 criterion for the recently proposed heuristic
                 approaches to {"}persistent naming,{"} and explains the
                 difficulties in devising sufficient tests for
                 BR-deformation encountered in practice. Finally our
                 methods are also applicable to more general cellular
                 models of pointsets and should be useful in developing
                 universal standards in parametric modeling.",
  volume =       "17 (4)",
  keywords =     "Algebraic topology, boundary deformation, boundary
                 representation, cell complex, parametric editing,
                 persistent naming, soild modeling",
  booktitle =    "ACM Transactions on Graphics",
}

@InProceedings{EVL-1998-389,
  pages =        "9--20",
  year =         "1998",
  title =        "NeuroAnimator: Fast Neural Network Emulation and
                 Control of Physics-Based Models",
  author =       "Radek Grzeszczuk and Demetri Terzopoulos and Geoffrey
                 Hinton",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-389",
  abstract =     "Animation through the numerical simulation of
                 physics-based graphics models offers unsurpassed
                 realism, but it can be computationally demanding.
                 Likewise, the search for controllers that enable
                 physics-based models to produce desired animations
                 usually entails formidable computational cost. This
                 paper demonstrates the possibility of replacing the
                 numerical simulation and control of dynamic models with
                 a dramatically more efficient alternative. In
                 particular, we propose the NeuroAnimator, a novel
                 approach to creating physically realistic animation
                 that exploits neural networks. NeuroAnimators are
                 automatically trained off-line to emulate physical
                 dynamics through the observation of physics-based
                 models in action. Depending on the model, its neural
                 net-work emulator can yield physically realistic
                 animation one or two orders of magnitude faster than
                 conventional numerical simulation. Furthermore, by
                 exploiting the network structure of the NeuroAnimator,
                 we introduce a fast algorithm for learning controllers
                 that enables either physics-based models or their
                 neural network emulators to synthesize motions
                 satisfying prescribed animation goals. We demonstrate
                 NeuroAnimators for a variety of physics-based models.",
  editor =       "Michael Cohen",
  keywords =     "Physics-based animation, neural networks, learning,
                 motion control, backpropagation, dynamical systems,
                 simulation.",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@TechReport{EVL-1998-39,
  year =         "1998",
  title =        "Exact Arithmetic at Low Cost -- a Case Study in Linear
                 Programming",
  type =         "Technical Report",
  author =       "Bernd G{\"a}rtner",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-39",
  language =     "en",
  abstract =     "We describe a new exact-arithmetic approach to linear
                 programming when the number of variables n is much
                 larger than the number of constraints m (or vice
                 versa). The algorithm is an implementation of the
                 simplex method which combines exact (multiple
                 precision) arithmetic with inexact (floating point)
                 arithmetic, where the number of exact arithmetic
                 operations is small and usually bounded by a function
                 of min(n,m). Combining this with a ``partial pricing''
                 scheme (based on a result by Clarkson ) which is
                 particularly tuned for the problems under
                 consideration, we obtain a correct and practically
                 efficient algorithm that even competes with the inexact
                 state-of-the-art solver CPLEX for small values of
                 min(n,m) and and is far superior to methods that use
                 exact arithmetic in any operation. The main
                 applications lie in computational geometry.",
  month =        jan,
  keywords =     "linear programming, simplex algorithm, exact
                 arithmetic, computational geometry",
  number =       "TR 283",
  institution =  "ETH Z{\"u}rich,",
}

@InProceedings{EVL-1998-390,
  pages =        "21--32",
  year =         "1998",
  title =        "A Beam Tracing Approach to Acoustic Modeling for
                 Interactive Virtual Environments",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-390",
  author =       "Thomas Funkhouser and Ingrid Carlbom and Gary Elko and
                 Gopal Pingali and Mohan Sondhi and Jim West",
  abstract =     "Virtual environment research has focused on
                 interactive image generation and has largely ignored
                 acoustic modeling for spatialization of sound. Yet,
                 realistic auditory cues can complement and enhance
                 visual cues to aid navigation, comprehension, and sense
                 of presence in virtual environments. A primary
                 challenge in acoustic modeling is computation of
                 reverberation paths from sound sources fast enough for
                 real-time auralization. We have developed a system that
                 uses precomputed spatial subdivision and 'beam tree'
                 data structures to enable real-time acoustic modeling
                 and auralization in interactive virtual environments.
                 The spatial subdivision is a partition of 3D space into
                 convex polyhedral regions (cells) represented as a cell
                 adjacency graph. A beam tracing algorithm recursively
                 traces pyramidal beams through the spatial subdivision
                 to construct a beamtree data structure representing the
                 regions of space reachable by each potential sequence
                 of transmission and specular reflection events at cell
                 boundaries. From these precomputed data structures, we
                 can generate high-order specular reflection and
                 transmission paths at interactive rates to spatialize
                 fixed sound sources in real-time as the user moves
                 through a virtual environment. Unlike previous acoustic
                 modeling work, our beam tracing method: 1) supports
                 evaluation of reverberation paths at interactive rates,
                 2) scales to compute high-order reflections and large
                 environments, and 3) extends naturally to compute paths
                 of diffraction and diffuse reflection efficiently. We
                 are using this system to develop interactive
                 applications in which a user experiences a virtual
                 environment immersively via simultaneous auralization
                 and visualization.",
  editor =       "Michael Cohen",
  keywords =     "Beam tracing, acoustic modeling, auralization,
                 spatialized sound, virtual environment systems, virtual
                 reality",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-391,
  pages =        "33--42",
  year =         "1998",
  title =        "Retargeting Motion to New Characters",
  author =       "Michael Gleicher",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-391",
  abstract =     "In this paper, we present a technique for motion: the
                 problem of adapting an animated motion from one
                 character to another. Our focus is on adapting the
                 motion of one articulated figure to another figure with
                 identical structure but different segment lengths,
                 although we use this as a step when considering less
                 similar characters. Our method creates adaptations that
                 preserve desirable qualities of the original motion. We
                 identify specific features of the motion as constraints
                 that must be maintained. A spacetime constraints solver
                 computes an adapted motion that re-establishes these
                 constraints while preserving the frequency
                 characteristics of the original signal. We demonstrate
                 our approach on motion capture data.",
  editor =       "Michael Cohen",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-392,
  pages =        "43--54",
  year =         "1998",
  title =        "Large Steps in Cloth Simulation",
  author =       "David Baraff and Andrew Witkin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-392",
  abstract =     "The bottle-neck in most cloth simulation systems is
                 that time steps must be small to avoid numerical
                 instability. This paper describes a cloth simulation
                 system that can stably take large time steps. The
                 simulation systemcouples a new technique for enforcing
                 constraints on individual cloth particles with an
                 implicit integration method. The simulator models cloth
                 as a triangular mesh, with internal cloth forces
                 derived using a simple continuum formulation that
                 supports modeling operations such as local anisotropic
                 stretch or compression; a unified treatment of damping
                 forces is included as well. The implicit integration
                 method generates a large, unbanded sparse linear system
                 at each time step which is solved using a modified
                 conjugate gradient method that simultaneously enforces
                 particles' constraints. The constraints are always
                 maintained exactly, independent of the number of
                 conjugate gradient iterations, which is typically
                 small. The resulting simulation system is significantly
                 faster than previous accounts of cloth simulation
                 systems in the literature.",
  editor =       "Michael Cohen",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
}

@InProceedings{EVL-1998-393,
  pages =        "55--66",
  year =         "1998",
  title =        "Making Faces",
  author =       "Brian Guenter and Cindy Grimm and Daniel Wood and
                 Henrique Malvar and Fr{\'{e}}d{\'{e}}ric Pighin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-393",
  abstract =     "We have created a system for capturing both the
                 three-dimensional geometry and color and shading
                 information for human facial expressions. We use this
                 data to reconstruct photorealistic, 3D animations of
                 the captured expressions. The system uses a large set
                 of sampling points on the face to accurately track the
                 three dimensional deformations of the face.
                 Simultaneously with the tracking of the geometric data,
                 we capture multiple high resolution, registered video
                 images of the face. These images are used to create a
                 texture map sequence for a three dimensional polygonal
                 face model which can then be rendered on standard 3D
                 graphics hardware. The resulting facial animation is
                 surprisingly life-like and looks very much like the
                 original live performance. Separating the capture of
                 the geometry from the texture images eliminates much of
                 the variance in the image data due to motion, which
                 increases compression ratios. Although the primary
                 emphasis of our work is not compression we have
                 investigated the use of a novel method to compress the
                 geometric data based on principal components analysis.
                 The texture sequence is compressed using an MPEG4 video
                 codec. Animations reconstructed from 512×512 pixel
                 textures look good at data rates as low as 240 Kbits
                 per second.",
  editor =       "Michael Cohen",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-394,
  pages =        "67--74",
  year =         "1998",
  title =        "An Anthropometric Face Model Using Variational
                 Techniques",
  author =       "Douglas DeCarlo and Dimitris Metaxas and Matthew
                 Stone",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-394",
  abstract =     "We describe a system that automatically generates
                 varied geometric models of human faces. A collection of
                 random measurements of the face is generated according
                 to anthropometric statistics for likely face
                 measurements in a population. These measurements are
                 then treated as constraints on a parameterized surface.
                 Variational modeling is used to find a smooth surface
                 that satisfies these constraints while using a
                 prototype shape as a reference.",
  editor =       "Michael Cohen",
  keywords =     "Face modeling, anthropometry, variational modeling,
                 crowd generation",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-395,
  pages =        "75--84",
  year =         "1998",
  title =        "Synthesizing Realistic Facial Expressions From
                 Photographs",
  author =       "Fr{\'{e}}d{\'{e}}ric Pighin and Jamie Hecker and Dani
                 Lischinski and Richard Szeliski and David H. Salesin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-395",
  abstract =     "We present new techniques for creating photorealistic
                 textured 3D facial models from photographs of a human
                 subject, and for creating smooth transitions between
                 different facial expressions by morphing between these
                 different models. Starting from several uncalibrated
                 views of a human subject, we employ a user-assisted
                 technique to recover the camera poses corresponding to
                 the views as well as the 3D coordinates of a sparse set
                 of chosen locations on the subject's face. A scattered
                 data interpolation technique is then used to deform a
                 generic face mesh to fit the particular geometry of the
                 subject's face. Having recovered the camera poses and
                 the facial geometry, we extract from the input images
                 one or more texture maps for the model. This process is
                 repeated for several facial expressions of a particular
                 subject. To generate transitions between these facial
                 expressions we use 3D shape morphing between the
                 corresponding face models, while at the same time
                 blending the corresponding textures. Using our
                 technique, we have been able to generate highly
                 realistic face models and natural looking animations.",
  editor =       "Michael Cohen",
  keywords =     "Facial modeling, facial expression generation, facial
                 animation, photogrammetry, morphing, view-dependent
                 texture-mapping",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-396,
  pages =        "85--94",
  year =         "1998",
  title =        "Subdivision Surfaces in Character Animation",
  author =       "Tony DeRose and Michael Kass and Tien Truong",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-396",
  abstract =     "The creation of believable and endearing characters in
                 computer graphics presents a number of technical
                 challenges, including the modeling, animation and
                 rendering of complex shapes such as heads, hands, and
                 clothing. Traditionally, these shapes have been modeled
                 with NURBS surfaces despite the severe topological
                 restrictions that NURBS impose. In order to move beyond
                 these restrictions, we have recently introduced
                 subdivision surfaces into our production environment.
                 Subdivision surfaces are not new, but their use in
                 high-end CG production has been limited. Here we
                 describe a series of developments that were required in
                 order for subdivision surfaces to meet the demands of
                 high-end production. First, we devised a practical
                 technique for construct ing provably smooth
                 variable-radius fillets and blends. Second, we
                 developed methods for using subdivision surfaces in
                 clothing simulation including a new algorithm for
                 efficient collision detection. Third, we developed a
                 method for constructing smooth scalar fields on
                 subdivision surfaces, thereby enabling the use of a
                 wider class of programmable shaders. These
                 developments, which were used extensively in our
                 recently completed short film 'Geri's Game,' have
                 become a highly valued feature of our production
                 environment.",
  editor =       "Michael Cohen",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-397,
  pages =        "95--104",
  year =         "1998",
  title =        "{MAPS}: Multiresolution Adaptive Parameterization of
                 Surfaces",
  author =       "Aaron W. F. Lee and Wim Sweldens and Peter
                 Schr{\"{o}}der and Lawrence Cowsar and David Dobkin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-397",
  abstract =     "We construct smooth parameterizations of irregular
                 connectivity triangulations of arbitrary genus
                 2-manifolds. Our algorithm uses hierarchical
                 simplifications to efficiently induce a
                 parameterization of the original mesh over a base
                 domain consisting of a small number of triangles. This
                 initial parameterization is further improved through a
                 hierarchical smoothing procedure based on Loop
                 subdivision applied in the parameter domain. Our method
                 supports both fully automatic and user constrained
                 operations. In the latter, we accommodate point and
                 edge constraints to forst the alignment of
                 iso-parameter lines with desired features. We show how
                 to use the parameterization for fast, hierarchical
                 subdivision connectivity remeshing with guaranteed
                 error bounds. The remeshing algorithm constructs an
                 adaptively subdivided mesh directly without first
                 resorting to uniform subdivision followed by subsequent
                 sparsification. It thus avoids the exponential cost of
                 the latter. Our parameterizations are also useful for
                 texture mapping and morphing applications, among
                 others.",
  editor =       "Michael Cohen",
  keywords =     "Meshes, surface parameterization, mesh simplification,
                 remeshing, texture mapping, multiresolution,
                 subdivision surfaces, Loop scheme",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-398,
  pages =        "105--114",
  year =         "1998",
  title =        "Interactive Multi-Resolution Modeling on Arbitrary
                 Meshes",
  author =       "Leif Kobbelt and Swen Campagna and Jens Vorsatz and
                 Hans-Peter Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-398",
  abstract =     "During the last years the concept of multi-resolution
                 modeling has gained special attention in many fields of
                 computer graphics and geometric modeling. In this paper
                 we generalize powerful multi-resolution techniques to
                 arbitrary triangle meshes without requiring subdivision
                 connectivity. Our major observation is that the
                 hierarchy of nested spaces which is the structural core
                 element of most multi-resolution algorithms can be
                 replaced by the sequence of intermediate meshes
                 emerging from the application of incremental mesh
                 decimation. Performing such schemes with local frame
                 coding of the detail coefficients already provides
                 effective and efficient algorithms to extract
                 multi-resolution information from unstructured meshes.
                 In combination with discrete fairing techniques, i.e.,
                 the constrained minimization of discrete energy
                 functionals, we obtain very fast mesh smoothing
                 algorithms which are able to reduce noise from a
                 geometrically specified frequency band in a
                 multi-resolution decomposition. Putting mesh
                 hierarchies, local frame coding and multi-level
                 smoothing together allows us to propose a flexible and
                 intuitive paradigm for interactive detail-preserving
                 mesh modification. We show examples generated by our
                 mesh modeling tool implementation to demonstrate its
                 functionality.",
  editor =       "Michael Cohen",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-399,
  pages =        "115--122",
  year =         "1998",
  title =        "Appearance-Preserving Simplification",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-399",
  author =       "Jonathan Cohen and Marc Olano and Dinesh Manocha",
  abstract =     "We present a new algorithm for appearance-preserving
                 simplification. Not only does it generate a
                 low-polygon-count approximation of a model, but it also
                 preserves the appearance. This is accomplished for a
                 particular display resolution in the sense that we
                 properly sample the surface position, curvature, and
                 color attributes of the input surface. We convert the
                 input surface to a representation that decouples the
                 sampling of these three attributes, storing the colors
                 and normals in texture and normal maps, respectively.
                 Our simplification algorithm employs a new texture
                 deviation metric, which guarantees that these maps
                 shift by no more than a user-specified number of pixels
                 on the screen. The simplification process filters the
                 surface position, while the run-time system filters the
                 colors and normals on a per-pixel basis. We have
                 applied our simplification technique to several large
                 models achieving significant amounts of simplification
                 with little or no loss in rendering quality.",
  editor =       "Michael Cohen",
  keywords =     "Simplification, attributes, parameterization, color,
                 normal, texture, maps",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@TechReport{EVL-1998-4,
  year =         "1998",
  title =        "Enhancing the Visualization of Characteristic
                 Structures in Dynamical Systems",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-4",
  author =       "Helwig L{\"o}ffelmann and Eduard Gr{\"o}ller",
  abstract =     "We present a thread of streamlets as a new technique
                 to visualize dynamical systems in three-space. A
                 trade-off is made between solely visualizing a
                 mathematical abstraction through lower-dimensional
                 manifolds, i.e., characteristic structures such as
                 fixed point, separatrices, etc., and directly encoding
                 the flow through stream lines or stream surfaces.
                 Bundlers of streamlets are selectively placed near
                 characteristic trajectories. An over-population of
                 phase space with occlusion problems as a consequence is
                 omitted. On the other hand, information loss is
                 minimized since characteristic structures of the flow
                 are still illustrated in the visualization.",
  language =     "en",
  month =        jan,
  keywords =     "visualization, dynamical systems",
  number =       "TR-186-2-98-05",
  institution =  "Visualisation and Animation Group, Vienna University
                 of Technology",
}

@TechReport{EVL-1998-40,
  year =         "1998",
  title =        "A Bernstein-Bezier Based Approach to Soft Tissue
                 Simulation",
  type =         "Technical Report",
  author =       "Samuel Hans Martin Roth and Markus Hans Gross and
                 Silvio Turello and Friedrich Robert Carls",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-40",
  language =     "en",
  abstract =     "This paper discusses a Finite Element approach for
                 volumetric soft tissue modeling in the context of
                 facial surgery simulation. We elaborate on the
                 underlying physics and address some computational
                 aspects of the finite element discretization. In
                 contrast to existing approaches speed is not our first
                 concern, but we strive for the highest possible
                 accuracy of simulation. We therefore propose an
                 extension of linear elasticity towards
                 incompressibility and nonlinear material behavior, in
                 order to describe the complex properties of human soft
                 tissue more accurately. Furthermore, we incorporate
                 higher order interpolation functions using a
                 Bernstein-B{\'{e}}zier formulation, which has various
                 advantageous properties, such as its integral
                 polynomial form of arbitrary degree, efficient
                 subdivision schemes, and suitability for geometric
                 modeling and rendering. In addition, the use of
                 tetrahedral Finite Elements does not put any
                 restriction on the geometry of the simulated volumes.
                 Experimental results obtained from a synthetic block of
                 soft tissue and from the Visible Human Data Set
                 illustrate the performance of the envisioned model",
  month =        jan,
  keywords =     "finite element, facial surgery simulation",
  number =       "TR 282",
  institution =  "ETH Z{\"u}rich, Institut of Scientific Computing",
}

@InProceedings{EVL-1998-400,
  pages =        "123--132",
  year =         "1998",
  title =        "Progressive Forest Split Compression",
  author =       "Gabriel Taubin and Andr{\'{e}} Gueziec and William
                 Horn and Francis Lazarus",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-400",
  abstract =     "In this paper we introduce the Progressive Forest
                 Split (PFS) representation, a new adaptive refinement
                 scheme for storing and transmitting manifold triangular
                 meshes in progressive and highly compressed form. As in
                 the Progressive Mesh (PM) method of Hoppe, a triangular
                 mesh is represented as a low resolution polygonal model
                 followed by a sequence of refinement operations, each
                 one specifying how to add triangles and vertices to the
                 previous level of detail to obtain a new level. The PFS
                 format shares with PM and other refinement schemes the
                 ability to smoothly interpolate between consecutive
                 levels of detail. However, it achieves much higher
                 compression ratios than PM by using a more complex
                 refinement operation which can, at the expense of
                 reduced granularity, be encoded more efficiently. A
                 forest split operation doubling the number n of
                 triangles of a mesh requires a maximum of approximately
                 3.5n bits to represent the connectivity changes, as
                 opposed to approximately (5 + log2(n)) bits in PM. We
                 describe algorithms to efficiently encode and decode
                 the PFS format. We also show how any surface
                 simplification algorithm based on edge collapses can be
                 modified to convert single resolution triangular meshes
                 to the PFS format. The modifications are simple and
                 only require two additional topological tests on each
                 candidate edge collapse. We show results obtained by
                 applying these modifications to the Variable Tolerance
                 method of Gueziec.",
  editor =       "Michael Cohen",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-401,
  pages =        "133--140",
  year =         "1998",
  title =        "Real Time Compression of Triangle Mesh Connectivity",
  author =       "Stefan Gumhold and Wolfgang Stra{\ss{}}er",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-401",
  abstract =     "In this paper we introduce a new compressed
                 representation for the connectivity of a triangle mesh.
                 We present local compression and decompression
                 algorithms which are fast enough for real time
                 applications. The achieved space compression rates keep
                 pace with the best rates reported for any known global
                 compression algorithm. These nice properties have great
                 benefits for several important applications. Naturally,
                 the technique can be used to compress triangle meshes
                 without significant delay before they are stored on
                 external devices or transmitted over a network. The
                 presented decompression algorithm is very simple
                 allowing a possible hardware realization of the
                 decompression algorithm which could significantly
                 increase the rendering speed of pipelined graphics
                 hardware.",
  editor =       "Michael Cohen",
  keywords =     "Mesh Compression, Algorithms, 3D Graphics Hardware,
                 Graphics",
  series =       "Annual Conference Series, Addison Weslry",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-402,
  pages =        "141--150",
  year =         "1998",
  title =        "The Design of a Parallel Graphics Interface",
  author =       "Homan Igehy and Gordon Stoll and Pat Hanrahan.",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-402",
  abstract =     "It has become increasingly difficult to drive a modern
                 high-performance graphics accelerator at full speed
                 with a serial immediate-mode graphics interface. To
                 resolve this problem, retained-mode constructs have
                 been integrated into graphics interfaces. While
                 retained-mode constructs provide a good solution in
                 many cases, at times they provide an undesirable
                 interface model for the application programmer, and in
                 some cases they do not solve the performance problem.
                 In order to resolve some of these cases, we present a
                 parallel graphics interface that may be used in
                 conjunction with the existing API as a new paradigm for
                 high-performance graphics applications. The parallel
                 API extends existing ideas found in OpenGL and X11 that
                 allow multiple graphics contexts to simultaneously draw
                 into the same image. Through the introduction of
                 synchronization primitives, the parallel API allows
                 parallel traversal of an explicitly ordered scene. We
                 give code examples which demonstrate how the API can be
                 used to expose parallelism while retaining many of the
                 desirable features of serial immediate-mode
                 programming. The viability of the API is demonstrated
                 by the performance of our implementation which achieves
                 scalable performance on a 24 processor system.",
  editor =       "Michael Cohen",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-403,
  pages =        "151--158",
  year =         "1998",
  title =        "The Clipmap: {A} Virtual Mipmap",
  author =       "Christopher C. Tanner and Christopher J. Migdal and
                 Michael T. Jones",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-403",
  abstract =     "We describe the clipmap, a dynamic texture
                 representation that efficiently caches textures of
                 arbitrarily large size in a finite amount of physical
                 memory for rendering at real-time rates. Further, we
                 describe a software system for managing clipmaps that
                 supports integration into demanding real-time
                 applications. We show the scale and robustness of this
                 integrated hardware/software architecture by reviewing
                 an application virtualizing a 170 gigabyte texture at
                 60 Hertz. Finally, we suggest ways that other rendering
                 systems may exploit the concepts underlying clipmaps to
                 solve related problems.",
  editor =       "Michael Cohen",
  keywords =     "Clipmap, mipmap, texture, image exploitation, terrain
                 visualization, load management, visual simulation",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
}

@InProceedings{EVL-1998-404,
  pages =        "159--168",
  year =         "1998",
  title =        "A Shading Language on Graphics Hardware: The PixelFlow
                 Shading System",
  author =       "Marc Olano and Anselmo Lastra",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-404",
  abstract =     "Over the years, there have been two main branches of
                 computer graphics image-synthesis research; one focused
                 on interactivity, the other on image quality.
                 Procedural shading is a powerful tool, commonly used
                 for creating high-quality images and production
                 animation. A key aspect of most procedural shading is
                 the use of a shading language, which allows a
                 high-level description of the color and shading of each
                 surface. However, shading languages have been beyond
                 the capabilities of the interactive graphics hardware
                 community. We have created a parallel graphics
                 multi-computer, PixelFlow, that can render images at 30
                 frames per second using a shading language. This is the
                 first system to be able to support a shading language
                 in real-time. In this paper, we describe some of the
                 techniques that make this possible.",
  editor =       "Michael Cohen",
  keywords =     "Real-time image generation, procedural shading,
                 shading language",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-405,
  pages =        "169--178",
  year =         "1998",
  title =        "Efficiently Using Graphics Hardware in Volume
                 Rendering Applications",
  author =       "R{\"{u}}diger Westermann and Thomas Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-405",
  abstract =     "OpenGL and its extensions provide access to advanced
                 per-pixel operations available in the rasterization
                 stage and in the frame buffer hardware of modern
                 graphics workstations. With these mechanisms,
                 completely new rendering algorithms can be designed and
                 implemented in a very particular way. In this paper we
                 extend the idea of extensively using graphics hardware
                 for the rendering of volumetric data sets in various
                 ways. First, we introduce the concept of clipping
                 geometries by means of stencil buffer operations, and
                 we exploit pixel textures for the mapping of volume
                 data to spherical domains. We show ways to use 3D
                 textures for the rendering of lighted and shaded
                 iso-surfaces in real-time without extracting any
                 polygonal representation. Second, we demonstrate that
                 even for volume data on unstructured grids, where only
                 software solutions exist up to now, both methods,
                 iso-surface extraction and direct volume rendering, can
                 be accelerated to new rates of interactivity by simple
                 polygon drawing and frame buffer operations.",
  editor =       "Michael Cohen",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-406,
  pages =        "179--188",
  year =         "1998",
  title =        "The Office of the Future: {A} Unified Approach to
                 Image-Based Modeling and Spatially Immersive Displays",
  author =       "Ramesh Raskar and Greg Welch and Matt Cutts and Adam
                 Lake and Lev Stesin and Henry Fuchs",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-406",
  abstract =     "We introduce ideas, proposed technologies, and initial
                 results for an office of the future that is based on a
                 unified application of computer vision and computer
                 graphics in a system that combines and builds upon the
                 notions of the CAVE(tm), tiled display systems, and
                 image-based modeling. The basic idea is to use
                 real-time computer vision techniques to dynamically
                 extract per-pixel depth and reflectance information for
                 the visible surfaces in the office including walls,
                 furniture, objects, and people, and then to either
                 project images on the surfaces, render images of the
                 surfaces, or interpret changes in the surfaces. In the
                 first case, one could designate everyday (potentially
                 irregular) real surfaces in the office to be used as
                 spatially immersive display surfaces, and then project
                 high-resolution graphics and text onto those surfaces.
                 In the second case, one could transmit the dynamic
                 image-based models over a network for display at a
                 remote site. Finally, one could interpret dynamic
                 changes in the surfaces for the purposes of tracking,
                 interaction, or augmented reality applications. To
                 accomplish the simultaneous capture and display we
                 envision an office of the future where the ceiling
                 lights are replaced by computer controlled cameras and
                 {"}smart{"} projectors that are used to capture dynamic
                 image-based models with imperceptible structured light
                 techniques, and to display high-resolution images on
                 designated display surfaces. By doing both
                 simultaneously on the designated display surfaces, one
                 can dynamically adjust or autocalibrate for geometric,
                 intensity, and resolution variations resulting from
                 irregular or changing display surfaces, or overlapped
                 projector images. Our current approach to dynamic
                 image-based modeling is to use an optimized structured
                 light scheme that can capture per-pixel depth and
                 reflectance at interactive rates. Our system
                 implementation is not yet imperceptible, but we can
                 demonstrate the approach in the laboratory. Our
                 approach to rendering on the designated (potentially
                 irregular) display surfaces is to employ a two-pass
                 projective texture scheme to generate images that when
                 projected onto the surfaces appear correct to a moving
                 head-tracked observer. We present here an initial
                 implementation of the overall vision, in an office-like
                 setting, and preliminary demonstrations of our dynamic
                 modeling and display techniques.",
  editor =       "Michael Cohen",
  keywords =     "Display, spatially immersive display, intensity
                 blending, image-based modeling, image-based rendering,
                 range, depth, reflectance, projection, virtual
                 environments, calibration, autocalibration",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-407,
  pages =        "189--198",
  year =         "1998",
  title =        "Rendering Synthetic Objects Into Real Scenes: Bridging
                 Traditional and Image-Based Graphics With Global
                 Illumination and High Dynamic Range Photography",
  author =       "Paul Debevec",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-407",
  abstract =     "We present a method that uses measured scene radiance
                 and global illumination in order to add new objects to
                 light-based models with correct lighting. The method
                 uses a high dynamic range image-based model of the
                 scene, rather than synthetic light sources, to
                 illuminate the new objects. To compute the
                 illumination, the scene is considered as three
                 components: the distant scene, the local scene, and the
                 synthetic objects. The distant scene is assumed to be
                 photometrically unaffected by the objects, obviating
                 the need for reflectance model information. The local
                 scene is endowed with estimated reflectance model
                 information so that it can catch shadows and receive
                 reflected light from the new objects. Renderings are
                 created with a standard global illumination method by
                 simulating the interaction of light amongst the three
                 components. A differential rendering technique allows
                 for good results to be obtained when only an estimate
                 of the local scene reflectance properties is known. We
                 apply the general method to the problem of rendering
                 synthetic objects into real scenes. The light-based
                 model is constructed from an approximate geometric
                 model of the scene and by using a light probe to
                 measure the incident illumination at the location of
                 the synthetic objects. The global illumination solution
                 is then composited into a photograph of the scene using
                 the differential rendering technique. We conclude by
                 discussing the relevance of the technique to recovering
                 surface reflectance properties in uncontrolled lighting
                 situations. Applications of the method include visual
                 effects, interior design, and architectural
                 visualization.",
  editor =       "Michael Cohen",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-408,
  pages =        "199--206",
  year =         "1998",
  title =        "Multiple-Center-of-Projection Images",
  author =       "Paul Rademacher and Gary Bishop",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-408",
  abstract =     "In image-based rendering, images acquired from a scene
                 are used to represent the scene itself. A number of
                 reference images are required to fully represent even
                 the simplest scene. This leads to a number of problems
                 during image acquisition and subsequent reconstruction.
                 We present the multiple-center-of-projection image, a
                 single image acquired from multiple locations, which
                 solves many of the problems of working with multiple
                 range images. This work develops and discusses
                 multiple-center-of-projection images, and explains
                 their advantages over conventional range images for
                 image-based rendering. The contributions include
                 greater flexibility during image acquisition and
                 improved image reconstruction due to greater
                 connectivity information. We discuss the acquisition
                 and rendering of multiple-center-of-projection
                 datasets, and the associated sampling issues. We also
                 discuss the unique epipolar and correspondence
                 properties of this class of image.",
  editor =       "Michael Cohen",
  keywords =     "Image-based rendering, multiple-center-of-projection
                 images",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-409,
  pages =        "207--218",
  year =         "1998",
  title =        "Recovering Photometric Properties of Architectural
                 Scenes from Photographs",
  author =       "Yizhou Yu and Jitendra Malik",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-409",
  abstract =     "In this paper, we present a new approach to producing
                 photorealistic computer renderings of real
                 architectural scenes under novel lighting conditions,
                 such as at different times of day, starting from a
                 small set of photographs of the real scene. Traditional
                 texture mapping approaches to image-based modeling and
                 rendering are unable to do this because texture maps
                 are the product of the interaction between lighting and
                 surface reflectance and one cannot deal with novel
                 lighting without dissecting their respective
                 contributions. To obtain this decomposition into
                 lighting and reflectance, our basic approach is to
                 solve a series of optimization problems to find the
                 parameters of appropriate lighting and reflectance
                 models that best explain the measured values in the
                 various photographs of the scene. The lighting models
                 include the radiance distributions from the sun and the
                 sky, as well as the landscape to consider the effect of
                 secondary illumination from the environment. The
                 reflectance models are for the surfaces of the
                 architecture. Photographs are taken for the sun, the
                 sky, the landscape, as well as the architecture at a
                 few different times of day to collect enough data for
                 recovering the various lighting and reflectance models.
                 We can predict novel illumination conditions with the
                 recovered lighting models and use these together with
                 the recovered reflectance values to produce renderings
                 of the scene. Our results show that our goal of
                 generating photorealistic renderings of real
                 architectural scenes under novel lighting conditions
                 has been achieved.",
  editor =       "Michael Cohen",
  keywords =     "Photometric Properties, Image-based Rendering,
                 Illumination, Sky Model, Reflectance, BRDF, Photometric
                 Stereo",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@TechReport{EVL-1998-41,
  year =         "1998",
  title =        "Dava Visualization, Indexing and Mining Engine --- {A}
                 Parallel Computing Architecture for Information
                 Processing Over the Internet",
  type =         "Technical Report",
  author =       "Xiannong Meng and Zhixiang Chen and Richard Fowler and
                 Richard Fox and Wendy Lawrence-Fowler",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-41",
  language =     "en",
  abstract =     "DaVIME (Data Visualization, Indexing and Mining
                 Engine) is a software architecture that does data
                 visualization, indexing and mining in an integraged
                 environment. It serves user's various types of
                 information requests by invoking different components
                 of DaVIME, many of which work in parallel such as
                 search, indexing and categorization.",
  month =        feb,
  number =       "CS-98-17",
  institution =  "University of Texas Pan American, Department of
                 Computer Science",
}

@InProceedings{EVL-1998-410,
  pages =        "219--230",
  year =         "1998",
  title =        "Visibility Sorting and Compositing Without Splitting
                 for Image Layer Decomposition",
  author =       "John Snyder and Jed Lengyel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-410",
  abstract =     "We present an efficient algorithm for visibility
                 sorting a set of moving geometric objects into a
                 sequence of image layers which are composited to
                 produce the final image. Instead of splitting the
                 geometry as in previous visibility approaches, we
                 detect mutual occluders and resolve them using an
                 appropriate image compositing expression or merge them
                 into a single layer. Such an algorithm has many
                 applications in computer graphics; we demonstrate two:
                 rendering acceleration using image interpolation and
                 visibility-correct depth of field using image blurring.
                 We propose a new, incremental method for identifying
                 mutually occluding sets of objects and computing a
                 visibility sort among these sets. Occlusion queries are
                 accelerated by testing on convex bounding hulls; less
                 conservative tests are also discussed. Kd-trees formed
                 by combinations of directions in object or image space
                 provide an initial cull on potential occluders, and
                 incremental collision detection algorithms are adapted
                 to resolve pairwise occlusions, when necessary. Mutual
                 occluders are further analyzed to generate an image
                 compositing expression; in the case of nonbinary
                 occlusion cycles, an expression can always be generated
                 without merging the objects into a single layer.
                 Results demonstrate that the algorithm is practical for
                 real-time animation of scenes involving hundreds of
                 objects each comprising hundreds or thousands of
                 polygons.",
  editor =       "Michael Cohen",
  keywords =     "Visibility sorting, compositing, nonsplitting layered
                 decomposition, occlusion cycle, occlusion graph,
                 sprite, kd-tree",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-411,
  pages =        "231--242",
  year =         "1998",
  title =        "Layered Depth Images",
  author =       "Jonathan Shade and Steven J. Gortler and Li-wei He and
                 Richard Szeliski",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-411",
  abstract =     "In this paper we present a set of efficient image
                 based rendering methods capable of rendering multiple
                 frames per second on a PC. The first method warps
                 Sprites with Depth representing smooth surfaces without
                 the gaps found in other techniques. A second method for
                 more general scenes performs warping from an
                 intermediate representation called a Layered Depth
                 Image (LDI). An LDI is a viewof the scene from a single
                 input camera view, but with multiple pixels along each
                 line of sight. The size of the representation grows
                 only linearly with the observed depth complexity in the
                 scene. Moreover, because the LDI data are represented
                 in a single image coordinate system, McMillan's warp
                 ordering algorithm can be successfully adapted. As a
                 result, pixels are drawn in the output image in
                 back-to-front order. No z-buffer is required, so
                 alpha-compositing can be done efficiently without depth
                 sorting. This makes splatting an efficient solution to
                 the resampling problem.",
  editor =       "Michael Cohen",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-412,
  pages =        "243--254",
  year =         "1998",
  title =        "Multiple Viewpoint Rendering",
  author =       "Michael Halle",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-412",
  abstract =     "This paper presents an algorithm for rendering a
                 static scene from multiple perspectives. While most
                 current computer graphics algorithms render scenes as
                 they appear from a single viewpoint (the location of
                 the camera) multiple viewpoint rendering (MVR) renders
                 a scene from a range of spatially-varying viewpoints.
                 By exploiting perspective coherence, MVR can produce a
                 set of images orders of magnitude faster than
                 conventional rendering methods. Images produced by MVR
                 can be used as input to multiple-perspective displays
                 such as holographic stereograms, lenticular sheet
                 displays, and holographic video. MVR can also be used
                 as a geometry-to-image prefilter for image-based
                 rendering algorithms. MVR techniques are adapted from
                 single viewpoint computer graphics algorithms and can
                 be accelerated using existing hardware graphics
                 subsystems. This paper describes the characteristics of
                 MVR algorithms in general, along with the design,
                 implementation, and applications of a particular MVR
                 rendering system.",
  editor =       "Michael Cohen",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-413,
  pages =        "255--266",
  year =         "1998",
  title =        "Progressive Radiance Evaluation Using Directional
                 Coherence Maps",
  author =       "Baining Guo",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-413",
  abstract =     "We develop a progressive refinement algorithm that
                 generates an approximate image quickly, then gradually
                 refines it towards the final result. Our algorithm can
                 reconstruct a high-quality image after evaluating only
                 a small percentage of the pixels. For a typical scene,
                 evaluating only 6% of the pixels yields an approximate
                 image that is visually hard to distinguish from an
                 image with all the pixels evaluated. At this low
                 sampling rate, previous techniques such as adaptive
                 stochastic sampling suffer from artifacts including
                 heavily jagged edges, missing object parts, and missing
                 high-frequency details. A key ingredient of our
                 algorithm is the directional coherence map (DCM), a new
                 technique for handling general radiance discontinuities
                 in a progressive ray tracing framework. Essentially an
                 encoding of the directional coherence in image space,
                 the DCM performs well on discontinuities that are
                 usually considered extremely difficult, e.g. those
                 involving non-polygonal geometry or caused by secondary
                 light sources. Incorporating the DCM into a ray tracing
                 system incurs only a negligible amount of additional
                 computation. More importantly, the DCM uses little
                 memory and thus it preserves the strengths of ray
                 tracing systems in dealing with complex scenes. We have
                 implemented our algorithm on top of RADIANCE. Our
                 enhanced system can produce high-quality images
                 significantly faster than RADIANCE - sometimes by
                 orders of magnitude. Moreover, when the baseline system
                 becomes less effective as its Monte Carlo components
                 are challenged by difficult lighting configurations,
                 our system will still produce high quality images by
                 redistributing computation to the small percentage of
                 pixels as dictated by the DCM.",
  editor =       "Michael Cohen",
  keywords =     "Progressive refinement, image-space discontinuities,
                 directional coherence, radiance evaluation, rendering",
  series =       "Annual Conference Series",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-414,
  pages =        "267--274",
  year =         "1998",
  title =        "Reproducing Color Images Using Custom Inks",
  author =       "Eric J. Stollnitz and Victor Ostromoukhov and David H.
                 Salesin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-414",
  abstract =     "We investigate the general problem of reproducing
                 color images on an offset press using custom inks in
                 any combination and number. While this problem has been
                 explored previously for the case of two inks, there are
                 a number of new mathematical and algorithmic challenges
                 that arise as the number of inks increases. These
                 challenges include more complex gamut mapping
                 strategies, more efficient ink selection strategies,
                 and fast and numerically accurate methods for computing
                 ink separations in situations that may be either over-
                 or under-constrained. In addition, the demands of
                 high-quality color printing require an accurate
                 physical model of the colors that result from
                 overprinting multiple inks using halftoning, including
                 the effects of trapping, dot gain, and the
                 interreflection of light between ink layers. In this
                 paper, we explore these issues related to printing with
                 multiple custom inks, and address them with new
                 algorithms and physical models. Finally, we present
                 some printed examples demonstrating the promise of our
                 methods.",
  editor =       "Michael Cohen",
  keywords =     "Color reproduction, color printing, gamut mapping, ink
                 selection, Kubelka-Munk model, Neugebauer model,
                 separations",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-415,
  pages =        "275--286",
  year =         "1998",
  title =        "Realistic Modeling and Rendering of Plant Ecosystems",
  author =       "Oliver Deussen and Patrick Hanrahan and Bernd
                 Lintermann and Radom{\'{i}}r Mech and Matt Pharr and
                 Przemyslaw Prusinkiewicz",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-415",
  abstract =     "Modeling and rendering of natural scenes with
                 thousands of plants poses a number of problems. The
                 terrain must be modeled and plants must be distributed
                 throughout it in a realistic manner, reflecting the
                 interactions of plants with each other and with their
                 environment. Geometric models of individual plants,
                 consistent with their positions within the ecosystem,
                 must be synthesized to populate the scene. The scene,
                 which may consist of billions of primitives, must be
                 rendered efficiently while incorporating the subtleties
                 of lighting in a natural environment. We have developed
                 a system built around a pipeline of tools that address
                 these tasks. The terrain is designed using an
                 interactive graphical editor. Plant distribution is
                 determined by hand (as one would do when designing a
                 garden), by ecosystem simulation, or by a combination
                 of both techniques. Given parametrized procedural
                 models of individual plants, the geometric complexity
                 of the scene is reduced by approximate instancing, in
                 which similar plants, groups of plants, or plant organs
                 are replaced by instances of representative objects
                 before the scene is rendered. The paper includes
                 examples of visually rich scenes synthesized using the
                 system.",
  editor =       "Michael Cohen",
  keywords =     "Realistic image synthesis, modeling of natural
                 phenomena, ecosystem simulation, self-thinning, plant
                 model, vector quantization, approximate instancing",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-416,
  pages =        "287--298",
  year =         "1998",
  title =        "A Multiscale Model of Adaptation and Spatial Vision
                 for Realistic Image Display",
  author =       "Sumanta N. Pattanaik and James A. Ferwerda and Mark D.
                 Fairchild and Donald P. Greenberg",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-416",
  abstract =     "In this paper we develop a computational model of
                 adaptation and spatial vision for realistic tone
                 reproduction. The model is based on a multiscale
                 representation of pattern, luminance, and color
                 processing in the human visual system. We incorporate
                 the model into a tone reproduction operator that maps
                 the vast ranges of radiances found in real and
                 synthetic scenes into the small fixed ranges available
                 on conventional display devices such as CRTs and
                 printers. The model allows the operator to address the
                 two major problems in realistic tone reproduction: wide
                 absolute range and high dynamic range scenes can be
                 displayed; and the displayed images match our
                 perceptions of the scenes at both threshold and
                 suprathreshold levels to the degree possible given a
                 particular display device. Although in this paper we
                 apply our visual model to the tone reproduction
                 problem, the model is general and can be usefully
                 applied to image quality metrics, image compression
                 methods, and perceptually-based image synthesis
                 algorithms.",
  editor =       "Michael Cohen",
  keywords =     "Realistic imaging, visual perception, tone
                 reproduction, adaptation, spatial vision",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-417,
  pages =        "299--310",
  year =         "1998",
  title =        "A Perceptually Based Adaptive Sampling Algorithm",
  author =       "Mark R. Bolin and Gary W. Meyer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-417",
  abstract =     "A perceptually based approach for selecting image
                 samples has been developed. An existing image
                 processing vision model has been extended to handle
                 color and has been simplified to run efficiently. The
                 resulting new image quality model was inserted into an
                 image synthesis program by first modifying the
                 rendering algorithm so that it computed a wavelet
                 representation. In addition to allowing image quality
                 to be determined as the image was generated, the
                 wavelet representation made it possible to use
                 statistical information about the spatial frequency
                 distribution of natural images to estimate values where
                 samples were yet to be taken. Tests on the image
                 synthesis algorithm showed that it correctly handled
                 achromatic and chromatic spatial detail and that it was
                 able predict and compensate for masking effects. The
                 program was also shown to produce images of equivalent
                 visual quality while using different rendering
                 techniques.",
  editor =       "Michael Cohen",
  keywords =     "Adaptive Sampling, Perception, Masking, Vision
                 Models",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-418,
  pages =        "311--320",
  year =         "1998",
  title =        "Efficient Simulation of Light Transport in Scenes With
                 Participating Media Using Photon Maps",
  author =       "Henrik Wann Jensen and Per H. Christensen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-418",
  abstract =     "This paper presents a new method for computing global
                 illumination in scenes with participating media. The
                 method is based on bidirectional Monte Carlo ray
                 tracing and uses photon maps to increase efficciency
                 and reduce noise. We remove previous restrictions
                 limiting the photon map method to surfaces by
                 introducing a volume photon map containing photons in
                 participating media. We also derive a new radiance
                 estimate for photons in the volume photon map. The
                 method is fast and simple, but also general enough to
                 handle nonhomogeneous media and anisotropic scattering.
                 It can efficiently simulate effects such as multiple
                 volume scattering, color bleeding between volumes and
                 surfaces, and volume caustics (light reflected from or
                 transmitted through specular surfaces and then
                 scattered by a medium). The photon map is decoupled
                 from the geometric representation of the scene, making
                 the method capable of simulating global illumination in
                 scenes containing complex objects. These objects do not
                 need to be tessellated; they can be instanced, or even
                 represented by an implicit function. Since the method
                 is based on a bidirectional simulation, it
                 automatically adapts to illumination and view.
                 Furthermore, because the use of photon maps reduces
                 noise and aliasing, the method is suitable for
                 rendering of animations.",
  editor =       "Michael Cohen",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
}

@InProceedings{EVL-1998-419,
  pages =        "321--332",
  year =         "1998",
  title =        "Fast Calculation of Soft Shadow Textures Using
                 Convolution",
  author =       "Cyril Soler and Fran{\c{c}}ois X. Sillion",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-419",
  abstract =     "The calculation of detailed shadows remains one of the
                 most difficult challenges in computer graphics,
                 especially in the case of extended (linear or area)
                 light sources. This paper introduces a new tool for the
                 calculation of shadows cast by extended light sources.
                 Exact shadows are computed in some constrained
                 configurations by using a convolution technique,
                 yielding a fast and accurate solution. Approximate
                 shadows can be computed for general configurations by
                 applying the convolution to a representative
                 {"}ideal{"} configuration. We analyze the various
                 sources of approximation in the process and derive a
                 hierarchical, error-driven algorithm for fast shadow
                 calculation in arbitrary configurations using a
                 hierarchy of object clusters. The convolution is
                 performed on omages rendered in an offscreen buffer and
                 produces a shadow map used as a texture to modulate the
                 unoccluded illumination. Light sources can have any 3D
                 shape as well as arbitrary emission characteristics,
                 while shadow maps can be applied to groups of objects
                 at once. The method can be employed in a hierarchical
                 radiosity system, or directly as a shadowing technique.
                 We demonstrate results for various scenes, showing that
                 soft shadows can be generated at interactive rates for
                 dynamic environments.",
  editor =       "Michael Cohen",
  keywords =     "Soft shadows, convolution, shadow map, error-driven
                 illumination, texture",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-42,
  year =         "1998",
  title =        "Particle Tracing on Sparse Grids",
  author =       "C. Teitzel and R. Grosso and T. Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-42",
  language =     "en",
  abstract =     "These days sparse grids are of increasing interest in
                 numerical simulations. Based upon hierarchical tensor
                 product bases, the sparse grid approach is a very
                 efficient one improving the ratio of invested storage
                 and computing time to the achieved accuracy for many
                 problems in the area of numerical solution of
                 differential equations, for instance in numerical fluid
                 mechanics. The particle tracing algorithms that are
                 available so far cannot cope with sparse grids. Now we
                 present an approach that directly works on sparse
                 grids. As a second aspect in this paper, we suggest to
                 use sparse grids as a data compression method in order
                 to visualize huge data sets even on small workstations.
                 Because the size of data sets used in numerical
                 simulations is still growing, this feature makes it
                 possible that workstations can continue to handle these
                 data sets.",
  organization = "Eurographics",
  editor =       "D. Bartz",
  copyright =    "Springer Verlag, Wien, New York",
  booktitle =    "Visualization in Scientific Computing '98, Proceedings
                 of the Eurographics Workshop in Blaubeuren, Germany",
  publisher =    "Springer Verlag, Wien, New York",
}

@InProceedings{EVL-1998-420,
  pages =        "333--342",
  year =         "1998",
  title =        "Interactive Reflections on Curved Objects",
  author =       "Eyal Ofek and Ari Rappoport",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-420",
  abstract =     "Global view-dependent illumination phenomena, in
                 particular reflections, greatly enhance the realism of
                 computer-generated imagery. Current interactive
                 rendering methods do not provide satisfactory support
                 for reflections on curved objects. In this paper we
                 present a novel method for interactive computation of
                 reflections on curved objects. We transform potentially
                 reflected scene objects according to reflectors, to
                 generate virtual objects. These are rendered by the
                 graphics system as ordinary objects, creating a
                 reflection image that is blended with the primary
                 image. Virtual objects are created by tessellating
                 scene objects and computing a virtual vertex for each
                 resulting scene vertex. Virtual vertices are computed
                 using a novel space subdivision, the reflection
                 subdivision. For general polygonal mesh reflectors, we
                 present an associated approximate acceleration scheme,
                 the explosion map. For specific types of objects (e.g.,
                 linear extrusions of planar curves) the reflection
                 subdivision can be reduced to a 2-D one that is
                 utilized more accurately and efficiently.",
  editor =       "Michael Cohen",
  keywords =     "Ray tracing, interactive reflections, virtual objects
                 method, reflection subdivision, explosion map. dynamic
                 scenes",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-421,
  pages =        "343--352",
  year =         "1998",
  title =        "Non-Distorted Texture Mapping for Sheared Triangulated
                 Meshes",
  author =       "Bruno L{\'{e}}vy and Jean-Laurent Mallet",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-421",
  abstract =     "This article introduces new techniques for
                 non-distorted texture mapping on complex triangulated
                 meshes. Texture coordinates are assigned to the
                 vertices of the triangulation by using an iterative
                 optimization algorithm, honoring a set of constraints
                 minimizing the distortions. As compared to other global
                 optimization techniques, our method allows the user to
                 specify the surface zones where distortions should be
                 minimized in order of preference. The modular approach
                 described in this paper results in a highly flexible
                 method, facilitating a customized mapping construction.
                 For instance, it is easy to align the texture on the
                 surface with a set of user defined isoparametric
                 curves. Moreover, the mapping can be made continuous
                 through cuts, allowing to parametrize in one go complex
                 cut surfaces. It is easy to specify other constraints
                 to be honored by the so-constructed mappings, as soon
                 as they can be expressed by linear (or linearizable)
                 relations. This method has been integrated successfully
                 within a widely used CAD software dedicated to
                 geosciences. In this context, applications of the
                 method comprise numerical computations of physical
                 properties stored in fine grids within texture space,
                 unfolding geological layers and generating grids that
                 are suitable for finite element analysis. The impact of
                 the method could be also important for 3D paint
                 systems.",
  editor =       "Michael Cohen",
  keywords =     "Non Distorted Texture Mapping, Parametrization,
                 Discrete Smooth Interpolation, Optimization",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-422,
  pages =        "353--360",
  year =         "1998",
  title =        "Techniques for Handling Video in Virtual
                 Environments",
  author =       "Gianpaolo U. Carraro and John T. Edmark and J. Robert
                 Ensor",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-422",
  abstract =     "This paper discusses ways to incorporate video
                 displays into virtual environments. It focuses on the
                 virtual worlds created by a distributed multi-user
                 simulator. Still images or video streams represent
                 spaces within these three-dimensional worlds. The paper
                 introduces techniques to deal with avatar movement into
                 and out of video regions. In one technique - media
                 melding - when an object moves from one region to
                 another, the media used to represent that object
                 correspondingly change. In a second technique - object
                 tracing - when an object moves from one region to
                 another, its actions in the second region are
                 represented by a trace object in the first region.
                 Pyramidic panels provide a means of dealing with
                 viewpoint changes so that two-dimensional images and
                 video clips can successfully simulate three-dimensional
                 spaces. The paper concludes by suggesting ways to
                 extend our techniques and by listing possible future
                 studies.",
  editor =       "Michael Cohen",
  keywords =     "Virtual worlds, virtual environments, camera
                 placement, VRML",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-423,
  pages =        "361--370",
  year =         "1998",
  title =        "A Distributed 3{D} Graphics Library",
  author =       "Blair MacIntyre and Steven Feiner",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-423",
  abstract =     "We present Repo-3D, a general-purpose, object-oriented
                 library for developing distributed, interactive 3D
                 graphics applications across a range of heterogeneous
                 workstations. Repo-3D is designed to make it easy for
                 programmers to rapidly build prototypes using a
                 familiar multi-threaded, object-oriented programming
                 paradigm. All data sharing of both graphical and
                 non-graphical data is done via general-purpose remote
                 and replicated objects, presenting the illusion of a
                 single distributed shared memory. Graphical objects are
                 directly distributed, circumventing the {"}duplicate
                 database{"} problem and allowing programmers to focus
                 on the application details. Repo-3D is embedded in
                 Repo, an interpreted, lexically-scoped, distributed
                 programming language, allowing entire applications to
                 be rapidly prototyped. We discuss Repo-3D's design, and
                 introduce the notion of local variations to the
                 graphical objects, which allow local changes to be
                 applied to shared graphical structures. Local
                 variations are needed to support transient local
                 changes, such as highlighting, and responsive local
                 editing operations. Finally, we discuss how our
                 approach could be applied using other programming
                 languages, such as Java.",
  editor =       "Michael Cohen",
  keywords =     "Object-oriented graphics, distributed shared memory,
                 distributed virtual environments, shared-data object
                 model",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-424,
  pages =        "371--378",
  year =         "1998",
  title =        "Constellation: {A} Wide-Range Wireless Motion-Tracking
                 System for Augmented Reality and Virtual Set
                 Application",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-424",
  author =       "Eric Foxlin and Michael Harrington and George
                 Pfeifer",
  abstract =     "We present a new tracking system for augmented reality
                 and virtual set applications, based on an inertial
                 navigation system aided by ultrasonic time-of-flight
                 range measurements to a constellation of wireless
                 transponder beacons. An extended Kalman filter
                 operating on 1-D range measurements allows the inertial
                 sensors to filter out corrupt range measurements and
                 perform optimal smoothing and prediction, while at the
                 same time using the pre-screened range measurements to
                 correct the drift of the inertial system. The use of
                 inside-out ultrasonic tracking allows for tetherless
                 tracking over a building-wide range with no acoustic
                 propagation latency. We have created a simulation to
                 account for error sources in the ultrasonic ranging
                 system. The fully implemented tracking system is tested
                 and found to have accuracy consistent with the
                 simulation results. The simulation also predicts that
                 with some further compensation of transducer
                 misalignment, accuracies better than 2 mm can be
                 achieved.",
  editor =       "Michael Cohen",
  keywords =     "Motion tracking, inertial, ultrasonic, kalman
                 filtering, augmented reality, virtual sets, accuracy,
                 latency, sensor fusion",
  series =       "Annual Conference Series",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-425,
  pages =        "379--386",
  year =         "1998",
  title =        "mediaBlocks: Physical Containers, Transports, and
                 Controls for Online Media",
  author =       "Brygg Ullmer and Hiroshi Ishii and Dylan Glas",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-425",
  abstract =     "We present a tangible user interface based upon
                 mediaBlocks: small, electronically tagged wooden blocks
                 that serve as physical icons ({"}phicons{"}) for the
                 containment, transport, and manipulation of online
                 media. MediaBlocks interface with media input and
                 output devices such as video cameras and projectors,
                 allowing digital media to be rapidly {"}copied{"} from
                 a media source and {"}pasted{"} into a media display.
                 MediaBlocks are also compatible with traditional GUIs,
                 providing seamless gateways between tangible and
                 graphical interfaces. Finally, mediaBlocks act as
                 physical {"}controls{"} in tangible interfaces for
                 tasks such as sequencing collections of media
                 elements.",
  editor =       "Michael Cohen",
  keywords =     "Tangible user interface, tangible bits, phicons,
                 physical constraints, ubiquitous computing",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-426,
  pages =        "387--394",
  year =         "1998",
  title =        "Non-Uniform Recursive Subdivision Surfaces",
  author =       "Thomas W. Sederberg and Jianmin Zheng and David Sewell
                 and Malcolm Sabin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-426",
  abstract =     "Sabin and Catmull-Clark subdivision surfaces are based
                 on the notion of repeated knot insertion of uniform
                 tensor product B-spline surfaces. This paper develops
                 rules for non-uniform Doo-Sabin and Clark surfaces that
                 generalize non-uniform tensor product spline surfaces
                 to arbitrary topologies. This added flexibility allows,
                 among other things, the natural introduction of
                 features such as cusps, creases, and darts, while
                 elsewhere maintaining the same order of tinuity as
                 their uniform counterparts.",
  editor =       "Michael Cohen",
  keywords =     "B-splines, Doo-Sabin surfaces, Catmull-Clark surface",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-427,
  pages =        "395--404",
  year =         "1998",
  title =        "Exact Evaluation of Catmull-Clark Subdivision Surfaces
                 at Arbitrary Parameter Values",
  author =       "Jos Stam",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-427",
  abstract =     "In this paper we disprove the belief widespread within
                 the computer graphics community that Catmull-Clark
                 subdivision surfaces cannot be evaluated directly
                 without explicitly subdividing. We show that the
                 surface and all its derivatives can be evaluated in
                 terms of a set of eigenbasis functions which depend
                 only on the subdivision scheme and we derive analytical
                 expressions for these basis functions. In particular,
                 on the regular part of the control mesh where
                 Catmull-Clark surfaces are bi-cubic B-splines, the
                 eigenbasis is equal to the power basis. Also, our
                 technique is both easy to implement and efficient. We
                 have used our implementation to compute high quality
                 curvature plots of subdivision surfaces. The cost of
                 our evaluation scheme is comparable to that of a
                 bicubic spline. Therefore, our method allows many
                 algorithms developed for parametric surfaces to be
                 applied to Catmull-Clark subdivision surfaces. This
                 makes subdivision surfaces an even more attractive tool
                 for free-form surface modeling.",
  editor =       "Michael Cohen",
  keywords =     "Subdivision surfaces, eigenanalysis, linear algebra,
                 parametrizations, surface evaluation, Catmull-Clark
                 surfaces",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-428,
  pages =        "405--414",
  year =         "1998",
  title =        "Wires: {A} Geometric Deformation Technique",
  author =       "Karan Singh and Eugene Fiume",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-428",
  abstract =     "Finding effective interactive deformation techniques
                 for complex geometric objects continues to be a
                 challenging problem in modeling and animation. We
                 present an approach that is inspired by armatures used
                 by sculptors, in which wire curves give definition to
                 an object and shape its deformable features. We also
                 introduce domain curves that define the domain of
                 deformation about an object. A wire together with a
                 collection of domain curves provide a new basis for an
                 implicit modeling primitive. Wires directly reflect
                 object geometry, and as such they provide a coarse
                 geometric representation of an object that can be
                 created through sketching. Furthermore, the aggregate
                 deformation from several wires is easy to define. We
                 show that a single wire is an appealing direct
                 manipulation deformation technique; we demonstrate that
                 the combination of wires and domain curves provide a
                 new way to outline the shape of an implicit volume in
                 space; and we describe techniques for the aggregation
                 of deformations resulting from multiple wires, domain
                 curves and their interaction with each other and other
                 deformation techniques. The power of our approach is
                 illustrated using applications of animating figures
                 with flexible articulations, modeling wrinkled surfaces
                 and stitching geometry together",
  editor =       "Michael Cohen",
  keywords =     "Deformations, implicit models, interactive graphics,
                 animation",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-429,
  pages =        "415--422",
  year =         "1998",
  title =        "A New Voronoi-Based Surface Reconstruction Algorithm",
  author =       "Nina Amenta and Marshall Bern and Manolis
                 Kamvysselis",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-429",
  abstract =     "Abstract We describe our experience with a new
                 algorithm for the reconstruction of surfaces from
                 unorganized sample points in R3. The algorithm is the
                 first for this problem with provable guarantees. Given
                 a {"}good sample{"} from a smooth surface, the output
                 is guaranteed to be topologically correct and
                 convergent to the original surface as the sampling
                 density increases. The definition of a good sample is
                 itself interesting: the required sampling density
                 varies locally, rigorously capturing the intuitive
                 notion that featureless areas can be reconstructed from
                 fewer samples. The output mesh interpolates, rather
                 than approximates, the input points. Our algorithm is
                 based on the three-dimensional Voronoi diagram. Given a
                 good program for this fundamental subroutine, the
                 algorithm is quite easy to implement.",
  editor =       "Michael Cohen",
  keywords =     "Medial axis, Sampling, Delaunay triangulation,
                 Computational Geometry",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-43,
  year =         "1998",
  title =        "Adaptively adjusting Marching Cubes output to fit a
                 trilinear reconstruction filte",
  author =       "F. Allamandri and P. Cignoni and C. Montani and R.
                 Scopigno",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-43",
  language =     "en",
  abstract =     "The paper focuses on the improvement of the quality of
                 isosurfaces fitted on volume datasets with respect to
                 standard MC solutions. The new solution presented
                 improves the precision in the reconstruction process
                 using an approach based on mesh refinement and driven
                 by the evaluation of the trilinear reconstruction
                 filter. The iso-surface reconstruction process is
                 adaptive, to ensure that the complexity of the fitted
                 mesh will not become excessive. The proposed approach
                 has been tested on many datasets; we discuss the
                 precision of the obtained meshs and report data on
                 fitted meshes complexity and processing times.",
  organization = "Eurographics",
  editor =       "D. Bartz",
  copyright =    "Springer Verlag, Wien, New York",
  booktitle =    "Visualization in Scientific Computing '98, Proceedings
                 of the Eurographics Workshop in Blaubeuren, Germany",
  publisher =    "Springer Verlag, Wien, New York",
}

@InProceedings{EVL-1998-430,
  pages =        "423--434",
  year =         "1998",
  title =        "Computer-Generated Floral Ornament",
  author =       "Michael T. Wong and Douglas E. Zongker and David H.
                 Salesin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-430",
  abstract =     "This paper describes some of the principles of
                 traditional floral ornamental design, and explores ways
                 in which these designs can be created algorithmically.
                 It introduces the idea of {"}adaptive clip art,{"}
                 which encapsulates the rules for creating a specific
                 ornamental pattern. Adaptive clip art can be used to
                 generate patterns that are tailored to fit a
                 particularly shaped region of the plane. If the region
                 is resized or reshaped, the ornament can be
                 automatically re-generated to fill this new area in an
                 appropriate way. Our ornamental patterns are created in
                 two steps: first, the geometry of the pattern is
                 generated as a set of two-dimensional curves and filled
                 boundaries; second, this geometry is rendered in any
                 number of styles. We demonstrate our approach with a
                 variety of floral ornamental designs.",
  editor =       "Michael Cohen",
  keywords =     "Adaptive clip art, conventionalization, pattern
                 generation, plant development, ornamentation, texture
                 generation",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-431,
  pages =        "435--446",
  year =         "1998",
  title =        "Texture Mapping for Cel Animation",
  author =       "Wagner Toledo Corr{\^{e}}a and Robert J. Jensen and
                 Craig E. Thayer and Adam Finkelstein",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-431",
  abstract =     "We present a method for applying complex textures to
                 hand-drawn characters in cel animation. The method
                 correlates features in a simple, textured, 3-D model
                 with features on a hand-drawn figure, and then distorts
                 the model to conform to the hand-drawn artwork. The
                 process uses two new algorithms: a silhouette detection
                 scheme and a depth-preserving warp. The silhouette
                 detection algorithm is simple and efficient, and it
                 produces continuous, smooth, visible contours on a 3-D
                 model. The warp distorts the model in only two
                 dimensions to match the artwork from a given camera
                 perspective, yet preserves 3-D effects such as
                 self-occlusion and foreshortening. The entire process
                 allows animators to combine complex textures with
                 hand-drawn artwork, leveraging the strengths of 3-D
                 computer graphics while retaining the expressiveness of
                 traditional hand-drawn cel animation.",
  editor =       "Michael Cohen",
  keywords =     "Cel animation, texture mapping, silhouette detection,
                 warp, metamorphosis, morph, non-photorealistic
                 rendering",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-432,
  pages =        "447--452",
  year =         "1998",
  title =        "A Non-Photorealistic Lighting Model for Automatic
                 Technical Illustration",
  author =       "Amy Gooch and Bruce Gooch and Peter Shirley and Elaine
                 Cohen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-432",
  abstract =     "Phong-shaded 3D imagery does not provide geometric
                 information of the same richness as human-drawn
                 technical illustrations. A non-photorealistic lighting
                 model is presented that attempts to narrow this gap.
                 The model is based on practice in traditional technical
                 illustration, where the lighting model uses both
                 luminance and changes in hue to indicate surface
                 orientation, reserving extreme lights and darks for
                 edge lines and highlights. The lighting model allows
                 shading to occur only in mid-tones so that edge lines
                 and highlights remain visually prominent. In addition,
                 we show how this lighting model is modified when
                 portraying models of metal objects. These illustration
                 methods give a clearer picture of shape, structure, and
                 material composition than traditional computer graphics
                 methods.",
  editor =       "Michael Cohen",
  keywords =     "Illustration, non-photorealistic rendering,
                 silhouettes, lighting models, tone, color, shading",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-433,
  pages =        "453--460",
  year =         "1998",
  title =        "Painterly Rendering with Curved Brush Strokes of
                 Multiple Sizes",
  author =       "Aaron Hertzmann.",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-433",
  abstract =     "We present a new method for creating an image with a
                 hand-painted appearance from a photograph, and a new
                 approach to designing styles of illustration. We
                 {"}paint{"} an image with a series of spline brush
                 strokes. Brush strokes are chosen to match colors in a
                 source image. A painting is built up in a series of
                 layers, starting with a rough sketch drawn with a large
                 brush. The sketch is painted over with progressively
                 smaller brushes, but only in areas where the sketch
                 differs from the blurred source image. Thus, visual
                 emphasis in the painting corresponds roughly to the
                 spatial energy present in the source image. We
                 demonstrate a technique for painting with long, curved
                 brush strokes, aligned to normals of image gradients.
                 Thus we begin to explore the expressive quality of
                 complex brush strokes. Rather than process images with
                 a single manner of painting, we present a framework for
                 describing a wide range of visual styles. A style is
                 described as an intuitive set of parameters to the
                 painting algorithm that a designer can adjust to vary
                 the style of painting. We show examples of images
                 rendered with different styles, and discuss long-term
                 goals for expressive rendering styles as a
                 general-purpose design tool for artists and
                 animators.",
  editor =       "Michael Cohen",
  keywords =     "Non-photorealistic rendering",
  series =       "Annual Conference Series, Addison Wesley",
  booktitle =    "Proceedings of SIGGRAPH 98",
  publisher =    "Addison Wesley",
}

@InProceedings{EVL-1998-434,
  pages =        "29--54",
  year =         "1998",
  title =        "Adaptive Supersampling in Object Space Using Pyramidal
                 Rays",
  author =       "Jon Genetti and Dan Gordon and Glen Williams",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-434",
  abstract =     "We introduce a new approach to three important
                 problems in ray tracing: antialiasing, distributed
                 light sources, and fuzzy reflections of lights and
                 other surfaces. For antialiasing, our approach combines
                 the quality of supersampling with the advantages of
                 adaptive supersampling. In adaptive supersampling, the
                 decision to partition a ray is taken in image-space,
                 which means that small or thin objects may be missed
                 entirely. This is particularly problematic in
                 animation, where the intensity of such objects may
                 appear to vary. Our approach is based on considering
                 pyramidal rays (pyrays) formed by the viewpoint and the
                 pixel. We test the proximity of a pyray to the boundary
                 of an object, and if it is close (or marginal), the
                 pyray splits into 4 sub-pyrays; this continues
                 recursively with each marginal sub-pyray until the
                 estimated change in pixel intensity is sufficiently
                 small. The same idea also solves the problem of soft
                 shadows from distributed light sources, which can be
                 calculated to any required precision. Our approach also
                 enables a method of defocusing reflected pyrays,
                 thereby producing realistic fuzzy reflections of light
                 sources and other objects. An interesting byproduct of
                 our method is a substantial speedup over regular
                 supersampling even when all pixels are supersampled.
                 Our algorithm was implemented on polygonal and circular
                 objects, and produced images comparable in quality to
                 stochastic sampling, but with greatly reduced run
                 times.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(1)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-435,
  pages =        "3--15",
  year =         "1998",
  title =        "Color Fidelity in Computer Graphics: a Survey",
  author =       "Gilles Rougeron and Bernard P{\'{e}}roche",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-435",
  abstract =     "The purpose of this paper is to make a state of the
                 art for color fidelity in computer graphics. Color
                 fidelity includes three steps. The first one is the
                 spectral rendering phase which attributes a spectrum to
                 each pixel of a picture. During the second step, a
                 spectral data is transformed into a set of tristimulus
                 values in the XYZ color space. The purpose of the third
                 step, called Color Reproduction Function, is to
                 determine the RGB values displayable on the screen, in
                 such a way that subjective fidelity is reached. We
                 especially detail the two last steps of the color
                 fidelity process; we also point out the work still
                 remaining to be done in this field and we propose some
                 research ways.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(1)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-436,
  pages =        "55--71",
  year =         "1998",
  title =        "The Priority Face Determination Tree for Hidden
                 Surface Removal",
  author =       "A. James and A. M. Day",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-436",
  abstract =     "Many virtual environments are built from a set of
                 polygons that form the basis of objects in the scene.
                 Using priority-list algorithms, the sequence in which
                 these polygons are drawn is dependent upon the location
                 of an observer; the polygons must be ordered correctly
                 before a realistic image can be displayed. It is
                 necessary for a scene to be drawn correctly in real
                 time from all locations before the observer can move
                 interactively around the scene with complete freedom.
                 The binary-space partitioning (BSP) tree developed by
                 Fuchs, Kedem and Naylor in 1980 stores the view
                 independent priority of a set of polygons which can be
                 used to obtain the correct order for any given
                 view-point. However, the number of polygons grows
                 significantly due to the BSP splitting stage,
                 increasing the number of nodes in the tree. This
                 affects linearly the number of tests necessary to
                 traverse the tree to obtain the priority of the set of
                 polygons. The algorithm presented here is built using
                 its associated BSP tree, but attempts to reduce the
                 number of tests to, log4&sol;3n, at the cost of a tree
                 of size of O(N1.5log4&sol;3n&minus;1), where n is the
                 initial number of polygons in the scene, and N the
                 resulting number after BSP splitting. To achieve the
                 increase in run-time efficiency, a height plane is used
                 to restrict the view point of the observer to a fixed
                 height, but the key to the efficiency of the algorithm
                 is in the use of polygonal dependencies. In the scene;
                 if we know our location relative to the front or back
                 of a polygon, then our position relative to one-quarter
                 of the remaining polygons, in the expected worst-case,
                 can be determined.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(1)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-437,
  pages =        "73--82",
  year =         "1998",
  title =        "A Modelling Method and User Interface for Creating
                 Plants",
  author =       "Bernd Lintermann and Oliver Deussen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-437",
  abstract =     "We present a modelling method and graphical user
                 interface for the creation of natural branching
                 structures such as plants. Structural and geometric
                 information is encapsulated in objects that are
                 combined to form a description of the model. The model
                 is represented graphically as a structure graph and can
                 be edited interactively. Global and partial constraint
                 techniques are integrated on the basis of tropisms,
                 free-form deformations and pruning operations to allow
                 the modelling of specific shapes. We show examples to
                 illustrate the design process and evaluate the user
                 interface.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(1)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-438,
  pages =        "17--27",
  year =         "1998",
  title =        "An Algorithm for Dynamic Color Management",
  author =       "Martin Fischer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-438",
  abstract =     "Though the problem of color quantization in static
                 images is not completely solved, there are a few
                 algorithms that produce good results. Color
                 quantization with dynamic color requests is an unsolved
                 problem. In this paper we present an algorithm that
                 manages dynamic color allocations without advance
                 knowledge of the sequence of color requests. It uses a
                 dynamic color table that can be used simultaneously in
                 multiple windows from one or more applications. Advance
                 knowledge about color requirements will improve the
                 results of the algorithm.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(1)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-439,
  pages =        "113--120",
  year =         "1998",
  title =        "Creating and Rendering Convolution Surfaces",
  author =       "Jon McCormack and Andrei Sherstyuk",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-439",
  abstract =     "Implicit surfaces obtained by convolution of
                 multi-dimensional primitives with some potential
                 function, are a generalisation of popular implicit
                 surface models: blobs, metaballs and soft objects.
                 These models differ in their choice of potential
                 function but agree upon the use of underlying modelling
                 primitives, namely, points. In this paper a method is
                 described for modelling and rendering implicit surfaces
                 built upon an expanded set of skeletal primitives:
                 points, line segments, polygons, arcs and planes. An
                 analytical solution to the convolution is described.
                 This solution offers a more accurate and robust
                 representation of the resultant implicit surface than
                 previous methods. An algorithm for ray-tracing the
                 surfaces formed through convolution of any combination
                 of these primitives is also outlined.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(2)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-44,
  year =         "1998",
  title =        "Surface Reconstruction from Multiple Stereo Images",
  author =       "A. Gaich and M. Berger and M. Gruber",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-44",
  language =     "en",
  abstract =     "The computation of a closed surface from three
                 dimensional sample points is currently a not completely
                 solved problem, and is particularly problematic in the
                 case of incomplete, irregular, or faulty point data. In
                 this contribution we present a survey about different
                 methods for reconstructing a surface from 3D point
                 data. Two different approaches have to be discriminated
                 whether if there is no a priori information about the
                 data points used (unstructured data), or if additional
                 data about the 3D reconstruction process, i. e. the
                 generation of the point data, is available. Depending
                 on the complexity of the object's shape and the
                 underlying data acquisition process several methods for
                 3D scanning principles are appropriated. The best
                 approach can only be defined in context with the
                 application. We have tested different procedures for
                 surface reconstruction to find a method which is suited
                 best for 3D point data derived from a 3D object
                 scanner, which was developed at our laboratory. This
                 scanner works with high-resolution images and shape
                 from stereo. In order to fully reconstruct an object,
                 it is necessary to find points distributed over the
                 entire surface of the object, so the stereo images have
                 to be taken from different views around the specimen.
                 The resulting point cloud is compounded by several
                 parts and every part respectively every data point can
                 be assigned uniquely to its underlying stereo image
                 pair. This additional information shall be exploited
                 within the surface reconstruction process for improving
                 the result especially if the existing data is
                 scattered, noisy, and non-uniformly distributed which
                 aggravates the task for the surface reconstruction
                 algorithm considerably. In this paper we show our
                 experience with different surface reconstruction
                 methods and why we want to find a new approach suitable
                 for our data.",
  keywords =     "surface reconstruction, shape from stereo",
  booktitle =    "Proceedings 22nd OAGM/AAPR Workshop, Illmitz,
                 Austria",
}

@InProceedings{EVL-1998-440,
  pages =        "121--134",
  year =         "1998",
  title =        "Fast Collision Detection Algorithms with Applications
                 to Particle Flow",
  author =       "B. C. Vemuri and Y. Cao and L. Chen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-440",
  abstract =     "In this paper, we present efficient algorithms for
                 collision detection of arbitrarily shaped rigid moving
                 objects in a variety of interactive as well as
                 non-interactive environments. The algorithms primarily
                 consist of two stages. The first stage involves finding
                 candidate objects for possible collisions. The second
                 stage involves detecting exact (within a prespecified
                 tolerance) collision between these candidates. The
                 primary data structure used in the algorithms is an
                 octree. In the first stage, we build an octree for the
                 enclosure containing the objects, which is used to
                 detect possible collisions. Assuming
                 spatial&sol;temporal coherence i.e., that the particles
                 move slowly or that the time sampling is fast enough,
                 the average time complexity of this stage can be shown
                 to be O(n) (excluding the time complexity for a one
                 time octree construction), where n is the number of
                 particles. In the second stage, we build a
                 surface-octree for each object. If the objects are
                 convex and assuming coherence, the expected time
                 complexity to detect precise (within a prespecified
                 tolerance) collision for each pair is a constant
                 (excluding the time complexity for a one time
                 surface-octree construction). Therefore, the overall
                 expected time complexity for convex object collision
                 detection is linear with respect to n. For the concave
                 objects, complexity analysis is nontrivial to perform
                 and instead we provide a very practical (almost linear
                 time) algorithm. We apply our algorithms to particle
                 flow simulations by simulating flow density conditions
                 often arising in granular flows.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(2)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-441,
  pages =        "135--152",
  year =         "1998",
  title =        "Interactive Construction and Animation of Layered
                 Elastically Deformable Characters",
  author =       "Russell Turner and Enrico Gobbetti",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-441",
  abstract =     "An interactive system is described for creating and
                 animating deformable 3D characters. By using a hybrid
                 layered model of kinematic and physics-based components
                 together with an immersive 3D direct manipulation
                 interface, it is possible to quickly construct
                 characters that deform naturally when animated and
                 whose behavior can be controlled interactively using
                 intuitive parameters. In this layered construction
                 technique, called the elastic surface layer model, a
                 simulated elastically deformable skin surface is
                 wrapped around a kinematic articulated figure. Unlike
                 previous layered models, the skin is free to slide
                 along the underlying surface layers constrained by
                 geometric constraints which push the surface out and
                 spring forces which pull the surface in to the
                 underlying layers. By tuning the parameters of the
                 physics-based model, a variety of surface shapes and
                 behaviors can be obtained such as more
                 realistic-looking skin deformation at the joints, skin
                 sliding over muscles, and dynamic effects such as
                 squash-and-stretch and follow-through. Since the
                 elastic model derives all of its input forces from the
                 underlying articulated figure, the animator may specify
                 all of the physical properties of the character once,
                 during the initial character design process, after
                 which a complete animation sequence can be created
                 using a traditional skeleton animation technique.
                 Character construction and animation are done using a
                 3D user interface based on two-handed manipulation
                 registered with head-tracked stereo viewing. In our
                 configuration, a six degree-of-freedom head-tracker and
                 CrystalEyes shutter glasses are used to display stereo
                 images on a workstation monitor that dynamically follow
                 the user head motion. 3D virtual objects can be made to
                 appear at a fixed location in physical space which the
                 user may view from different angles by moving his head.
                 To construct 3D animated characters, the user interacts
                 with the simulated environment using both hands
                 simultaneously: the left hand, controlling a Spaceball,
                 is used for 3D navigation and object movement, while
                 the right hand, holding a 3D mouse, is used to
                 manipulate through a virtual tool metaphor the objects
                 appearing in front of the screen. Hand-eye coordination
                 is made possible by registering virtual space to
                 physical space, allowing a variety of complex 3D tasks
                 necessary for constructing 3D animated characters to be
                 performed more easily and more rapidly than is possible
                 using traditional interactive techniques.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(2)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-442,
  pages =        "153--165",
  year =         "1998",
  title =        "Tree Visualisation and Navigation Clues for
                 Information Visualisation",
  author =       "Ivan Herman and Maylis Delest and Guy Melancon",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-442",
  abstract =     "Information visualisation often requires good
                 navigation aids on large trees, which represent the
                 underlying abstract information. Using trees for
                 information visualisation requires novel user interface
                 techniques, visual clues, and navigational aids. This
                 paper describes a visual clue: using the so-called
                 Strahler numbers, a map is provided that indicates
                 which parts of the tree are interesting. A second idea
                 is that of &ldquo;folding&rdquo; away subtrees that are
                 too &ldquo;different&rdquo; in some sense, thereby
                 reducing the visual complexity of the tree. Examples
                 are given demonstrating these techniques, and what the
                 further challenges in this area are.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(2)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-443,
  pages =        "167--174",
  year =         "1998",
  title =        "Measuring Error on Simplified Surfaces",
  author =       "P. Cignoni and C. Rocchini and R. Scopigno",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-443",
  abstract =     "This paper presents a new tool, Metro, designed to
                 compensate for a deficiency in many simplification
                 methods proposed in literature. Metro allows one to
                 compare the difference between a pair of surfaces (e.g.
                 a triangulated mesh and its simplified representation)
                 by adopting a surface sampling approach. It has been
                 designed as a highly general tool, and it does no
                 assumption on the particular approach used to build the
                 simplified representation. It returns both numerical
                 results (meshes areas and volumes, maximum and mean
                 error, etc.) and visual results, by coloring the input
                 surface according to the approximation error.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(2)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-444,
  year =         "1998",
  title =        "Frontiers in User-Computer Interaction",
  author =       "Andries van Dam",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-444",
  abstract =     "In this age of (near-)adequate computing power, the
                 power and usability of the user interface is as key to
                 an applicationAEs success as its functionality. Most of
                 the code in modern desktop productivity applications
                 resides in the user interface. But despite its
                 centrality, the user interface field is currently in a
                 rut: the WIMP (Windows, Icons, Menus, Point-and-Click
                 GUI based on keyboard and mouse) has evolved little
                 since it was pioneered by Xerox PARC in the early
                 AE70s. Computer and display form factors will change
                 dramatically in the near future and new kinds of
                 interaction devices will soon become available. Desktop
                 environments will be enriched not only with PDAs such
                 as the Newton and Palm Pilot, but also with wearable
                 computers and large-screen displays produced by new
                 projection technology, including office-based immersive
                 virtual reality environments. On the input side, we
                 will finally have speech-recognition and force-feedback
                 devices. Thus we can look forward to user interfaces
                 that are dramatically more powerful and better matched
                 to human sensory capabilities than those dependent
                 solely on keyboard and mouse. 3D interaction widgets
                 controlled by mice or other interaction devices with
                 three or more degrees of freedom are a natural
                 evolution from their two-dimensional WIMP counterparts
                 and can decrease the cognitive distance between widget
                 and task for many tasks that are intrinsically 3D, such
                 as scientific visualization and MCAD. More radical
                 post-WIMP UIs are needed for immersive virtual reality
                 where keyboard and mouse are absent. Immersive VR
                 provides good driving applications for developing
                 post-WIMP UIs based on multimodal interaction that
                 involve more of our senses by combining the use of
                 gesture, speech, and haptics.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-445,
  year =         "1998",
  title =        "The Convergence of Graphics and Imaging",
  author =       "James D. Foley",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-445",
  abstract =     "Over twenty years ago a SIGGRAPH panel session
                 addressed the convergence of computer graphics and
                 image processing. At that time the emphasis was on
                 low-level operations such as filtering to avoid
                 anti-aliasing, and related psycho-physics issues. More
                 recently, Graphics and Imaging are converging at a
                 higher level as we move toward blending the synthetic
                 world of computer-generated images with the real world
                 of computer-captured images. In this talk we describe
                 several research directions that relate to this
                 convergence, and illustrate with specific examples of
                 work at MERL - A Mitsubishi Electric Research
                 Laboratory. These research directions are: Analyzing
                 images of the human face to determine identity and
                 orientation and ultimately to reconstruct the shape of
                 the face. Reconstruction of static and dynamic 3D
                 geometries from 2D images separated in time or space:
                 here the objective is to take multiple images of a
                 real-world scene and recreate the 3D geometry of the
                 scene. If objects in the scene are moving, then the
                 objective is extracting the dynamic geometry. Once the
                 geometry has been reconstructed, editing and relighting
                 of the scene becomes possible. Display of 3D scalar
                 fields (also known as volume graphics) concerns 3D as
                 opposed to 2D images, such as CT and MRI scans. These
                 scans can be thought of as 3D images in that they are
                 point samples of a 3D scalar field, just as a
                 computer-captured image is a point sample of a 2D
                 sample field. The objective of volume graphics is to
                 create and display the 3D geometries that underly 3D
                 images. An inexpensive yet real-time (30 fps for a 256
                 x 256 x 256 image) implementation of Pfister and
                 KaufmanAEs Cube-4 rendering architecture will be
                 described.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-446,
  year =         "1998",
  title =        "Anno 2010 - Remembering Our Future: Challenges and
                 Frontiers of Human-Media Technology as the Kernel for
                 Human-Centered Computing",
  author =       "Jos{\'{U}} Encarnaþ{\~{a}}o",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-446",
  abstract =     "Based on a forecast by Battelle, Human-Media
                 Technology is one of {"}technology?s top 10 challenges
                 and opportunities{"} for the year 2010. The subject
                 hereby is: {"}Humans live, science finds out how
                 technology conforms{"}. The goal is to develop
                 environments that allow users to cooperate in the most
                 efficient and natural way. Human-centered systems will
                 have to incorporate people as an explicit design
                 component. This lecture will address the main goals in
                 developing such systems based on their general
                 characteristics and the corresponding enabling
                 technologies: Visualization (seeing the unseen),
                 Virtual and Augmented Reality (environment must be
                 immersive), and also Multimedia (to introduce the
                 combination of visual, auditory and voice data).
                 Examples from applications and case studies will
                 support the clarification of ideas and goals. Several
                 videos will be used to show the impact. So far, the
                 human-centered interfaces to accommodate human
                 perception and human response capabilities and
                 limitations will have been presented and discussed.
                 These interfaces allow to integrate the desired amount
                 of immersion and cooperation (CSCW). Based on this,
                 some {"}hands-on{"} life demos will be shown to discuss
                 the state of the art of these technologies, like
                 Virtual Tables (responsive workbenches), special I/O
                 technologies, etc. Some of these demos will be
                 stand-alone demos; others will show the potential of
                 telecommunication for collaboration by connecting the
                 floor with other locations to demonstrate CSCW-based
                 visual tele-applications.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-447,
  pages =        "1--13",
  year =         "1998",
  title =        "Animation of Biological Organ Growth Based on
                 {L}-systems",
  author =       "Roman Durikovic and Kazufumi Kaneda and Hideo
                 Yamashita",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-447",
  abstract =     "In contrast with the growth of plants and trees, human
                 organs can undergo significant changes in shape through
                 a variety of global transformations during the growth
                 period, such as bending or twisting. In our approach,
                 the topology of a human organ is represented by a
                 skeleton in the form of a tree or cycled graph. The
                 length of skeleton growth can be simulated by an
                 algebraic L-system that also produces discrete events.
                 The paper shows how to include global transformations
                 into the formalism of L-systems to obtain a continuous
                 process. The shape of the organ is approximated by a
                 number of ellipsoidal clusters centred at points on the
                 skeleton. The proposed growth model of the organ
                 continually responds to the positional changes of
                 surrounding organs, thereby changing the organ shape
                 locally. In our study, the stomach of a human embryo is
                 used for the demonstration of organ development, and
                 the methodology employed is also applicable to the
                 animation of animal organs and their development.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-448,
  pages =        "15--22",
  year =         "1998",
  title =        "Fast Feature-Based Metamorphosis and Operator Design",
  author =       "Tong-Yee Lee and Young-Ching Lin and Y. N. Sun
                 andLeeween Lin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-448",
  abstract =     "Metamorphosis is a powerful visual technique, for
                 producing interesting transition between two images or
                 volume data. Image or volume metamorphosis using simple
                 features provides flexible and easy control of visual
                 effect. The feature-based image warping proposed by
                 Beier and Neely is a brute-force approach. In this
                 paper, first, we propose optimization methods to reduce
                 their warping time without noticeable loss of image
                 quality. Second, we extend our methods to 3D volume
                 data and propose several interesting warping operators
                 allowing global and local metamorphosis of volume
                 data.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-449,
  pages =        "23--30",
  year =         "1998",
  title =        "Interactive 3{D} Morphing",
  author =       "Hujun Bao and Qunsheng Peng",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-449",
  abstract =     "A new 3D morphing algorithm for polyhedral objects
                 with the same genus is presented in the paper. Our main
                 contribution is an efficient and general algorithm for
                 setting up the vertex correspondence between the
                 polyhedra. The proposed algorithm first interactively
                 partitions the two original polyhedra into the same
                 number of polygonal patches, the patch correspondence
                 is also established during partitioning. Each pair of
                 corresponding patches is then parametrized and
                 resampled by using the harmonic maps. A feature
                 polyhedron is finally constructed for each original
                 polyhedron, and the vertex correspondence between each
                 original polyhedron and its feature polyhedron is
                 automatically established following a cluster scheme.
                 The shape transition between the original polyhedral
                 models is accomplished by composing three successive
                 transformations using their feature polyhedra as the
                 bridges. Experimental results demonstrate that our
                 algorithm is very robust, and can deal with very
                 general cases (non-zero genus polyhedral cases).",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InCollection{EVL-1998-45,
  pages =        "295--314",
  year =         "1998",
  title =        "Fast {LIC} with Piecewise Polynomial Filter Kernels",
  author =       "Hans-Christian Hege and Detlev Stalling",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-45",
  language =     "en",
  abstract =     "Line integral convolution (LIC) has become a
                 well-known and popular method for visualizing vector
                 fields. The method works by convolving a random input
                 texture along the integral curves of the vector field.
                 In order to accelerate image synthesis significantly,
                 an efficient algorithm has been proposed that utilizes
                 pixel coherence in field line direction. This
                 algorithm, called {"}fast LIC{"}, originally was
                 restricted to simple box-type filter kernels. Here we
                 describe a generalization of fast LIC for piecewise
                 polynomial filter kernels. Expanding the filter kernels
                 in terms of truncated power functions allows us to
                 exploit a certain convolution theorem. The convolution
                 integral is expressed as a linear combination of
                 repeated integrals (or repeated sums in the discrete
                 case). Compared to the original algorithm the
                 additional expense for using higher order filter
                 kernels, e.g. of B-spline type, is very low. Such
                 filter kernels produce smoother, less noisier results
                 than a box filter. This is evident from visual
                 investigation, as well as from analysis of pixel
                 correlations. Thus, our method represents a useful
                 extension of the fast LIC algorithm for the creation of
                 high-quality LIC images",
  editor =       "H.-C. Hege and K. Polthier",
  booktitle =    "Mathematical Visualization - Algorithms and
                 Applications",
  publisher =    "Springer",
}

@InProceedings{EVL-1998-450,
  pages =        "31--40",
  year =         "1998",
  title =        "A Framework for Synchronized Editing of Multiple Curve
                 Representations",
  author =       "Cindy Grimm and Matthew Ayers",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-450",
  abstract =     "Editing curves and surfaces is difficult in part
                 because their mathematical representations rarely
                 correspond to most people&rsquo;s idea of a curve or
                 surface. The implementation (and hence, behavior) of
                 most manipulation tools is intertwined with a
                 particular curve or surface representation; this can
                 make reimplementing the tool with a different
                 representation problematic. A system using a single
                 representation must therefore either limit the types of
                 tools available or convert existing tools to work on
                 the system&rsquo;s representation. In this paper we
                 present a framework for editing curves or surfaces
                 which supports multiple representations and ensures
                 that they stay synchronized. As a proof of concept, we
                 have created a curve editor which contains several
                 tools each of which manipulate one of three different
                 curve representations: polylines, NURBs, and
                 multi-resolution B-splines.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-451,
  pages =        "41--52",
  year =         "1998",
  title =        "Egocentric Object Manipulation in Virtual
                 Environments: Evaluation of Interaction Techniques",
  author =       "I. Poupyrev and S. Weghorst and M. Billinghurst and T.
                 Ichikawa",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-451",
  abstract =     "The acceptance of virtual environment (VE) technology
                 requires scrupulous optimization of the most basic
                 interactions in order to maximize user performance and
                 provide efficient and enjoyable virtual interfaces.
                 Motivated by insufficient understanding of the human
                 factors design implications of interaction techniques
                 and tools for virtual interfaces, this paper presents
                 results of a formal study that compared two basic
                 interaction metaphors for egocentric direct
                 manipulation in VEs, virtual hand and virtual pointer,
                 in object selection and positioning experiments. The
                 goals of the study were to explore immersive direct
                 manipulation interfaces, compare performance
                 characteristics of interaction techniques based on the
                 metaphors of interest, understand their relative
                 strengths and weaknesses, and derive design guidelines
                 for practical development of VE applications.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-452,
  pages =        "53--61",
  year =         "1998",
  title =        "A Collaborative Scene Editor for {VRML} Worlds",
  author =       "Tain-chi Lu and Chuanwen Chiang and Ming-tang Lin and
                 Chungnan Lee",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-452",
  abstract =     "In this paper, we analyze the requirements for a
                 Web-based collaborative infrastructure within a virtual
                 world. Additionally, we combine several tools and
                 methodologies to propose a flexible and fluid
                 collaborative environment using Java language to create
                 a VRML scene graph. The proposed prototype aims at four
                 aspects: a shared workspace of scene editor, an active
                 entity composition algorithm in Java, collaborative
                 control in the multi-user environment, and access
                 control mechanism toward the shared data.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-453,
  pages =        "63--74",
  year =         "1998",
  title =        "A Light Hierarchy for Fast Rendering of Scenes with
                 Many Lights",
  author =       "Eric Paquette and Pierre Poulin and George Drettakis",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-453",
  abstract =     "We introduce a new data structure in the form of a
                 light hierarchy for efficiently ray-tracing scenes with
                 many light sources. An octree is constructed with the
                 point light sources in a scene. Each node represents
                 all the light sources it contains by means of a virtual
                 light source. We determine bounds on the error
                 committed with this approximation to shade a point,
                 both for the cases of diffuse and specular reflections.
                 These bounds are then used to guide a hierarchical
                 shading algorithm. If the current level of the light
                 hierarchy provides shading of sufficient quality, the
                 approximation is used, thus avoiding the cost of
                 shading for all the light sources contained below this
                 level. Otherwise the descent into the light hierarchy
                 continues. Our approach has been implemented for scenes
                 without occlusion. The results show important
                 acceleration compared to standard ray-tracing (up to 90
                 times faster) and an important improvement compared to
                 Ward&rsquo;s adaptive shadow testing.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-454,
  pages =        "75--85",
  year =         "1998",
  title =        "Optical Flow Rendering",
  author =       "Tae-Joon Park and Sung Yong Shin and Seungyong Lee",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-454",
  abstract =     "This paper proposes a new approach to image-based
                 rendering that generates an image viewed from an
                 arbitrary camera position and orientation by rendering
                 optical flows extracted from reference images. To
                 derive valid optical flows, we develop an analysis
                 technique that improves the quality of stereo matching.
                 Without using any special equipments such as range
                 cameras, this technique constructs reliable optical
                 flows from a sequence of matching results between
                 reference images. We also derive validity conditions of
                 optical flows and show that the obtained flows satisfy
                 those conditions. Since environment geometry is
                 inferred from the optical flows, we are able to
                 generate more accurate images with this additional
                 geometric information. Our approach makes it possible
                 to combine an image rendered from optical flows with an
                 image generated by a conventional rendering technique
                 through a simple Z-buffer algorithm.",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-455,
  pages =        "87--95",
  year =         "1998",
  title =        "Anisotropic Solid Texture Synthesis Using Orthogonal
                 2{D} Views",
  author =       "J. M. Dischler and D. Ghazanfarpour and R. Freydier",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-455",
  abstract =     "Analytical approaches, based on digitised 2D texture
                 models, for an automatic solid (3D) texture synthesis
                 have been recently introduced to Computer Graphics.
                 However, these approaches cannot provide satisfactory
                 solutions in the usual case of natural anisotropic
                 textures (wood grain for example). Indeed, solid
                 texture synthesis requires particular care, and
                 sometimes external knowledge to {"}guess{"} the
                 internal structure of solid textures because only 2D
                 texture models are used for analysis. By making some
                 basic assumptions about the internal structure of solid
                 textures, we propose a very efficient method based on a
                 hybrid analysis (spectral and histogram) for an
                 automatic synthesis of solid textures. This new method
                 allows us to obtain high precision solid textures
                 (closely resembling initial models) in a large number
                 of cases, including the difficult case of anisotropic
                 textures.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-456,
  pages =        "97--104",
  year =         "1998",
  title =        "Importance Driven Texture Coordinate Optimization",
  author =       "Peter-Pike J. Sloan and David M. Weinstein and J. Dean
                 Brederson",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-456",
  abstract =     "Traditionally, texture coordinates have been generated
                 based solely on the model's geometry, often even before
                 a model's textures have been created. With the arrival
                 of new technologies, such as 3D paint programs,
                 weaknesses of a static optimization pre-process are
                 becoming apparent. These weaknesses arise from
                 constructing a parameterization based solely on the
                 model's geometry, ignoring the fact that detail is not
                 uniformly spaced throughout the texture space. In fact,
                 certain regions of the texture are more important than
                 other regions. In this paper we introduce the notion of
                 the {"}importance map{"} and describe how importance
                 values are derived from both intrinsic properties of
                 the texture and user-guided highlights. Furthermore, we
                 describe how importance maps are used to drive the
                 texture coordinate optimization. Finally, we show how
                 this optimization process can be integrated into a 3D
                 painting environment, enabling periodic optimization at
                 any stage of texture design.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-457,
  pages =        "105--112",
  year =         "1998",
  title =        "Simulating Wood Using a Voxel Approach",
  author =       "John W. Buchanan",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-457",
  abstract =     "In this paper we present a technique for generating
                 three-dimensional wood textures using a regular texture
                 array. Currently three-dimensional wood textures are
                 generated using procedural textures. Procedural
                 textures are flexible and require little memory,
                 however the modeling of local artifacts such as knots
                 is difficult using the procedural approach. By
                 representing the wood as a texture array and growing
                 the wood in this array we can easily simulate local
                 phenomena such as knots. Our growth model is an
                 approximation to the biological model and assumes that
                 there are several similar wood cells per array element.
                 This means that we can model artifacts that are defined
                 by groups of similar cells. In particular our model is
                 well suited for the modeling of soft-woods.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-458,
  pages =        "125--135",
  year =         "1998",
  title =        "Progressive Iso-Surface Extraction from Hierarchical
                 3{D} Meshes",
  author =       "Roberto Grosso and Thomas Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-458",
  abstract =     "A multiresolution data decomposition offers a
                 fundamental framework supporting compression,
                 progressive transmission, and level-of-detail (LOD)
                 control for large two or three dimensional data sets
                 discretized on complex meshes. In this paper we extend
                 a previously presented algorithm for 3D mesh reduction
                 for volume data based on multilevel finite element
                 approximations in two ways. First, we present efficient
                 data structures which allow to incrementally construct
                 approximations of the volume data at lower or higher
                 resolutions at interactive rates. An abstract
                 description of the mesh hierarchy in terms of a coarse
                 base mesh and a set of integer records offers a high
                 compression potential which is essential for an
                 efficient storage and a progressive network
                 transmission. Based on this mesh hierarchy we then
                 develop a new progressive iso-surface extraction
                 algorithm. For a given iso-value, the corresponding
                 iso-surface can be computed at different levels of
                 resolution. Changing to a higher or coarser resolution
                 will update the surface only in those regions where the
                 volume data is being refined or coarsened. Our approach
                 allows to interactively visualize very large scalar
                 fields like medical data sets, whereas the conventional
                 algorithms would have required at least an order of
                 magnitude more resources.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-459,
  pages =        "137--147",
  year =         "1998",
  title =        "Multiresolution Isosurface Extraction with Adaptive
                 Skeleton Climbing",
  author =       "Tim Poston and Tien-Tsin Wong and Pheng-Ann Heng",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-459",
  abstract =     "An isosurface extraction algorithm which can directly
                 generate multiresolution isosurfaces from volume data
                 is introduced. It generates low resolution isosurfaces,
                 with 4 to 25 times fewer triangles than that generated
                 by marching cubes algorithm, in comparable running
                 times. By climbing from vertices (0-skeleton) to edges
                 (1-skeleton) to faces (2-skeleton), the algorithm
                 constructs boxes which adapt to the geometry of the
                 true isosurface. Unlike previous adaptive marching
                 cubes algorithms, the algorithm does not suffer from
                 the gap-filling problem. Although the triangles in the
                 meshes may not be optimally reduced, it is much faster
                 than postprocessing triangle reduction algorithms.
                 Hence the coarse meshes it produces can be used as the
                 initial starts for the mesh optimization, if mesh
                 optimality is the main concern.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@Book{EVL-1998-46,
  year =         "1998",
  title =        "Mathematical Visualization - Algorithms and
                 Applications",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-46",
  language =     "en",
  month =        aug,
  editor =       "H.-C. Hege and K. Polthier",
  publisher =    "Springer",
}

@InProceedings{EVL-1998-460,
  pages =        "113--124",
  year =         "1998",
  title =        "Maximum Intensity Projection Using Splatting in
                 Sheared Object Space",
  author =       "Wenli Cai and Georgios Sakas",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-460",
  abstract =     "In this paper we present a new Maximum Intensity
                 Projection (MIP) algorithm which was implemented
                 employing splatting in a shear-warp context. This
                 algorithm renders a MIP image by first splatting each
                 voxel on two intermediate spaces called {"}worksheet{"}
                 and {"}shear image{"}. Then, the maximum value is
                 evaluated between worksheet and shear image. Finally,
                 shear image is warped on the screen to generate the
                 result image. Different footprints implementing
                 different quality modes are discussed. In addition, we
                 introduced a line encoded indexing speed-up method to
                 obtain interactive speed. This algorithm allows for a
                 quantitative, predictable trade-off between
                 interactivity and image quality.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-461,
  pages =        "149--158",
  year =         "1998",
  title =        "A Vector Approach for Global Illumination in Ray
                 Tracing",
  author =       "Jacques Zaninetti and Xavier Serpaggi and Bernard
                 P{\'{e}}roche",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-461",
  abstract =     "This paper presents a method taking global
                 illumination into account in a ray tracing environment.
                 A vector approach is introduced, which allows to deal
                 with all the types of light paths and the directional
                 properties of materials. Three types of vectors are
                 defined: Direct Light Vectors associated to light
                 sources, Indirect Light Vectors which correspond to
                 light having been diffusely reflected at least once and
                 Caustic Light Vectors which are associated to light
                 rays emitted by sources and reflected and/or
                 transmitted by specular surfaces. These vectors are
                 estimated at a small number of points in the scene. A
                 weighted interpolation between known values allows to
                 reconstruct these vectors for the other points, with
                 the help of a gradient computation for the indirect
                 component. This approach also allows to take uniform
                 area light sources (spherical, rectangular and
                 circular) into account for all the types of vectors.
                 Computed images are thus more accurate and no
                 discretizing of the geometry of the scene is needed.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-462,
  pages =        "159--164",
  year =         "1998",
  title =        "A Two-Pass Hardware-Based Method for Hierarchical
                 Radiosity",
  author =       "I. Mart{\'{i}}n and X. Pueyo and D. Tost",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-462",
  abstract =     "Finite elements methods for radiosity are aimed at
                 computing global illumination solutions efficiently.
                 However these methods are not suitable for obtaining
                 high quality images due to the lack of error control.
                 Two-pass methods allow to achieve that level of quality
                 computing illumination at each pixel and thus
                 introducing a high computing overhead. We present a
                 two-pass method for radiosity that allows to produce
                 high quality images avoiding most of the per-pixel
                 computations. The method computes a coarse hierarchical
                 radiosity solution and then performs a second pass
                 using current graphics hardware accelerators to
                 generate illumination as high definition textures.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-463,
  pages =        "165--174",
  year =         "1998",
  title =        "Getting Rid of Links in Hierarchical Radiosity",
  author =       "M. Stamminger and H. Schirmacher and Ph. Slusallek and
                 H.-P. Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-463",
  abstract =     "Hierarchical radiosity with clustering has positioned
                 itself as one of the most efficient algorithms for
                 computing global illumination in non-trivial
                 environments. However, using hierarchical radiosity for
                 complex scenes is still problematic due to the
                 necessity of storing a large number of transport
                 coefficients between surfaces in the form of links. In
                 this paper, we eliminate the need for storage of links
                 through the use of a modified shooting method for
                 solving the radiosity equation. By distributing only
                 unshot radiosity in each step of the iteration, the
                 number of links decreases exponentially. Recomputing
                 these links instead of storing them increases
                 computation time, but reduces memory consumption
                 dramatically. Caching may be used to reduce the time
                 overhead. We analyze the error behavior of the new
                 algorithm in comparison with the normal gathering
                 approach for hierarchical radiosity. In particular, we
                 consider the relation between the global error of a
                 hierarchical radiosity solution and the local error
                 threshold for each link.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-464,
  pages =        "175--186",
  year =         "1998",
  title =        "Screen-Space Constraints for Camera Movements: the
                 Virtual Cameraman",
  author =       "Frank Jardillier and Eric Langu{\'{e}}nou",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-464",
  abstract =     "This article presents a virtual cameraman which allows
                 us to obtain the whole set of camera movements
                 satisfying user defined constraints specified in the
                 image space and/or constraints on the objects of the
                 scene. This research follows the {"}Declarative
                 Modelling{"} approach, which focuses on a 3-phase
                 modeller concept: description; generation; result
                 exploration. Our tool is based on a solver using
                 interval arithmetic. The time dimension is treated as
                 another variable, thus constraints can be specified for
                 the total duration of the animation or could last only
                 for a given amount of time. There is no keyframing and
                 no interpolation, thereby, for the solutions obtained,
                 the satisfaction of the specified constraints are
                 guaranteed. Several ways to include time dimension
                 efficiently are discussed. We claim that the method is
                 simple enough to be implemented easily without the need
                 of any external solver.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-465,
  pages =        "187--194",
  year =         "1998",
  title =        "Accelerated Walkthroughs of Virtual Environments Based
                 on Visibility Preprocessing and Simplification",
  author =       "Yigang Wang and Hujun Bao and Qunsheng Peng",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-465",
  abstract =     "This paper proposes a new preprocessing method for
                 interactive rendering of complex polygonal virtual
                 environments. The approach divides the space that
                 observer can reach into many rectangular viewpoint
                 regions. For each region, an outer rectangular volume
                 (ORV) is established to surround it. By adaptively
                 partitioning the boundary of the ORV together with the
                 viewpoint region, all the rays that originate from the
                 viewpoint region are divided into the beams whose
                 potentially visible polygon number is less than a
                 preset threshold. If a resultant beam is the smallest
                 and intersects many potentially visible polygons, the
                 beam is simplified as a fixed number of rays and the
                 averaged color of the hit polygons is recorded. For
                 other beams, their potentially visible sets (PVS) of
                 polygons are stored respectively. During an interactive
                 walkthrough, the visual information related to the
                 current viewpoint is retrieved from the storage. The
                 view volume clipping, visibility culling and detail
                 simplification are efficiently supported by these
                 stored data. The rendering time is independent of the
                 scene complexity.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-466,
  pages =        "195--206",
  year =         "1998",
  title =        "Space Discretization for Efficient Human Navigation",
  author =       "Srikanth Bandi and Daniel Thalmann",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-466",
  abstract =     "There is a large body of research on motion control of
                 legs in human models. However, they require
                 specification of global paths in which to move. A
                 method for automatically computing a global motion path
                 for a human in 3D environment of obstacles is
                 presented. Object space is discretized into a 3D grid
                 of uniform cells and an optimal path is generated
                 between two points as a discrete cell path. The grid is
                 treated as graph with orthogonal links of uniform cost.
                 A&ast; search method is applied for path finding. By
                 considering only the cells on the upper surface of
                 objects on which human walks, a large portion of the
                 grid is discarded from the search space, thus boosting
                 efficiency. This is expected to be a higher level
                 mechanism for various local foot placement methods in
                 human animation.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-467,
  pages =        "207--217",
  year =         "1998",
  title =        "Importance Driven Halftoning",
  author =       "L. Streit and J. Buchanan",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-467",
  abstract =     "Most halftoning techniques have been primarily
                 concerned with achieving an accurate reproduction of
                 local gray-scale intensities while avoiding the
                 introduction of artifacts. A secondary concern in
                 halftoning has been the preservation of edges in the
                 halftoned image. In this paper, we will introduce a new
                 halftoning technique that utilizes a bandpass pyramid
                 to achieve an accurate reproduction of important
                 attributes in the image. Ink is distributed through the
                 bandpass pyramid primarily according to a user defined
                 importance function. This technique has three main
                 characteristics. First, our technique can produce
                 results similar to many other halftoning techniques by
                 allowing a generic importance function to be specified.
                 If the chosen importance function is average intensity
                 we obtain results similar to traditional halftoning. We
                 also show how the importance function can be changed to
                 highlight areas with high variance. Second, in addition
                 to changing the importance function, the drawing
                 primitives can also be changed. By using line segments
                 instead of single pixels as drawing primitives we
                 illustrate how edge enhancement can be achieved. Third,
                 this technique allows the user to easily limit the
                 number drawing primitives used. This is useful in
                 limited resource rendering. In addition to providing a
                 tailorable halftoning technique our method can easily
                 be adapted to produce two tone non-photorealistic (NPR)
                 images. We illustrate this by showing how sketched
                 effects can be achieved by aligning the drawing
                 primitives according to different image attributes.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-468,
  pages =        "219--231",
  year =         "1998",
  title =        "Dithered Color Quantization",
  author =       "L{\'{a}}szlJ. M. Buhmann and D. W. Fellner and M. Held
                 and J. Ketterer and J. Puzicha",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-468",
  abstract =     "Image quantization and digital halftoning are
                 fundamental problems in computer graphics, which arise
                 when displaying high-color images on non-truecolor
                 devices. Both steps are generally performed
                 sequentially and, in most cases, independent of each
                 other. Color quantization with a pixel-wise defined
                 distortion measure and the dithering process with its
                 local neighborhood optimize different quality criteria
                 or, frequently, follow a heuristic without reference to
                 any quality measure. In this paper we propose a new
                 method to simultaneously quantize and dither color
                 images. The method is based on a rigorous cost-function
                 approach which optimizes a quality criterion derived
                 from a generic model of human perception. A highly
                 efficient algorithm for optimization based on a
                 multiscale method is developed for the dithered color
                 quantization cost function. The quality criterion and
                 the optimization algorithms are evaluated on a
                 representative set of artificial and real-world images
                 as well as on a collection of icons. A significant
                 image quality improvement is observed compared to
                 standard color reduction approaches.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-469,
  pages =        "233--241",
  year =         "1998",
  title =        "Perception Based Color Image Difference",
  author =       "L{\'{a}}szl{\'{o}} Neumann and Kresimir Matkovic and
                 Werner Purgathofer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-469",
  abstract =     "A good image metric is often needed in digital image
                 synthesis. It can be used to check the convergence
                 behavior in progressive methods, to compare images
                 rendered using various rendering methods etc. Since
                 images are rendered to be observed by humans, an image
                 metric should correspond to human perception as well.
                 We propose here a new algorithm which operates in the
                 original image space. There is no need for Fourier or
                 wavelet transforms. Furthermore, the new metric is view
                 distance dependent. The new method uses the contrast
                 sensitivity function. The main idea is to place a
                 number of various rectangles in images, and to compute
                 the CIE LUV average color difference between
                 corresponding rectangles. Errors are then weighted
                 according to the rectangle size and the contrast
                 sensitivity function.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@Book{EVL-1998-47,
  year =         "1998",
  title =        "Mathematical Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-47",
  language =     "en",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  publisher =    "Springer-Verlag",
}

@InProceedings{EVL-1998-470,
  pages =        "243--253",
  year =         "1998",
  title =        "Conservative Visibility and Strong Occlusion for
                 Viewspace Partitioning of Densely Occluded Scenes",
  author =       "Daniel Cohen-Or and Gadi Fibich and Dan Halperin and
                 Eyal Zadicario",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-470",
  abstract =     "Computing the visibility of out-door scenes is often
                 much harder than of in-door scenes. A typical urban
                 scene, for example, is densely occluded, and it is
                 effective to precompute its visibility space, since
                 from a given point only a small fraction of the scene
                 is visible. The difficulty is that although the
                 majority of objects are hidden, some parts might be
                 visible at a distance in an arbitrary location, and it
                 is not clear how to detect them quickly. In this paper
                 we present a method to partition the viewspace into
                 cells containing a conservative superset of the visible
                 objects. For a given cell the method tests the
                 visibility of all the objects in the scene. For each
                 object it searches for a strong occluder which
                 guarantees that the object is not visible from any
                 point within the cell. We show analytically that in a
                 densely occluded scene, the vast majority of objects
                 are strongly occluded, and the overhead of using
                 conservative visibility (rather than visibility) is
                 small. These results are further supported by our
                 experimental results. We also analyze the cost of the
                 method and discuss its effectiveness.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-471,
  pages =        "255--265",
  year =         "1998",
  title =        "Using Wavefront Tracing for the Visualization and
                 Optimization of Progressive Lenses",
  author =       "J. Loos and Ph. Slusallek and H.-P. Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-471",
  abstract =     "Progressive addition lenses are a relatively new
                 approach to compensate for defects of the human visual
                 system. While traditional spectacles use rotationally
                 symmetric lenses, progressive lenses require the
                 specification of free-form surfaces. This poses
                 difficult problems for the optimal design and its
                 visual evaluation. This paper presents two new
                 techniques for the visualization of optical systems and
                 the optimization of progressive lenses. Both are based
                 on the same wavefront tracing approach to accurately
                 evaluate the refraction properties of complex optical
                 systems. We use the results of wavefront tracing for
                 continuously re-focusing the eye during rendering.
                 Together with distribution ray tracing, this yields
                 high-quality images that accurately simulate the visual
                 quality of an optical system. The design of progressive
                 lenses is difficult due to the trade-off between the
                 desired properties of the lens and unavoidable optical
                 errors, such as astigmatism and distortions. We use
                 wavefront tracing to derive an accurate error
                 functional describing the desired properties and the
                 optical error across a lens. Minimizing this error
                 yields optimal free-form lens surfaces. While the basic
                 approach is much more general, in this paper, we
                 describe its application to the particular problem of
                 designing and evaluating progressive lenses and
                 demonstrate the benefits of the new approach with
                 several example images.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-472,
  pages =        "267--273",
  year =         "1998",
  title =        "Molecular Dynamics Simulation in Virtual
                 Environments",
  author =       "Z. Ai",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-472",
  abstract =     "A virtual environment for interactive molecular
                 dynamics simulation has been designed and implemented
                 at the Fraunhofer Institute for Computer Graphics.
                 Different kinds of virtual reality devices are used in
                 the environment for immersive display and interaction
                 with the molecular system. A parallel computer is used
                 to simulate the physical and chemical properties of the
                 molecular system dynamically. A high-speed network
                 exchanges data between the simulation program and the
                 modeling program. Molecular dynamics simulation virtual
                 environment provides scientists with a powerful tool to
                 study immersively the world of molecules. The dynamic
                 interaction between an AIDS antiviral drug and reverse
                 transcriptase enzyme is illustrated in the paper.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-473,
  pages =        "275--284",
  year =         "1998",
  title =        "Real-time Biomechanically-based Muscle Volume
                 Deformation using {FEM}",
  author =       "Qing-hong Zhu and Yan Chen and Arie Kaufman",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-473",
  abstract =     "This paper presents a voxel-based biomechanical model
                 for muscle deformation using finite element method
                 (FEM) and volume graphics. Hierarchical voxel meshes
                 are reconstructed from filtered segmented muscle images
                 followed by FEM simulation and volume rendering.
                 Physiological muscle force is considered and linear
                 elastic muscle models for both static and dynamic cases
                 are simulated by FEM. Voxel-based wireframe, polygon
                 surface rendering, and volume rendering techniques are
                 applied to show real-time muscle deformation processes
                 as well as realistic animations.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-474,
  pages =        "285--294",
  year =         "1998",
  title =        "A Bernstein-{B}{\'{e}}zier Based Approach to Soft
                 Tissue Simulation",
  author =       "S. H. Martin Roth and Markus H. Gross and Silvio
                 Turello and Friedrich R. Carls",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-474",
  abstract =     "This paper discusses a Finite Element approach for
                 volumetric soft tissue modeling in the context of
                 facial surgery simulation. We elaborate on the
                 underlying physics and address some computational
                 aspects of the finite element discretization. In
                 contrast to existing approaches speed is not our first
                 concern, but we strive for the highest possible
                 accuracy of simulation. We therefore propose an
                 extension of linear elasticity towards
                 incompressibility and nonlinear material behavior, in
                 order to describe the complex properties of human soft
                 tissue more accurately. Furthermore, we incorporate
                 higher order interpolation functions using a
                 Bernstein-B{\'{e}}zier formulation, which has various
                 advantageous properties, such as its integral
                 polynomial form of arbitrary degree, efficient
                 subdivision schemes, and suitability for geometric
                 modeling and rendering. In addition, the use of
                 tetrahedral Finite Elements does not put any
                 restriction on the geometry of the simulated volumes.
                 Experimental results obtained from a synthetic block of
                 soft tissue and from the Visible Human Data Set
                 illustrate the performance of the envisioned model.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-475,
  pages =        "295--302",
  year =         "1998",
  title =        "Emotion Editing using Finite Elements",
  author =       "Rolf M. Koch and Albert A. Bosshard",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-475",
  abstract =     "This paper describes the prototype of a facial
                 expression editor. In contrast to existing systems the
                 presented editor takes advantage of both medical data
                 for the simulation and the consideration of facial
                 anatomy during the definition of muscle groups. The
                 C^1-continuous geometry and the high degree of
                 abstraction for the expression editing sets this system
                 apart from others. Using finite elements we achieve a
                 better precision in comparison to particle systems.
                 Furthermore, a precomputing of facial action units
                 enables us to compose facial expressions by a
                 superposition of facial action geometries in real-time.
                 The presented model is based on a generic facial model
                 using a thin plate and membrane approach for the
                 surface and elastic springs for facial tissue modeling.
                 It has been used successfully for performing facial
                 surgery simulation. We illustrate features of our
                 system with examples from the Visible Human Dataset.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-476,
  pages =        "303--313",
  year =         "1998",
  title =        "Subdivision Schemes for Thin Plate Splines",
  author =       "Henrik Weimer and Joe Warren",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-476",
  abstract =     "Thin plate splines are a well known entity of
                 geometric design. They are defined as the minimizer of
                 a variational problem whose differential operators
                 approximate a simple notion of bending energy.
                 Therefore, thin plate splines approximate surfaces with
                 minimal bending energy and they are widely considered
                 as the standard {"}fair{"} surface model. Such surfaces
                 are desired for many modeling and design applications.
                 Traditionally, the way to construct such surfaces is to
                 solve the associated variational problem using finite
                 elements or by using analytic solutions based on radial
                 basis functions. This paper presents a novel approach
                 for defining and computing thin plate splines using
                 subdivision methods. We present two methods for the
                 construction of thin plate splines based on
                 subdivision: A globally supported subdivision scheme
                 which exactly minimizes the energy functional as well
                 as a family of strictly local subdivision schemes which
                 only utilize a small, finite number of distinct
                 subdivision rules and approximately solve the
                 variational problem. A tradeoff between the accuracy of
                 the approximation and the locality of the subdivision
                 scheme is used to pick a particular member of this
                 family of subdivision schemes. Later, we show
                 applications of these approximating subdivision schemes
                 to scattered data interpolation and the design of fair
                 surfaces. In particular we suggest an efficient
                 methodology for finding control points for the local
                 subdivision scheme that will lead to an interpolating
                 limit surface and demonstrate how the schemes can be
                 used for the effective and efficient design of fair
                 surfaces.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-477,
  pages =        "315--326",
  year =         "1998",
  title =        "Rapid and Accurate Contact Determination between
                 Spline Models using ShellTrees",
  author =       "S. Krishnan and M. Gopi and M. Lin and D. Manocha and
                 A. Pattekar",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-477",
  abstract =     "In this paper, we present an efficient algorithm for
                 contact determination between spline models. We make
                 use of a new hierarchy, called ShellTree, that
                 comprises of spherical shells and oriented bounding
                 boxes. Each spherical shell corresponds to a portion of
                 the volume between two concentric spheres. Given large
                 spline models, our algorithm decomposes each surface
                 into B{\'{e}}zier patches as part of pre-processing. At
                 runtime it dynamically computes a tight fitting
                 axis-aligned bounding box across each B{\'{e}}zier
                 patch and efficiently checks all such boxes for
                 overlap. Using off-line and on-line techniques for tree
                 construction, our algorithm computes ShellTrees for
                 B{\'{e}}zier patches and performs fast overlap tests
                 between them to detect collisions. The overall approach
                 can trade off runtime performance for reduced memory
                 requirements. We have implemented the algorithm and
                 tested it on large models, each composed of hundred of
                 patches. Its performance varies with the configurations
                 of the objects. For many complex models composed of
                 hundreds of patches, it can accurately compute the
                 contacts in a few milliseconds.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-478,
  pages =        "327--334",
  year =         "1998",
  title =        "A New Approach for Direct Manipulation of Free-Form
                 Curve",
  author =       "J. M. Zheng and K. W. Chan and I. Gibson",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-478",
  abstract =     "There is an increasing demand for more intuitive
                 methods for creating and modifying free-form curves and
                 surfaces in CAD modeling systems. The methods should be
                 based not only on the change of the mathematical
                 parameters, such as control points, knots, and weights,
                 but also on the user&rsquo;s specified constraints and
                 shapes. This paper presents a new approach for directly
                 manipulating the shape of a free-form curve, leading to
                 a better control of the curve deformation and a more
                 intuitive CAD modeling interface. The user&rsquo;s
                 intended deformation of a curve is automatically
                 converted into the modification of the corresponding
                 NURBS control points and knot sequence of the curve.
                 The algorithm for this approach includes curve
                 elevation, knot refinement, control point
                 repositioning, and knot removal. Several examples shown
                 in this paper demonstrate that the proposed method can
                 be used to deform a NURBS curve into the desired shape.
                 Currently, the algorithm concentrates on the purely
                 geometric consideration. Further work will include the
                 effect of material properties.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-479,
  pages =        "335--344",
  year =         "1998",
  title =        "An Enhanced Spring Model for Information
                 Visualization",
  author =       "Holger Theisel and Matthias Kreuseler",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-479",
  abstract =     "In this paper we present a new technique for
                 visualizing multidimensional information. We describe
                 objects of a higher dimensional information space as
                 small closed free-form-surfaces in the visualization.
                 The location, size and shape of these surfaces describe
                 the original objects in information space uniquely. The
                 underlying enhanced spring model is introduced. The
                 technique is applied to two test data sets",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InCollection{EVL-1998-48,
  pages =        "3--18",
  year =         "1998",
  title =        "Tetrahedra Based Volume Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-48",
  author =       "Paolo Cignoni and Claudio Montani and Roberto
                 Scopigno",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InProceedings{EVL-1998-480,
  pages =        "345--353",
  year =         "1998",
  title =        "Mass-Spring Simulation using Adaptive Non-Active
                 Points",
  author =       "P. Howlett and W. T. Hewitt",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-480",
  abstract =     "This paper introduces an adaptive component to a
                 mass-spring system as used in the modelling of cloth
                 for computer animation. The new method introduces
                 non-active points to the model which can adapt the
                 shape of the cloth at inaccuracies. This improves on
                 conventional uniform mass-spring systems by producing
                 more visually pleasing results when simulating the
                 drape of cloth over irregular objects. The
                 computational cost of simulation is decreased by
                 reducing the complexity of collision handling and
                 enabling the use of coarser mass-spring networks.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-481,
  pages =        "355--362",
  year =         "1998",
  title =        "The Art of Knitted Fabrics, Realistic & Physically
                 Based Modelling of Knitted Patterns",
  author =       "M. Mei{\ss{}}ner and B. Eberhardt",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-481",
  abstract =     "In this paper we will present a system to use three
                 dimensional computer graphics in garment design. This
                 system is capable to visualize the {"}real{"}, i.e. the
                 physically correct, appearance of a knitted fabric. A
                 fast visualization of a physically correct
                 micro-structure garment is of crucial importance in
                 textile industry, since it enables fast and less
                 expensive product development. This system may be
                 either used in the design of new products or teaching
                 the art of knitted fabrics. We use in our system
                 directly the produced machine-code of the design system
                 for knitting machines. A physical model, a particle
                 system, is used to calculate the dynamics of the
                 micro-structure of the knitted garment.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(3)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-482,
  pages =        "197--218",
  year =         "1998",
  title =        "An Exhaustive Error-Bounding Algorithm for
                 Hierarchical Radiosity",
  author =       "Nicolas Holzschuch and Fran{\c{c}}ois Sillion",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-482",
  abstract =     "This paper presents a complete algorithm for the
                 evaluation and control of error in radiosity
                 calculations. Providing such control is both extremely
                 important for industrial applications andd one of the
                 most challenging issues remaining in global
                 illumination research. In order to control the error,
                 we need to estimate the accuracy of the calculation
                 while computing the energy exchanged between two
                 objects. Having this information for each radiosity
                 interaction allows to allocate more resources to refine
                 interactions with greater potential error, and to avoid
                 spending more time to refine interactions already
                 represented with sufficient accuracy. Until now, the
                 accuracy of the computed energy exchange could only be
                 approximated using heuristic algorithms. This paper
                 presents the first exhaustive algorithm to compute
                 fully reliable upper and lower bounds on the energy
                 being exchanged in each interaction. This is
                 accomplished by computing first and second derivatives
                 of the radiosity function where appropriate, and making
                 use of two concavity conjectures. These bounds are then
                 used in a refinement criterion for hierarchical
                 radiosity, resulting in a global illumination algorithm
                 with complete control of the error incurred. Results
                 are presented, demonstrating the possibility to create
                 radiosity solutions with guaranteed precision. We then
                 extend our algorithm to consider linear bounding
                 functions instead of constant functions, thus creating
                 simpler meshes in regions where the function is
                 concave, without loss of precision. Our experiments
                 show that the computation of radiosity derivatives
                 along with the radiosity values only requires a modest
                 extra cost, with the advantage of a much greater
                 precision.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(4)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-483,
  pages =        "219--233",
  year =         "1998",
  title =        "Reference Models for Distributed Cooperative
                 Visualization",
  author =       "D. A. Duce and D. Giorgetti and C. S. Cooper andJ. R.
                 Gallop and I. J. Johnson and K. Robinson and C. D.
                 Seelig",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-483",
  abstract =     "In this paper reference models for visualization
                 systems that have appeared in the literature are
                 surveyed and a new reference model for distributed
                 cooperative visualization developed in the MANICORAL
                 project (funded by the EU Telematics Programme) is
                 described. The relationship of earlier models to the
                 new model is discussed. A number of cooperative
                 visualization systems that have been reported in the
                 literature are compared in the framework of the
                 MANICORAL model.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(4)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-484,
  pages =        "235--247",
  year =         "1998",
  title =        "Incident Light Metering in Computer Graphics",
  author =       "L{\'{a}}szl{\'{o}} Neumann and Kresimir Matkovi and
                 Attila Neumann and Werner Purgathofer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-484",
  abstract =     "Every rendering process consists of two steps. The
                 first is the computing of luminance values by methods
                 like ray tracing or radiosity, and the second step is
                 the mapping of the computed values to values
                 appropriate for displaying. In the last years, as
                 alternative to simple linear scaling which maps the
                 average value to the medium luminance, some new ways of
                 mapping were introduced. These new methods are based on
                 photography analogies and on human vision models. All
                 existing methods follow, implicitly or explicitly, the
                 reflected light metering principle. The method
                 introduced in this paper is the first that follows the
                 incident light metering used in professional
                 photography and in the movie industry. Actually the
                 irradiances are measured using a set of diffusors,
                 which are placed automatically in the scene, and a
                 linear scale factor based on these measurements is used
                 to map the computed radiances to the display device.
                 The diffusors act as half space integrators, they
                 collect the light energy from all half space
                 directions. The light comes from the primary light
                 sources, or it is the result of various
                 interreflections. The newly introduced method
                 reproduces original colors faithfully even for scenes
                 with very low or very high average reflectivity.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(4)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-485,
  pages =        "249--261",
  year =         "1998",
  title =        "Programming Paradigms in an Object-Oriented Multimedia
                 Standard",
  author =       "D. J. Duke and I. Herman",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-485",
  abstract =     "Of the various programming paradigms in use today,
                 object-orientation is probably the most successful in
                 terms of industrial take-up and application,
                 particularly in the field of multimedia. It is
                 therefore unsurprising that this technology has been
                 adopted by ISO&sol;IEC JTC1&sol;SC24 as the foundation
                 for a forthcoming International Standard for
                 Multimedia, called PREMO. Two important design aims of
                 PREMO are that it be distributable, and that it
                 provides a set of media-related services that can be
                 extended in a disciplined way to support the needs of
                 future applications and problem domains. While key
                 aspects of the object-oriented paradigm provide a sound
                 technical basis for achieving these aims, the need to
                 balance extensibility and a high-level programming
                 interface against the realities of efficiency and ease
                 of implementation in a distributed setting meant that
                 the task of synthesising a Standard from existing
                 practice was non-trivial. Indeed, in order to meet the
                 design aims of PREMO is was found necessary to augment
                 the basic object infrastructure with facilities and
                 ideas drawn from other programming paradigms, in
                 particular concepts from constraint management and data
                 flow. This paper describes the important trade-offs
                 that have affected the development of PREMO and
                 explains how these are addressed through the use of
                 specific programming paradigms.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(4)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-486,
  pages =        "261--271",
  year =         "1998",
  title =        "A Fast Algorithm for Inverse Colormap Computation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-486",
  author =       "L. Brun and C. Secroun",
  abstract =     "The inverse colormap operation is the process which
                 allows an image to be displayed with a limited set of
                 colors. In order to obtain a minimal visual distortion
                 between the input image annd the one displayed, inverse
                 colormap algorithms associate each color with its
                 nearest representative. The method presented in this
                 paper is carried out in two steps. First, the 3D
                 Voronoi diagram implicitly used by inverse colormap
                 algorithms is approximated using a Karhunen-Lo{\`{e}}ve
                 transformation. Then, a correcting step is carried out
                 in order to reduce the influence of the first
                 approximation. The complexity of our algorithm is
                 independent of the size of the colormap. Moreover, its
                 results are equal or quite close to the optimal
                 solution.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "17(4)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1998-487,
  pages =        "210--218",
  year =         "1998",
  title =        "{Straight Skeleton Implementation}",
  author =       "P. Felkel and \v{S}. Obdr\v{z}{\'a}lek",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-487",
  address =      "Budmerice, Slovakia",
  month =        apr,
  editor =       "L{\'a}zl{\'o} Szirmay-Kalos",
  booktitle =    "Spring Conference on Computer Graphics",
}

@Book{EVL-1998-488,
  year =         "1998",
  title =        "{Modern Computer Graphics (Moderni pocitacova
                 grafika)}",
  author =       "J. {\v{Z}}{\'a}ra and B. Bene\v{s} and P. Felkel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-488",
  note =         "In Czech.",
  publisher =    "Computer Press, Brno, Czech Republic",
}

@InProceedings{EVL-1998-489,
  pages =        "241--242",
  year =         "1998",
  title =        "{3D Reconstruction from Cross Sections by means of
                 Contour Tiling}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-489",
  author =       "P. Felkel",
  month =        feb,
  editor =       "A. Strejc",
  volume =       "I",
  booktitle =    "Workshop '98",
  publisher =    "Czech Technical University, Prague, Czech Republic",
}

@InCollection{EVL-1998-49,
  pages =        "19--30",
  year =         "1998",
  title =        "Mesh Optimization and Multilevel Finite Element
                 Approximations",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-49",
  author =       "Roberto Grosso and Thomas Ertl",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InProceedings{EVL-1998-490,
  pages =        "301--312",
  year =         "1998",
  title =        "Accuracy in Contour Drawing",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-490",
  author =       "Adriano Lopes and Ken Brodlie",
  abstract =     "Contouring remains a major application of computer
                 graphics. Contours are typically modelled, first as
                 conic sections, and then as straight-line
                 approximations to the conics. In this paper, it is
                 shown how the accuracy of this straight-line
                 approximation can be indicated. In addition, a
                 refinement is proposed that yields a better
                 straight-line approximation without major cost
                 overhead.",
  organization = "University of Leeds",
  month =        mar,
  note =         "ISBN 0-952 1097-7-8",
  keywords =     "accuracy, contour, contouring",
  booktitle =    "Proceedings of Eurographics UK Conference",
}

@InProceedings{EVL-1998-491,
  pages =        "3--15",
  year =         "1998",
  title =        "Acceleration Radiosity Solutions Through the Use of
                 Hemisphere-base Formfactor Calculation",
  author =       "Akio Doi and Takayuki Itoh",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-491",
  abstract =     "In this paper, we propose a novel formfactor
                 calculation algorithm for acceleration radiosity
                 solutions in complex environments. Our basic algorithm
                 is an improved version of Spencer's (S.N. Spencer, The
                 hemisphere radiosity method: a tale of two algorithms,
                 in Photorealism in Computer Graphics, Spencer, 1992,
                 pp. 127-135) and Van Wyk's (G.C. Van Wyk Jr., A
                 geometry-based insolution model for computer-aided
                 design, Ph.D. Thesis, The University of Michigan,
                 1998.) methods, which fail to remove hidden surfaces
                 for relatively large patches and cause large
                 discretization errors in formfactors. We also
                 demonstrate that our technique is superior to the
                 hemi-cube method in terms of the computation time.
                 Moreover, we parallelize our approach on a parallel
                 computer with shared memory, and obtain a high
                 performance with our radiosity rendering system. Our
                 method divides a hemisphere-base into regions, and
                 assigns a region to each processor. The approach can be
                 applied to geometrical data generated by CAD systems,
                 and is evaluated in terms of the computation time, the
                 visual effects, and the parallelization performance.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  volume =       "9(1)",
  keywords =     "Global illumination, progressive refinement radiosity,
                 formfactor, image synthesis, parallelization",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1998-492,
  pages =        "17--31",
  year =         "1998",
  title =        "Fast Rendering of Relativistic Objects",
  author =       "Christopher Betts",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-492",
  abstract =     "This paper presents a number of simple techniques that
                 allow very fast rendering of multiple independently
                 moving objects under the conditions of special
                 relativity, as well as a brief tutorial in the theory
                 of special relativity for non-experts. In addition to
                 geometric rendering, a simplified lighting model is
                 presented, which allows the apparent colour changes of
                 objects to be observed, along with changes in the
                 apparent direction of illumination. The geometric
                 rendering can be summarized as a two-stage process. In
                 the first, the Lorentz contraction is applied to each
                 object in the observers reference frame, and in the
                 second the apparent visual position is found for every
                 object. Once the visual geometry is found, the
                 coloration of each object is handled, with a
                 traditional graphics engine such as a z-buffer
                 performing the final rendering. Rendering speeds in
                 excess of 30,000 polygons a second were obtained on a
                 133 MHz Silicon Graphics Indigo2 workstation.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  volume =       "9(1)",
  keywords =     "Relativity, simulation, animation",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1998-493,
  pages =        "33--47",
  year =         "1998",
  title =        "A Geometric Algorithm to Predict the Arm Reach Posture
                 for Computer-aided Ergonomic Evaluation",
  author =       "Xuguang Wang and Jean Pierre Verriest",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-493",
  abstract =     "Simulation of task-oriented human postures is one of
                 the essential functions of a computerized human model
                 for making reach, vision and fit analyses in a
                 computer-aided design environment. After reviewing
                 different existing methods of posture prediction, a
                 geometric inverse kinematic algorithm to predict arm
                 reach postures has been proposed, based on the
                 criterion of minimization of the norm of joint angular
                 velocities. The arm is modelled as a
                 four-degrees-of-freedom kinematic linkage system, three
                 for the shoulder and one for the elbow. A detailed
                 three-dimensional description of the shoulder joint
                 motion range is given. The main advantage of the
                 proposed method is that it can take into account the
                 non-linearity of the shoulder joint limit in a direct
                 and easy way. Another important advantage is that no
                 matrix inverse calculation is needed, thus reducing the
                 calculation time. Experimental validation shows that
                 the arm reach postures predicted by the proposed method
                 are very close to the real ones in a large
                 arm-reachable space. If initial arm postures are not
                 too awkward, no additional manipulation is needed to
                 correct the predicted arm reach posture from a visual
                 criterion. The proposed method of arm posture
                 prediction can be used as an efficient arm posture
                 manipulation primitive.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  volume =       "9(1)",
  keywords =     "Posture prediction, inverse kinematics, computerized
                 human model, shoulder joint motion range,
                 computer-aided ergonomic evaluation",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1998-494,
  pages =        "53--63",
  year =         "1998",
  title =        "Tighter Error Bounds and Weighted Error Metrics for
                 hierarchical radiosity",
  author =       "Chin-Chen Chang and Zen-Chung Shih",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-494",
  abstract =     "In this paper we first derive a tighter error bound on
                 form factors as a subdivision criterion for the
                 hierarchical radiosity algorithm. Such an error bound
                 can reduce more unnecessary links and improve the
                 performance of the hierarchical radiosity algorithm to
                 meet a user-specified error tolerance. We then propose
                 a weighted error metric in form factor computation such
                 that more effort is automatically applied to shadow
                 boundaries. Evaluating form factors along shadow
                 boundaries with a higher degree of precision should
                 enhance the quality of human perception. Using the
                 proposed tighter error bound on the weighted error
                 metric, we not only improve the performance but also
                 increase the accuracy of the hierarchical radiosity
                 algorithm.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  volume =       "9(2)",
  keywords =     "Global illumination, error analysis, radiosity,
                 visibility, shadow",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1998-495,
  pages =        "65--94",
  year =         "1998",
  title =        "Constraint-Based Motion Adaptation",
  author =       "Michael Gleicher and Peter Litwinowicz",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-495",
  abstract =     "Todays computer animators have access to many systems
                 and techniques to author high-quality motion.
                 Unfortunately, available techniques typically produce a
                 particular motion for a specific character. In this
                 paper we present a constraint-based approach to adapt
                 previously created motions to new situations and
                 characters. We combine constraint methods that compute
                 changes to motion to meet specified needs with motion
                 signal processing methods that modify signals yet
                 preserve desired properties of the original motion. The
                 combination allows the adaptation of motions to meet
                 new goals while retaining much of the motions original
                 quality.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  volume =       "9(2)",
  keywords =     "Computer animation, motion adaptation, constraint
                 solving, motion displacement and warping",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1998-496,
  pages =        "95--107",
  year =         "1998",
  title =        "Periodic Motion Synthesis and Fourier Compression",
  author =       "Mikio Shinya and Takeaki Mori and Noriyoshi Osumi",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-496",
  abstract =     "Periodic motion is an important class of motion to
                 synthesize, but it is not easy to compute it robustly
                 and efficiently. In this paper we propose a simple,
                 robust and efficient method to compute periodic motion
                 from linear equation systems. The method first
                 calculates the response of the system when an external
                 periodic force is applied during one period, and then
                 sums up the periodically shifted versions of the system
                 response to provide the periodic solution. It is also
                 shown that Fourier decomposition is very effective to
                 compress the motion data without a drop in visual
                 fidelity.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  volume =       "9(2)",
  keywords =     "Physically based modelling, linear systems, stationary
                 motion, wind models",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1998-497,
  pages =        "113--127",
  year =         "1998",
  title =        "Illumination of Image-based Objects",
  author =       "Tien-Tsin Wong and Pheng-Ann Heng and Siu-Hang Or and
                 Wai-Yin Ng",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-497",
  abstract =     "A new data representation of image-based objects is
                 presented. With this representation, the user can
                 change the illumination as well as the viewpoint of an
                 image-based scene. Physically correct imagery can be
                 generated without knowing any geometrical information
                 (e.g. depth or surface normal) of the scene. By
                 treating each pixel on the image plane as a surface
                 element, we can measure its apparent BRDF
                 (bidirectional reflectance distribution function) by
                 collecting information in the sampled images. These
                 BRDFs allow us to calculate the correct pixel colour
                 under a new illumination set-up by fitting the
                 intensity, direction and number of the light sources.
                 We demonstrate that the proposed representation allows
                 re-rendering of the scene illuminated by different
                 types of light sources. Moreover, two compression
                 schemes, spherical harmonics and discrete cosine
                 transform, are proposed to compress the huge amount of
                 tabular BRDF data.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann and Yeong
                 Gil Shin and James K. Hahn",
  volume =       "9(3)",
  keywords =     "Image-based rendering, BRDF, illumination, spherical
                 harmonics, light field rendering, lumigraph",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1998-498,
  pages =        "129--143",
  year =         "1998",
  title =        "Incremental View-dependent Multiresolution
                 Triangulation of Terrain",
  author =       "Reinhard Klein and Daniel Cohen-Or and Tobias
                 H{\"{u}}ttner",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-498",
  abstract =     "A view-dependent multiresolution triangulation
                 algorithm is presented for a real-time fly-through. The
                 triangulation of the terrain is generated incrementally
                 on-the-fly during the rendering time. We show that
                 since the view changes smoothly, only a few incremental
                 modifications are required to update the triangulation
                 to a new view. The resulting triangles form a
                 multiresolution Delaunay triangulation which satisfies
                 a predetermined view-dependent error tolerance. The
                 presented method provides a guaranteed-quality mesh
                 since it has control over the global geometric
                 approximation error of the multiresolution
                 view-dependent triangulation",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  volume =       "9(3)",
  keywords =     "Hierarchical approximation, model simplification,
                 levels-of-detail generation, shape approximation,
                 terrain modelling, view-dependent refinement",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1998-499,
  pages =        "145--161",
  year =         "1998",
  title =        "Template-based Rendering of Run-length-encoded
                 Volumes",
  author =       "Cheol-Hi Lee and Yun-Mo Koo and Yeong Gil Shin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-499",
  abstract =     "Template-based volume rendering is a technique to
                 accelerate volume ray casting. It does not trade off
                 image quality for rendering speed. However, it still
                 falls short of interactive manipulation of volume data,
                 mainly owing to the ray-by-ray volume access pattern
                 and the long ray path in the transparent regions. In
                 this paper we present an object-order template-based
                 volume rendering method that uses run-length encoding
                 to enable skipping highly transparent regions. We
                 present three algorithms, one for each principal axis
                 direction. By combining the advantages of object-order
                 volume traversal and run-length encoded volumes, the
                 algorithms achieve high quality rendering in a much
                 shorter time than the original template-based volume
                 rendering.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  volume =       "9(3)",
  keywords =     "Volume rendering, template based, run-length encoding,
                 object-order algorithm",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InCollection{EVL-1998-50,
  pages =        "31--44",
  year =         "1998",
  title =        "Efficient Visualization of Data on Sparse Grids",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-50",
  author =       "Norbert Heu{\ss}er and Martin Rumpf",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InProceedings{EVL-1998-500,
  pages =        "163--182",
  year =         "1998",
  title =        "SharedWeb - a Shared Virtual Environment over the
                 World Wide Web",
  author =       "Jiung-Yao Huang and Chao-Tsou Fang-Tsou and Jia-Lin
                 Chang and Ai-Jye Lee",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-500",
  abstract =     "The design, implementation and experimental results of
                 a multiple-user 3D web-browsing system, called the
                 SharedWeb system, are described in this paper. The
                 system is based on the request-response method of the
                 WWW server. The key features of this system include the
                 ability of the WWW server to remember all the browsers
                 connected to it in order to transmit the messages
                 required to maintain consistent states among browsers.
                 The dead-reckoning technique of distributive
                 interactive simulation (DIS) is used by browsers in
                 order to reduce the number of messages sent onto the
                 network and to satisfy the timing requirement demanded
                 by the virtual environment. The protocol data unit
                 (PDU) defined in DIS is also adapted by the SharedWeb
                 system not only to communicate information among
                 browsers but also to integrate with the DIS environment
                 in the near future.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  volume =       "9(3)",
  keywords =     "World Wide Web, common gateway interface, distributive
                 interactive simulation, virtual reality,
                 dead-reckoning",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1998-501,
  pages =        "185--194",
  year =         "1998",
  title =        "An Erosion Model Based on Velocity Fields for the
                 Visual Simulation of Mountain Scenery",
  author =       "N. Chiba and K. Muraoka and K. Fujita",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-501",
  abstract =     "Visual simulation of natural scenery using computer
                 graphics is an interesting research field with wide
                 applications such as flight simulation and special
                 effects in movies. There have been many studies of
                 fractal techniques that use 1/f noise generated by FFT
                 or the midpoint displacement method as modelling
                 methods for imaginary mountain scenery. These methods
                 are suitable for creating impressive mountain scenes,
                 but they cannot create clear ridge and valley lines,
                 which are notable features of mountains produced by
                 erosion processes. Although a few reports have
                 presented modelling methods that take erosion processes
                 into account, the results have not been satisfactory.
                 In this paper we present a simple quasi-physically
                 based method for simulating the topography of eroded
                 mountains based on velocity fields of water flow.",
  volume =       "9(4)",
  keywords =     "Computer graphics, visual simulation, natural
                 phenomena, terrain, erosion processes",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1998-502,
  pages =        "195--213",
  year =         "1998",
  title =        "A New Partitioning Method for Architectural
                 Environments",
  author =       "D. Meneveaux and K. Bouatouch and E. Maisel and R.
                 Delmont",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-502",
  abstract =     "Computing global illumination for complex environments
                 in moderate time and walking through them is one of the
                 challenges in computer graphics. To meet this goal,
                 preprocessing is necessary. This preprocessing consists
                 in partitioning the environment into cells and
                 determining visibility between these cells. Most of the
                 existing partitioning methods rely on the binary space
                 partitioning (BSP) technique which can be easily
                 applied to axial environments. However, for non-axial
                 scenes, BSP has a high complexity of O (n3) in time to
                 construct a tree of size at worst O(n2), n being the
                 total number of input polygons. Moreover, this
                 technique entails a large number of cells that do not
                 necessarily fit with the topology of the environment.
                 We propose in this paper a partitioning method which
                 can be applied to non-axial buildings with several
                 floors. It consists of two steps. In the first step
                 each floor is extracted by applying a BSP technique
                 using the most occlusive horizontal polygons for
                 splitting. In the second step each floor is in turn
                 partitioned with a model-based method operating in a
                 dual 2D space. The result is a low number of cells
                 fitting at best with the environment topology.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  volume =       "9(4)",
  keywords =     "Computer graphics, complex scenes, radiosity, lighting
                 simulation",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1998-503,
  pages =        "215--227",
  year =         "1998",
  title =        "A Spring-mass Model-based Approach for Warping Cloth
                 Patterns on 3{D} Objects",
  author =       "Jin Fan and Qifu Wang and Shiang-Fong Chen and Matthew
                 M. F. Yuen and C. C. Chan",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-503",
  abstract =     "This paper presents a general method for
                 transformations of an object between 2D and 3D which
                 can be used efficiently in computer-aided garment
                 design. A uniform triangular spring-mass-based
                 deformable model is employed. Both 2D-to-3D and
                 3D-to-2D transformations can be implemented in the same
                 model. An efficient collision detection method is also
                 discussed",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  volume =       "9(4)",
  keywords =     "2D/3D transformation, deformable model, spring-mass
                 system, collision detection, computer-aided garment
                 design",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1998-504,
  pages =        "229--242",
  year =         "1998",
  title =        "An Efficient Algorithm for Ray Casting of {CSG}
                 animation frames",
  author =       "Andrea Sanna and Paolo Montuschi",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-504",
  abstract =     "This paper presents a new algorithm to generate
                 ray-cast CSG animation frames. We consider sequences of
                 frames where only the objects can move; in this way, we
                 take advantage of the high screen area coherence of
                 this kind of animation. A new definition of bounding
                 box allows us to reduce the number of pixels to be
                 computed for the frames after the first. We associate a
                 CSG subtree and two new flags, denoting if the box has
                 changed in the current frame and if it will change in
                 the next frame, with each box. We show with three
                 examples the advantages of our technique when compared
                 with an algorithm which entirely renders each frame of
                 an animation. Intersections with CSG objects may be
                 reduced to about one-fifth, while the rendering may be
                 computed up to four times faster for the test
                 sequences.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  volume =       "9(4)",
  keywords =     "Animation rendering, bounding box computation,
                 constructive solid geometry",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1998-505,
  pages =        "243--257",
  year =         "1998",
  title =        "Interactive Rigid Body Manipulation with Obstacle
                 Contacts",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-505",
  author =       "Matthias Buck and Elmar Sch{\"{o}}mer",
  abstract =     "The interactive manipulation of rigid objects in
                 virtual reality environments requires an object
                 behaviour which is at least physically plausible to be
                 useful for applications such as interactive assembly
                 simulation and virtual training. Physically plausible
                 behaviour implies that collisions between simulated
                 solid objects are taken into account and that the
                 motion of objects with obstacle contacts can be
                 controlled without force feedback mechanisms in an
                 intuitively correct manner. We present a real time
                 framework which enables the simulation of interactively
                 controlled solid objects with a dynamically changing
                 set of contact constraints. In this paper all contact
                 configurations are replaced by a canonical set of point
                 contacts which is updated dynamically. The basic step
                 to determine the contact forces and object motion
                 consists in the solution of a non-linear
                 complementarity problem (NCP), which results from the
                 unilateral contact conditions together with an adequate
                 discretization of the corresponding differential
                 equations of motion",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  keywords =     "Virtual reality, motion simulation, contact
                 constraints, non-linear complementarity problem, NCP
                 functions, Newton iteration",
  volume =       "9(4)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1998-506,
  pages =        "259--272",
  year =         "1998",
  title =        "Extension of Validity Calculation to Moving Objects
                 within a Virtual Reality System using Frame-to-frame
                 Coherence",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-506",
  author =       "F. Duckstein",
  abstract =     "Generating pictures of a complex scene in real time
                 with constant frame rates still overwhelms modern
                 rendering systems. The exploitation of frame-to-frame
                 coherence offers a solution to this problem by reusing
                 the images of rendered objects multiple times.
                 Combining this with a cost metric and an LOD system
                 delivers a uniform frame rate system with little
                 additional effort. The basis of reusing images is a
                 validity calculation to decide how far the original and
                 the image might differ. The application is restricted
                 to fixed objects and fixed spaces. This paper presents
                 an extension of the existing validity calculation to
                 handle moving objects, which are now standard e.g. in
                 the virtual reality description language VRML 2.0.11
                 The additional deviation resulting from object movement
                 is estimated together with viewpoint movement,
                 resulting in one scalar error value for further
                 processing.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  keywords =     "Frame coherence, image-based rendering, animated
                 objects, computational geometry, virtual reality",
  volume =       "9(4)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1998-507,
  year =         "1998",
  title =        "Solving Point and Plane vs. Orthogonal Polyhedra Using
                 the Extreme Vertices Model ({EVM})",
  author =       "Antonio Aguilera and Dolors Ayala",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-507",
  abstract =     "In a previous work, Orthogonal Polyhedra (OP) were
                 proposed as geometric bounds in CSG. Primitives in the
                 CSG model were approximated by their respective
                 bounding boxes. The polyhedrical bound for the CSG
                 object was obtained by applying the corresponding
                 Boolean Algebra to those boxes. Also in that paper, a
                 specific and very concise model for representing and
                 handling OP was presented: the Extreme Vertices Model
                 (EVM). The EVM allows simple and robust algorithms for
                 performing the most usual and demanding tasks. This
                 paper deals with the classification of point, and plane
                 vs. OP. These operations can be done on the EVM in
                 linear time. Furthermore, a very important feature of
                 EVM algorithms is that, even though their input data
                 (i.e., vertices'coordinates) can be floating-point
                 values, no time-consuming floating-point arithmetic is
                 ever performed (except when explicitly noted), so there
                 are absolutely no propagation errors due to partial
                 results (which do not exist). All results are obtained
                 by just classifying and selecting vertices' coordinates
                 of the initial data.",
  editor =       "V. Skala",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-508,
  year =         "1998",
  title =        "{IRIS}: Intelligent Visualization for Data Exploration
                 Support in {GIS}",
  author =       "Gennady L. Andrienko and Natalia V. Andrienko",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-508",
  abstract =     "Our work is focused on assisting users in exploration
                 of spatially referenced data, i.e., economical,
                 demographic, ecological, etc. data referring to
                 geographical objects or locations. Analysis of such
                 data is impossible without representing them on maps.
                 The main weakness of Geographic Information Systems
                 (GISes) in support of data mapping is that the user is
                 not given any guidance in designing presentations
                 whereas improper selection of visualization methods can
                 impede subsequent analysis or even result in wrong
                 conclusions. Correct map design requires special
                 knowledge from the field of thematic cartography. One
                 cannot presume that any GIS user has this knowledge. We
                 introduce the software system IRIS that incorporates
                 the knowledge on map design in the form of generic,
                 domain-independent rules. On this basis it
                 automatically generates thematic maps properly
                 presenting user's data. Another distinctive feature of
                 IRIS is that it supports subsequent data analysis with
                 the use of generated maps. The user can interactively
                 manipulate the presentations, and in response they
                 dynamically change making more salient various features
                 of the data under analysis.",
  editor =       "V. Skala",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-509,
  year =         "1998",
  title =        "Extending the Zonal Method to Specular Surfaces",
  author =       "Didier Arques and Sylvain Michelin and Benoit
                 Piranda",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-509",
  abstract =     "This paper focuses on how to calculate, with radiosity
                 techniques, images associated to different scenes which
                 take into account both an isotropic diffuse
                 participating media and specular surfaces. To do so, we
                 propose a new expression of the form factors defined in
                 the zonal method. The first part of this article
                 briefly quotes the expression of energetic exchanges by
                 using operators and zonal method equations. The second
                 part deals with the different changes brought to the
                 zonal method in order to express it under the form of
                 operators first and then redefine its form factors.
                 Finally, we will treat of computational considerations
                 and see how different images enable us to show the many
                 possibilities of our algorithm.",
  editor =       "V. Skala",
  keywords =     "Radiosity, participating media, specular reflection,
                 zonal method, extended form factors",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InCollection{EVL-1998-51,
  pages =        "45--59",
  year =         "1998",
  title =        "A Meta Scheme for Iterative Refinement of Meshes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-51",
  author =       "Markus Kohler",
  language =     "en",
  address =      "Heidelberg",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InProceedings{EVL-1998-510,
  year =         "1998",
  title =        "A Motion Blur Method for Animated Radiosity
                 Environments",
  author =       "Gonzalo Besuievsky and Xavier Pueyo",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-510",
  abstract =     "A method to generate motion blur images for radiosity
                 scenes is presented. The method, based on Global Monte
                 Carlo techniques, is intended to perform an accurate
                 lighting simulation of the effect. Once the
                 illumination is computed a rendering step based on an
                 accumulation buffer approach is used to obtain the
                 motion blur images. Due to the fact that radiosity
                 solutions are view independent, motion blur images can
                 be obtained from different points of view only with
                 this rendering step. Results show that the illumination
                 computed with the method is correct. The proposed
                 method is well suited for being included in the whole
                 computation of animations in a radiosity environment.",
  editor =       "V. Skala",
  keywords =     "Radiosity, Motion Blur, Monte Carlo, Accumulation
                 Buffer, Animation",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-511,
  year =         "1998",
  title =        "A Framework for Data Visualization Based on Particle
                 Systems",
  author =       "Fernando Pedro Birra and Manuel Prospero dos Santos",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-511",
  abstract =     "This paper describes an object oriented framework
                 based on particle systems and suitable for scientific
                 visualization and simulation. The environment is
                 composed by a small kernel and an upgradable collection
                 of dynamically loaded classes where the specific
                 behaviors of the elements are described and
                 implemented. Full benefits of handling particle systems
                 to create physical based simulations are also expected.
                 The use of a general purpose programming language
                 rather than a particular built-in language gives the
                 application programmer the advantage of avoiding the
                 inevitable learning time, in addition to skim over
                 eventual limitations. The conceptual model is described
                 along the paper and it is founded on four abstract
                 classes: particles, fields, particle sources, and
                 interactions. The main application areas exemplified in
                 the text are direct volume rendering, flow field
                 visualization, and simulation of several phenomena
                 involving interactions between particles. All these
                 examples were built on top of the framework as a proof
                 of its wide range of applications.",
  editor =       "V. Skala",
  keywords =     "Visualization, Particle systems, Simulation",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-512,
  year =         "1998",
  title =        "Interactive Rigid Body Manipulation with Obstacle
                 Contacts",
  author =       "Matthias Buck and Elmar Sch{\"o}mer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-512",
  abstract =     "The interactive manipulation of rigid objects in
                 virtual reality environments requires an object
                 behaviour which is at least physically plausible to be
                 useful for applications like interactive assembly
                 simulation or virtual training. Physically plausible
                 behaviour implies that collisions between simulated
                 solid objects are taken into account, and that the
                 motion of objects with obstacle contacts can be
                 controlled without force feedback mechanisms in an
                 intuitively correct manner. We present a real-time
                 framework which enables the simulation of interactively
                 controlled solid objects with a dynamically changing
                 set of contact constraints. In this paper, all contact
                 configurations are replaced by a canonical set of point
                 contacts, which is updated dynamically. The basic step
                 to determine the contact forces and the object motion
                 consists in the solution of a non-linear
                 complementarity problem (NCP), which results from the
                 unilateral contact conditions together with an adequate
                 discretization of the corresponding differential
                 equations of motion.",
  editor =       "V. Skala",
  keywords =     "Virtual reality, motion simulation, contact
                 constraints, non-linear complementarity problem,
                 NCP-functions, Newton iteration",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-513,
  year =         "1998",
  title =        "Quantitative Visualisation of Surfaces from Volumetric
                 Data",
  author =       "Jonathan C. Carr and Andrew H. Gee1 and Richard W.
                 Prager and Kevin J. Dalton",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-513",
  abstract =     "A 3D B-spline quasi-interpolant is used to extract
                 smooth iso-value surfaces from volume data. In this
                 technique discretization and partial voluming artifacts
                 are reduced by approximating sampled data at voxel
                 centres. Surface normals, necessary for realistic
                 shading, are analytically defined by the approximating
                 function rather than estimated by in an ad hoc way from
                 the volume data. We consider an application where bone
                 surfaces are revealed from CT data by ray-casting and
                 the surfaces are then used to construct models and
                 prostheses. Accurate determination and rendering of
                 bone surfaces is required. A z-buffer shading technique
                 is also described for improved rendering of surface
                 depth-maps.",
  volume =       "V. Skala",
  keywords =     "Medical imaging, surface rendering, ray-casting,
                 Computed Tomography (CT), prosthesis design",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-514,
  year =         "1998",
  title =        "Optimization and Parallelization of a Geographical
                 Stereo Vision Code",
  author =       "Sylvain Contassot-Vivier and Serge Miguet",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-514",
  abstract =     "This paper describes the sequential and parallel
                 versions of a stereo vision algorithm. In this study,
                 we address the particular stereo vision problem of
                 accurate Digital Elevation Models (DEMs) reconstruction
                 from a pair of images of the SPOT satellite. We start
                 from an existing algorithm made by M.Memier, we
                 optimize it while focusing on the cross-correlation
                 problem based on a statistical operator. And we propose
                 a parallel implementation with special care to load
                 balancing.",
  editor =       "V. Skala",
  keywords =     "Stereo Vision, Computational Optimization,
                 Parallelism, Load Balancing",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-515,
  year =         "1998",
  title =        "{JAZ}: Java Algorithm Visualizer. {A} Multi-platform
                 Collaborative Tool for Teaching Graph Algorithms",
  author =       "Giancarlo Bongiovanni and Pierluigi Crescenzi",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-515",
  abstract =     "In this paper we propose a new graph algorithm
                 visualizer, called JAZ (Java Algorithm visualiZer),
                 whose main features are: (a) JAZ is basically
                 machine-independent; (b) JAZ is a post-mortem SV
                 system; (c) JAZ heavily uses colors and it allows to
                 show multiple views of program executions; (d) JAZ is
                 based on an imperative approach; (e) the interaction
                 between the user and the system is based on the Java
                 Application Programming Interface; (f) the main purpose
                 of JAZ is novice classroom demonstration and algorithm
                 development and debugging. In our opinion, JAZ is very
                 simple to be used. The main characteristic of JAZ,
                 however, is that it can also be used as a
                 multi-platform distributed collaborative tool for
                 teaching graph algorithms (over any network supporting
                 the TCP protocol, such as Internet or any intranet).",
  editor =       "V. Skala",
  keywords =     "Graph algorithms, Algorithm animation, Visualization",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-516,
  year =         "1998",
  title =        "Extension of Validity Calculation to Moving Objects
                 within a Virtual Reality System Using Frame to Frame
                 Coherence",
  author =       "Franz Duckstein",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-516",
  abstract =     "Generating pictures of a complex scene in real time
                 with constant frame rate still overwhelms modern
                 rendering systems. The exploitation of frame to frame
                 coherence proposes a solution to this problem by
                 reusing the images of rendered objects multiple times.
                 Combining this with a cost-metric and a LOD-system
                 delivers a uniform frame rate system with less
                 additional effort. The fundament of reusing images is a
                 validity calculation, deciding in how far the original
                 and the image might differ. The application is
                 restricted to fixed objects respectively fixed spaces.
                 This paper presents extensions for existing validity
                 calculations to handle moving objects, which e.g. are
                 now standard in the virtual reality description
                 language VRML 2.0. The additional deviation resulting
                 from object movements is estimated together with
                 viewpoint movement.",
  editor =       "V. Skala",
  keywords =     "Image based rendering, frame coherence, animated
                 objects, virtual reality, LOD management",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-517,
  year =         "1998",
  title =        "Volumetric-{CSG} -- {A} Model-based Volume
                 Visualization Approach",
  author =       "Shiaofen Fang and Rajagopalan Srinivasan",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-517",
  abstract =     "This paper presents a Volumetric-CSG (VCSG) method for
                 the representation of volumetric objects and their
                 operation, such as transformations, cutting and Boolean
                 operations. A new volume rendering algorithm is
                 developed for visualizing the VCSG models. The
                 algorithm first generates optimal target blocks for
                 efficient model operations by adaptive subdivision of
                 the target volume, and then volume renders the target
                 blocks using a template-based octree projection
                 process. Both the raycasting block projection and
                 hardware assisted 3D texture mapping rendering methods
                 are implemented.",
  editor =       "V. Skala",
  keywords =     "Volume rendering, Volumetric modeling, Raycasting, 3D
                 texture mapping, CSG",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-518,
  year =         "1998",
  title =        "A {GIS}-based Environment for Telecommunications
                 Engineering",
  author =       "Wilson de Paula Padua Filho and Berthier Ribeiro and
                 Adriana Andrade Oliveira and Guilherme P. S. Padua",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-518",
  abstract =     "This work describes a large GIS-based environment for
                 the support of planning and design activities in
                 Telecommunications Engineering. This environment
                 supports various activities such as the acquisition of
                 geographic and engineering data, the design of outside
                 plants for telecommunications companies, and the design
                 of wireless transmission systems. To support such
                 activities, the geographic information system must be
                 customized. This paper focuses on tools for the
                 customization of the geographic information system, on
                 the design of wireless transmission systems and on the
                 lessons learned during the development of the whole
                 environment. Among these lessons, we distinguish: (a)
                 workstations are not always the most appropriate
                 platform, (b) the GIS environment might require the
                 implementation of special purpose software modules if
                 acceptable performance is to be obtained, (c) a
                 requirements analysis phase must be concluded in the
                 very beginning if users are to be kept satisfied with
                 the system, and (d) application of methodology to the
                 software engineering process is an absolute must.",
  editor =       "V. Skala",
  keywords =     "Geographic information systems, telecommunications
                 engineering, computer graphics applications, software
                 engineering, spatial data bases",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-519,
  year =         "1998",
  title =        "Automated Finite Element Modeling of a Human Mandible
                 with Dental Implants",
  author =       "Stefan F{\"u}tterling and Reinhard Klein and Wolfgang
                 Strasser and Heiner Weber",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-519",
  abstract =     "This paper presents an automated procedure to generate
                 a three-dimensional finite element model of an
                 individual patient's mandible with dental implants
                 inserted. The reconstruction of the geometry as well as
                 the modeling of the material properties for the
                 different types of bone in the jaw is based on CT data.
                 For this purpose various methods of image processing,
                 geometric modeling and finite element analysis are
                 combined and extended. Special emphasis is given to the
                 automated assignment of the material properties based
                 on the density values of the CT data, a technique that
                 replaces the geometric modeling of the inner structures
                 of the bone and makes it possible to run the generation
                 process of the model in an automated way. Finally we
                 focus on a comparison of a mandible with different
                 material modeling strategies that shows the quality of
                 the finite element models.",
  editor =       "V. Skala",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InCollection{EVL-1998-52,
  pages =        "73--87",
  year =         "1998",
  title =        "Finite Element Approximations and the Dirichlet
                 Problem for Surfaces of Prescribed Mean Curvature",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-52",
  author =       "Gerhard Dziuk and John E. Hutchinson",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InProceedings{EVL-1998-520,
  year =         "1998",
  title =        "Pixel Classification in Nosiy Digital Pictures Using
                 Fuzzy Arithmetic",
  author =       "Giovanni Gallo and Salvatore Spinello",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-520",
  abstract =     "This paper presents a new technique to extract, in
                 noisy digital pictures, regions whose pixels fall, with
                 a degree of uncertainty, in a given range of gray
                 levels. The proposed method uses fuzzy numbers to
                 describe in a compact way, at the early vision stage,
                 the relevant information of the picture together with
                 the uncertainty due to noise. This fuzzy model of the
                 original picture is hence interrogated with a
                 Marching-Cube-like algorithm, for a specified level of
                 presumption, the pixels in a prescribed range. The
                 quality of the obtained result is comparable with those
                 obtained with more traditional, but less efficient,
                 non-linear smoothing techniques.",
  editor =       "V. Skala",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-521,
  year =         "1998",
  title =        "Computation and Maintenance of Visibility and Shadows
                 in the Plane",
  author =       "Sherif Ghali",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-521",
  abstract =     "We consider visibility and shadow problems in the
                 plane under static and dynamic conditions. We show that
                 several problems in this domain are intimately related
                 and that a careful formulation of the requirements of
                 each problem can lead to a great deal of reuse in the
                 implementation. This is of interest to the practitioner
                 who may find it difficult to implement visibility and
                 shadow algorithms given their complexity. A solution of
                 these problems in the plane turns out to be a necessary
                 component of their solution in space and by separating
                 these components, difficult issues of implementation
                 are taken care of at the design level. We also give
                 specific algorithmic results. We show a reduction
                 between shadow and visibility computations under a
                 point light source, between static and dynamic
                 computations under a point light source, and visibility
                 and shadow computations between a point and a linear
                 light source.",
  editor =       "V. Skala",
  keywords =     "Computational Geometry, Visibility, Shadows, Dynamic
                 Algorithms, Class Hierarchy",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-522,
  year =         "1998",
  title =        "Numerically Stable Direct Least Squares Fitting of
                 Ellipses",
  author =       "Radim Halir and Jan Flusser",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-522",
  abstract =     "This paper presents a numerically stable non-iterative
                 algorithm for fitting an ellipse to a set of data
                 points. The approach is based on a least squares
                 minimization and it guarantees an ellipse-specific
                 solution even for scattered or noisy data. The optimal
                 solution is computed directly, no iterations are
                 required. This leads to a simple, stable and robust
                 fitting method which can be easily implemented. The
                 proposed algorithm has no computational ambiguity and
                 it is able to fit more than 100,000 points in a
                 second.",
  editor =       "V. Skala",
  keywords =     "Ellipses, fitting, least squares, eigenvector",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-523,
  year =         "1998",
  title =        "Reconstructing Radiosity by Scattered Data
                 Interpolation",
  author =       "Andre Hinkenjann and Georg Pietrek",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-523",
  abstract =     "We discuss interpolation methods for the
                 reconstruction of the radiosity function across a
                 patch. Two groups of methods are compared: One group
                 based on regular grids and one based on hierarchical
                 subdivisions. We handle points on hierarchical
                 subdivisions as scattered data points which opens the
                 field of scattered data interpolation. These different
                 methods were implemented and characteritic images that
                 show the (dis)advantages are discussed and compared.
                 Additionally, we calculated errors in standard error
                 measures. It shows that some scattered data
                 interpolation methods produce acceptable images.",
  editor =       "V. Skala",
  keywords =     "Radiosity reconstruction, interpolation, scattered
                 data",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-524,
  year =         "1998",
  title =        "Reconstruction of Outer Surfaces by Bi-directional
                 Rays",
  author =       "Hisayoshi Zaima and Tsuyoshi Yamamoto",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-524",
  abstract =     "We propose a method for reconstructing the shape and
                 color of objects from ordinary video frames. On the
                 assumption that the objects are located in voxel space,
                 we reconstruct the objects as voxel models by
                 identifying individual voxel properties. Since this
                 method allows a high degree of freedom when taking
                 pictures, this method offers a wider range of
                 applications than conventional methods. Since our
                 method does not involve extracting and tracking feature
                 points, the algorithm does not suffer from the problems
                 of corresponding feature points and human intervention.
                 In addition, as there is no need to pay attention to
                 correlation between frames, our approach is easy to
                 deal with. Our method assumes that the objects exist in
                 voxel space. This method identifies the voxel
                 properties so as to keep the appearance from frame to
                 frame consistent. The voxel properties are state
                 variables that represent shape and color, and they are
                 calculated by the backward and forward shooting of
                 rays. During the calculation each voxel has several
                 candidate colors with the correct one determined at the
                 end. The reconstructed voxel models are rendered from
                 new viewpoints to generate new views.",
  editor =       "V. Skala",
  keywords =     "Computer graphics, 3 dimensional modeling, voxel
                 space, ray tracing, rendering, reconstruction",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-525,
  year =         "1998",
  title =        "Interactive Design Tools for Minimal Energy Splines",
  author =       "Jyun-Ming Chen and Bing-Hong Wu",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-525",
  abstract =     "The use of minimal energy spline (MES) curves in shape
                 design produces smoooth geometry. This paper describes
                 a prototype system implmenting such a technology. The
                 section curves of three-dimensional solids is designed
                 using the MES curves incorporating constraint
                 management technology for both unilateral and bilateral
                 geometric constraints. A new shape design method, the
                 mold cavity deformation, is developed to mimic the
                 force-based deformation process inside a mold cavity.
                 It is conjectured that this suite of intuitive (reality
                 mimicking) and interactive design tools would allow
                 stylists to work more effectively on the creative
                 aspect of the design.",
  editor =       "V. Skala",
  keywords =     "Geometric Constraints, Physically-Based Modeling,
                 Geometric Modeling",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-526,
  year =         "1998",
  title =        "Selecting Independent Vertices for Terrain
                 Simplification",
  author =       "Bernd J{\"u}nger and Jack Snoeyink",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-526",
  abstract =     "In this paper we investigate decimation algorithms for
                 simplifying triangulated terrain models in order to
                 support progressive transmission of GIS terrain models
                 over the web. We report on experiments with heuristics
                 that select an independent set of vertices to be
                 deleted, while trying to preserve terrain
                 characteristics.",
  editor =       "V. Skala",
  keywords =     "Level of detail, surface simplification, progressive
                 transmission, accuracy, GIS, TIN",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-527,
  year =         "1998",
  title =        "Rational Biangle Surface Patches",
  author =       "Kestutis Karciauskas and Rimvydas Krasauskas",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-527",
  abstract =     "The concept of the rational biangle surface patch of
                 the degree 2n is introduced. The construction is most
                 close to (n,n) tensor product surface because it has
                 (n+1)^{2} control points and the implicit degree 2n^{2}
                 in general. The biangle has many similar properties: a
                 convex hull property; Boundary Bezier curves can be
                 easily calculated; a subdivision and degree evaluation
                 algorithms are available. The quadratic biangle (when n
                 = 1) is a patch on an oval quadratic surface with four
                 control points. In particukar, one can realize any
                 biangle with two cyclic arcs on the sphere.",
  editor =       "V. Skala",
  keywords =     "Rational surface patch, Quadratic, Complex projective
                 line",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-528,
  year =         "1998",
  title =        "3{D} Furlike Texture Generation by a 2{D}
                 Autoregressive Synthesis",
  author =       "Leila Khouas and Christophe Odet and Denis Friboulet",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-528",
  abstract =     "This paper presents a texture model that allows a two
                 dimensional synthesis of 3D furlike texture. The model
                 is based on the modelling of the texture
                 autocorrelation function (ACF) and a two dimensional
                 AutoRegressive synthesis (2D AR). We consider that the
                 appearance of a furlike texture is defined by two
                 characteristics: the orientation and the length of each
                 individual element (i.e. filament). The values of these
                 characteristics are constant in a stationary texture.
                 The AR synthesis model of a given texture is defined by
                 a number of parameters which can be computed from the
                 texture ACF. Therefore, we have built a mathematical
                 model of the ACF with respect to orientation and
                 length. The model obtained consists of an analytical
                 expression of ACF with respect to orientation. For the
                 length, we have used a lookup table. Indeed, the ACF
                 expression includes some parameters which are
                 numerically estimated for several values of length.
                 This allows to build a lookup table indexed by length
                 and containing at each entry a set of ACF parameters.
                 In this way, a furlike texture synthesis based on this
                 table can be implemented. We show that with an
                 appropriate choice of other parameters of the AR
                 synthesis, this approach yields satisfying results,
                 essentially for the simulation of fur appearance on 3D
                 surfaces.",
  editor =       "V. Skala",
  keywords =     "Furlike texture, texture modelling, AR modelling,
                 texture synthesis, texture mapping, fur appearance
                 simulation.",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-529,
  year =         "1998",
  title =        "Visualization of the Nonlinear Dynamical System Using
                 {WWW}",
  author =       "S. V. Klimenko and V. A. Litvin and V. V Smirnova",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-529",
  abstract =     "Various 3d-view and 2d-projections of the abstract
                 mathematical objects (strange attractors) have been
                 discussed. We are studying the nonlinear dynamical
                 systems using WWW technologies VRML, HTML and HTTP.
                 Such visualization technique gives an essential
                 improvement in the scientific investigation of the
                 nonlinear dynamical systems. The search of the
                 homoclinic points of such software have been presented
                 at WWW.",
  editor =       "V. Skala",
  keywords =     "WWW, VRML. visualization, dynamical system, homoclinic
                 point",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InCollection{EVL-1998-53,
  pages =        "107--116",
  year =         "1998",
  title =        "Constant Mean Curvature Surfaces with Cylindrical
                 Ends",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-53",
  author =       "Karsten Gro{\ss}e-Brauckmann and Robert B. Kusner and
                 John M. Sullivan",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InProceedings{EVL-1998-530,
  year =         "1998",
  title =        "Genetic Approach to the Minimum Weight triangulation",
  author =       "I. Kolingerova",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-530",
  abstract =     "The weight of a triangulation is the sum of the
                 Euclidean lengths of its edges. A minimum weight
                 triangulation is such a triangulation which minimizes
                 the weight among all triangulations of the given point
                 set. Finding the MWT is difficult as for general data
                 sets, no exact polynomial algorithm is known. This
                 paper desribes how to find an approximate solution to
                 the MWT by a genetic approach, i.e., by a probabilistic
                 method which maintains a set of potential solutions and
                 tries to improve them so that at the end at least one
                 triangulation is close or equal to the optimum. The
                 operators used for improvement are quite general and
                 were derived from genetics. The results obtained by
                 this approach are compared either with optima if
                 available or with results of already existing heuristic
                 algorithms.",
  editor =       "V. Skala",
  keywords =     "Computational geometry, computer graphics, minimum
                 weight triangulation, genetic algorithms,
                 optimalization, greedy triangulation",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-531,
  year =         "1998",
  title =        "Interleaved Dimension Decomposition: {A} New
                 Decomposition Method for Wavelets and its Application
                 to Computer Graphics",
  author =       "Manfred Kopp and Werner Purgathofer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-531",
  abstract =     "Wavelets in 2D or higher dimensions are often
                 generated by a decomposition scheme from 1D wavelets.
                 There are two decomposition schemes called the standard
                 and the nonstandard decomposition which are used in
                 most applications of higher dimensional wavelets. This
                 paper introduces a new decomposition method, the
                 interleaved dimension decomposition and compares its
                 advantages and disadvantages with the other
                 decompositions. Based on the properties of the
                 interleaved dimension decomposition, applications to
                 computer graphics are sketched including
                 multiresolution painting, morphing in 2D and 3D, and
                 image compression.",
  editor =       "V. Skala",
  keywords =     "Wavelets, morphing, multiresolution painting, image
                 compression",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-532,
  year =         "1998",
  title =        "A Performance Prediction Tool for Parallel Ray
                 Tracing",
  author =       "M. Krajecki and Z. Habbas and F. Herrmann Y. Gardan",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-532",
  abstract =     "This paper proposes a general matrix iterative model
                 to represent a range of dynamic load balancing for MIMD
                 parallel ray tracing. Different measure parameters are
                 expressed only on theoretical data. A parallel ray
                 tracing application is implemented. Both results
                 obtained from the model and from parallel
                 implementation are used to compare different load
                 balancing strategies and validate the model. As a main
                 result, we observe no significant error between model
                 and measured value. We can deduce that our model is an
                 interesting and accurate tool to select the best
                 algorithm before implementing it.",
  editor =       "V. Skala",
  keywords =     "Load balancing, parallel algorithms, matrix iterative
                 model, parallel ray tracing, parallel and distributed
                 graphics, MIMD application.",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-533,
  year =         "1998",
  title =        "Biomedical Data Interpolation for 3{D} Visual Models",
  author =       "M. H. Kuo and M. C. Chen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-533",
  abstract =     "Computed Tomography (CT) is an X-ray technique that
                 produces 2D biomedical slice image data. In such
                 images, the slice spacing is much greater than the
                 spacing of individual samples on a slice. Thus, a
                 robust interpolation method to generate quality
                 intermediate images for the reconstruction of 3D
                 biomedical model is necessary. The subject of this
                 paper is to compare three different interpolation
                 techniques: linear, cubic spline, and Fourier
                 interpolation for generating slices. Linear
                 interpolation is shown to be the best of the three
                 interpolation techniques. This research also shows that
                 without the image segmentation or the matching process
                 poor intermediate images wiil be generated.",
  editor =       "V. Skala",
  keywords =     "Computed Tomography, interpolation, 3D biomedical
                 model",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-534,
  year =         "1998",
  title =        "Structural Description of Digitized Straight Line
                 Segments",
  author =       "Jacek Lebied",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-534",
  abstract =     "This paper presents a structural description of
                 digitized straight line segments and shows a scan
                 converting algorithm based on this description. At the
                 end, the obtained structural algorithm is compared with
                 Bresenham's algorithm. The former seems to be faster
                 than the latter.",
  editor =       "V. Skala",
  keywords =     "Structural description, discrete line segments, scan
                 conversion, raster graphics",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-535,
  year =         "1998",
  title =        "{PICS}earch - {A} Platform for Image Content-based
                 Searching Algorithms",
  author =       "Kjell Lemstr{\"o}m and Jouni Korte and Pyry Kuusi Pasi
                 Kyher{\"o}inen Pekka P{\"a}ivi{\"a}kumpuand",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-535",
  abstract =     "An environment for content-based pictorial retrieval
                 algorithms, called PICSearch, is introduced. In this
                 context, the retrieval is based on color distribution,
                 texture or edges of a query image, or a sketch; i.e.
                 the boundary information of a shape. PICSearch is
                 designed to serve as a platform for any kind of
                 pictorial matching algorithm. The system is designed
                 for researchers developing such algorithms. PICSearch
                 provides an easy-to-use graphical interface and a
                 platform, where the researcher can easily embed his
                 algorithm without the need to create a whole system
                 from scratch. PICSearch is very independent on the
                 underlying operating system and window manager.
                 PICSearch is released to public use (under the GNU
                 General Public License) and, to our knowledge, it is
                 the first open platform to image retrieval systems
                 freely available.",
  editor =       "V. Skala",
  keywords =     "Content-based image retrieval, visual information
                 management, open platforms, image databases",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-536,
  year =         "1998",
  title =        "Multimedia Authoring Systems: {A} Proposal for a
                 Reference Model",
  author =       "Mario Lorenz and Rene' Schmalfuss",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-536",
  abstract =     "This paper describes a proposal for a reference model
                 for multimedia authoring systems. In this context a
                 multimedia authoring system is a combination of hard-
                 and software for making, presenting/running and
                 revising the entire multimedia document and its
                 informations. The reference model framework consists of
                 the user component, the knowledge component, the design
                 support component and the system component. The
                 framework specification was done top down in a modular
                 way and it includes support for various design
                 philosophies, design workgroups, networking and
                 document lifetime cycle models.",
  editor =       "V. Skala",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-537,
  year =         "1998",
  title =        "Fast Penumbra Calculation in Ray Tracing",
  author =       "Arno Formela and Andrzej Lukaszewski",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-537",
  abstract =     "Penumbras, or soft shadows, are an important means to
                 enhance the realistic appearance of computer generated
                 images. We present a fast method based on Minkowski
                 operators to reduce the run time for penumbra
                 calculations with stochastic ray tracing. Detailed run
                 time analysis on some examples shows that the new
                 method is significantly faster than the conventional
                 approach. Moreover, it adapts to the environment so
                 that small penumbras are calculated faster than larger
                 ones. The algorithm needs at most twice as much memory
                 as the underlying ray tracing algorithm.",
  editor =       "V. Skala",
  keywords =     "Shadow calculation, stochastic ray tracing, bounding
                 volumes, Minkowski operators, offsets",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-538,
  year =         "1998",
  title =        "Software Tools for Content Based Hypervideo",
  author =       "Joao Martins and Nuno Correia and Nuno Guimaraes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-538",
  abstract =     "In most hypermedia systems, video and audio data are
                 condidered still as navigationally opaque data, without
                 an accessible internal structure, and where no
                 hyperlinking is performed. This article describes an
                 object model, realized as a library of C++ classes, for
                 the development of annotation and navigation
                 applications in video spaces. The object model
                 considers not only aspects related to the structuring
                 of these materials, but ways in which they can be
                 integrated with others (images, text, etc), enriched
                 with automatically extracted information, as well as
                 presentation and interaction aspects.",
  editor =       "V. Skala",
  keywords =     "Video hyperlinking, video navigation, movie-only
                 spaces, automatic content extraction",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-539,
  year =         "1998",
  title =        "A Nearly Output Sensitive Parallel Hidden Surface
                 Removal Algorithm in Object Space",
  author =       "Klaus Meyer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-539",
  abstract =     "We present a new method for solving the hidden surface
                 removal (HSR) problem in parallel on a CREW PRAM using
                 a combination of new observations and known methods for
                 solving related geometric problems. The new algorithm
                 obtains the bounds of O(log 2 n) time and O(n log n +
                 I) processors, where n is the amount of given endpoints
                 of the input and I is the number of intersections in
                 the viewing plane. In contrast to most known algorithms
                 solving the HSR problem in object space, this method is
                 able to process input scenes where penetrations of line
                 segments and polygonal areas are allowed. The ability
                 to process such an input has an enormous advantage for
                 integrating three dimensional curves in IR3, which have
                 been approximated by segments. Since a penetration of
                 two polygonal areas will be detected during run time,
                 this algorithm could also be used for testing given
                 scenes for intersections of polygonal areas. In this
                 case the algorithm is able to solve the hidden line
                 removal problem in the same bounds. For practical
                 purposes this algorithm allows also non convex (but
                 simple and planar) polygons as input. You do not have
                 to have triangulated scenes. According to the
                 constraints of object space HSR algorithms, we will
                 construct the visibility graph of the given scene as a
                 planar graph in the viewing plane. Although the number
                 of processors depends on the number of intersections in
                 the viewing plane, in most given scenes this method
                 will work like an output sensitive algorithm. Typical
                 examples like {"}one big rectangle is covering all
                 intersections{"} will be detected and these
                 intersections are not computed.",
  editor =       "V. Skala",
  keywords =     "Parallel algorithm, parallel computational geometry,
                 parallel and distributed graphics, computer graphics,
                 hidden surface removal, hidden line removal",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InCollection{EVL-1998-54,
  pages =        "117--124",
  year =         "1998",
  title =        "Discrete Rotational {CMC} Surfaces and the Elliptic
                 Billiard",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-54",
  author =       "Tim Hoffmann",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InProceedings{EVL-1998-540,
  year =         "1998",
  title =        "Visualizing Changes in a Dynamic Voronoi Data
                 Structure via Time Travel",
  author =       "Darka Mioc and Francois Anton and Christopher Gold",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-540",
  abstract =     "In recent years there has been rapidly growing
                 interest by the GIS community in new visualization
                 methods for cartographic data. The visualization of map
                 changes is important for several reasons:
                 spatio-temporal analysis, process modelling, and
                 animated maps. It is now widely recognized that current
                 GIS software has no ability to maintain incremental
                 change of spatio-temporal data, and therefore
                 visualization of such data is limited to series of
                 `snapshots' of cartographic data (see [Peque94a]). The
                 growing amount of research on spatio-temporal databases
                 shows that today's world of spatial data handling
                 requires a dynamic and interactive environment for map
                 visualization. In this paper we will present a
                 conceptual approach for representing cartographic data
                 changing in time and space. The approach emphasizes
                 several research efforts: on the Voronoi spatial data
                 structure, the reversibility of its map construction
                 commands, and their applicability to map visualization
                 and map animation.",
  editor =       "V. Skala",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-541,
  year =         "1998",
  title =        "An Automatic Classification of Materials Degradation",
  author =       "L. Moltedo and D. Vitulano",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-541",
  abstract =     "A novel model to achieve a mathematical formalism for
                 an automatic classification of materials degradation
                 kinds is proposed. In the image analysis phase, wavelet
                 transform local maxima are computed in order to detect
                 the degradation shapes edges. In the second phase, at
                 first, the shapes are encoded by the chain code and,
                 then, a grammar is found to describe degradation kinds.
                 Finally, for simulating further degradation processes,
                 we are able to synthesize shapes boundaries examples,
                 by means of both the found grammar and some additional
                 rules.",
  editor =       "V. Skala",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-542,
  year =         "1998",
  title =        "Managing Complexity and Feature Placement in Planetary
                 Terrain Synthesis",
  author =       "Alan Murta and James Miller and Simon Embley",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-542",
  abstract =     "This paper presents a case study in the generation of
                 a planetary terrain model. Several novel techniques
                 were developed as part of the work. A new
                 view-dependent surface tiling method for representing
                 spherical bodies is first described. Secondly, a simple
                 method which uses image-driven modulation of fractal
                 functions to support the integration of synthetic
                 surface detail with planned topographical features is
                 presented. Finally a deterministic method for the
                 incorporation of randomly placed discrete structures is
                 outlined. A model of the lunar surface is presented as
                 an illustration of these techniques.",
  editor =       "V. Skala",
  keywords =     "Picture/Image Generation, Object Modeling,
                 Three-Dimensional Graphics and Realism",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-543,
  year =         "1998",
  title =        "Kinematic Chain Substitution for Geometric Constraint
                 Solving",
  author =       "G{\"u}l Ogan and Felix J. Metzger and Bernhard J.
                 Seybold and Max Engeli",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-543",
  abstract =     "Assembly modelling is an important task within the
                 design process for mechanical engineering.
                 Constraint-based modelling is an adequate approach to
                 this task. Kinematic mechanisms often have multiple
                 interacting loops which are difficult to solve by a
                 geometric constraint solver. In this paper, we
                 introduce kinematic chain substitution, which is an
                 elegant method to solve loops. Kinematic chains are
                 open paths of simply connected components in which --
                 unlike rigid chains -- relative mobility between their
                 components is possible. For kinematic chain
                 substitution, the chain between the first and last
                 component is substituted temporarily with one arc, by
                 eliminating the other components in between. Our
                 algorithm, called Concatenate, calculates the relative
                 mobility restriction between the first and last
                 component of a kinematic chain with 3 components. By
                 repeatedly applying Concatenate, a kinematic chain
                 substitution for chains with any number of components
                 is possible. The main application of this approach is
                 to solve loops. In addition, Concatenate can be used to
                 visualise the relative mobility restriction between any
                 two components, which is induced by a particular chain.
                 Our approach is an advantageous alternative with
                 respect to locus analysis presented by KRAMER, because
                 it is more flexible and has a more general application
                 scope.",
  editor =       "V. Skala",
  keywords =     "Geometric constraint solving, loops, kinematic
                 chains",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-544,
  year =         "1998",
  title =        "Interactive Modifying the Method Set of a Geometric
                 Constraint",
  author =       "David Podgorelec and Borut Zalik and Simon Kolmanic",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-544",
  abstract =     "In the paper, a way of making geometric constraints
                 more flexible is described. This approach is employed
                 in our new interactive 2D constraint-based drawing
                 system. Multidirectional constraints should be very
                 powerful to find exactly the results expected by a user
                 when more than one solution is possible. By defining
                 constraint method priorities, we can satisfy some
                 designer's intents but not all of them. So we leave a
                 possibility to reorder method lists of particular
                 constraints to the user. Two new terms: parallel and
                 alternative methods are also introduced. Finally, we
                 present some basic characteristics of two our geometric
                 constraint solving systems.",
  editor =       "V. Skala",
  keywords =     "Geometric constraints, geometry, geometric modelling,
                 CAD, constraint solving, graphs",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-545,
  year =         "1998",
  title =        "Constrained Deformation for Geometric Modeling and
                 Object Reconstruction",
  author =       "Roamin Raffin and Marc Neveu and Brahim Derdouri",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-545",
  abstract =     "Deformations are a powerful modeling tool. The model
                 we present here is able to produce interactive control
                 on the constraint points that rule the deformation of
                 any object. Given a desired displacement we can define
                 the local shape of the deformed object while satisfying
                 the constraints. This model deforms the whole space,
                 the image of one point is a blend of deformations
                 fonctions with a projection matrix computed to satisfy
                 the constraints. Deformation extent is simply
                 controlled by an influence hull of the constraint.
                 Self-influences of constraints are automatically ruled.
                 Deformations functions can be edited, modified to
                 create deformation profiles.",
  editor =       "V. Skala",
  keywords =     "Solid geometry modeling, deformations, space
                 deformation, free form deformation",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-546,
  year =         "1998",
  title =        "Growth in Complex Exponential Dynamics",
  author =       "M. Romera and G. Pastor and G. Alvarez and F.
                 Montoya",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-546",
  abstract =     "Both the computer drawing of the complement of the
                 Mandelbrot-like set of a one-parameter dependent
                 complex exponential family of maps and the computer
                 drawing of the Julia sets of the maps of this family,
                 grow with the maximal number of iterations we choose.
                 Some graphic examples of this growth, which evoke the
                 image of a garden, are shown here.",
  editor =       "V. Skala",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-547,
  year =         "1998",
  title =        "Real-Time Generation of Continuous Levels of Detail
                 for Height Fields",
  author =       "Stefan R{\"o}ttger and Wolfgang Heidrich and Philipp
                 Slusallek and Hans-Peter Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-547",
  abstract =     "Height fields play an important role in the fast
                 growing domain of Geographic Information Systems (GIS).
                 For exploring different kinds of geographic-based data
                 sets on screen it is necessary to display height fields
                 at interactive frame rates. Because of the inherent
                 geometric complexity, this goal is often unachievable
                 even with new generations of powerful graphics
                 computers, unless the original height field data is
                 approximated in order to reduce the number of geometric
                 primitives that need to be rendered without
                 compromising visual quality. So far most algorithms
                 have focused on global reduction or multi-resolution
                 techniques, which reduce resolution on the basis of
                 surface roughness. A recent new approach called
                 Continuous Levels of Detail [LKR+96] introduced a
                 hierarchical quadtree technique. In order to reduce the
                 projected pixel error, the height field is dynamically
                 triangulated in a bottom up fashion according to the
                 distance to the point of view. Since resolution is
                 allowed to change smoothly, the result is a much better
                 image quality. However, this algorithm still has a
                 major disadvantage. With the viewpoint moving, the
                 triangulation is continuously changing, resulting in a
                 phenomenon called vertex popping. As the observer
                 approaches an area with detail information, this detail
                 will suddenly appear at a certain distance. To
                 eliminate these artifacts we introduce a new, rapid
                 geomorphing algorithm, which operates top down on a
                 quadtree data structure.",
  editor =       "V. Skala",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-548,
  year =         "1998",
  title =        "An efficient Algorithm for Ray Casting of {CSG}
                 Animation Frames",
  author =       "Andrea Sanna and Paolo Montuschi",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-548",
  abstract =     "This paper presents a new algorithm to generate ray
                 casted CSG animation frames. We consider sequences of
                 frames where only the objects can move, in this way, we
                 take advantage of the high screen area coherence of
                 this kind of animation. A new definition of bounding
                 box allows us to reduce the number of pixels to be
                 computed for the frames after the first. We associate
                 with each box a CSG sub-tree encapsulated and two flags
                 denoting if the box has changed in the current frame
                 and if it will change in the next frame, respectively.
                 We show with two examples the advantages of our
                 technique when compared with an algorithm which
                 entirely renders each frame of an animation. For the
                 test sequences the intersections with CSG objects may
                 be reduced about up to one fifth, while the rendering
                 may be computed up to four times faster.",
  editor =       "V. Skala",
  keywords =     "Animation rendering, bounding box computation,
                 constructive solid geometry",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-549,
  year =         "1998",
  title =        "Gathering Multi-path: {A} New Monte Carlo Algorithm
                 for Radiosity",
  author =       "Mateu Sbert and Roel Martinez and Xavier Puey",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-549",
  abstract =     "In this paper we present a new Monte Carlo technique
                 for radiosity. This technique is shown to be the dual
                 of the previous multi-path technique, in the same way
                 as a gathering random walk technique is dual to a
                 shooting one. We also show how the same global density
                 of lines can be used to trace paths and thus to obtain
                 estimators of radiosity simultaneously in both dual
                 techniques. Although the gathering multi-path technique
                 does not appear to improve the shooting technique, it
                 can be used to compute efficiently the importance
                 matrix.",
  editor =       "V. Skala",
  keywords =     "Rendering, Integral Geometry, Radiosity, Monte Carlo",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InCollection{EVL-1998-55,
  pages =        "125--134",
  year =         "1998",
  title =        "Zonotope Dynamics in Numerical Quality Control",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-55",
  author =       "Wolfgang K{\"u}hn",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InProceedings{EVL-1998-550,
  year =         "1998",
  title =        "A New Look At Mipmap Level Estimation Techniques",
  author =       "Leon Shirman and Yakov Kamen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-550",
  abstract =     "In this work, we study various methods of mipmap level
                 estimation. We show that despite their differences,
                 these methods depend on the interpolated homogeneous
                 coordinate. We introduce a new method based om
                 homogeneous coordinates only that has functionality and
                 efficiency advantages over traditional approaches.",
  editor =       "V. Skala",
  keywords =     "Texture mapping, mipmapping, mipmap level, inverse
                 homogeneous coordinates",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-551,
  year =         "1998",
  title =        "Vector Field Topology with Clifford Algebra",
  author =       "Gerik Scheuermann and Hans Hagen and Heinz
                 Kr{\"u}ger",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-551",
  abstract =     "One way to visualize vector fields was based on their
                 qualitative structure by showing the field topology
                 after linear or bilinear interpolation. This paper
                 extends this to cases of nonlinear behavior in the grid
                 cells. It uses new ideas about the analysis of some
                 polynomial vector fields which have been proved by
                 using Clifford algebra and analysis. They are used as
                 local model for the visualization and allow general
                 positions and arbitrary Poincare indices of the
                 critical points in the interesting regions.",
  editor =       "V. Skala",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-552,
  year =         "1998",
  title =        "Surfaces To Lines: Rendering Rich Line Drawings",
  author =       "Stefan Schlechtweg and Bert Sch{\"o}nw{\"a}lder and
                 Lars Schumann and Thomas Strothotte",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-552",
  abstract =     "Rendering algorithms have tended to restrict
                 themselves to represent the effect of light sources on
                 scenes as could be observed by the human eye. For
                 certain applications, like teaching surgery and
                 anatomy, somewhat more schematic renditions are called
                 for. Such graphics tend to be line-oriented and encode
                 other information than just the effect of light. In the
                 lack of appropriate computer-based tools, such images
                 are practically always drawn by hand by a scientific
                 illustrator. In this paper, we study techniques for
                 rendering what we call rich line drawings. We develop
                 tools for selectively mapping attributes of surfaces of
                 an object onto lines which depict it. This enables us
                 to render images which encode only those properties
                 which are needed for the application at hand.",
  editor =       "V. Skala",
  keywords =     "Rendering Pipeline, Non-Photorealistic Rendering, Line
                 Drawings, Shading",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-553,
  year =         "1998",
  title =        "An Interactive Visualization and Navigation Tool for
                 Medical Volume Data",
  author =       "O. Sommer and A. Dietz and R. Westermann and T. Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-553",
  abstract =     "Interactive direct volume rendering by hardware
                 assisted 3D texture mapping has become a powerful
                 visualization method in many different fields. However,
                 to make this technique fully practicable convenient
                 visualization options and data analysis tools have to
                 be integrated. For example, direct rendering of
                 semi-transparent volume objects with integrated display
                 of lighted iso-surfaces is one important challenge
                 especially in medical applications. Furthermore,
                 explicit use of multi-dimensional image processing
                 operations often helps to optimize the exploration of
                 the available datasets. On the other hand, only if
                 interactive frame rates can be guaranteed, such
                 visualization tools will be accepted in medical planing
                 and surgery simulation systems. In this paper we
                 propose a volume visualization tool for large scale
                 medical volume data which takes advantage of hardware
                 assisted 3D texture interpolation and convolution
                 operations. We demonstrate how to use the 3D texture
                 mapping capability of high-end graphics workstations to
                 display arbitrary iso-surfaces which can be directly
                 illuminated to enhance the spatial relations between
                 objects. Back-to-front 3D texture slicing is used to
                 simultaneously display semi-transparent material
                 densities. Using this approach similar image quality
                 can be achieved as with conventional software-based
                 ray-casting techniques, but the rendering process is
                 accelerated to a considerable extent. In order to
                 enable on-the-fly data analysis, first approaches using
                 hardware assisted convolution operations have been
                 integrated. An implementation of the proposed method
                 based on the OpenInventor rendering toolkit is
                 described offering interactive frame rates at high
                 image quality including sophisticated user
                 interactions.",
  editor =       "V. Skala",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-554,
  year =         "1998",
  title =        "Impressionistic Techniques for Rendering Woodland
                 Scenes",
  author =       "Michael Starling and Neil Ashdown",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-554",
  abstract =     "The authors have been working on the simulation of
                 trees and in particular their representation in the
                 landscsape when massed as woodland or forest. The
                 purpose of this work is to provide a representation of
                 trees en masse that is more visually convincing when
                 used in architectural and environmental computer aided
                 design applications - architectural models, landscape
                 design and evaluation, and historical reconstructions.
                 Although our research was directed initially at
                 producing a better compromise between realism and the
                 economical use of polygons, we found that we were
                 developing an impressionistic approach to modeling
                 elements in the landscape which, if applied in other
                 contexts, challenged the conventional approach of
                 creating simulations from a literal translation of the
                 geometry of the subject. The paper explains the process
                 by which the software was developed, describes the
                 interaction by which the user produces the scenes, and
                 ends by processing tools which would allow the user
                 greater determination of those factors which create a
                 more visually convincing simulation.",
  editor =       "V. Skala",
  keywords =     "Impressionistic techniques, Landscapes, Woodland",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-555,
  year =         "1998",
  title =        "Importance Driven Quasi-Random Walk Solution of the
                 Rendering Equation",
  author =       "Laslo Szirmay-Kalos and Balazs Csebfalvi and Werner
                 Purgathofer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-555",
  abstract =     "This paper presents a new method that combines
                 quasi-Monte Carlo quadrature with importance sampling
                 to solve the general rendering equation efficiently.
                 Since classical importance sampling has been proposed
                 for Monte-Carlo integration, first an appropriate
                 formulation is elaborated for deterministic sample sets
                 used in quasi-Monte Carlo methods. This formulation is
                 based on integration by variable transformation. It is
                 also shown that instead of multi-dimensional inversion
                 methods, the variable transformation can be executed
                 iteratively where each step works only with
                 2-dimensional mappings. Since the integrands of the
                 Neumann expansion of the rendering equation is not
                 available explicitely, some approximations are used,
                 that are based on a particle-shooting step. Although
                 the complete method works for the original geometry, in
                 order to store the results of the initial particle
                 shooting, surfaces are decomposed into patches and the
                 patches are interconnected by links.",
  editor =       "V. Skala",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-556,
  year =         "1998",
  title =        "Quasi-Monte Carlo Global Light Tracing with Infinite
                 Number of Rays",
  author =       "Laszlo Szirmay-Kalos and Tibor Foris and Werner
                 Purgathofer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-556",
  abstract =     "The paper presents an efficient method to solve the
                 general rendering equation, using a combined finite
                 element and quasi-random walk approach. Applying finite
                 element techniques, the surfaces are decomposed into
                 planar patches that are assumed to have position
                 independent, but not direction independent (that is
                 non-diffuse) radiance. The direction dependent radiance
                 function is then computed by quasi-random walk. Since
                 quasi-Monte Carlo quadrature is applied here to an
                 integrand of finite variation, this method can take
                 advantage of the superior convergence of quasi-Monte
                 Carlo integration.",
  editor =       "V. Skala",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-557,
  year =         "1998",
  title =        "Linearly Combining Shading Models for Texturing in
                 Global Illumination Algoritms",
  author =       "Robert F. Tobler and Stefan Hynst and Werner
                 Purgathofer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-557",
  abstract =     "We propose a simple method to generate a wide variety
                 of different, texturing shading models by linearly
                 combining simpler shading models. This makes it
                 possible to specify complex shading operations in a
                 very concise and simple form without explicitly
                 programming shaders, and facilitates the use of this
                 generalized shading model in global illumination
                 algorithms. We also show the programming interfaces of
                 this new method for distribution ray tracing, path
                 tracing, and stochastic radiosity, and some results
                 using these algorithms.",
  editor =       "V. Skala",
  keywords =     "Texturing, shading models, global illumination",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-558,
  year =         "1998",
  title =        "Geology Meets Virtual Relity: {VRML} Visualization
                 Server Applications",
  author =       "Ch. Lindenbeck and H. Ulmer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-558",
  abstract =     "The setup of a VRML visualization server is discussed.
                 The so called VRengine is embedded in a World Wide Web
                 server site accessible by the Internet community. Two
                 geological applications show the advantages of the
                 concept for the visualization of extended data sets.
                 Model specific HTML forms and clickable image maps are
                 provided to build up a visualization request. On the
                 server site a VRML scene is generated by a CGI program
                 using the Visualization Toolkit. To render the scene on
                 the client site VRML plugins are available on most
                 platforms. In addition to the visualization server
                 concept, a short overview about the applied methods of
                 geometric modeling is given.",
  editor =       "V. Skala",
  keywords =     "Visualization server, geometric modeling, VRML, HTML,
                 CGI, vtk, Visualization Toolkit, Geo3View, geology,
                 silver mine, sandbox experiment",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-559,
  year =         "1998",
  title =        "Scaffold Design by a Virtual Genetic Algorithm",
  author =       "Yoshiaki Usami and Munetoshi Unuma and Misato Nio and
                 Toshiaki Yoshinaga",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-559",
  abstract =     "We propose a scaffold design technique using a
                 visualized genetic algorithm for construction of power
                 plants. Scaffolds are placed in order that workers can
                 install plant parts such as pipes or other equipment.
                 In our technique, the parts that need scaffolds are
                 extracted from plant CAD data, and the necessity
                 degrees for scaffold placement are calculated. A
                 genetic algorithm that simulates the evolution of
                 creatures is applied to the necessity degrees, and we
                 can obtain the optimum scaffold placement. On the other
                 hand, placement information has to be expressed as gene
                 data of the genetic algorithm. The gene data are stored
                 in computer image memories. By using the image
                 memories, we can visualize the process of the genetic
                 algorithm. In particular, convergence of a calculation
                 is displayed as a change of an image pattern.
                 Therefore, we can understand the behavior of the
                 genetic algorithm intuitively. Because scaffold
                 placement is optimized easily, this technique is useful
                 to shorten operation time of scaffold design.",
  editor =       "V. Skala",
  keywords =     "CAD, visualization, computer graphics, scaffold,
                 design, placement, genetic algorithm, optimization,
                 image",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InCollection{EVL-1998-56,
  pages =        "135--150",
  year =         "1998",
  title =        "Straightest Geodesics on Polyhedral Surfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-56",
  author =       "Konrad Polthier and Markus Schmies",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InProceedings{EVL-1998-560,
  year =         "1998",
  title =        "Cellular Automaton as a Fast Tool for Animation of
                 Liquid in Multi-object Scenes",
  author =       "Rafal Wcislo and Jacek Kitowski and Jacek Moscinski",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-560",
  abstract =     "The paper presents the usage of a cellular automaton
                 (CA) in three-dimensional computer animations. The main
                 goal is to achieve a fast, realistic motion of a set of
                 bodies. In order to fulfil the demand of {"}reality{"}
                 we try to employ the simulation methods based on
                 physical laws. However, in scenes where different kinds
                 of bodies (rigid, elastic and liquid) are animated
                 together it is hard to find one universal paradigm for
                 their simulation. Instead, we propose to use CA for
                 simulating the liquid and the molecular dynamics (MD)
                 method for other bodies. The paper describes the CA
                 rules and its interaction with particles used in the MD
                 method. Finally we present the animation algorithm
                 (composed of CA, MD and visualization) distributed on
                 networked Unix workstations.",
  editor =       "V. Skala",
  keywords =     "Computer animation, cellular automaton, molecular
                 dynamics, parallel algorithms",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-561,
  year =         "1998",
  title =        "Ray tracing of Nonlinear Fractals",
  author =       "Peter Wonka and Michael Gervautz",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-561",
  abstract =     "We present a new kind of parametric
                 Lindenmeyer-systems called nonlinear CSG-pL-Systems,
                 which are useful for the modeling of nonlinear fractals
                 and fractal like objects. Nonlinear CSG-pL-systems
                 describe cyclic CSG-graphs, which can include several
                 nonlinear transformations. To render the given objects
                 a ray tracing algorithm is introduced, that is
                 independent of the transformation and the under laying
                 CSG-primitives. We use tapering, twist and bend as
                 nonlinear transformations for our implementation. The
                 new modeling possibilities and their visualization for
                 abstract fractals and natural phenomena are
                 investigated.",
  editor =       "V. Skala",
  keywords =     "Nonlinear fractals, nonlinear ray tracing,
                 CSG-pL-systems, natural phenomena",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-562,
  year =         "1998",
  title =        "A New Rendering Technique for Water Droplet Using
                 Metaball in the Gravitation Force",
  author =       "Young-Jung Yu and Ho-Youl Jung and Hwan-Gue Cho",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-562",
  abstract =     "Till now there are many rendering models for water and
                 other fluids and their dynamics. Especially in order to
                 generate the curved surface of flexible objects such as
                 water, jelly, and snow, the implicit metaball
                 formulation is widely used in favor of its simplicity
                 and flexibility. This paper proposes one novel method
                 for generating water droplets, which would be deformed
                 in a gravitation field. In previous works, a water
                 droplet was simply represented by approximated curved
                 surfaces of a symmetric and quite a simple metaball. So
                 the finally rendered water droplet was far from a
                 realistic droplet, because they did not consider the
                 gravitation force in droplets attached on a surface. We
                 give a new generalized metaball model for rendering
                 water droplets placed on an arbitrary surface
                 considering the gravitation and friction between
                 droplets and the plate. Our new metaball model uses a
                 new vector field isosurface function controlling the
                 basic scalar metaball, with respect to the norm of
                 gravitation force. In several experiments, we could
                 render a photo-realistic water droplets with
                 natural-looking shadows by applying ray-tracing
                 techniques.",
  editor =       "V. Skala",
  keywords =     "Metaball, droplet model, curved surface, rendering",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-563,
  year =         "1998",
  title =        "A Deformation Model of Thin Flexible Surfaces",
  author =       "Biao Wang and Zhuang Wu and Qingping Sun and Matthew
                 M. F. Yuen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-563",
  abstract =     "In engineering it is often needed to determine the
                 deformed shape of thin flexible surfaces, such as
                 fabrics, paper, etc., under the action of applied
                 loads. In this paper, a variational formulation was
                 developed for such cases. It serves as the foundation
                 for developing the numerical solution scheme. Through
                 the variational formulation, the governing differential
                 equation of motion was derived. The widely used
                 Terzopoulos model is shown to be an approximated
                 solution. The constitutive relationship for thin
                 flexible surfaces was established. Finally the draping
                 effect of a skirt was simulated based on this model.
                 The formulation is suitable for most large deformation
                 problems and is extendible to cover the non-linear
                 materials.",
  editor =       "V. Skala",
  keywords =     "Variational formulation, flexible surface,
                 differential equation of motion, finite difference
                 solution, deformation model",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-564,
  year =         "1998",
  title =        "A Vector Model for Global Illumination in Ray
                 Tracing",
  author =       "Jacques Zanietti and Bernard Peroche",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-564",
  abstract =     "This paper presents a method taking global
                 illumination in ray tracing into account. A vector
                 approach is introduced which allows to use any type of
                 material, in particular with directional properties.
                 This vector is decomposed into a direct component
                 associated to light sources and an indirect one which
                 corresponds to light having been reflected at least
                 once. These components are estimated at a small number
                 of points within the scene. A weighted interpolation
                 between known values allows to reconstruct these
                 components for the other points, with the help of a
                 gradient computation for the indirect component.
                 Computed images are thus more accurate, at most no
                 additional cost, and no discretizing of the geometry of
                 the scene is needed.",
  editor =       "V. Skala",
  keywords =     "Rendering Algorithms, Ray Tracing, Shading, Global
                 Illumination",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-565,
  year =         "1998",
  title =        "Navigation of Intelligent Characters in Complex 3{D}
                 Synthetic Environments in Real-time Applications",
  author =       "S. Zhukov and A. Jones and G. Kronin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-565",
  abstract =     "The problem of intelligent agent navigation in unknown
                 and dynamic synthetic environments is a subject of a
                 number of recent AI researches. Even though many
                 solutions have been proposed to address it,
                 ever-growing complexity of virtual environments and
                 development of sophisticated characters make it
                 necessary to further elaborate computational models
                 used to control character behavior making it more
                 robust and believable. In this paper we describe our
                 technique for navigation of intelligent characters with
                 significantly different locomotion capabilities in
                 complex virtual environments. The approach is based on
                 the use of so-called accessibility graphs. We depict a
                 general scheme applicable to control significantly
                 different characters in a similar fashion even in the
                 most challenging circumstances. Then, the description
                 of different behavior pattern creation is given within
                 the framework of the model proposed. The technology
                 outlined in the paper is perfectly suited for the use
                 in modern 3D video games.",
  editor =       "V. Skala",
  keywords =     "AI applications, Navigation in 3D, Real-time
                 navigation, Accessibility graphs",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-566,
  year =         "1998",
  title =        "Using Light Maps to Create Realistic Lighting in
                 Real-time Applications",
  author =       "S. Zhukov and A. Iones and G. Kronin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-566",
  abstract =     "One of the well-known methods of adding realism to
                 static scene lighting in real-time applications is
                 based on the idea of the use of pre-calculated lighting
                 data. Usually high-quality scene lighting can be
                 calculated with some global illumination methods such
                 as radiosity or ray tracing. For real-time application,
                 lighting data can be stored in textures called light
                 maps. Then these textures are used to modulate the
                 original textures of the scene to produce nice looking
                 lighting with fine details (highlights and shadows).
                 Light map textures are usually calculated at much lower
                 resolution than the ordinary textures to save up the
                 required memory storage. In this paper we extend our
                 previous work and further develop the techniques of
                 light mapping for real-time applications.",
  editor =       "V. Skala",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-567,
  year =         "1998",
  title =        "3{D} Object Reconstruction from Aerial Stereo Images",
  author =       "S. Zlatanova and J. Paintsil and K. Tempfli",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-567",
  abstract =     "Among the variety of problems in providing 3D
                 information about topographic objects, efficient data
                 collection and model construction are issues for
                 research. The efforts are directed towards improving
                 the tedious, time and man-power consuming process of
                 data generation by applying automation. In this paper,
                 we present a semi-automatic method for acquiring 3D
                 topologically structured data from aerial stereo
                 images. The process involves the manual digitising of a
                 minimum number of points necessary for automatically
                 reconstructing the objects of interest. Validation of
                 each reconstructed object is done by superimposition of
                 its wire frame graphics in the stereo model. The 3D
                 topologically structured data are stored in a database
                 and also used for visualisation of the objects.",
  editor =       "V. Skala",
  keywords =     "Object reconstruction, 3D modelling, data structuring,
                 feature extraction, DTM, virtual reality",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-568,
  year =         "1998",
  title =        "Parallel Construction and Isosurface Extraction of
                 Recursive Tree Structures",
  author =       "Dirk Bartz and Wolfgang Strasser and Roberto Grosso
                 and Thomas Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-568",
  abstract =     "The visualization of volumetric datasets is usually
                 limited by the amount of memory and processing power of
                 computer systems. Several multiresolution methods have
                 been developed in order to adapt the necessary work;
                 recursive spatial tree structures, such as octrees, are
                 among the most popular. The exploration of a dataset
                 frequently requires a change of parameters, such as
                 color table entries or isovalues. Therefore, the costly
                 update of an octree becomes necessary. To overcome this
                 drawback, we propose the parallel construction of
                 octrees to improve their suitability for interactive
                 volume visualization. Based on the thread model of the
                 shared-memory paradigm, we developed a scheme for a
                 balanced parallel construction. We apply this new
                 scheme to generate isosurfaces in parallel, using the
                 Marching Cubes algorithm.",
  editor =       "V. Skala",
  keywords =     "Volume visualization, octrees, hierarchical data
                 structures, thread model, shared-memory paradigm",
  booktitle =    "WSCG'98 Conference Proceedings of Recursive Tree
                 Structures",
}

@InProceedings{EVL-1998-569,
  year =         "1998",
  title =        "Animation of Dust Behaviors in a Networked Virtual
                 Environment",
  author =       "Jim X. Chen and Edward J. Wegman and Jingfang Wanf",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-569",
  abstract =     "Real-time simulation of physically realistic complex
                 dust behaviors is very useful in computer simulation,
                 training, education, art, advertising, and
                 entertainment. There is no successful model for
                 realistic dust behaviors generated by a traveling
                 vehicle. In this paper, we use particle systems and
                 behavioral simulation techniques to simulate these dust
                 behaviors in real-time. First we analyze the forces and
                 factors which affect the dust generation and the
                 behaviors after dust particles are generated. Then we
                 construct physically-based empirical models to generate
                 dust particles and control the behaviors accordingly.
                 After that, we further simplify the numerical
                 calculations by dividing the dust behaviors into three
                 stages, and establishing simplified particle system
                 models for each stage. In addition, we discuss the
                 methods to simulate the behaviors in a networked
                 virtual environment. Our major contribution includes
                 analyzing dust behaviors in detail, constructing
                 physically-based empirical models that correlate the
                 behaviors to the dust generating forces and other
                 factors, and that achieve simulations in a networked
                 virtual environment.",
  editor =       "V. Skala",
  keywords =     "Particle Systems, Dust, Real-time Simulation,
                 Physically Inspired Modeling, DIS",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InCollection{EVL-1998-57,
  pages =        "153--166",
  year =         "1998",
  title =        "Support of Explicit Time and Event Flows in the
                 Object-Oriented Visualization Toolkit {MAM}/{VRS}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-57",
  author =       "J{\"u}rgen D{\"o}llner and Klaus Hinrichs",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InProceedings{EVL-1998-570,
  year =         "1998",
  title =        "Efficient Volume Rendering by IsoRegion Leaping
                 Acceleration",
  author =       "Ping-Fu Fung and Pheng-Ann Heng",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-570",
  abstract =     "This paper introduces a new acceleration technique for
                 ray casting type volume rendering called IsoRegion
                 Leaping. A factorization of the sample composition
                 equation leads to our new algorithm which is based on
                 homogeneity inside a volume. We create an IsoRegion
                 data structure to help identifying homogeneous voxel
                 cubes, which is independent of the viewing parameters
                 and shading conditions, thus can be pre-processed.
                 Accumulated colors and transparencies of homogeneous
                 ray segments are pre-computed into a look-up table
                 according to different voxel values and ray segment
                 lengths. Attributes of ray segments piercing through
                 IsoRegion blocks are looked-up from the table.
                 Therefore, unnecessary sample compositions within
                 IsoRegions are saved. This voxel leaping technique is
                 experimentally proved to be efficient even for
                 real-life medical volume data. A speedup of 2 to 3
                 times is measured while preserving the image quality.
                 Images generated by our algorithm and the ones from the
                 original algorithm without IsoRegion Leaping
                 acceleration are identical, both theoretically and
                 experimentally.",
  editor =       "V. Skala",
  keywords =     "Volume visualization, ray casting, IsoRegion",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-571,
  year =         "1998",
  title =        "Fundamental Algorithms for Projective Voxelization",
  author =       "Reginald C. Jegathese and Eustace Painkras and Edmond
                 C. Prakash",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-571",
  abstract =     "We develop a voxel-based approach to volume modeling,
                 in which the 3D object is represented as a set of
                 voxels rather than a collection of surfaces.
                 Interpretation with voxel-based representations is an
                 intuitive paradigm , which has been shown to be
                 theoretically sound and possesses enormous
                 computational advantages over modeling with
                 surface-based representations of 3D objects. However,
                 there is no efficient algorithm for volume modeling of
                 graphics primitives. In this paper, we attempt to
                 bridge the gap by an efficient projective voxelization
                 technique suitable for all existing 3D graphics
                 primitives.",
  editor =       "V. Skala",
  keywords =     "Volume Graphics, Voxelization, CAD/CAM",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-572,
  year =         "1998",
  title =        "Selection of the Number of Control Points for Spline
                 Surface Approximation",
  author =       "Nirant V. Puntambekar and Andrei G. Jablokow",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-572",
  abstract =     "Parametric spline surfaces represent an important
                 surface type in the process of reverse engineering.
                 Various surface fitting techniques are available for
                 fitting of splines to sets of data points, here we
                 consider a least squares fitting technique. A user
                 input to this algorithm is the number of control points
                 to be used for the fitting in the two parametric
                 directions. Using a larger number of points, gives a
                 more accurate fitted surface but also results in a
                 surface with a number of undulations or surface wiggles
                 which may not be desired. This paper is concerned with
                 the optimal selection of the number of control points
                 to be used for the least squares surface fitting of B
                 spline surface patches. An optimal method based on
                 multicriteria optimization is presented to decide on
                 the number of control points to be used in the
                 fitting.",
  editor =       "V. Skala",
  keywords =     "Geometric modeling, Reverse engineering, Surface
                 fitting, Splines",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-573,
  year =         "1998",
  title =        "A Unified Framework for Collision Detection,
                 Avoidance, and Response",
  author =       "Kevin L. Steele and Parris K. Egbert",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-573",
  abstract =     "Collision detection and collision avoidance algorithms
                 are necessary for accurate and realistic animation, but
                 are presently implemented separately and independently.
                 This is a disadvantage when designing some simulations
                 or animations which would otherwise benefit from the
                 availability of both algorithms during run-time. We
                 propose a unified algorithm incorporating both
                 collision detection and collision avoidance
                 simultaneously during an animation. Our algorithm
                 benefits animations whose moving objects generally
                 require collision-free movement, but under certain
                 circumstances may collide with other objects within the
                 scene. The algorithm utilizes vector fields as its
                 basis, and we present a supporting algebra that
                 facilitates the design of a simulation's behavioral
                 interaction. Two sample simulations are presented, and
                 their implementation and performance is discussed.",
  editor =       "V. Skala",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-574,
  year =         "1998",
  title =        "A Visualization System for Operational Meteorological
                 Use",
  author =       "Miriam Lux and Thoams Fr{\"u}hauf",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-574",
  abstract =     "RASSIN is a system for professional meteorological
                 visualization that has been developed by the
                 Fraunhofer-IGD together with the German National
                 Weather Service DWD. The software is being used in
                 DWD's research department. Its scope is the
                 visualization of numerical simulation data and measured
                 data. This results in a very precise and highly
                 interactive weather forecasting. Furthermore, the
                 system serves the meteorologists in the evaluation of
                 new forecast methods. RASSIN follows a consistent
                 strategy to perform calculation and visualization only
                 on the original, irregular, dynamic grid of the
                 meteorological data. The integration of a new concept
                 for time-critical visualization allows scientific
                 animations. The realized HCI-concept is tailored for
                 users from meteorological sciences.",
  editor =       "V. Skala",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-575,
  year =         "1998",
  title =        "Extending the {VGRAPH} Algorithm for Robot Path
                 Planning",
  author =       "Alade Tokuta",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-575",
  abstract =     "This paper presents an O(n log m) method to
                 incorporate start and goal points of a robot into the
                 roadmap of a two-dimensional workspace to form a
                 VGRAPH. A VGRAPH Point Incorporation Algorithm (VPIA)
                 incorporates a point in free-space into a roadmap. This
                 VPIA divides the free space around an obstacle vertex
                 into an ordered set of areas. A search is used to
                 determine the containment of the point. Containment
                 implies visibility of the point from the vertex. A
                 point is incorporated by determining its visibility
                 from all obstacle vertices. The VPIA is inherently
                 parallel and the implementation can reduce this
                 complexity from O(n log m) to O(log m). Most existing
                 techniques for incorporating points into a roadmap
                 perform in O(n^2). The roadmap's data structure is
                 modified to support the VPIA. The VPIA, employing the
                 VGRAPH approach should enhance the worst case runtime
                 of find-path algorithms closer to real-time for 2-D
                 workspaces cluttered with obstacles. Sample VGRAPH
                 diagram generated by an implementation of the VPIA is
                 shown in Appendix A.",
  editor =       "V. Skala",
  keywords =     "Visibility graph, path planning, VGRAPH 1.0",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-576,
  year =         "1998",
  title =        "Establishing a 3{D} Human Gait and Knee Model",
  author =       "Ying Zhu and Jim X. Chen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-576",
  abstract =     "This paper ppresemts a system interactively simulating
                 the human gait cycle and knee joint. The goal of the
                 system is to help understanding and analyzing the knee
                 behaviors during gait cycle. We establish a model of
                 the lower half of the human body. The model can
                 simulate human gait with quite high realism. Moreover,
                 users can manipulate the model to demonstrate various
                 normal or abnormal gait behavior. A 3D knee joint model
                 is reconstructed fromm knee intersection images. This
                 model gives highly realistic and more detailed
                 simulation of the behavior of human femur and tibia
                 during gait cycle. The model is physically-based as
                 well as biomechanically-based.",
  editor =       "V. Skala",
  keywords =     "Computer graphics, simulation, human model, gait, knee
                 joint",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-577,
  year =         "1998",
  title =        "Constrained Surface Modeling and Deformation",
  author =       "Fr{\'{e}}d{\'{e}}ric Jaar and Brahim Derdouri and Marc
                 Neveu",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-577",
  abstract =     "Soft surface modeling allows creation or
                 reconstruction of various objects. We propose a
                 deformable surface model, relying upon operations on
                 local and global surface properties. This model
                 includes an initial shape, various constraints on the
                 surface and external operators. Besides, surface
                 hierarchy confers local properties that allows the easy
                 satisfaction of various constraints. The originality of
                 our work includes the addition of patterns in surface
                 representation, the use of an alphabet to represent the
                 resulting hierarchical surface and new 3D development
                 tools such as the {"}profiler{"}.",
  editor =       "V. Skala",
  keywords =     "Surface modeling, deformation, minimal energy under
                 constraints, B-Splines, refinement.",
  booktitle =    "WSCG'98 Conference Proceedings",
}

@InProceedings{EVL-1998-578,
  pages =        "391--396",
  year =         "1998",
  title =        "Interpolation of triangle hierarchies",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-578",
  author =       "Axel Friedrich and Konrad Polthier and Markus
                 Schmies",
  abstract =     "We consider interpolation between keyframe
                 hierarchies. We impose a set of weak constraints that
                 allows smooth interpolation between two keyframe
                 hierarchies in an animation or, more generally, allows
                 the interpolation in an n-parameter family of
                 hierarchies. We use hierarchical triangulations
                 obtained by the Rivara element bisection algorithm (M.
                 Rivara, 1984) and impose a weak compatibility
                 constraint on the set of root elements of all keyframe
                 hierarchies. We show that the introduced constraints
                 are rather weak. The strength of our approach is that
                 the interpolation works in the class of conforming
                 triangulations and simplifies the task of finding the
                 intermediate hierarchy, which is the union of the two
                 (or more) keyframe hierarchies involved in the
                 interpolation process. This allows for an efficient
                 generation of the intermediate connectivity and
                 additionally ensures that the intermediate hierarchy is
                 again a conforming hierarchy satisfying the same
                 constraints.",
  organization = "IEEE Computer Society",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InProceedings{EVL-1998-579,
  pages =        "463--466",
  year =         "1998",
  title =        "Battlefield visualization on the responsive
                 workbench",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-579",
  author =       "Jim Durbin and J. Edward Swan II and Brad Colbert and
                 John Crowe and Rob King and Tony King and Christopher
                 Scannell and Zachary Wartell and Terry Welsh",
  abstract =     "In this paper we describe a battlefield visualization
                 system, called Dragon, which we have implemented on a
                 virtual reality responsive workbench. The Dragon system
                 has been successfully deployed as part of two large
                 military exercises: the Hunter Warrior advanced
                 warfighting experiment, in March 1997, and the Joint
                 Counter Mine advanced concept tactical demonstration,
                 in August and September 1997. We describe battlefield
                 visualization, the Dragon system, and the workbench,
                 and we describe our experiences as part of these two
                 real-world deployments, with an emphasis on lessons
                 learned and needed future work.",
  organization = "IEEE Computer Society",
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "IEEE Visualization '98",
}

@InCollection{EVL-1998-58,
  pages =        "167--179",
  year =         "1998",
  title =        "A Survey of Parallel Coordinates",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-58",
  author =       "Alfred Inselberg",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InCollection{EVL-1998-59,
  pages =        "195--206",
  year =         "1998",
  title =        "Two-Dimensional Image Rotation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-59",
  author =       "Ivan Sterling and Thomas Sterling",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@TechReport{EVL-1998-6,
  year =         "1998",
  title =        "Interleaved Dimension Decomposition",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-6",
  author =       "Manfred Kopp and Werner Purgathofer",
  abstract =     "Wavelets in 2D or higher dimensions are often
                 generated by a decomposition scheme from 1D wavelets.
                 There are two decomposition schemes called the standard
                 and the nonstandard decomposition which are used in
                 most applications of higher dimensional wavelets. This
                 paper introduces a new decomposition method, the
                 interleaved dimension decomposition and compares its
                 advantages and disadvantages with the other
                 decompositions. Based on the properties of the
                 interleaved dimension decomposition, applications to
                 computer graphics are sketched including
                 multiresolution painting, morphing in 2D and 3D, and
                 image compression.",
  language =     "en",
  month =        jan,
  keywords =     "Wavelets, morphing, multiresolution painting, image
                 compression",
  number =       "TR-186-2-98-03",
  institution =  "Vienna University of Technology, Computer Graphics,
                 Visualisation and Animation Group",
}

@InCollection{EVL-1998-60,
  pages =        "207--220",
  year =         "1998",
  title =        "An Object-Oriented Interactive System for Scientific
                 Simulations: Design and Applications",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-60",
  author =       "Alexandru C. Telea and Cornelius W. A. M. van
                 Overveld",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InCollection{EVL-1998-61,
  pages =        "223--236",
  year =         "1998",
  title =        "Auditory Morse Analysis of Triangulated Manifolds",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-61",
  author =       "Ulrike Axen and Herbert Edelsbrunner",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InCollection{EVL-1998-62,
  pages =        "237--255",
  year =         "1998",
  title =        "Computing Sphere Eversions",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-62",
  author =       "George Francis and John M. Sullivan and Chris
                 Hartman",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InCollection{EVL-1998-63,
  pages =        "269--279",
  year =         "1998",
  title =        "Special Relativity in Virtual Reality",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-63",
  author =       "Ren{\'e} T. Rau and Daniel Weiskopf and Hanns Ruder",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InCollection{EVL-1998-64,
  pages =        "281--291",
  year =         "1998",
  title =        "Exploring Low Dimensional Objects in High Dimensional
                 Spaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-64",
  author =       "Dennis Roseman",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InCollection{EVL-1998-65,
  pages =        "329--341",
  year =         "1998",
  title =        "Accuracy in 3{D} Particle Tracing",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-65",
  author =       "Adriano Lopes and Ken Brodlie",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InCollection{EVL-1998-66,
  pages =        "343--351",
  year =         "1998",
  title =        "Clifford Algebra in Vector Field Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-66",
  author =       "Gerik Scheuermann and Hans Hagen and Heinz
                 Kr{\"u}ger",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InCollection{EVL-1998-67,
  pages =        "353--362",
  year =         "1998",
  title =        "Visualization of Complex {ODE} Solutions",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-67",
  author =       "Laurent Testard",
  language =     "en",
  address =      "Heidelberg",
  note =         "Vismath97",
  editor =       "Hans-Christian Hege and Konrad Polthier",
  booktitle =    "Mathematical Visualization",
  publisher =    "Springer-Verlag",
}

@InProceedings{EVL-1998-68,
  year =         "1998",
  title =        "Texture Mapping for Cel Animation",
  author =       "Wagner Toledo Correa and Robert J. Jensen and Craig E.
                 Thayer and Adam Finkelstein",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-68",
  language =     "en",
  abstract =     "We present a method for applying complex textures to
                 hand-drawn characters in cel animation. The method
                 correlates features in a simple, textured, 3-D model
                 with features on a hand-drawn figure, and then distorts
                 the model to conform to the hand-drawn artwork. The
                 process uses two new algorithms: a silhouette detection
                 scheme and a depth-preserving warp. The silhouette
                 detection algorithm is simple and efficient, and it
                 produces continuous, smooth, visible contours on a 3-D
                 model. The warp distorts the model in only two
                 dimensions to match the artwork from a given camera
                 perspective, yet preserves 3-D effects such as
                 self-occlusion and foreshortening. The entire process
                 allows animators to combine complex textures with
                 hand-drawn artwork, leveraging the strengths of 3-D
                 computer graphics while retaining the expressiveness of
                 traditional hand-drawn cel animation.",
  organization = "ACM SIGGRAPH",
  keywords =     "Cel animation, texture mapping, silhouette detection,
                 warp, metamorphosis, morph, non-photorealistic
                 rendering",
  copyright =    "ACM",
  booktitle =    "SIGGRAPH '98 Conference Proceedings",
}

@TechReport{EVL-1998-7,
  year =         "1998",
  title =        "A Network Architecture for Remote Rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-7",
  author =       "Gerd Hesina and Dieter Schmalstieg",
  abstract =     "We present an approach to the construction of a
                 virtual environment (VE) that makes use of local
                 coherence and pays attention to the issue of
                 distribution of geometric models along with the
                 information regarding the simulation. We outline a
                 network architecture and an application level protocol
                 for these purposes, and report on some experiments and
                 experiences with this implementation.",
  language =     "en",
  month =        jan,
  keywords =     "distributed virtual environments, smooth levels of
                 details, remote rendering",
  number =       "TR-186-2-98-02",
  institution =  "Vienna University of Technology, Computer Graphics,
                 Visualisation and Animation Group",
}

@TechReport{EVL-1998-76,
  year =         "1998",
  title =        "Interactive Segmentation of Medical Images based on
                 Intelligent Scissors",
  type =         "Technical Report",
  author =       "A. Vilanova",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-76",
  language =     "en",
  abstract =     "This document describes an interactive tool for
                 segmentation of medical image data based on Intelligent
                 Scissors. The two-dimensional approach, as presented by
                 Mortensen and Barrett [9], was extended by several
                 features in order to allow easy and fast segmentation
                 of three-dimensional medical data sets. The integration
                 into SegMed which is a general tool for the
                 segmentation of medical image data, allows application
                 within a large context.",
  number =       "9",
  institution =  "Computer Graphics Group, University of Erlangen,
                 Germany",
}

@InProceedings{EVL-1998-77,
  title =        "A Higher-Order Method For Finding Vortex Core Lines",
  language =     "en",
  month =        oct,
  editor =       "David Ebert and Hans Hagen and Holly Rushmeier",
  booktitle =    "Proceedings of IEEE Visualization",
  publisher =    "IEEE CS Press",
  pages =        "143--150",
  year =         "1998",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-77",
  author =       "Martin Roth and Ronald Peikert",
  abstract =     "This paper presents a novel method to extract vortical
                 structures from 3D CFD vector field automatically. It
                 discusses the underlying theory and some aspects of the
                 implementation. Making use of higher-order derivatives,
                 the method is able to locate bent vortices. In order to
                 structure the recognition procedure, we distinguish
                 locating the core line from calculating attributes of
                 strength and quality. Results are presented on several
                 flow fields from the field of turbomachinery.",
  organization = "IEEE Computer Society",
  keywords =     "vortex cores, 3D vector fields, CFD analysis, feature
                 extraction, feature-based visualization",
}

@TechReport{EVL-1998-79,
  year =         "1998",
  title =        "Parameterizing meshes with arbitrary topology",
  type =         "Technical Report",
  author =       "S. Campagna and H.-P. Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-79",
  language =     "en",
  abstract =     "Parameterizing meshes is a basic requirement for many
                 applications, including, e.g., reverse engineering,
                 texture mapping, and re-meshing. We present a new fast
                 algorithm that uses the hierarchical representation of
                 a polygonal mesh with arbitrary topology for generating
                 a geometrydriven parameterization.",
  number =       "2",
  institution =  "Computer Graphics Group, University of Erlangen,
                 Germany",
}

@TechReport{EVL-1998-8,
  year =         "1998",
  title =        "Collaborative Visualization in Augmented Reality",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-8",
  author =       "Anton Fuhrmann and Michael Gervautz and Helwig
                 L{\"o}ffelmann and Dieter Schmalstieg",
  abstract =     "STUDIERSTUBE is an augmented reality system that has
                 several advantages over conventional desktop and other
                 virtual reality environments, including true
                 stereoscopy, 3D-interaction, individual viewpoints and
                 customized views for multiple users, unhindered natural
                 collaboration and low cost. We demonstrate the
                 application of this concept for the interaction of
                 multiple users and illustrate it with several
                 visualizations of dynamical systems in DynSys3D, a
                 visualization system running on top of AVS. We also
                 show how the integration of AR into a commercial
                 visualization system can be achieved. Several examples
                 constructed in DynSys3D - developed for the
                 visualization of complex dynamical systems in AVS -
                 will complement the presentation.",
  language =     "en",
  month =        jan,
  keywords =     "Visualization, Augmented Reality",
  number =       "TR-186-2-98-01",
  institution =  "Vienna University of Technology, Computer Graphics,
                 Visualisation and Animation Group",
}

@TechReport{EVL-1998-80,
  year =         "1998",
  title =        "Efficient Decimation of Complex Triangle Meshes",
  type =         "Technical Report",
  author =       "S. Campagna and L. Kobbelt and H.-P. Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-80",
  language =     "en",
  abstract =     "Due to their simplicity triangle meshes are used to
                 represent surfaces in many applications. Since the
                 number of triangles often goes beyond the capabilities
                 of computer graphics hardware, a large variety of mesh
                 simplification algorithms has been proposed in the last
                 years. In this paper we identify major requirements for
                 the practical usability of general purpose mesh
                 reduction algorithms. The driving idea is to understand
                 mesh reduction algorithms as a software extension to
                 make more complex meshes accessible with limited
                 hardware resources. We show how these requirements can
                 be efficiently attained and discuss implementation
                 aspects in detail. We present a mesh decimation scheme
                 that fulfills these design goals and which has already
                 been evaluated by several users from different
                 application areas. We apply this algorithm to typical
                 meshes to demonstrate its performance.",
  number =       "3",
  institution =  "Computer Graphics Group, University of Erlangen,
                 Germany",
}

@TechReport{EVL-1998-81,
  year =         "1998",
  title =        "Three Point Clustering for Radiance Computations",
  type =         "Technical Report",
  author =       "M. Stamminger and Ph. Slusallek and H.-P. Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-81",
  language =     "en",
  abstract =     "There has been great success in speeding up global
                 illumination computation in diffuse environments. The
                 concept of clustering allows radiosity computations
                 even for scenes of high complexity. However, for
                 lighting simulations in complex non-diffuse scenes,
                 Monte-Carlo sampling methods are currently the first
                 choice, because non-diffuse finite element approaches
                 still exhibit enormous computation times and are thus
                 only applicable to scenes of very modest complexity. In
                 this paper we present a novel clustering approach for
                 radiance computations, by which we overcome some of the
                 problems of previous methods. The algorithm computes a
                 radiance solution within a line space hierarchy, that
                 allows us to efficiently represent light propagation
                 and reflection between arbitrary non-diffuse surfaces
                 and clusters.",
  number =       "4",
  institution =  "Computer Graphics Group, University of Erlangen,
                 Germany",
}

@TechReport{EVL-1998-82,
  year =         "1998",
  title =        "Volume Ray Casting on Sparse Grids",
  type =         "Technical Report",
  author =       "C. Teitzel and M. Hopf and R. Grosso and T. Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-82",
  language =     "en",
  abstract =     "These days sparse grids are of increasing interest in
                 numerical simulations. Based upon hierarchical tensor
                 product bases, the sparse grid approach is a very
                 efficient one improving the ratio of invested storage
                 and computing time to the achieved accuracy for many
                 problems in the area of numerical solution of partial
                 differential equations. The volume visualization
                 algorithms that are available so far cannot cope with
                 sparse grids. Now we present an approach that directly
                 works on sparse grids. As a second aspect in this
                 paper, we suggest to use sparse grids as a data
                 compression method in order to visualize huge data sets
                 even on workstations with low main memory. Because the
                 size of data sets used in numerical simulations is
                 still growing, this feature makes it possible that
                 workstations can continue to handle these data sets.
                 Besides the standard sparse grid interpolation
                 algorithm and the so called combination approach, we
                 have developed a new sparse grid interpolation method,
                 which harnesses the texture-mapping hardware of Silicon
                 Graphics workstations for accelerating purposes.
                 Therefore, hardware based volume rendering becomes
                 possible on compressed data sets at interactive frame
                 rates.",
  number =       "5",
  institution =  "Computer Graphics Group, University of Erlangen,
                 Germany",
}

@TechReport{EVL-1998-83,
  year =         "1998",
  title =        "Volume Visualization on Sparse Grids",
  type =         "Technical Report",
  author =       "C. Teitzel and M. Hopf and R. Grosso and T. Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-83",
  language =     "en",
  abstract =     "Volume rendering is an important technique of
                 displaying volumetric three-dimensional scalar data
                 sets resulting from measurement or simulation.
                 Additionally, sparse grids are of increasing interest
                 in numerical simulations. Based upon hierarchical
                 tensor product bases, the sparse grid approach is a
                 very efficient one improving the ratio of invested
                 storage and computing time to the achieved accuracy for
                 many problems in the area of numerical solution of
                 partial differential equations, for instance in
                 numerical fluid mechanics. The volume visualization
                 algorithms that are available so far cannot cope with
                 sparse grids. We present an approach that directly
                 works on these grids. As a second aspect in this paper,
                 we suggest to use sparse grids as a data compression
                 method in order to visualize huge data sets even on
                 workstations with low main memory. Because the size of
                 data sets used in numerical simulations is still
                 growing, this feature makes it possible that
                 workstations can continue to handle these data sets. In
                 addition to the standard sparse grid interpolation
                 algorithm and the so-called combination approach, we
                 have developed a new sparse grid interpolation method,
                 which harnesses the texture-mapping hardware of Silicon
                 Graphics workstations for acceleration purposes.
                 Therefore, hardware based volume rendering becomes
                 possible on compressed data sets at interactive frame
                 rates. This is not possible if other compression
                 methods like wavelet or fractal compression are used.",
  number =       "8",
  institution =  "Computer Graphics Group, University of Erlangen,
                 Germany",
}

@InProceedings{EVL-1998-84,
  pages =        "11--18",
  year =         "1998",
  title =        "Traversal-based Visualization of Data Structures",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-84",
  author =       "Jeffrey L. Korn and Andrew W. Appel",
  language =     "en",
  abstract =     "Algorithm animation systems and graphical debuggers
                 perform the task of translating program state into
                 visual representations. While algorithm animation
                 typically rely on user augmented source code to produce
                 visualizations, debuggers make use of symbolic
                 information in the target program. As a result,
                 visualizations produced by debuggers often lack
                 important semantic content, making them inferior to
                 algorithm animation systems. This paper presents a
                 method to provide higher-level, more informative
                 visualizations in a debugger using a technique called
                 traversal-based visualization. The debugger traverses a
                 data structure using a set of user-supplied patterns to
                 identify parts of the data structure to be drawn a
                 similar way. A declarative language is used to specify
                 the patterns and the actions to take when patterns are
                 encountered. Alternatively, the user can construct
                 traversal specifications through a graphical user
                 interface to the declarative language. Furthermore, the
                 debugger supports modification of data. Changes made to
                 the on-screen representation are reflected in the
                 underlying data.",
  organization = "IEEE",
  booktitle =    "Proceedings IEEE Symposium on Information
                 Visualization 1998",
}

@InProceedings{EVL-1998-85,
  pages =        "3--10",
  year =         "1998",
  title =        "{WEBPATH} -- {A} Three Dimensional Web History",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-85",
  author =       "Emmanuel Fr{\'e}con and Gareth Smith",
  language =     "en",
  abstract =     "A number of usability studies report that many users
                 of the WWW cannot find pages already visited,
                 additionaly many users cannot visualise where they are,
                 or where they have been browsing. Currently, readily
                 available WWW browsers provide history mechanisms that
                 offer little or no support in the presentation and
                 manipulation of visited sites. Manipulation and
                 presentation of usage data, such as a browse history
                 has been used in a number of cases to aid users in
                 searching for previously attained data, and to teach or
                 assist other users in their browse or searching
                 techniques. This paper presents a virtual reality (VR)
                 based application to be used alongside traditional Web
                 browsers, which provides them with a flexibly
                 tailorable real-time visualization of their history.",
  organization = "IEEE",
  booktitle =    "Proceedings IEEE Symposium on Information
                 Visualization 1998",
}

@InProceedings{EVL-1998-86,
  pages =        "19--25",
  year =         "1998",
  title =        "Reconfigurable Disc Trees for Visualizing Large
                 Hierarchical Information Space",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-86",
  author =       "Chang-Sung Jeong and Alex Pang",
  language =     "en",
  abstract =     "We present a new visualization technique, called RDT
                 (Reconfigurable Disc Tree) which can alleviate the
                 disadvantages of cone trees significantly for large
                 hierarchies while maintaining its context of using 3D
                 depth. In RDT, each node is associated with a disc
                 around which its children are placed. Using discs
                 instead of cones as the basic shape in RDT has several
                 advantages: significant reduction of occluded region,
                 sharp increase in number of displayed nodes, and easy
                 projection onto plane without visual overlapping. We
                 show that RDT can greatly enhance user perception by
                 transforming its shapes dynamically in several ways:
                 (1) disc tree which can significantly reduce the
                 occluded region by the foreground objects, (2) compact
                 disc tree which can increase the number of nodes
                 displayed on the screen, and (3) plane disc tree which
                 can be mapped onto the plane without visual
                 overlapping. We describe an implementation of our
                 visualization system called VISIT (Visual Information
                 System for reconfigurable dIsc Tree). It provides 2D
                 and 3D layouts for RDT and various user interface
                 features such as tree reconfiguration, tree
                 transformation, tree shading, viewing transformation,
                 animation, selection and browsing which can enhance the
                 user perception and navigation capabilities. We also
                 evaluate our system using the following three metrics:
                 percentage of occlusion, density of displayed nodes on
                 a screen. number of identifiable nodes.",
  organization = "IEEE",
  booktitle =    "Proceedings IEEE Symposium on Information
                 Visualization 1998",
}

@InProceedings{EVL-1998-87,
  pages =        "26--31",
  year =         "1998",
  title =        "An Interactive View for Hierarchical Clustering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-87",
  author =       "Graham J. Wills",
  language =     "en",
  abstract =     "This paper describes a visualization of a general
                 hierarchical clustering algorithm that allows the user
                 to manipulate the number of classes produced by the
                 clustering method without requiring a radical
                 re-drawing of the clustering tree. The visual method
                 used, a space-filling recursive division of a
                 rectangular area, keeps the items under consideration
                 at the same screen position even while the number of
                 classes is under interactive control. As well as
                 presenting a compact representation of the clustering
                 with different cluster numbers, this method is
                 particularly useful in a linked views environment where
                 additional information can be added to a display to
                 encode other information, without this added level of
                 detail being perturbed when changes are made to the
                 number of clusters.",
  organization = "IEEE",
  booktitle =    "Proceedings IEEE Symposium on Information
                 Visualization 1998",
}

@InProceedings{EVL-1998-88,
  pages =        "35--43",
  year =         "1998",
  title =        "Dynamic Aggregation with Circular Visual Designs",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-88",
  author =       "Mei Chuah",
  language =     "en",
  abstract =     "One very effective method for managing large data sets
                 is aggregation or binning. In this paper we consider
                 methods for making data aggregation more simple and
                 intuitive so that general users may benefit from this
                 powerful technique. We present two interactive visual
                 displays, each of which is tightly coupled with data
                 aggregation. Both visualizations support: 1) automatic
                 aggregation, 2) continuous change and control of the
                 aggregation level, 3) spatially based aggregates, 4)
                 context maintenance across different aggregate levels,
                 and 5) feedback on the level of aggregation.",
  organization = "IEEE",
  booktitle =    "Proceedings IEEE Symposium on Information
                 Visualization 1998",
}

@InProceedings{EVL-1998-89,
  pages =        "44--51",
  year =         "1998",
  title =        "The Generalized Detail-In-Context Problem",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-89",
  author =       "T. Alan Keahey",
  language =     "en",
  abstract =     "This paper describes a general formulation of the
                 {"}detail-in-context{"} problem, which is a central
                 issue of fundamental importance to a wide variety of
                 nonlinear magnification systems. A number of tools are
                 described for dealing with this problem effectively.
                 These tools can be applied to any continuous nonlinear
                 magnification system, and are not tied to specific
                 implementation features of the system that produced the
                 original transformation. Of particular interest is the
                 development of {"}seamless multi-level views{"}, which
                 allow multiple global views of an information space
                 (each having different information content) to be
                 integrated into a single view without discontinuity.",
  organization = "IEEE",
  booktitle =    "Proceedings IEEE Symposium on Information
                 Visualization 1998",
}

@TechReport{EVL-1998-9,
  year =         "1998",
  title =        "Avoiding Errors in Progressive Tetrahedralizations",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-9",
  author =       "Oliver G. Staadt and Markus H. Gross",
  abstract =     "This paper describes some fundamental issues for
                 robust implementations of progressively refined
                 tetrahedralizations generated through sequences of edge
                 collapses. We address the definition of appropriate
                 cost functions and explain on various tests which are
                 necessary to preserve the consistency of the mesh when
                 collapsing edges. Although being considered a special
                 case of progressive simplicial complexes the results of
                 our method are of high practical importance and can be
                 used in many different applications, such as finite
                 element meshing, scattered data interpolation or
                 rendering of irregular volume data",
  language =     "en",
  month =        jan,
  keywords =     "mesh simplification, multiresolution, FEM meshing",
  number =       "287",
  institution =  "ETH Z{\"{u}}rich, Institute of Scientific Computing",
}

@InProceedings{EVL-1998-90,
  pages =        "52--60",
  year =         "1998",
  title =        "Similarity Clustering of Dimensions for an Enhanced
                 Visualization of Multidimensional Data",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-90",
  author =       "Mihael Ankerst and Stefan Berchtold and Daniel A. Keim
                 Mihael",
  language =     "en",
  abstract =     "The order and arrangement of dimensions (variates) is
                 crucial for the effectiveness of a large number of
                 visualization techniques such as parallel coordinates,
                 scatterplots, recursive pattern, and many others. In
                 this paper, we describe a systematic approach to
                 arrange the dimensions according to their similarity.
                 The basic idea is to rearrange the data dimensions such
                 that dimensions showing a similar behavior are
                 positioned next to each other. For the similarity
                 clustering of dimensions we need to define similarity
                 measures which determine the partial or global
                 similarity of dimensions. We then consider the problem
                 of finding an optimal one- or two-dimensional
                 arrangement of the dimensions based on their
                 similarity. Theoretical considerations show that both,
                 the one- and the two-dimensional arrangement problem
                 are surprisingly hard problems, i.e. they are
                 NP-complete. Our solution of the problem is therefore
                 based on heuristic algorithms. An empirical evaluation
                 using a number of different visualization techniques
                 shows the high impact of our similarity clustering of
                 dimensions on the visualization results.",
  organization = "IEEE",
  booktitle =    "Proceedings IEEE Symposium on Information
                 Visualization 1998",
}

@InProceedings{EVL-1998-91,
  pages =        "63--70",
  year =         "1998",
  title =        "An Operator Interaction Framework for Visualization
                 Systems",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-91",
  author =       "Ed Huai-hsin Chi and John T. Riedl",
  language =     "en",
  abstract =     "Information visualization encounters a wide variety of
                 different data domains. The visualization community has
                 developed representation methods and interactive
                 techniques. As a community, we have realized that the
                 requirements in each domain are often dramatically
                 different. In order to easily apply existing methods,
                 researchers have developed a semiology of graphic
                 representations. We have extended this research into a
                 framework that includes operators and interactions in
                 visualization systems, such as a visualization
                 spreadsheet. We discuss properties of this framework
                 and use it to characterize operations spanning a
                 variety of different visualization techniques. The
                 framework developed in this paper enables a new way of
                 exploring and evaluating the design space of
                 visualization operators, and helps end-users in their
                 analysis tasks.",
  organization = "IEEE",
  booktitle =    "Proceedings IEEE Symposium on Information
                 Visualization 1998",
}

@InProceedings{EVL-1998-92,
  pages =        "71--78",
  year =         "1998",
  title =        "Algorithm Visualization For Distributed Environments",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-92",
  author =       "Yoram Moses and Zvi Polunsky and Ayellet Tal",
  language =     "en",
  abstract =     "This paper investigates the visualization of
                 distributed algorithms. We present a conceptual model
                 and a system, VADE, that realizes this model. Since in
                 asynchronous distributed systems there is no way of
                 knowing (let alone, visualizing) the {"}real{"}
                 execution, we show how to generate a visualization
                 which is consistent with the execution of the
                 distributed algorithm. We also present the design and
                 implementation of our system. VADE is designed so that
                 the algorithm runs on the server's machines while the
                 visualization is executed on a web page on the client's
                 machine. Programmers can write animations quickly and
                 easily with the assistant of the VADE's libraries.",
  organization = "IEEE",
  booktitle =    "Proceedings IEEE Symposium on Information
                 Visualization 1998",
}

@InProceedings{EVL-1998-93,
  pages =        "87--94",
  year =         "1998",
  title =        "Geographic Visualization: Designing Manipulable Maps
                 for Exploring Temporally Varying Georeferenced
                 Statistics",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-93",
  author =       "Alan M. MacEachren and Francis P. Boscoe and Daniel
                 Haug and Linda W. Pickle",
  language =     "en",
  abstract =     "Geographic Visualization, sometimes called
                 cartographic visualization, is a form of information
                 visualization in which principles from cartography,
                 geographic information systems (GIS), Exploratory Data
                 Analysis (EDA), and information visualization more
                 generally are integrated in the development and
                 assessment of visual methods that facilitate the
                 exploration, analysis, synthesis, and presentation of
                 georeferenced information. We report on development and
                 use of one component of a prototype GVis environment
                 designed to facilitate exploration, by domain experts,
                 of time series multivariate georeferenced health
                 statistics. Emphasis is on how manipulable dynamic GVis
                 tools may facilitate visual thinking, pattern noticing,
                 and hypothesis generation. The prototype facilitates
                 the highlighting of data extremes, examination of
                 change in geographic patterns over time, and
                 exploration of similarity among georeferenced
                 variables. A qualitative exploratory analysis of verbal
                 protocols and transaction logs is used to characterize
                 system use. Evidence produced through the
                 characterization highlights differences among experts
                 in data analysis strategies (particularly in relation
                 to the use of attribute {"}focusing{"} combined with
                 time series animation) and corresponding differences in
                 success at noticing spatiotemporal patterns.",
  organization = "IEEE",
  booktitle =    "Proceedings IEEE Symposium on Information
                 Visualization 1998",
}

@InProceedings{EVL-1998-94,
  pages =        "97--101",
  year =         "1998",
  title =        "Saying it in graphics: from intentions to
                 visualizations",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-94",
  author =       "Stephan Kerpedjiev and Giuseppe Carenini and Nancy
                 Green and Johanna Moore and Steven Roth",
  language =     "en",
  abstract =     "We propose a methodology for automatically realizing
                 communicative goals in graphics. It features a task
                 model that mediates the communicative intent and the
                 selection of graphical techniques. The methodology
                 supports the following functions: isolating assertions
                 presentable in graphics; mapping such assertions into
                 tasks for the potential reader, and selecting graphical
                 techniques that support those tasks. We illustrate the
                 methodology by redesigning a textual argument into a
                 multimedia one with the same rhetorical and content
                 structures but employing graphics to achieve some of
                 the intentions.",
  organization = "IEEE",
  booktitle =    "Proceedings IEEE Symposium on Information
                 Visualization 1998",
}

@InProceedings{EVL-1998-95,
  pages =        "102--105",
  year =         "1998",
  title =        "Visualizing Decision Table Classifiers",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-95",
  author =       "Barry G. Becker",
  language =     "en",
  abstract =     "Decision tables[1], like decision trees[2] or neural
                 nets[3], are classification models used for prediction.
                 They are induced by machine learning algorithms. A
                 decision table consists of a hierarchical table in
                 which each entry in a higher level table gets broken
                 down by the values of a pair of additional attributes
                 to form another table. The structure is similar to
                 dimensional stacking [4]. Presented here is a
                 visualization method that allows a model based on many
                 attributes to be understood even by those unfamiliar
                 with machine learning. Various forms of interaction are
                 used to make this visualization more useful than other
                 static designs.",
  organization = "IEEE",
  booktitle =    "Proceedings IEEE Symposium on Information
                 Visualization 1998",
}

@InProceedings{EVL-1998-96,
  pages =        "106--110",
  year =         "1998",
  title =        "Comparative Visualization of Protein
                 Structure-Sequence Alignments",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-96",
  author =       "Marc Hansen and Doanna Meads and Alex Pang",
  language =     "en",
  abstract =     "Protein fold recognition (threading) involves the
                 prediction of a protein's three-dimensional shape based
                 on its similarity to a protein whose structure is
                 known. Fold predictions are low resolution; no effort
                 is made to rotate the protein's component amino acid
                 side chains into their correct spatial orientations.
                 Rather, the goal is to recognize the protein family
                 member that most closely resembles the target sequence
                 of unknown structure and to create a sensible alignment
                 of the target to the structure (i.e., a
                 structure-sequence alignment). To complement this
                 structure prediction method we have implemented a low
                 resolution molecular graphics tool. Since amino acid
                 side chain orientation is not relevant in fold
                 recognition, amino acid residues are represented by
                 abstract shapes or glyphs much like Lego (tm) blocks.
                 We also borrow techniques from comparative streamline
                 visualization to provide clean depictions of the entire
                 protein structure model. By creating a low resolution
                 representation of protein structure, we are able to
                 approximately double the amount of information on the
                 screen. This implementation also possesses the
                 advantage of eliminating distracting and possibly
                 misleading visual clutter resulting from the mapping of
                 protein alignment information onto a high resolution
                 display of a known structure.",
  organization = "IEEE",
  booktitle =    "Proceedings IEEE Symposium on Information
                 Visualization 1998",
}

@InProceedings{EVL-1998-97,
  pages =        "113--120",
  year =         "1998",
  title =        "LensBar - Visualization for Browsing and Filtering
                 Large Lists of Data",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-97",
  author =       "Toshiyuki Masui",
  language =     "en",
  abstract =     "We propose a simple and powerful graphical interface
                 tool called the LensBar, for filtering and visualizing
                 a large list of data. Browsing and querying are the
                 most important techniques for information retrieval,
                 and LensBar integrates the two techniques into a
                 simple-looking scroll window with a slider. While it
                 looks familiar to users of conventional graphical
                 interface tools, its filtering and zooming mechanism
                 offers sophisticated handling of large lists of
                 text-oriented data.",
  organization = "IEEE",
  booktitle =    "Proceedings IEEE Symposium on Information
                 Visualization 1998",
}

@InProceedings{EVL-1998-98,
  pages =        "121--129",
  year =         "1998",
  title =        "The Shape Of Shakespeare: Visualizing Text Using
                 Implicit Surfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-98",
  author =       "Randall M. Rohrer and David S. Ebert and John L.
                 Sibert",
  language =     "en",
  abstract =     "Information visualization focuses on the use of visual
                 means for exploring non-visual information. While
                 free-form text is a rich, common source of information,
                 visualization of text is a challenging problem since
                 text is inherently non-spatial. This paper explores the
                 use of implicit surface models for visualizing text. We
                 describe several techniques for text visualization that
                 aid in understanding document content and document
                 relationships. A simple method is defined for mapping
                 document content to shape. By comparing the shapes of
                 multiple documents, global content similarities and
                 differences may be noted. In addition, we describe a
                 visual clustering method in which documents are
                 arranged in 3D based upon similarity scoring. Documents
                 deemed closely related blend together as a single
                 connected shape. Hence, a document corpus becomes a
                 collection of shapes that reflect inter-document
                 relationships. These techniques provide methods to
                 visualize individual documents as well as corpus
                 meta-data. We then combine the two techniques to
                 produce transparent clusters enclosing individual
                 document shapes. This provides a way to visualize both
                 local and global contextual information. Finally, we
                 elaborate on several potential applications of these
                 methods.",
  organization = "IEEE",
  booktitle =    "Proceedings IEEE Symposium on Information
                 Visualization 1998",
}

@InProceedings{EVL-1998-99,
  pages =        "130--136",
  year =         "1998",
  title =        "BiblioMapper: {A} Cluster-based Information
                 Visualization Technique",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1998-99",
  author =       "Min Song",
  language =     "en",
  abstract =     "The purpose of the paper is to develop a visualization
                 system of a document space, called BiblioMapper, for
                 CISI collections, one of the bibliographic databases
                 available on the Internet. The major function of
                 BiblioMapper is to visualize the document space with a
                 cluster-based visualization technique. The
                 cluster-based visualization technique assembles a set
                 of documents according to semantic similarities. One
                 advantage of this technique is that users are able to
                 focus on and assess each cluster and the documents
                 which the cluster comprises according to their
                 information needs.",
  organization = "IEEE",
  booktitle =    "Proceedings IEEE Symposium on Information
                 Visualization 1998",
}
