@inproceedings{Cao:2010:TCN:1787275.1787277,
 author = {Cao, Yong and Patnaik, Debprakash and Ponce, Sean and Archuleta, Jeremy and Butler, Patrick and Feng, Wu-chun and Ramakrishnan, Naren},
 title = {Towards chip-on-chip neuroscience: fast mining of neuronal spike streams using graphics hardware},
 abstract = {Computational neuroscience is being revolutionized with the advent of multi-electrode arrays that provide real-time, dynamic perspectives into brain function. Mining neuronal spike streams from these chips is critical to understand the firing patterns of neurons and gain insight into the underlying cellular activity. To address this need, we present a solution that uses a massively parallel graphics processing unit (GPU) to mine the stream of spikes. We focus on mining frequent episodes that capture coordinated events across time even in the presence of intervening background events. Our contributions include new computation-to-core mapping schemes and novel strategies to map finite state machine-based counting algorithms onto the GPU. Together, these contributions move us towards a real-time 'chip-on-chip' solution to neuroscience data mining, where one chip (the multi-electrode array) supplies the spike train data and another chip (the GPU) mines it at a scale previously unachievable.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787277},
 doi = {http://doi.acm.org/10.1145/1787275.1787277},
 acmid = {1787277},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computation-to-core mapping, computational neuroscience, graphics processing unit, temporal data mining},
} 

@inproceedings{Navaridas:2010:SIT:1787275.1787278,
 author = {Navaridas, Javier and Plana, Luis A. and Miguel-Alonso, Jose and Luj\'{a}n, Mikel and Furber, Steve B.},
 title = {SpiNNaker: impact of traffic locality, causality and burstiness on the performance of the interconnection network},
 abstract = {The SpiNNaker system is a biologically-inspired massively parallel architecture of bespoke multi-core System-on-Chips. The aim of its design is to simulate up to a billion spiking neurons in (biological) real-time. Packets, in SpiNNaker, represent neural spikes and these travel through the two-dimensional triangular torus network that connects the over 65 thousand nodes housed in the largest size of SpiNNaker. The research question that we explore is the impact that spatial locality, temporal causality and burstiness of the traffic have on the performance of such interconnection network. Given the limited knowledge of neuron activity patterns, we propose and use synthetic traffic patterns which resemble biological neural traffic and allow tuning of spatial locality. Causality is explored by means of temporal patterns that maintain a specified overall network load while allowing at the node level autonomous causal traffic generation. Part of the traffic is generated automatically, but the remaining traffic is triggered by a spike arrival in the form of a packet or a burst of packets; as neural stimuli do. In this way, we generate non-uniform traffic patterns with an evolving concentration of activity at nodes which contain more active parts of the spiking neural network. Given the application domain, the simulation-based study focuses on the real-time behavior of the system rather than focusing on standard HPC network metrics. The results show that the interconnection network of SpiNNaker can operate without dropping packets with traffic loads that exceed more than 3.5 times those required to simulate 10<sup>9</sup> spiking neurons, despite using non-local traffic. We also find that increments in the degree of traffic causality do not affect the performance of the system, but burstiness in the traffic can hurt performance.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {11--20},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787278},
 doi = {http://doi.acm.org/10.1145/1787275.1787278},
 acmid = {1787278},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {interconnection networks, massively parallel systems, performance evaluation, power-efficient architectures, real-time applications, spiking neural networks, system-on-chip, traffic characterization},
} 

@inproceedings{Rast:2010:SEN:1787275.1787279,
 author = {Rast, Alexander D. and Jin, Xin and Galluppi, Francesco and Plana, Luis A. and Patterson, Cameron and Furber, Steve},
 title = {Scalable event-driven native parallel processing: the SpiNNaker neuromimetic system},
 abstract = {Neural networks present a fundamentally different model of computation from the conventional sequential digital model. Modelling large networks on conventional hardware thus tends to be inefficient if not impossible. Neither dedicated neural chips, with model limitations, nor FPGA implementations, with scalability limitations, offer a satisfactory solution even though they have improved simulation performance dramatically. SpiNNaker introduces a different approach, the "neuromimetic" architecture, that maintains the neural optimisation of dedicated chips while offering FPGA-like universal configurability. Central to this parallel multiprocessor is an asynchronous event-driven model that uses interrupt-generating dedicated hardware on the chip to support real-time neural simulation. While this architecture is particularly suitable for spiking models, it can also implement "classical" neural models like the MLP efficiently. Nonetheless, event handling, particularly servicing incoming packets, requires careful and innovative design in order to avoid local processor congestion and possible deadlock. Using two exemplar models, a spiking network using Izhikevich neurons, and an MLP network, we illustrate how to implement efficient service routines to handle input events. These routines form the beginnings of a library of "drop-in" neural components. Ultimately, the goal is the creation of a library-based development system that allows the modeller to describe a model in a high-level neural description environment of his choice and use an automated tool chain to create the appropriate SpiNNaker instantiation. The complete system: universal hardware, automated tool chain, embedded system management, represents the "ideal" neural modelling environment: a general-purpose platform that can generate an arbitrary neural network and run it with hardware speed and scale.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {21--30},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787279},
 doi = {http://doi.acm.org/10.1145/1787275.1787279},
 acmid = {1787279},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asynchronous, event-driven, universal neural processor},
} 

@inproceedings{Saez:2010:OSS:1787275.1787281,
 author = {Saez, Juan Carlos and Fedorova, Alexandra and Prieto, Manuel and Vegas, Hugo},
 title = {Operating system support for mitigating software scalability bottlenecks on asymmetric multicore processors},
 abstract = {Asymmetric multicore processors (AMP) promise higher performance per watt than their symmetric counterparts, and it is likely that future processors will integrate a few fast</i> out-of-order cores, coupled with a large number of simpler, slow</i> cores, all exposing the same instruction-set architecture (ISA). It is well known that one of the most effective ways to leverage the effectiveness of these systems is to use fast cores to accelerate sequential phases of parallel applications, and to use slow cores for running parallel phases. At the same time, we are not aware of any implementation of this parallelism-aware</i> (PA) scheduling policy in an operating system. So the questions as to whether this policy can be delivered efficiently by the operating system</i> to unmodified applications, and what the associated overheads are remain open. To answer these questions we created two different implementations of the PA policy in OpenSolaris and evaluated it on real hardware, where asymmetry was emulated via CPU frequency scaling. This paper reports our findings with regard to benefits and drawbacks of this scheduling policy.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {31--40},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787281},
 doi = {http://doi.acm.org/10.1145/1787275.1787281},
 acmid = {1787281},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asymmetric multicore, operating systems, scheduling},
} 

@inproceedings{Huang:2010:ECD:1787275.1787282,
 author = {Huang, Miaoqing and Serres, Olivier and Narayana, Vikram K. and El-Ghazawi, Tarek and Newby, Gregory},
 title = {Efficient cache design for solid-state drives},
 abstract = {Solid-State Drives (SSDs) are data storage devices that use solid-state memory to store persistent data. Flash memory is the de facto nonvolatile technology used in most SSDs. It is well known that the writing performance of flash-based SSDs is much lower than the reading performance due to the fact that a flash page can be written only after it is erased. In this work, we present an SSD cache architecture designed to provide a balanced read/write performance for flash memory. An efficient automatic updating technique is proposed to provide a more responsive SSD architecture by writing back stable but dirty flash pages according to a predetermined set of policies during the SSD device idle time. Those automatic updating policies are also tested and compared. Simulation results demonstrate that both reading and writing performance are improved significantly by incorporating the proposed cache with automatic updating feature into SSDs.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {41--50},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787282},
 doi = {http://doi.acm.org/10.1145/1787275.1787282},
 acmid = {1787282},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache design, simulation, solid-state drive},
} 

@inproceedings{Rus:2010:PMS:1787275.1787284,
 author = {Rus, Daniela},
 title = {Programmable matter with self-reconfiguring robots},
 abstract = {Programmable matter aims to bring machines and materials closer together. We wish to create smart materials whose properties can be programmed, or, alternatively, machines that look and feel more like materials. Programmable matter will be achieved when we will have the ability to create objects whose physical properties, for example shape, stiffness, optical characteristics, acoustic characteristics, and viscosity can be programmed. We are working toward creating materials with embedded sensing, actuation, communication, computation, and connection, which we call SAC<sup>3</sup> materials. We are developing two concepts: smart SAC<sup>3</sup> sheets that self-fold into origami shapes, and smart SAC<sup>3</sup> pebbles that self-sculpt into desired objects. This work is at the intersection of theory, algorithms, device design, and control. This talk will survey the history of programmable matter. We start by discussing robotic self-reconfiguration whose aim is to create modular robots capable of changing shape: hundreds of small modules autonomously organize and reorganize as geometric structures to best fit the terrain on which the robot has to move, the shape of the object the robot has to manipulate, or the sensing needs of the given task. Self-reconfiguration leads to versatile robots that can support multiple modalities of locomotion, manipulation, and perception. We will discuss a spectrum of mechanical and computational capabilities for such systems and detail some recent self-reconfiguring robots. We then discuss programmable matter by smart sheets and smart pebbles. Finally, we discuss the theoretical and systems challenges for realizing the full potential of programmable matter.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {51--52},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787284},
 doi = {http://doi.acm.org/10.1145/1787275.1787284},
 acmid = {1787284},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {programmable matter, robotics},
} 

@inproceedings{Cederman:2010:SLC:1787275.1787286,
 author = {Cederman, Daniel and Tsigas, Philippas},
 title = {Supporting lock-free composition of concurrent data objects},
 abstract = {Lock-free data objects offer several advantages over their blocking counterparts, such as being immune to deadlocks and convoying and, more importantly, being highly concurrent. However, composing the operations they provide into larger atomic operations, while still guaranteeing efficiency and lock-freedom, is a challenging algorithmic task. We present a lock-free methodology for composing highly concurrent linearizable objects together by unifying their linearization points. This makes it possible to relatively easily introduce atomic lock-free move operations to a wide range of concurrent objects. Experimental evaluation has shown that the operations originally supported by the data objects keep their performance behavior under our methodology.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {53--62},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787286},
 doi = {http://doi.acm.org/10.1145/1787275.1787286},
 acmid = {1787286},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {composition, data structures, lock-free},
} 

@inproceedings{Xia:2010:CSD:1787275.1787287,
 author = {Xia, Yinglong and Prasanna, Viktor K.},
 title = {Collaborative scheduling of DAG structured computations on multicore processors},
 abstract = {Many computational solutions can be expressed as directed acyclic graphs (DAGs), in which the nodes represent tasks to be executed. A fundamental challenge in parallel computing is to schedule such DAGs onto multicore processors while preserving the precedence constraints. In this paper, we propose a lightweight scheduling method for DAG structured computations on multicore processors. We distribute the scheduling activities across the cores and let the schedulers collaborate with each other to balance the workload. In addition, we develop a software lock-free local task list for the scheduler to reduce the scheduling overhead. We experimentally evaluated the proposed method by comparing with various baseline methods on state-of-the-art multicore processors. For a representative set of DAG structured computations from both synthetic and real problems, the proposed scheduler with lock-free local task lists achieved 15.12x average speedup on a platform with four quadcore processors, compared to 8.77x achieved by lock-based baseline methods. The observed scheduling overhead of the proposed scheduler was less than 1\% of the overall execution time.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {63--72},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787287},
 doi = {http://doi.acm.org/10.1145/1787275.1787287},
 acmid = {1787287},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {collaborative scheduling, dag structured computations, lock free structures, task sharing},
} 

@inproceedings{Abellan:2010:ESB:1787275.1787289,
 author = {Abell\'{a}n, Jos\'{e} L. and Fern\'{a}ndez, Juan and Acacio, Manuel E.},
 title = {Efficient and scalable barrier synchronization for many-core CMPs},
 abstract = {We present in this work a novel hardware-based barrier mechanism for synchronization on many-core CMPs. In particular, we leverage global interconnection lines (G-lines</i>) and S-CSMA</i> technique, which have been used to overcome some limitations of a flow control mechanism (EVC) in the context of Networks-on-Chip, to develop a simple G-lines</i>-based network that operates independently of the main data network in order to carry out barrier synchronizations. Next, we evaluate our approach by running several applications on top of the Sim-PowerCMP performance simulator. Our method only takes 4 cycles to carry out the synchronization once all cores or threads have arrived at the barrier. Hence, we obtain much better performance results than software-based barrier implementations in terms of scalability and efficiency.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {73--74},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787289},
 doi = {http://doi.acm.org/10.1145/1787275.1787289},
 acmid = {1787289},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {g-line-based barrier synchronization, global interconnection lines, many-core cmps, s-csma technique},
} 

@inproceedings{Brown:2010:CIM:1787275.1787290,
 author = {Brown, Andrew D. and Furber, Steven B. and Reeve, Jeff S. and Wilson, Peter R. and Zwolinski, Mark and Chad, John E. and Plana, Luis and Lester, David R.},
 title = {A communication infrastructure for a million processor machine},
 abstract = {SpiNNaker (Spiking Neural Network architecture) is a massively parallel computing machine, comprising a million ARM9 cores. These are realised on 50000 chips, 20 cores/chip. While it could be classed as a MIMD machine, there is no unifying bus structure, and there is no attempt to maintain cross-system memory coherence. Inter-core communication is brokered by a fast message-passing system, built in and managed at the hardware level - thus there is an inevitable tension between speed and flexibility. The message passing infrastructure was designed to be fast and have a high bandwidth; a consequence of this design decision is that the effective data payload is only 32 bits/packet. Whilst this is ample for a wide range of applications, when the system is initialising, it is necessary to transport relatively large and sophisticated data structures across the system. This can be slow and cumbersome, and makes some form of internal self-organisation extremely attractive. This is described in outline here.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {75--76},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787290},
 doi = {http://doi.acm.org/10.1145/1787275.1787290},
 acmid = {1787290},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multi-core, self-organisation},
} 

@inproceedings{Chiu:2010:HMA:1787275.1787291,
 author = {Chiu, Jih-Ching and Chou, Yu-Liang and Su, Ding-Siang},
 title = {A hyperscalar multi-core architecture},
 abstract = {This paper proposes a reconfigurable multi-core architecture, called hyperscalar</i> that enables many scalar cores to be united dynamically as a larger superscalar processor to accelerate a thread. To accomplish this, we propose the virtual shared register files (VSRF) that allow the instructions of a thread executed in the united cores to logically face a uniform set of register files. We also propose the instruction analyzer (IA) with the capability of detecting and tagging the dependence information to the newly fetched instructions. According to the tags, instructions in the united cores can issue requests to obtain their remote operands via the VSRF. The reconfigurable feature of hyperscalar can cover a spectrum of workloads well, providing high single-thread performance when TLP is low and high throughput when TLP is high. Simulation results show that the a 8-core hyperscalar chip multiprocessor's 2, 4, and 8-core-united configurations archive 94\%, 90\%, and 83\% of the performance of the monolithic 2, 4, and 8-issue out-of-order superscalar processors with lower area costs and better support for software diversity.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {77--78},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787291},
 doi = {http://doi.acm.org/10.1145/1787275.1787291},
 acmid = {1787291},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chip multiprocessors, cmps, dynamic multi-core chips, reconfigurable multi-core architectures},
} 

@inproceedings{Sui:2010:ACP:1787275.1787292,
 author = {Sui, Xiufeng and Wu, Junmin and Chen, Guoliang and Tang, Yixuan and Zhu, Xiaodong},
 title = {Augmenting cache partitioning with thread-aware insertion/promotion policies to manage shared caches},
 abstract = {In this paper, we augment traditional cache partitioning with thread-aware adaptive insertion and promotion policies to manage shared L2 caches. The proposed mechanism can mitigate destructive inter-thread interference, and meanwhile retain some fraction of the working set in the cache, therefore results in better performance.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {79--80},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787292},
 doi = {http://doi.acm.org/10.1145/1787275.1787292},
 acmid = {1787292},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache partitioning, replacement, shared caches},
} 

@inproceedings{Rico:2010:PPE:1787275.1787293,
 author = {Rico, Alejandro and Derby, Jeff H. and Montoye, Robert K. and Heil, Timothy H. and Cher, Chen-Yong and Bose, Pradip},
 title = {Performance and power evaluation of an in-line accelerator},
 abstract = {In this paper we evaluate the performance and power of a processor-attached in-line accelerator. The accelerator provides high-performance SIMD computing and power efficiency by means of a very large register file and a set of vector multimedia extensions based on IBM's PowerPC VMX. Our experiments show significant performance improvements and power reduction, compared to a baseline vector execution unit, mainly due to the drastic decrease of memory accesses caused by the software-managed locality of the very large register file. Total execution time is, on average, reduced by 61\%, while consuming 55\% less energy.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {81--82},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787293},
 doi = {http://doi.acm.org/10.1145/1787275.1787293},
 acmid = {1787293},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {accelerator, powerpc, simd, vmx},
} 

@inproceedings{Nallamuthu:2010:EEB:1787275.1787294,
 author = {Nallamuthu, Ananth and Smith, Melissa C. and Hampton, Scott and Agarwal, Pratul K. and Alam, Sadaf R.},
 title = {Energy efficient biomolecular simulations with FPGA-based reconfigurable computing},
 abstract = {Reconfigurable computing (RC) is being investigated as a hardware solution for improving time-to-solution for biomolecular simulations. A number of popular molecular dynamics (MD) codes are used to study various aspects of biomolecules. These codes are now capable of simulating nanosecond time-scale trajectories per day on conventional microprocessor-based hardware, but biomolecular processes often occur at the microsecond time-scale or longer. A wide gap exists between the desired and achievable simulation capability; therefore, there is considerable interest in alternative algorithms and hardware for improving the time-to-solution of MD codes. The fine-grain parallelism provided by Field Programmable Gate Arrays (FPGA) combined with their low power consumption make them an attractive solution for improving the performance of MD simulations. In this work, we use an FPGA-based coprocessor to accelerate the compute-intensive calculations of LAMMPS, a popular MD code, achieving up to 5.5 fold speed-up on the non-bonded force computations of the particle mesh Ewald method and up to 2.2 fold speed-up in overall time-to-solution, and potentially an increase by a factor of 9 in power-performance efficiencies for the pair-wise computations. The results presented here provide an example of the multi-faceted benefits to an application in a heterogeneous computing environment.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {83--84},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787294},
 doi = {http://doi.acm.org/10.1145/1787275.1787294},
 acmid = {1787294},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {biomolecular simulations, fpga, lammps, molecular dynamics, reconfigurable computing},
} 

@inproceedings{Gummaraju:2010:EIG:1787275.1787295,
 author = {Gummaraju, Jayanth and Sander, Ben and Morichetti, Laurent and Gaster, Benedict and Howes, Lee},
 title = {Efficient implementation of GPGPU synchronization primitives on CPUs},
 abstract = {The GPGPU model represents a style of execution where thousands of threads execute in a data-parallel fashion, with a large subset (typically 10s to 100s) needing frequent synchronization. As the GPGPU model evolves target both GPUs and CPUs as acceleration targets, thread synchronization becomes an important problem when running on CPUs. CPUs have little hardware support for synchronization and must be emulated in software, reducing application performance. This paper presents software techniques to implement the GPGPU synchronization primitives on CPUs, while maintaining application debug-ability. Performing limit studies using real hardware, we evaluate the potential performance benefits of an efficient <b>barrier</b> primitive.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {85--86},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787295},
 doi = {http://doi.acm.org/10.1145/1787275.1787295},
 acmid = {1787295},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {gpgpu, multicore, synchronization},
} 

@inproceedings{Tumeo:2010:EPM:1787275.1787296,
 author = {Tumeo, Antonino and Villa, Oreste and Sciuto, Donatella},
 title = {Efficient pattern matching on GPUs for intrusion detection systems},
 abstract = {In this paper we present an efficient implementation of the Aho-Corasick pattern matching algorithm on Graphics Processing Units (GPU), showing how we redesigned the algorithm and the data structures to fit on the architecture and comparing it with an equivalent implementation on the CPU. We show that with a synthetic dataset, our implementation obtains a speedup up to 6.67 with respect to the CPU solution.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {87--88},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787296},
 doi = {http://doi.acm.org/10.1145/1787275.1787296},
 acmid = {1787296},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {aho corasick, cuda, gpgpu, pattern matching},
} 

@inproceedings{Jin:2010:EPI:1787275.1787297,
 author = {Jin, Xin and Luj\'{a}n, Mikel and Plana, Luis A. and Rast, Alexander D. and Welbourne, Stephen R. and Furber, Steve B.},
 title = {Efficient parallel implementation of multilayer backpropagation networks on SpiNNaker},
 abstract = {This paper presents an efficient implementation and performance analysis of mapping multilayer perceptron networks with the backpropagation learning rule on SpiNNaker - a massively parallel architecture dedicated for neural network simulation. A new algorithm called pipelined checker-boarding partitioning scheme is proposed for efficient mapping. The new mapping algorithm relies on a checker-board partitioning scheme, but the key advantage comes from introducing a pipelined mode. The six-stage pipelined mode captures the parallelism within each partition of the weight matrix, allowing the overlapping of communication and computation. Not only does the proposed mapping localize communication, but it can also hide a part of or even all the communication for high efficiency.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {89--90},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787297},
 doi = {http://doi.acm.org/10.1145/1787275.1787297},
 acmid = {1787297},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {backpropagation, mapping, mlp, parallel, pipeline, spinnaker},
} 

@inproceedings{Heinecke:2010:PEC:1787275.1787298,
 author = {Heinecke, Alexander and Trinitis, Carsten and Weidendorfer, Josef},
 title = {Porting existing cache-oblivious linear algebra HPC modules to larrabee architecture},
 abstract = {Cache-obliviousness represents an important but relatively new concept for cache optimization. As cache-oblivious algorithms perform well on architectures with arbitrary cache configurations, the programming effort required for porting and optimizing for future architectures can be significantly reduced. In [8] and [9], fast parallel cache-oblivious linear algebra modules have been presented. The underlying matrix storing schemes are based on space filling curves. For matrix multiplication, all cache misses can be avoided, whereas for the LU decomposition algorithm the number of cache misses is minimized. It has been shown that the resulting codes work very well on several kinds of systems ranging from laptops to supercomputers. In this paper, we will show that the runtime characteristics of our existing cache-oblivious codes can be preserved on newer Intel processors. Special emphasis is put on the first many-core processor architecture with complete hardware-based cache coherency: The Larrabee Architecture. As the latter is expected to be available as a PCIe card connected to the host system, porting had to take into account transfer of data structures between different memory address spaces. Unfortunately, Larrabee was canceled as a graphics device for 2010, but Intel is expected to outline futher steps about Larrabee during 2010.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {91--92},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787298},
 doi = {http://doi.acm.org/10.1145/1787275.1787298},
 acmid = {1787298},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {accelerator space-filling curve, cache-oblivious, lu decomposition, manycore, matrix multiplication, openmp},
} 

@inproceedings{Omana:2010:NLA:1787275.1787299,
 author = {Oma\~{n}a, Martin and Rossi, Daniele and Bosio, Nicol\`{o} and Metra, Cecilia},
 title = {Novel low-cost aging sensor},
 abstract = {Performance degradation of integrated circuits due to aging effects, such as Negative Bias Temperature Instability (NBTI), is becoming of great concern for current and future CMOS technology. Here we introduce an aging sensor able to detect such degradations in the combinational part of a critical data-path. It requires lower area than recently proposed alternative solutions, and a lower or comparable power consumption.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {93--94},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787299},
 doi = {http://doi.acm.org/10.1145/1787275.1787299},
 acmid = {1787299},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {aging sensor, nbti, performance degradation},
} 

@inproceedings{Opritoiu:2010:HAA:1787275.1787300,
 author = {Opritoiu, Flavius and Vladutiu, Mircea and Prodan, Lucian and Udrescu, Mihai},
 title = {A high-speed AES architecture implementation},
 abstract = {We present in this paper a high performance implementation for the Advanced Encryption Standard (AES) standard. The design goal is directed toward efficient implementation of an AES cryptocore. The proposed architecture exhibits parallelism by concurrently processing all the bytes of a data block and computes each round key on-the-fly. The design implements both AES encryption and decryption by efficiently sharing the complex design modules. The proposed high-speed iterative implementation performing the AES operations in 11 clock cycles was synthesized for ALTERA's Cyclone II FPGA.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {95--96},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787300},
 doi = {http://doi.acm.org/10.1145/1787275.1787300},
 acmid = {1787300},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {aes, cryptochip, fpga, hardware, optimization},
} 

@inproceedings{Shabbir:2010:PCA:1787275.1787301,
 author = {Shabbir, Ahsan and Stuijk, Sander and Kumar, Akash and Theelen, Bart and Mesman, Bart and Corporaal, Henk},
 title = {A predictable communication assist},
 abstract = {Modern multi-processor systems need to provide guaranteed services to their users. A communication assist (CA) helps in achieving tight timing guarantees. In this paper, we present a CA for a tile-based MP-SoC. Our CA has smaller memory requirements and a lower latency than existing CAs. The CA has been implemented in hardware. We compare it with two existing DMA controllers. When compared with these DMAs, our CA is up-to 44\% smaller in terms of equivalent gate count.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {97--98},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787301},
 doi = {http://doi.acm.org/10.1145/1787275.1787301},
 acmid = {1787301},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ca, communication, dma, fpga's, mp-soc, predictable},
} 

@inproceedings{Kejariwal:2010:ENT:1787275.1787302,
 author = {Kejariwal, Arun and Girkar, Milind and Tian, Xinmin and Saito, Hideki and Nicolau, Alexandru and Veidenbaum, Alexander V. and Banerjee, Utpal and Polychronopoulos, Constantine D.},
 title = {Exploitation of nested thread-level speculative parallelism on multi-core systems},
 abstract = {Multi-cores such as the Intel Core 2 Duo, AMD Barcelona and IBM POWER6 are becoming ubiquitous. The number of cores and the resulting hardware parallelism is poised to increase rapidly in the foreseeable future. Nested thread-level speculative parallelization has been proposed as a means to exploit the hardware parallelism of such systems. In this paper, we present a methodology to gauge the efficacy of nested thread-level speculation with increasing level of nesting.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {99--100},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787302},
 doi = {http://doi.acm.org/10.1145/1787275.1787302},
 acmid = {1787302},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {performance, thread-level speculation},
} 

@inproceedings{Lakshman:2010:CDD:1787275.1787303,
 author = {Lakshman, Pandya},
 title = {Combining deblurring and denoising for handheld HDR imaging in low light conditions},
 abstract = {This paper proposes a probability formulation that unifies both single-image deblurring and multi-image denoising using variational inference. Based on this formulation, a new algorithm for deblurring a noisy and blurry image pair is presented. Besides, we provide also an approach that combines existing optical flow and image denoising techniques for High Dynamic Range imaging.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {101--102},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787303},
 doi = {http://doi.acm.org/10.1145/1787275.1787303},
 acmid = {1787303},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {deblurring, denoising, high dynamic range imaging},
} 

@inproceedings{Cunsolo:2010:VCC:1787275.1787304,
 author = {Cunsolo, Vincenzo D. and Distefano, Salvatore and Puliafito, Antonio and Scarpa, Marco},
 title = {From volunteer to cloud computing: cloud@home},
 abstract = {Even though mainly commercial Cloud solutions have been implemented so far, the Cloud computing approach is quickly and widely spreading in open contexts such as scientific and academic communities. Two main research directions can be identified in such context: to provide an open</i> Cloud infrastructure able to share resources to the community; and to implement an interoperable</i> framework, allowing commercial and open Cloud infrastructures to interact. In this paper we present the Cloud@Home</i> paradigm as an effective solution to the problem of building open and interoperable Clouds. In this new paradigm, users' hosts are not passive interface to Cloud services anymore, but they can interact (for free or by charge) with other Clouds, that therefore must be able to interoperate.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {103--104},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787304},
 doi = {http://doi.acm.org/10.1145/1787275.1787304},
 acmid = {1787304},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cloud, infrastructure as a service, interoperability, volunteer computing},
} 

@inproceedings{Celesti:2010:DCN:1787275.1787305,
 author = {Celesti, Antonio and Villari, Massimo and Puliafito, Antonio},
 title = {Design of a cloud naming framework},
 abstract = {In cloud computing environments, naming and resource location become critical issues. Exploiting the concept of resource virtualization, a cloud offers a variety of entities which can be moved from a place to another, needing to be identified with one or more names and logical representations in various contexts. In such scenario, the management of cloud name spaces may become very difficult. This paper proposes a Cloud Naming System Framework</i> (CNSF) able to: I) integrate independent cloud name spaces in federated scenario, II) support the "mounting" of names from a cloud name space to another, III) interact with other URI-based naming systems.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {105--106},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787305},
 doi = {http://doi.acm.org/10.1145/1787275.1787305},
 acmid = {1787305},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cloud computing, cloud federation, cloud name space, cloud naming system, xri},
} 

@inproceedings{Bosin:2010:SAE:1787275.1787306,
 author = {Bosin, Andrea and Dess\`{\i}, Nicoletta and Bairappan, Madusudhanan},
 title = {A service-based approach for the execution of scientific workflows in grids},
 abstract = {Scientific workflows may be deployed on heterogeneous distributed environments, including Grid resources whose access is not straightforward when it is non compliant with the specific Grid middleware. This paper addresses this complexity and investigates the feasibility of a service oriented approach that takes advantage of the standardized resource access provided by BPEL. The paper proposes specialized Web services, namely BPEL partners, that support the discovering of suitable resources, monitor the execution of each single workflow task in the resource and aggregate results of the execution. Some implementation hints are presented.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {107--108},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787306},
 doi = {http://doi.acm.org/10.1145/1787275.1787306},
 acmid = {1787306},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bpel, grid computing, scientific workflows},
} 

@inproceedings{Paterna:2010:VRW:1787275.1787307,
 author = {Paterna, Francesco and Acquaviva, Andrea and Caprara, Alberto and Papariello, Francesco and Desoli, Giuseppe and Benini, Luca},
 title = {Variability-tolerant run-time workload allocation for MPSoC energy minimization under real-time constraints},
 abstract = {},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {109--110},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787307},
 doi = {http://doi.acm.org/10.1145/1787275.1787307},
 acmid = {1787307},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy minimization, mpsoc, real-time, variability},
} 

@inproceedings{Nie:2010:PPF:1787275.1787308,
 author = {Nie, Yufeng and Wang, Lei and Zhang, Weiwei},
 title = {A portable parallel finite element simulation system},
 abstract = {In order to adapt various complex parallel environments, increase the utilization ratio of current software and hardware, as well as improve the overall parallel efficiency of parallel FEM, the Parallel Node-based Local Finite Element (<b>PNLFE</b>) System is presented. The PNLFE system separates the whole computing task to N unit tasks according to the number of mesh nodes N. For each mesh node, the unit task is to generate the corresponding local mesh, which meets conforming requirement, then compute the global stiffness matrix row of the node. After this is done, processors can immediately pre-process the stiffness matrix rows and obtain the numerical solution with the least communication compared with other known systems. Unit tasks are separated and small, thus the system is applicable to various parallel architectures.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {111--112},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787308},
 doi = {http://doi.acm.org/10.1145/1787275.1787308},
 acmid = {1787308},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {load balance, parallel efficiency, parallel fem, portable system},
} 

@inproceedings{Rossi:2010:LCL:1787275.1787309,
 author = {Rossi, Daniele and Omana, Martin E. and Berghella, Gianluca and Metra, Cecilia and Jas, Abhijit and Tirumurti, Chandra and Galivanche, Rajesh},
 title = {Low cost and low intrusive approach to test on-line the scheduler of high performance microprocessors},
 abstract = {We propose a low cost and low intrusive approach to test on line the scheduler of high performance microprocessors. Differently from traditional approaches, it is based on looking for the information redundancy that the scheduler inherently has due to its performed functionality, rather than adding such a redundancy for on line test purposes.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {113--114},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787309},
 doi = {http://doi.acm.org/10.1145/1787275.1787309},
 acmid = {1787309},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {control logic, microprocessor, on-line testing},
} 

@inproceedings{Pellegrini:2010:ATM:1787275.1787310,
 author = {Pellegrini, Simone and Fahringer, Thomas and Jordan, Herbert and Moritsch, Hans},
 title = {Automatic tuning of MPI runtime parameter settings by using machine learning},
 abstract = {MPI implementations provide several hundred runtime parameters that can be tuned for performance improvement. The ideal parameter setting does not only depend on the target multiprocessor architecture but also on the application, its problem and communicator size. This paper presents <b>ATune</b>, an automatic performance tuning tool that uses machine learning techniques to determine the program-specific optimal settings for a subset of the Open MPI's runtime parameters. <b>ATune</b> learns the behaviour of a target system by means of a training phase where several MPI benchmarks and MPI applications are run on a target architecture for varying problem and communicator sizes. For new input programs, only one run is required in order for <b>ATune</b> to deliver a prediction of the optimal runtime parameters values. Experiments based on the NAS Parallel Benchmarks performed on a cluster of SMP machines are shown that demonstrate the effectiveness of <b>ATune</b>. For these experiments, <b>ATune</b> derives MPI runtime parameter settings that are on average within 4\% of the maximum performance achievable on the target system resulting in a performance gain of up to 18\% with respect to the default parameter setting.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {115--116},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787310},
 doi = {http://doi.acm.org/10.1145/1787275.1787310},
 acmid = {1787310},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {machine learning, mpi runtime parameters, tuning},
} 

@inproceedings{Penry:2010:EPL:1787275.1787311,
 author = {Penry, David A. and Richins, Daniel J. and Harris, Tyler S. and Greenland, David and Reh Koy D.},
 title = {Exposing parallelism and locality in a runtime parallel optimization framework},
 abstract = {Runtime parallel optimization has been suggested as a means to overcome the difficulties of parallel programming. For runtime parallel optimization to be effective, parallelism and locality that are expressed in the programming model need to be communicated to the runtime system. We suggest that the compiler should expose this information to the runtime using a representation that is independent of the programming model. Such a representation allows a single runtime environment to support many different models and architectures and to perform automatic parallelization optimization.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {117--118},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787311},
 doi = {http://doi.acm.org/10.1145/1787275.1787311},
 acmid = {1787311},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adopar, parallel optimization},
} 

@inproceedings{Miranda:2010:EDC:1787275.1787312,
 author = {Miranda, Cupertino and Dumont, Philippe and Cohen, Albert and Duranton, Marc and Pop, Antoniu},
 title = {ERBIUM: a deterministic, concurrent intermediate representation for portable and scalable performance},
 abstract = {Optimizing compilers and runtime libraries do not shield programmers from the complexity of multi-core hardware; as a result the need for manual, target-specific optimizations increases with every processor generation. High-level languages are being designed to express concurrency and locality without reference to a particular architecture. But compiling such abstractions into efficient code requires a portable, intermediate representation: this is essential for modular composition (separate compilation), for optimization frameworks independent of the source language, and for just-in-time compilation of bytecode languages. This paper introduces Erbium, an intermediate representation for compilers, a low-level language for efficiency programmers, and a lightweight runtime implementation. It relies on a data structure for scalable and deterministic concurrency, called Event Recording</i>, exposing the data-level, task and pipeline parallelism suitable to a given target. We provide experimental evidence of the productivity, scalability and efficiency advantages of Erbium, relying on a prototype implementation in GCC 4.3.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {119--120},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787312},
 doi = {http://doi.acm.org/10.1145/1787275.1787312},
 acmid = {1787312},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {intermediate representation, kpn, parallelism, streaming, synchronization},
} 

@inproceedings{Zhao:2010:NNC:1787275.1787314,
 author = {Zhao, Li and Iyer, Ravi and Makineni, Srihari and Newell, Don and Cheng, Liqun},
 title = {NCID: a non-inclusive cache, inclusive directory architecture for flexible and efficient cache hierarchies},
 abstract = {Chip-multiprocessor (CMP) architectures employ multi-level cache hierarchies with private L2 caches per core and a shared L3 cache like Intel's Nehalem processor and AMD's Barcelona processor. When designing a multi-level cache hierarchy, one of the key design choices is the inclusion policy: inclusive, non-inclusive or exclusive. Either choice has its benefits and drawbacks. An inclusive cache hierarchy (like Nehalem's L3) has the benefit of allowing incoming snoops to be filtered at the L3 cache, but suffers from (a) reduced space efficiency due to replication between the L2 and L3 caches and (b) reduced flexibility since it cannot bypass the L3 cache for transient or low priority data. In an inclusive L2/L3 cache hierarchy, it also becomes difficult to flexibly chop L3 cache size (or increase L2 cache size) for different product instantiations because the inclusion can start to affect performance (due to significant back-invalidates). In this paper, we present a novel approach to addressing the drawbacks of inclusive caches, while retaining its positive features of snoop filtering. We present NCID: a non-inclusive cache, inclusive directory architecture that allows data in the L3 to be non-inclusive or exclusive, but retains tag inclusion in the directory to support complete snoop filtering. We then describe and evaluate a range of NCID-based architecture options and policies. Our evaluation shows that NCID enables a flexible and efficient cache hierarchy for future CMP platforms and has the potential to improve performance significantly for several important server benchmarks.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {121--130},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787314},
 doi = {http://doi.acm.org/10.1145/1787275.1787314},
 acmid = {1787314},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache, directory},
} 

@inproceedings{Zahran:2010:GMC:1787275.1787315,
 author = {Zahran, Mohamed and McKee, Sally A.},
 title = {Global management of cache hierarchies},
 abstract = {Cache memories currently treat all blocks as if they were equally important. This assumption of equally important blocks is not always valid. For instance, not all blocks deserve to be in L1 cache. We therefore propose globalized block placement. We present a global placement algorithm for managing blocks in a cache hierarchy by deciding where in the hierarchy an incoming block should be placed. Our technique makes decisions by adapting to access patterns of different blocks. The contributions of this paper are fourfold. First, we motivate our solution by demonstrating the importance of a globalized placement scheme. Second, we present a method to categorize cache block behavior into one of four categories. Third, we present one potential design exploiting this categorization. Finally, we demonstrate the performance of our design. The proposed scheme enhances overall system performance (IPC) by an average of 12\% over a traditional LRU scheme while reducing traffic between L1 cache and L2 cache by an average of 20\%, using SPEC CPU benchmark suite. All of this is achieved with a table as small as 3 KB.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {131--140},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787315},
 doi = {http://doi.acm.org/10.1145/1787275.1787315},
 acmid = {1787315},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache memory, memory hierarchy},
} 

@inproceedings{Keramidas:2010:RAF:1787275.1787316,
 author = {Keramidas, Georgios and Petoumenos, Pavlos and Kaxiras, Stefanos},
 title = {Where replacement algorithms fail: a thorough analysis},
 abstract = {Cache placement and eviction, especially at the last level of the memory hierarchy, have received a flurry of research activity recently. The common perception that LRU is a well-performing algorithm has recently been discredited: many researchers have turned their attention to more sophisticated algorithms that are able to substantially improve cache performance. In this paper, we thoroughly examine four recently proposed replacement policies: the Dynamic Insertion Policy (DIP), the Shepherd Cache (SC), the MLP-aware replacement, and the Instruction-based Reuse Distance Prediction (IbRDP) replacement policy. Our experimental studies show that there is a great inconsistency</i> between the number of misses saved by each mechanism and the resulting improvement in IPC. This is particularly true for the DIP and the SC approach and indeed attest to the fact that these algorithms do not take into account the relative cost of each miss (i.e., whether it is an isolated or parallel miss). Their aim is to blindly lower the total number of misses. On the other hand, the MLP-aware replacement, although miss-cost-aware, cannot handle efficiently workloads which display LRU-hostile behavior and thus fails to reduce execution time even when there are ample opportunities to reduce cache misses. The IbRDP replacement policy shows both the ability to deal with non-LRU access patterns and MLP friendliness</i> leading to greater consistency between the reduction of misses and the corresponding increase in performance thus the largest IPC improvement among the studied mechanisms. So, what are the appropriate characteristics of a replacement algorithm targeting the lower levels of the memory hierarchy? In this paper we are shedding some light on this question.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {141--150},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787316},
 doi = {http://doi.acm.org/10.1145/1787275.1787316},
 acmid = {1787316},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {last-level caches, memory system, profiling, replacement/placement policies/algorithms},
} 

@inproceedings{Babaoglu:2010:NTS:1787275.1787318,
 author = {Babaoglu, Ozalp},
 title = {Nature-inspired techniques for self-organization in dynamic networks},
 abstract = {We examine problems that arise in dynamic network structures such as Peer-to-Peer and mobile ad hoc networks that are characterized by their extreme dynamism and large scale. In such systems, traditional techniques often prove inadequate towards providing simple solutions for their deployment, configuration and management. What is desirable is that these systems be self-configuring, self-monitoring, self-adapting, self-tuning, self-healing, and in general, self-managing. In this talk, I will put forth self-organization as a fundamental abstraction for achieving self-* properties in a bottom-up fashion without having to program them explicitly. I will support this view by illustrating completely decentralized, extremely robust and scalable solutions for important problems that draw inspiration from nature and that are based on a gossiping interaction model.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {151--152},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1787275.1787318},
 doi = {http://doi.acm.org/10.1145/1787275.1787318},
 acmid = {1787318},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {gossip-based protocols, peer-to-peer systems},
} 

@inproceedings{Moreto:2010:LBU:1787275.1787320,
 author = {Moreto, Miquel and Cazorla, Francisco J. and Sakellariou, Rizos and Valero, Mateo},
 title = {Load balancing using dynamic cache allocation},
 abstract = {Supercomputers need a huge budget to be built and maintained. To maximize the usage of their resources, application developers spend time to optimize the code of the parallel applications and minimize execution time. Despite this effort, load imbalance</i> still arises in many optimized applications due to causes not controlled by the application developer, resulting in significant performance degradation and waste of CPU time. If the nodes of the supercomputer use chip multiprocessors, this problem may become even worse, as the interaction between different threads inside the chip may affect their performance in an unpredictable way. Although there are many techniques to address load imbalance at run-time, as it happens, these techniques may not be particularly effective when the cause of the imbalance is due to the performance sensitivity of the parallel threads when accessing a shared cache. To this end, we present a novel run-time mechanism, with minimal hardware, that automatically tries to balance parallel applications using dynamic cache allocation. The mechanism detects which applications may be sensitive to cache allocation and reduces imbalance by assigning more cache space to the slowest threads. The efficiency of our proposed mechanism is demonstrated with both synthetic workloads and a real-world parallel application. In the former case, we reduce the execution time by up to 28.9\%; in the latter case, our proposal reduces the imbalance of a non-optimized version of the application to the values obtained with a hand-tuned version of the same application.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {153--164},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1787275.1787320},
 doi = {http://doi.acm.org/10.1145/1787275.1787320},
 acmid = {1787320},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache partitioning, cmp architectures, load balancing},
} 

@inproceedings{Al-Otoom:2010:EED:1787275.1787321,
 author = {Al-Otoom, Muawya and Forbes, Elliott and Rotenberg, Eric},
 title = {EXACT: explicit dynamic-branch prediction with active updates},
 abstract = {Branches that depend directly or indirectly on load instructions are a leading cause of mispredictions by state-of-the-art branch predictors. For a branch of this type, there is a unique dynamic instance of the branch for each unique combination of producer-load addresses. Based on this definition, a study of mispredictions reveals two related problems: (i) Global branch history often fails to distinguish between different dynamic branches. In this case, the predictor is unable to specialize predictions for different dynamic branches, causing mispredictions if their outcomes differ. Ideally, the remedy is to predict a dynamic branch using its program counter (PC) and the addresses of its producer loads, since this context uniquely identifies the dynamic branch. We call this context the identity, or ID, of the dynamic branch. In general, producer loads are unlikely to have generated their addresses when the dynamic branch is fetched. We show that the ID of a distant retired branch in the global branch stream combined with recent global branch history, is effective context for predicting the current branch. (ii) Fixing the first problem exposes another problem. A store to an address on which a dynamic branch depends may flip its outcome when it is next encountered. With conventional passive updates, the branch suffers a misprediction before the predictor is retrained. We propose that stores to the memory addresses on which a dynamic branch depends, directly update its prediction in the predictor. This novel "active update" concept avoids mispredictions that are otherwise incurred by conventional passive training. We highlight two practical features that enable large EXACT predictors: the prediction path is scalably pipelinable by virtue of its decoupled indexing strategy, and active updates are tolerant of 100s of cycles of latency making it ideal for virtualizing this component in the general-purpose memory hierarchy. We also present a compact form of the predictor that caches only dynamic instances of a static branch that differ from its overall bias.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {165--176},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1787275.1787321},
 doi = {http://doi.acm.org/10.1145/1787275.1787321},
 acmid = {1787321},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch prediction, microarchitecture, superscalar processors},
} 

@inproceedings{Dinan:2010:HPP:1787275.1787323,
 author = {Dinan, James and Balaji, Pavan and Lusk, Ewing and Sadayappan, P. and Thakur, Rajeev},
 title = {Hybrid parallel programming with MPI and unified parallel C},
 abstract = {The Message Passing Interface (MPI) is one of the most widely used programming models for parallel computing. However, the amount of memory available to an MPI process is limited by the amount of local memory within a compute node. Partitioned Global Address Space (PGAS) models such as Unified Parallel C (UPC) are growing in popularity because of their ability to provide a shared global address space that spans the memories of multiple compute nodes. However, taking advantage of UPC can require a large recoding effort for existing parallel applications. In this paper, we explore a new hybrid parallel programming model that combines MPI and UPC. This model allows MPI programmers incremental access to a greater amount of memory, enabling memory-constrained MPI codes to process larger data sets. In addition, the hybrid model offers UPC programmers an opportunity to create static UPC groups that are connected over MPI. As we demonstrate, the use of such groups can significantly improve the scalability of locality-constrained UPC codes. This paper presents a detailed description of the hybrid model and demonstrates its effectiveness in two applications: a random access benchmark and the Barnes-Hut cosmological simulation. Experimental results indicate that the hybrid model can greatly enhance performance; using hybrid UPC groups that span two cluster nodes, RA performance increases by a factor of 1.33 and using groups that span four cluster nodes, Barnes-Hut experiences a twofold speedup at the expense of a 2\% increase in code size.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {177--186},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787323},
 doi = {http://doi.acm.org/10.1145/1787275.1787323},
 acmid = {1787323},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hybrid parallel programming, mpi, pgas, upc},
} 

@inproceedings{Neill:2010:HPS:1787275.1787324,
 author = {Neill, Richard and Shabarshin, Alexander and Carloni, Luca P.},
 title = {A heterogeneous parallel system running open mpi on a broadband network of embedded set-top devices},
 abstract = {We present a heterogeneous parallel computing system that combines a traditional computer cluster with a broadband network of embedded set-top box (STB) devices. As multiple service operators (MSO) manage millions of these devices across wide geographic areas, the computational power of such a massively-distributed embedded system could be harnessed to realize a centrally-managed, energy-efficient parallel processing platform that supports a variety of application domains which are of interest to MSOs, consumers, and the high-performance computing research community. We investigate the feasibility of this idea by building a prototype system that includes a complete head-end cable system with a DOCSIS-2.0 network combined with an interoperable implementation of a subset of Open MPI running on the STB embedded operating system. We evaluate the performance and scalability of our system compared to a traditional cluster by solving approximately various instances of the Multiple Sequence Alignment bioinformatics problem, while the STBs continue simultaneously to operate their primary functions: decode MPEG streams for television display and run an interactive user interface. Based on our experimental results and given the technology trends in embedded computing we argue that our approach to leverage a broadband network of embedded devices in a heterogeneous distributed system offers the benefits of both parallel computing clusters and distributed Internet computing.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {187--196},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787324},
 doi = {http://doi.acm.org/10.1145/1787275.1787324},
 acmid = {1787324},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed embedded systems, mpi, multiple sequence alignment, multiple service operators, set-top box},
} 

@inproceedings{Trachsel:2010:VCP:1787275.1787325,
 author = {Trachsel, Oliver and Gross, Thomas R.},
 title = {Variant-based competitive parallel execution of sequential programs},
 abstract = {Competitive parallel execution (CPE</i>) is a simple yet attractive technique to improve the performance of sequential programs on multi-core and multi-processor systems. A sequential program is transformed into a CPE-enabled program by introducing multiple variants</i> for parts of the program. The performance of different variants depends on runtime conditions, such as program input or the execution platform, and the execution time of a CPE-enabled program is the sum of the shortest variants. Variants compete at run-time under the control of a CPE-aware run-time system. The run-time system ensures that the behavior and outcome of a CPE-enabled program is not distinguishable from the one of its original sequential counterpart. We present and evaluate a run-time system that is implemented as a user-space library and that closely interacts with the operating system. The paper discusses two strategies for the generation of variants and investigates the applicability of CPE for two usage scenarios: i) computation-driven CPE: a simple and straightforward parallelization of heuristic algorithms, and ii) compiler-driven CPE: generation of CPE-enabled programs as part of the compilation process using different optimization strategies. Using a state-of-the-art SAT solver as an illustrative example, we report for compiler-based CPE speedups of 4-6\% for many data sets, with a maximum slowdown of 2\%. Computation-driven CPE provides super-linear speedups for 5 out of 31 data sets (with a maximum speedup of 7.4) and at most a slow-down of 1\% for two data sets.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {197--206},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787325},
 doi = {http://doi.acm.org/10.1145/1787275.1787325},
 acmid = {1787325},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive execution, algorithmic choice, multi-core processors, multi-variant execution, sequential programs, speculative execution},
} 

@inproceedings{Tipparaju:2010:EHG:1787275.1787326,
 author = {Tipparaju, Vinod and Apr\'{a}, Edoardo and Yu, Weikuan and Vetter, Jeffrey S.},
 title = {Enabling a highly-scalable global address space model for petascale computing},
 abstract = {Over the past decade, the trajectory to the petascale has been built on increased complexity and scale of the underlying parallel architectures. Meanwhile, software developers have struggled to provide tools that maintain the productivity of computational science teams using these new systems. In this regard, Global Address Space (GAS) programming models provide a straightforward and easy to use addressing model, which can lead to improved productivity. However, the scalability of GAS depends directly on the design and implementation of the runtime system on the target petascale distributed-memory architecture. In this paper, we describe the design, implementation, and optimization of the Aggregate Remote Memory Copy Interface (ARMCI) runtime library on the Cray XT5 2.3 PetaFLOPs computer at Oak Ridge National Laboratory. We optimized our implementation with the flow intimation</i> technique that we have introduced in this paper. Our optimized ARMCI implementation improves scalability of both the Global Arrays (GA) programming model and a real-world chemistry application - NWChem - from small jobs up through 180,000 cores.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {207--216},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787326},
 doi = {http://doi.acm.org/10.1145/1787275.1787326},
 acmid = {1787326},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {armci, flow control, ga, gas, global address space, global arrays, nwchem, pgas, xt5},
} 

@inproceedings{Kavadias:2010:OCS:1787275.1787328,
 author = {Kavadias, Stamatis G. and Katevenis, Manolis G.H. and Zampetakis, Michail and Nikolopoulos, Dimitrios S.},
 title = {On-chip communication and synchronization mechanisms with cache-integrated network interfaces},
 abstract = {Per-core local (scratchpad) memories allow direct inter-core communication, with latency and energy advantages over coherent cache-based communication, especially as CMP architectures become more distributed. We have designed cache-integrated network interfaces (NIs), appropriate for scalable multicores, that combine the best of two worlds the flexibility of caches and the efficiency of scratchpad memories: on-chip SRAM is configurably shared among caching, scratchpad, and virtualized NI functions. This paper presents our architecture, which provides local and remote scratchpad access, to either individual words or multi-word blocks through RDMA copy. Furthermore, we introduce event responses</i>, as a mechanism for software configurable synchronization primitives. We present three event response mechanisms that expose NI functionality to software, for multiword transfer initiation, memory barriers for explicitly-selected accesses of arbitrary size, and multi-party synchronization queues. We implemented these mechanisms in a four-core FPGA prototype, and evaluated the on-chip communication performance on the prototype as well as on a CMP simulator with up to 128 cores. We demonstrate efficient synchronization, low-overhead communication, and amortized-overhead bulk transfers, which allow parallelization gains for fine-grain tasks, and efficient exploitation of the hardware bandwidth.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {217--226},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787328},
 doi = {http://doi.acm.org/10.1145/1787275.1787328},
 acmid = {1787328},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache, explicit communication, inter-processor synchronization, network interface},
} 

@inproceedings{LaFratta:2010:MGL:1787275.1787329,
 author = {La Fratta, Patrick Anthony and Kogge, Peter M.},
 title = {Models for generating locality-tuned traveling threads for a hierarchical multi-level heterogeneous multicore},
 abstract = {As heterogeneous multicore processors become more widespread, many options are emerging for producing efficient parallel code for such processors. Although parallel programming languages are improving, manual partitioning of computations and data across heterogeneous processing resources is proving extraordinarily difficult. Further, it is becoming increasingly important to consider locality when producing parallel code, as data transport is a primary source of performance overhead and energy consumption. To address these problems, we propose a novel model for extracting parallel computations from sequential code for a hierarchical multi-level heterogeneous processor which we present called the Passive/Active Multicore</i> (PAM). The computations take the form of short, fine-grained threads, which are generated with consideration to locality through cache profiling and have the ability to migrate from core to core up through the memory hierarchy based on the location of operands. Experimental results across both integer and floating point intensive standard and scientific workloads show that the architecture, execution model, and computational extraction techniques together offer computational offloads of up to 24\% (5.8\% on average). Through simulation, we estimate these offloads may translate into speedups of up to 19\% (4.0\% on average) and that negative effects on performance are negligible. Floating point applications seem to be most aided by these techniques.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {227--236},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787329},
 doi = {http://doi.acm.org/10.1145/1787275.1787329},
 acmid = {1787329},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asymmetric multicore architectures, cache hierarchy design, locality-cognizant parallelization, migrant threads, multithreaded architectures},
} 

@inproceedings{Michaud:2010:PSA:1787275.1787330,
 author = {Michaud, Pierre and Sazeides, Yiannakis and Seznec, Andr\'{e}},
 title = {Proposition for a sequential accelerator in future general-purpose manycore processors and the problem of migration-induced cache misses},
 abstract = {As the number of transistors on a chip doubles with every technology generation, the number of on-chip cores also increases rapidly, making possible in a foreseeable future to design processors featuring hundreds of general-purpose cores. However, though a large number of cores speeds up parallel code sections, Amdahl's law requires speeding up sequential sections too. We argue that it will become possible to dedicate a substantial fraction of the chip area and power budget to achieve high sequential performance. Current general-purpose processors contain a handful of cores designed to be continuously active and run in parallel. This leads to power and thermal constraints that limit the core's performance. We propose removing these constraints with a sequential accelerator</i> (SACC). A SACC consists of several cores designed</i> for ultimate sequential performance. These cores cannot run continuously. A single core is active at any time, the rest of the cores are inactive and power-gated. We migrate the execution periodically to another core to spread heat generation uniformly over the whole SACC area, thus addressing the temperature issue. The SACC will be viable only if it yields significant sequential performance. Migration-induced cache misses may limit performance gains. We propose some solutions to mitigate this problem. We also investigate a migration method using thermal sensors, such that the migration interval depends on the ambient temperature and the migration penalty is negligible under normal thermal conditions.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {237--246},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787330},
 doi = {http://doi.acm.org/10.1145/1787275.1787330},
 acmid = {1787330},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {activity migration, cache misses, manycore, multicore, power, sequential performance, temperature},
} 

@inproceedings{Vasilieva:2010:HPQ:1787275.1787332,
 author = {Vasilieva, Alina and Mischenko-Slatenkova, Taisia},
 title = {High precision quantum query algorithm for computing AND-based boolean functions},
 abstract = {Quantum algorithms can be analyzed in a query model to compute Boolean functions. Function input is provided in a black box, and the aim is to compute the function value using as few queries to the black box as possible. The complexity of the algorithm is measured by the number of queries on the worst-case input. In this paper we consider computing AND</i> Boolean function. First, we present a quantum algorithm for AND</i> of two bits. Our algorithm uses one quantum query and correct result is obtained with a probability p</i>=4/5, that improves previous results. The main result is generalization of our approach to design efficient quantum algorithms for computing composite function AND(f<sub>1</sub>,f<sub>2</sub>)</i> where f<sub>i</sub></i> is a Boolean function. Finally, we demonstrate another kind of an algorithm for AND</i> of two variables, that has a correct answer probability p</i>=9/10, but cannot be extended to compute AND(f<sub>1</sub>,f<sub>2</sub>)</i>.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {247--256},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787332},
 doi = {http://doi.acm.org/10.1145/1787275.1787332},
 acmid = {1787332},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {algorithm complexity, algorithm design, boolean function, quantum computing, query algorithm},
} 

@inproceedings{Chen:2010:ROB:1787275.1787333,
 author = {Chen, Jiaoyan and Vasudevan, Dilip P. and Popovici, Emanuel and Schellekens, Michel},
 title = {Reversible online BIST using bidirectional BILBO},
 abstract = {Test generation for reversible circuits is currently gaining interest due to its feasibility towards quantum implementation and asymptotically zero-power dissipation. A novel BIST (Built-In-Self-Test) method for reversible circuits is proposed in this paper. New bidirectional D-latch and D-flipflop designs are introduced. A Reversible BILBO (Built-in-Logic-Block-Observer) based on conventional BILBO is designed to facilitate the BIST procedure. The complete test procedure is executed and experimental results are analyzed for both stuck at and missing gate faults (MGF) with 100\% fault coverage.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {257--266},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787333},
 doi = {http://doi.acm.org/10.1145/1787275.1787333},
 acmid = {1787333},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bilbo, bist, reversible logic, testing},
} 

@inproceedings{Rahmani:2010:PPO:1787275.1787335,
 author = {Rahmani, Amir-Mohammad and Liljeberg, Pasi and Plosila, Juha and Tenhunen, Hannu},
 title = {Power and performance optimization of voltage/frequency island-based networks-on-chip using reconfigurable synchronous/bi-synchronous FIFOs},
 abstract = {Distributing a single global clock across a chip while meeting the power requirements of the design is a troublesome task due to shrinking technology nodes associated with high clock frequencies. To deal with this, network-on-chip (NoC) architectures partitioned into several voltage-frequency islands (VFIs) have been proposed. To interface the islands on a chip, operating at different frequencies, a complex bi-synchronous FIFO design is inevitable. However, these FIFOs are not needed if adjacent switches belong to the same clock domain. In this paper, a Reconfigurable Synchronous/Bi-Synchronous (RSBS) FIFO is proposed which can adapt its operation to either synchronous or bi-synchronous mode. The FIFO is presented by three different scalable and synthesizable design styles and, in addition, some techniques are suggested to show how the FIFO could be utilized in a VFI-based NoC. Our analysis reveal that the RSBS FIFOs can help to achieve up to 15\% savings in the average power consumption of NoC switches and 29\% improvement in the total average packet latency in the case of MPEG-4 encoder application, when compared to a non-reconfigurable architecture.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {267--276},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787335},
 doi = {http://doi.acm.org/10.1145/1787275.1787335},
 acmid = {1787335},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {globally asynchronous locally synchronous (gals), low-power and high-performance design, networks-on-chip (nocs), reconfigurable fifos, voltage/frequency islands (vfis)},
} 

@inproceedings{Moeng:2010:ASM:1787275.1787336,
 author = {Moeng, Michael and Melhem, Rami},
 title = {Applying statistical machine learning to multicore voltage \&\#38; frequency scaling},
 abstract = {Dynamic Voltage/Frequency Scaling (DVFS) is a useful tool for improving system energy efficiency, especially in multi-core chips where energy is more of a limiting factor. Per-core DVFS, where cores can independently scale their voltages and frequencies, is particularly effective. We present a DVFS policy using machine learning, which learns the best frequency choices for a machine as a decision tree. Machine learning is used to predict the frequency which will minimize the expected energy per user-instruction (epui)</i> or energy per (user-instruction)<sup>2</sup> (epui2)</i>. While each core independently sets its frequency and voltage, a core is sensitive to other cores' frequency settings. Also, we examine the viability of using only partial training to train our policy, rather than full profiling for each program. We evaluate our policy on a 16-core machine running multiprogrammed, multithreaded benchmarks from the PARSEC benchmark suite against a baseline fixed frequency as well as a recently-proposed greedy policy. For 1ms DVFS intervals, our technique improves system epui2</i> by 14.4\% over the baseline no-DVFS policy and 11.3\% on average over the greedy policy.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {277--286},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787336},
 doi = {http://doi.acm.org/10.1145/1787275.1787336},
 acmid = {1787336},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {decision tree, multicore, power management},
} 

@inproceedings{Keramidas:2010:IMR:1787275.1787338,
 author = {Keramidas, Georgios and Spiliopoulos, Vasileios and Kaxiras, Stefanos},
 title = {Interval-based models for run-time DVFS orchestration in superscalar processors},
 abstract = {We develop two simple interval-based models for dynamic superscalar processors. These models allow us to: i) predict with great accuracy performance and power consumption under various frequency and voltage combinations and ii) implement targeted DVFS policies at run-time. The models analyze program execution in intervals - steady-state and miss-event intervals. Intervals are signalled by miss events (L2-misses in our case) that upset the "steady state" execution of the program and are ended when the pipeline reaches again a steady state. The first model is fed by an approximation of the stall cycles (the time the processor instruction window is blocked) due to long-latency L2-misses. The second model improves on this approximation using as input the occupancy of the L2's miss-handling registers (MSHRs). Despite their simplicity these models prove to be accurate in predicting the performance (and energy) for any target frequency/voltage setting, yielding average errors of 2.1\% and 0.2\% respectively. Besides modelling, we show that the methodology we propose is powerful enough to implement (at run-time) various DVFS policies: "operate at optimal EDP" or "ED<sup>2</sup>P," or even "reduce ED<sup>2</sup>P within specific performance constraints." Approaches based on the two models require minimal hardware cost: two counters for measuring the duration of the steady state and the miss-event intervals and some comparison logic. To validate our methodology we use a cycle-accurate simulator and the benchmarks provided by the SPEC2K suite. Our results indicate that our proposed run-time mechanism is able to orchestrate different DVFS policies with great success yielding negligible errors - bellow 1.5\% on average.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {287--296},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787338},
 doi = {http://doi.acm.org/10.1145/1787275.1787338},
 acmid = {1787338},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic voltage and frequency scaling, performance and power modeling, superscalar out-of-order processors},
} 

@inproceedings{Homayoun:2010:MSM:1787275.1787339,
 author = {Homayoun, Houman and Sasan, Avesta and Gupta, Aseem and Veidenbaum, Alex and Kurdahi, Fadi and Dutt, Nikil},
 title = {Multiple sleep modes leakage control in peripheral circuits of a all major SRAM-based processor units},
 abstract = {Leakage currents in on-chip SRAMs: caches, branch predictor, register files and TLBs, are major contributors to the energy dissipated by processors in deep sub-micron technologies. High leakage also increases chip temperature and some SRAM-based structures become thermal hotspots. Previous work has addressed major sources of SRAM leakage in memory cells and bit-lines, making remaining SRAM components, in particular large drivers, the primary source of leakage. This paper proposes an approach to reduce this source of leakage in all major SRAM-based units of the processor, controlling them in a uniform way, yet treating each unit individually based on its behavior and memory organization. The new approach uses multiple bias voltages in sleep transistors allowing a trade-off between leakage reduction and wakeup delay in multi-stage peripheral drivers. Four low-power modes are defined, from basic to ultra low power, and SRAMs dynamically transition between these modes to minimize leakage without sacrificing performance. A novel control mechanism monitors and predicts future processor behavior for mode control. The leakage reduction in individual units is evaluated and shown to vary from 25\% for IL1 to 75\% for L2 caches. Resulting temperature reduction, including the effect of positive feedback between temperature and leakage power, is evaluated. A significant temperature reduction is achieved in each unit. It is also shown to reduce hot spots in the instruction TLB and branch predictor.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {297--308},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1787275.1787339},
 doi = {http://doi.acm.org/10.1145/1787275.1787339},
 acmid = {1787339},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {leakage power, multiple sleep mode, peripheral circuits, sram memory, temperature reduction.},
} 

@inproceedings{Doh:2010:TGD:1787275.1787340,
 author = {Doh, In Hwan and Kim, Young Jin and Park, Jung Soo and Kim, Eunsam and Choi, Jongmoo and Lee, Donghee and Noh, Sam H.},
 title = {Towards greener data centers with storage class memory: minimizing idle power waste through coarse-grain management in fine-grain scale},
 abstract = {Studies have shown much of today's data centers are over-provisioned and underutilized. Over-provisioning cannot be avoided as these centers must anticipate peak load with bursty behavior. Under-utilization, to date, has also been unavoidable as systems always had to be ready for that sudden burst of requests that loom just around the corner. Previous research has pointed to turning off systems as one solution, albeit, an infeasible one due to its irresponsiveness. In this paper, we present the feasibility of using new Storage Class Memory (SCM, which encompasses specific developments such as PCM, MRAM, or FeRAM) technology to turn systems on and off with minimum overhead. This feature is used to control systems on the whole (in comparison to previous fine-grained component-wise control) in finer time scale for high responsiveness with minimized power lost to idleness. Our empirical study is done by executing "real trace"-like workloads on a prototype "data center" of embedded systems deploying FeRAM. We quantify the energy savings and performance trade-off by turning idle systems off. We show that our energy savings approach consumes energy in proportion to user requests with configurable service of quality. Based on observations made on this data center, we discuss the requirements for real deployment. Finally, our conclusion is that SCM should not be viewed as just a replacement of RAM, but rather, as a component that could potentially open a whole new field of applications.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {309--318},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787340},
 doi = {http://doi.acm.org/10.1145/1787275.1787340},
 acmid = {1787340},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {non-volatile ram (nvram), power management, servers, storage class memory (scm)},
} 

@inproceedings{Borodin:2010:PRO:1787275.1787342,
 author = {Borodin, Demid and Juurlink, Ben H.H.},
 title = {Protective redundancy overhead reduction using instruction vulnerability factor},
 abstract = {Due to modern technology trends, fault tolerance (FT) is acquiring an ever increasing research attention. To reduce the overhead introduced by the FT features, several techniques have been proposed. One of these techniques is Instruction-Level Fault Tolerance Configurability (ILCOFT). ILCOFT enables application developers to protect different instructions at varying degrees, devoting more resources to protect the most critical instructions, and saving resources by weakening protection of other instructions. It is, however, not trivial to assign a proper protection level for every instruction. This work introduces the notion of Instruction Vulnerability Factor (IVF)</i>, which evaluates how faults in every instruction affect the final application output. The IVF is computed off-line, and is then used by ILCOFT-enabled systems to assign the appropriate protection level to every instruction. IVF releases the programmer from the need to assign the necessary protection level to every instruction by hand. Experimental results demonstrate that IVF-based ILCOFT reduces the instruction duplication performance penalty by up to 77\%, while the maximum output damage due to undetected faults does not exceed 0.6\% of the total application output.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {319--326},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1787275.1787342},
 doi = {http://doi.acm.org/10.1145/1787275.1787342},
 acmid = {1787342},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fault detection, instruction vulnerability, performance, redundancy, selective protection},
} 

@inproceedings{Pani:2010:SOS:1787275.1787343,
 author = {Pani, Danilo and Secchi, Simone and Raffo, Luigi},
 title = {Self organization on a swarm computing fabric: a new way to look at fault tolerance},
 abstract = {Recent studies have demonstrated the possibility to exploit Swarm Intelligence (SI) as an inspiration for the design of scalable VLSI tiled architectures exhibiting multitasking, adaptability, absence of centralized low-level control and fault-tolerance. SI approach to fault-tolerance, in principle, can be regarded as a reconfiguration-free cell-exclusion mechanism. The key elements at the basis of a reconfiguration free solution are: loose structure of the system, homogeneity, cooperative behaviors and self organization. In this paper, these self organization aspects, introduced in a recently developed multi-agent VLSI tiled architecture for array processing, expressly developed resorting to the SI inspiration, are presented along with some theoretical and experimental results. The architecture presents two forms of cell-exclusion (bypass and block of faulty elements), implementing self-adaptive behaviors rather than reconfiguration to face faults preserving system functionality. The proposed approach, exploiting indirect communications to provide workload spreading into the computing fabric, is also successful in reducing the effects of the presence of faulty elements without spare resources and with limited performance degradation.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {327--336},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787343},
 doi = {http://doi.acm.org/10.1145/1787275.1787343},
 acmid = {1787343},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fault tolerance, reconfiguration, self organization, swarm intelligence},
} 

@inproceedings{Jordan:2010:DLM:1787275.1787344,
 author = {Jordan, Herbert and Prodan, Radu and Nae, Vlad and Fahringer, Thomas},
 title = {Dynamic load management for MMOGs in distributed environments},
 abstract = {To support thousands of concurrent players in virtual worlds simulated by contemporary Massively Multiplayer Online Games, most implementations employ static game world partitioning for distributing the load among multiple game server instances. Further, the resources that manage the resulting subregions are statically allocated, independent of the actual game load. As a result, due to the high variability of the user demand, this approach leads to a low resource utilization causing much higher provisioning costs than necessary. In addition, the number of players supported by a region is limited by the maximum load that can be handled by a single server instance. We propose in this paper a novel game load management technique divided in two (global and a local) layers, capable of dynamically adjusting the amount of allocated resources to the present user demand. The global level assigns the responsibility of serving particular game regions to data centers using a peer-to-peer infrastructure, while the local level within individual facilities maintains the necessary server instances for the assigned obligations. We device two generic heuristics based on the well-known bin-packing problem to achieve the ultimate goal of maximizing the resource utilization on both levels while maintaining user-level Quality of Service (QoS). We evaluate the performance of our proposed solution using simulation-based experiments, which demonstrate a potential cost reduction in maintaining MMOG sessions by up to 60\% while maintaining QoS in 99\% of the cases.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {337--346},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787344},
 doi = {http://doi.acm.org/10.1145/1787275.1787344},
 acmid = {1787344},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {load balancing, mmogs},
} 

@inproceedings{Stone:2010:SSC:1787275.1787345,
 author = {Stone, Andrew Ian and DiBenedetto, Steven and Mills Strout, Michelle and Massey, Daniel},
 title = {Scalable simulation of complex network routing policies},
 abstract = {Modern routing protocols for the internet implement complex policies that take more into account than just path length. However, current routing protocol simulators are limited to either working with hard-coded policies or working on small networks (1000 nodes or less). It is currently not possible to ask questions about how the routing tables will change on all of the autonomous systems (e.g., AT\&#38;T, Sprint, etc.) in the internet, given a change in the routing protocol. This paper presents a routing policy simulation framework that enables such simulations to be done on resources that are readily available to researchers, such as a small set of typical desktops. We base the policy simulation framework on the Routing Algebra Meta-Language (RAML), which is a formal framework for specifying routing policies. Our theoretical contributions include proving that the signatures and the meet operation induced by the preference operator in RAML define a semilattice and that routing policy simulation frameworks are analogous to data-flow analysis frameworks. The main problem we address is that direct implementation of routing policy simulation has scaling issues due to the O(n^2) memory requirements for routing tables. However, due to properties of routing algebras specified in RAML, we are able to segment the simulation problem into multiple runs that propagate route information for subsets of the network on each run. This strategy enables us to perform a simulation that does not exceed system memory on typical desktops and enables the 43 minute, parallel simulation of a real network topology (33k nodes) and an approximation of the common BGP protocol.},
 booktitle = {Proceedings of the 7th ACM international conference on Computing frontiers},
 series = {CF '10},
 year = {2010},
 isbn = {978-1-4503-0044-5},
 location = {Bertinoro, Italy},
 pages = {347--356},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1787275.1787345},
 doi = {http://doi.acm.org/10.1145/1787275.1787345},
 acmid = {1787345},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data-flow analysis, metarouting, parallel, performance, routing, simulation},
} 

@inproceedings{RonnyRonen:2009:LMI:1531743.1531744,
 author = {Ronny Ronen, Ronny},
 title = {Larrabee: a many-core Intel\&\#174; architecture for visual computing},
 abstract = {The ample supply of transistors provided by advancements in process technology, combined with the increased difficultly to exploit single thread performance, moved the industry to populate several cores on a single die. This talk presents Larrabee -- the next bold step in this direction. Larrabee is a many-core visual computing architecture. Larrabee uses multiple in-order X86 CPU cores that are augmented by a wide vector processor unit, as well as some fixed function logic blocks. This provides dramatically higher performance per watt and per unit of area than out-of-order CPUs on highly parallel workloads. It also greatly increases the flexibility and programmability of the architecture as compared to standard GPUs. A coherent on-die 2nd level cache allows efficient inter-processor communication and high-bandwidth local data access by CPU cores. The customizable software graphics rendering pipeline for this architecture uses binning in order to reduce required memory bandwidth, and increase opportunities for parallelism relative to standard GPUs. The Larrabee native programming model supports a variety of highly parallel applications that use irregular data structures.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {225--225},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1531743.1531744},
 doi = {http://doi.acm.org/10.1145/1531743.1531744},
 acmid = {1531744},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {graphics architecture, many-core computing, parrallel processing, processor arechitecture, software rendering},
} 

@inproceedings{Jahre:2009:LFM:1531743.1531747,
 author = {Jahre, Magnus and Natvig, Lasse},
 title = {A light-weight fairness mechanism for chip multiprocessor memory systems},
 abstract = {Chip Multiprocessor (CMP) memory systems suffer from the effects of destructive thread interference. This interference reduces performance predictability because it depends heavily on the memory access pattern and intensity of the co-scheduled threads. In this work, we confirm that all shared units must be thread-aware in order to provide memory system fairness. However, the current proposals for fair memory systems are complex as they require an interference measurement mechanism and a fairness enforcement policy for all hardware-controlled shared units. Furthermore, they often sacrifice system throughput to reach their fairness goals which is not desirable in all systems. In this work, we show that our novel fairness mechanism, called the Dynamic Miss Handling Architecture (DMHA), is able to reduce implementation complexity by using a single fairness enforcement policy for the complete hardware-managed shared memory system. Specifically, it controls the total miss bandwidth available to each thread by dynamically manipulating the number of Miss Status Holding Registers (MSHRs) available in each private data cache. When fairness is chosen as the metric of interest and we compare to a state-of-the-art fairness-aware memory system, DMHA improves fairness by 26\% on average with the single program baseline. With a different configuration, DMHA improves throughput by 13\% on average compared to a conventional memory system.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1531743.1531747},
 doi = {http://doi.acm.org/10.1145/1531743.1531747},
 acmid = {1531747},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chip multiprocessor, dynamic miss handling architecture, fairness, interference, mechanism, miss status holding register},
} 

@inproceedings{Pant:2009:ECT:1531743.1531748,
 author = {Pant, Salil Mohan and Byrd, Gregory T.},
 title = {Extending concurrency of transactional memory programs by using value prediction},
 abstract = {Transactional Memory (TM) is an optimistic speculative synchronization scheme that provides atomic execution for a region of code marked as a transaction by the programmer. TM avoids many of the problems associated with lock-based synchronization and can make writing parallel programs relatively easier. Programs with critical sections that are not heavily contended benefit from the optimistic nature of TM systems. However, for heavily contended critical sections, performance can degrade due to conflicts leading to stalls and expensive rollbacks. In this paper, we look into the nature of the shared data involved in conflicts for TM systems. We find that most transactions have conflicts around a few shared addresses, and shared-conflicting data is often updated in a predictable manner by different transactions. We propose using a memory-level value predictor to capture this predictability for such data structures and increase overall concurrency by satisfying loads from conflicting transactions with predicted values, instead of stalling. In this paper, we present one possible design and implementation of a TM system with a value predictor. Our benchmark results show that the value predictor can capture this predictable behavior for most benchmarks and can improve performance of TM programs by improving concurrency and minimizing stalls and rollbacks.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {11--20},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1531743.1531748},
 doi = {http://doi.acm.org/10.1145/1531743.1531748},
 acmid = {1531748},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {logtm, transactional memory, value prediction},
} 

@inproceedings{Liu:2009:SEG:1531743.1531749,
 author = {Liu, Shaoshan and Wang, Ligang and Li, Xiao-Feng and Gaudiot, Jean-Luc},
 title = {Space-and-time efficient garbage collectors for parallel systems},
 abstract = {As multithreaded server applications and runtime systems prevail, garbage collection is becoming an essential feature to support high performance systems. The fundamental issue of garbage collector (GC) design is to maximize the recycled space with minimal time overhead. This paper proposes two innovative solutions: one to improve space efficiency, and the other to improve time efficiency. To achieve space efficiency, we propose the Space Tuner that utilizes the novel concept of allocation speed to reduce wasted space. Conventional static space partitioning techniques often lead to inefficient space utilization. The Space Tuner adjusts the heap partitioning dynamically such that when a collection is triggered, all space partitions are fully filled. To achieve time efficiency, we propose a novel parallelization method that reduces the compacting GC parallelization problem into a tree traversal parallelization problem. This method can be applied for both normal and large object compaction. Object compaction is hard to parallelize due to strong data dependencies such that the source object can not be moved to its target location until the object originally in the target location has been moved out. Our proposed algorithm overcomes the difficulties by dividing the heap into equal-sized blocks and parallelizing the movement of the independent blocks. It is noteworthy that these proposed algorithms are generic such that they can be utilized in different GC designs.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {21--30},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1531743.1531749},
 doi = {http://doi.acm.org/10.1145/1531743.1531749},
 acmid = {1531749},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {garbage collection, java virtual machine},
} 

@inproceedings{Shebanow:2009:PMM:1531743.1531745,
 author = {Shebanow, Michael C.},
 title = {Pervasive massively multithreaded GPU processors},
 abstract = {This talk presents an overview of NVIDIA's SIMT architecture and some brief insights on how some CUDA programming paradigms map onto it. A brief history of SIMT is provided to explain how NVIDIA ended up implementing a unified SIMT processor core in its GPUs including how graphics shaders are mapped onto SIMT threads. In addition, a conceptual view of how a SIMT microarchitecture executes threads in parallel is provided. The talk wraps up by describing some pitfalls related to thread synchronization, memory access, and cache management and describes some key problem areas in SIMT programming that NVIDIA would like to address in the future.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {227--227},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1531743.1531745},
 doi = {http://doi.acm.org/10.1145/1531743.1531745},
 acmid = {1531745},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CUDA, GPU, SIMT, parallel processing},
} 

@inproceedings{West:2009:CMM:1531743.1531751,
 author = {West, Paul E. and Peress, Yuval and Tyson, Gary S. and McKee, Sally A.},
 title = {Core monitors: monitoring performance in multicore processors},
 abstract = {As we reach the limits of single-core computing, we are promised more and more cores in our systems. Modern architectures include many performance counters per core, but few or no inter-core counters. In fact, performance counters were not designed to be exploited by users, as they now are, but simply as aids for hardware debugging and testing during system creation. As such, they tend to be an "after thought" in the design, with no standardization across or within platforms. Nonetheless, given access to these counters, researchers are using them to great advantage [17]. Furthermore, evaluating counters for multicore systems has become a complex and resource consuming task. We propose a Performance Monitoring System consisting of a specialized CPU core designed to allow efficient collection and evaluation of performance data for both static and dynamic optimizations. Our system provides a transparent mechanism to change architectural features dynamically, inform the Operating System of process behaviors, and assist in profiling and debugging. For instance, a piece of hardware watching snoop packets can determine when a write-update cache coherence protocol would be helpful or detrimental to the currently running program. Our system is designed to allow the hardware to feed performance statistics back to software, allowing dynamic architectural adjustments at runtime.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {31--40},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1531743.1531751},
 doi = {http://doi.acm.org/10.1145/1531743.1531751},
 acmid = {1531751},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache coherency, debugging, multicore, performance monitoring, profiling, realtime, scheduling},
} 

@inproceedings{Tian:2009:SOC:1531743.1531752,
 author = {Tian, Kai and Jiang, Yunlian and Shen, Xipeng},
 title = {A study on optimally co-scheduling jobs of different lengths on chip multiprocessors},
 abstract = {Cache sharing in Chip Multiprocessors brings cache contention among corunning processes, which often causes considerable degradation of program performance and system fairness. Recent studies have seen the effectiveness of job co-scheduling in alleviating the contention. But finding optimal schedules is challenging. Previous explorations tackle the problem under highly constrained settings. In this work, we show that relaxing those constraints, particularly the assumptions on job lengths and reschedulings, increases the complexity of the problem significantly. Subsequently, we propose a series of algorithms to compute or approximate the optimal schedules in the more general setting. Specifically, we propose an A*-based approach to accelerating the search for optimal schedules by as much as several orders of magnitude. For large problems, we design and evaluate two approximation algorithms, A*-cluster and local-matching algorithms, to effectively approximate the optimal schedules with good accuracy and high scalability. This study contributes better understanding to the optimal co-scheduling problem, facilitates the evaluation of co-scheduling systems, and offers insights and techniques for the development of practical co-scheduling algorithms.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {41--50},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1531743.1531752},
 doi = {http://doi.acm.org/10.1145/1531743.1531752},
 acmid = {1531752},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {a*-search, cache contention, cmp scheduling, co-scheduling, perfect matching},
} 

@inproceedings{Chiu:2009:MSA:1531743.1531753,
 author = {Chiu, Jih-Ching and Chou, Yu-Liang and Tzeng, Hua-Yi},
 title = {A multi-streaming SIMD architecture for multimedia applications},
 abstract = {Current MMX-like extensions provide a mechanism for general purpose processors to meet the growing performance demand of multimedia applications. However, the computing performance of these extensions is often limited because they only operate on a single data stream. To overcome this obstacle, this paper presents an architecture named "multi-streaming SIMD architecture" that enables one SIMD instruction to simultaneously manipulate multiple data streams. The proposed architecture is a Processor-In-Memory-like register-file architecture including SIMD operating logics for general-purposed processors to further extend current MMX-like extensions to obtain high performance. To efficiently and flexibly realize the proposed architecture, an operation cell is designed by fusing the logic gates and the storage cells together. The operation cells then are used to compose a register file with the ability of performing SIMD operations called "Multimedia Operation Storage Unit (MOSU)". Further, many MOSUs are used to compose a multi-streaming SIMD computing engine that can simultaneously manipulate multiple data streams and exploit the subword parallelisms of the elements in each data stream. Three instruction modes (global, coupling, and isolated modes) are defined for the MMX-like extensions to modulate the amount of parallel data streams and to efficiently utilize the computation resources. Simulation results show that when the multi-streaming SIMD architecture has four 4-register MOSUs, it provides a factor of 3.3x to 5.5x performance improvement compared with Intel's MMX extensions on eleven multimedia kernels.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {51--60},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1531743.1531753},
 doi = {http://doi.acm.org/10.1145/1531743.1531753},
 acmid = {1531753},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SIMD, mmx, multimedia extensions, pim, processor-in-memory, streaming computing, streaming processing},
} 

@inproceedings{SanchezCastano:2009:QAS:1531743.1531755,
 author = {S\'{a}nchez Casta\~{n}o, Friman and Ramirez, Alex and Valero, Mateo},
 title = {Quantitative analysis of sequence alignment applications on multiprocessor architectures},
 abstract = {The exponential growth of databases that contains biological information (such as protein and DNA data) demands great efforts to improve the performance of computational platforms. In this work we investigate how bioinformatics applications benefit from parallel architectures that combine different alternatives to exploit coarse- and fine-grain parallelism. As a case of analysis we study the performance behavior of the Ssearch application that implements the Smith-Waterman algorithm, which is a dynamic programing approach that explores the similarity between a pair of sequences. The inherent large parallelism of the algorithm makes it ideal for architectures supporting multiple dimensions of parallelism (TLP, DLP and ILP). We study how this algorithm can take advantage of different parallel machines like the SGI Altix, IBM Power6, Cell BE machines and MareNostrum. Our results show that a share memory architecture like the PowerPC 970MP of Marenostrum can surpass a heterogeneous machine like the current Cell BE. Our quantitative analysis includes not only a study of scalability of the performance in terms of speedup, but also includes the analysis of bottlenecks in the execution of the application. This analysis is carried out through the study of the execution phases that the application presents.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {61--70},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1531743.1531755},
 doi = {http://doi.acm.org/10.1145/1531743.1531755},
 acmid = {1531755},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bioinformatics applications, multiprocessor architectures, parallel architectures, sequence comparison},
} 

@inproceedings{Venetis:2009:MLD:1531743.1531756,
 author = {Venetis, Ioannis E. and Gao, Guang R.},
 title = {Mapping the LU decomposition on a many-core architecture: challenges and solutions},
 abstract = {Recently, multi-core architectures with alternative memory subsystem designs have emerged. Instead of using hardware-managed cache hierarchies, they employ software-managed embedded memory. An open question is what programming and compiling methods are effective to exploit the performance potential of this new class of architectures. Using the LU decomposition as a case study, we propose three techniques that combined achieve a 27 times speedup on the IBM Cyclops-64 many-core architecture, compared to the parallel LU implementation from the SPLASH-2 benchmarks suite. Our first method allows adaptive load distribution, which maximizes load-balance among cores - this is important to leverage the potential of the next two methods. Secondly, we developed a method for register tiling that determines the optimal data tile parameters and maximizes data reuse according to register file size constraints. We demonstrate that our method is inherently general and that it should have a much broader applicability beyond Cyclops-64. Thirdly, we present a register allocation method for register tiled loop bodies. We evaluate its effect through hand-tuned Cyclops-64 assembly code and observe a 6-fold reduction in load/store operations. We achieve a performance of 19.17 and 27.50 GFlops with double-precision floating point numbers, for a 700x700 and a 1000x1000 matrix respectively.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {71--80},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1531743.1531756},
 doi = {http://doi.acm.org/10.1145/1531743.1531756},
 acmid = {1531756},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {LU decomposition, load balancing, local memory, multi-core, register tiling},
} 

@inproceedings{Caraiman:2009:NAQ:1531743.1531757,
 author = {Caraiman, Simona and Manta, Vasile I.},
 title = {New applications of quantum algorithms to computer graphics: the quantum random sample consensus algorithm},
 abstract = {The inherent parallelism of quantum systems determined not only the investigation of innovative applications that can be developed using these high performance computing systems, but also of ways to improve the performances over the classical case. Exploiting this parallelism recently led to the emergence of innovative ideas in the field of computer graphics, sketching the development of quantum rendering and quantum computational geometry. Following these tracks, we propose a new quantum algorithm for the RANdom SAmple Consensus (RANSAC) voting scheme. In this paper we show that by exploiting the unique features of quantum computing, generating uniform superpositions of states in the problem space and applying quantum operators to all states simultaneously, the performance of our quantum algorithm is orders of magnitude faster than the classical variant.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {81--88},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1531743.1531757},
 doi = {http://doi.acm.org/10.1145/1531743.1531757},
 acmid = {1531757},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer graphics, quantum algorithms, quantum computing, ransac},
} 

@inproceedings{Rul:2009:TAP:1531743.1531759,
 author = {Rul, Sean and Vandierendonck, Hans and De Bosschere, Koen},
 title = {Towards automatic program partitioning},
 abstract = {There is a trend towards using accelerators to increase performance and energy efficiency of general-purpose processors. Adoption of accelerators, however, depends on the availability of tools to facilitate programming these devices. In this paper, we present techniques for automatically partitioning programs for execution on accelerators. We call the off-loaded code regions sub-algorithms, which are parts of the program that are loosely connected to the remainder of the program. We present three heuristics for automatically identifying sub-algorithms based on control flow and data flow properties. Analysis of SPECint and MiBench benchmarks shows that on average 12 sub-algorithms are identified (up to 54), covering the full execution time for 27 out of 30 benchmarks. We show that these sub-algorithms are suitable for off-loading them to accelerators by manually implementing sub-algorithms for 2 SPECint benchmarks on the Cell processor.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {89--98},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1531743.1531759},
 doi = {http://doi.acm.org/10.1145/1531743.1531759},
 acmid = {1531759},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {accelerators, off-loading, partitioning, sub-algorithms},
} 

@inproceedings{Sun:2009:NSS:1531743.1531760,
 author = {Sun, Hongyang and Cao, Yangjie and Hsu, Wen-Jing},
 title = {Non-clairvoyant speed scaling for batched parallel jobs on multiprocessors},
 abstract = {Energy consumption and heat dissipation have become key considerations for modern high performance computer systems. In this paper, we focus on non-clairvoyant speed scaling to minimize flow time plus energy for batched parallel jobs on multiprocessors. We consider a common scenario where the total power consumption cannot exceed a given budget and the power consumed on each processor is s<sup>\&#945;</sup> when running at speed s</i>. Extending the EQUI processor allocation policy, we propose two algorithms: U-EQUI and N-EQUI, which use respectively a uniform-speed and a non-uniform speed scaling function for the allocated processors. Using competitive analysis, we show that U-EQUI is O</i>(P<sup>\&#945;-1)/\&#945;<sup>2</sup>)</sup>-competitive for flow time plus energy, and N-EQUI is O</i>(<sup>\&#945;</sup>\&#8730;lnP</i>)-competitive for the same metric when given sufficient power, where P</i> is the total number of processors. Our simulation results confirm that U-EQUI and N-EQUI achieve better performance than a straightforward fixed-speed EQUI strategy. Moreover, moderate power constraint does not significantly affect the performance of our algorithms.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {99--108},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1531743.1531760},
 doi = {http://doi.acm.org/10.1145/1531743.1531760},
 acmid = {1531760},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {batched parallel jobs, energy consumption, flow time, multiprocessors, non-clairvoyant, power budget, scheduling, simulations, speed scaling},
} 

@inproceedings{Karantasis:2009:PCM:1531743.1531761,
 author = {Karantasis, Konstantinos I. and Polychronopoulos, Eleftherios D.},
 title = {Pleiad: a cross-environment middleware providing efficient multithreading on clusters},
 abstract = {The engagement of cluster and grid computing, two popular trends of today's high performance computation, has formed an imperative need for efficient utilization of the afforded resources. In this paper we present the concept, design and implementation of the Pleiad platform. Having its origin in the proposition of distributed shared memory (DSM), Pleiad is a cluster middleware that provides shared memory abstraction which enables transparent multithreaded execution across the cluster nodes. It belongs to the new generation of cluster middleware that aside from providing the proof of concept regarding unification of the cluster memory resources, they aim to achieve satisfactory levels of performance and scalability for a broad range of multithreaded applications. First results from the performance evaluation of Pleiad appear emboldening and they are presented in comparison with an efficient implementation of MPI for the Java platform.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {109--116},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1531743.1531761},
 doi = {http://doi.acm.org/10.1145/1531743.1531761},
 acmid = {1531761},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cluster middleware, java, shared memory programming},
} 

@inproceedings{Trancoso:2009:DPA:1531743.1531763,
 author = {Trancoso, Pedro and Othonos, Despo and Artemiou, Artemakis},
 title = {Data parallel acceleration of decision support queries using Cell/BE and GPUs},
 abstract = {Decision Support System (DSS) workloads are known to be one of the most time-consuming database workloads that processes large data sets. Traditionally, DSS queries have been accelerated using large-scale multiprocessor. The topic addressed in this work is to analyze the benefits of using high-performance/low-cost processors such as the GPUs and the Cell/BE to accelerate DSS query execution. In order to overcome the programming effort of developing code for different architectures, in this work we explore the use of a platform, Rapidmind, which offers the possibility of executing the same program on both Cell/BE and GPUs. To achieve this goal we propose data-parallel versions of the original database scan and join algorithms. In our experimental results we compare the execution of three queries from the standard DSS benchmark TPC-H on two systems with two different GPU models, a system with the Cell/BE processor, and a system with dual quad-core Xeon processors. The results show that parallelism can be well exploited by the GPUs. The speedup values observed were up to 21x compared to a single processor system.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {117--126},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1531743.1531763},
 doi = {http://doi.acm.org/10.1145/1531743.1531763},
 acmid = {1531763},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Cell/BE, GPU, data-parallel model, decision support system, performance evaluation, rapidmind},
} 

@inproceedings{Theodoropoulos:2009:WFS:1531743.1531764,
 author = {Theodoropoulos, Dimitris and Ciobanu, Catalin Bogdan and Kuzmanov, Georgi},
 title = {Wave field synthesis for 3D audio: architectural prospectives},
 abstract = {In this paper, we compare the architectural perspectives of the Wave Field Synthesis (WFS) 3D-audio algorithm mapped on three different platforms: a General Purpose Processor (GPP), a Graphics Processor Unit (GPU) and a Field Programmable Gate Array (FPGA). Previous related work reveals that, up to now, WFS sound systems are based on standard PCs. However, on one hand, contemporary GPUs consist of many multiprocessors that can process data concurrently. On the other hand, recent FPGAs provide huge level of parallelism, and reasonably high performance potentials, which can be exploited very efficiently by smart designers. Furthermore, new parallel programming environments, such as the Compute Unified Device Architecture (CUDA) from NVidia and the Stream from ATI, give to the researchers full access to the GPU resources. We use the CUDA to map the WFS kernel on a GeForce 8600GT GPU. Additionally, we implement a reconfigurable and scalable hardware accelerator for the same kernel, and map it onto Virtex4 FPGAs. We compare both architectural approaches against a baseline GPP implementation on a Pentium D at 3.4 GHz. Our conclusion is that in highly demanding WFS-based audio systems, a low-cost GeForce 8600GT desktop GPU can achieve a speedup of up to 8x comparing to a modern Pentium D implementation. An FPGA-based WFS hardware accelerator consisting of a single rendering unit (RU), can provide a speedup of up 10x comparing to the Pentium D approach. It can fit into small FPGAs and consumes approximately 3 Watts. Furthermore, cascading multiple RUs into a larger FPGA, can boost processing throughput up to more than two orders of magnitude higher than a GPP-based implementation and an order of magnitude better than a low-cost GPU one.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {127--136},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1531743.1531764},
 doi = {http://doi.acm.org/10.1145/1531743.1531764},
 acmid = {1531764},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3D audio, general purpose GPU computing, reconfigurable computing, wave field synthesis},
} 

@inproceedings{Moazeni:2009:ATV:1531743.1531765,
 author = {Moazeni, Maryam and Bui, Alex and Sarrafzadeh, Majid},
 title = {Accelerating total variation regularization for matrix-valued images on GPUs},
 abstract = {The advent of new matrix-valued magnetic resonance imaging modalities such as Diffusion Tensor Imaging (DTI) requires extensive computational acceleration. Computational acceleration on graphics processing units (GPUs) can make the regularization (denoising) of DTI images attractive in clinical settings, hence improving the quality of DTI images in a broad range of applications. Construction of DTI images consists of direction-specific Magnetic Resonance (MR) measurements. Compared with conventional MR, direction-sensitive acquisition has a lower signal-to-noise ratio (SNR). Therefore, high noise levels often limit DTI imaging. Advanced post-processing of imaging data can improve the quality of estimated tensors. However, the post-processing problem is only made more computationally difficult when considering matrix-valued imaging data. This paper describes the acceleration of a Total Variation regularization method for matrix-valued images, in particular, for DTI images on NVIDIA Quadro FX 5600. The TV regularization of a 3-D image with 128<sup>3</sup> voxels ultimately achieves 266X speedup and requires 1 minute and 30 seconds on the Quadro, while this algorithm on a dual-core CPU completes in more than 3 hours. In this application study we are aimed at analyzing the effective of excessive synchronization, which provides an insight into generally adapting Variational methods to the GPU architecture for other image processing algorithms designed for matrix-valued images.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {137--146},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1531743.1531765},
 doi = {http://doi.acm.org/10.1145/1531743.1531765},
 acmid = {1531765},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cuda, gpgpu, gpu computing, tv regularization, variational methods},
} 

@inproceedings{Carrillo:2009:CSO:1531743.1531766,
 author = {Carrillo, Snaider and Siegel, Jakob and Li, Xiaoming},
 title = {A control-structure splitting optimization for GPGPU},
 abstract = {Control statements in a GPU program such as loops and branches pose serious challenges for the efficient usage of GPU resources because those control statements will lead to the serialization of threads and consequently ruin the occupancy of GPU, that is, the number of threads running concurrently. Unlike traditional vector processing units that are inside a general purpose processor, the GPU cannot leave the control statements to the CPU because fine-grain statement scheduling between GPU and CPU is impossible. We need an effective method to handle the control statements "just in place" on the GPUs. In this paper, we propose novel techniques to transform control statements so that they can be executed efficiently on GPUs. Our techniques smartly increase code redundancy, which might be deemed as "de-optimization" for CPU, to improve the occupancy of a program on GPU and therefore improve performance. We focus our attention on how common programming structures such as loops and branches decrease the occupancy of single kernels and how to counter that. We demonstrate our optimizations on a synthetic benchmark and a complex parallel algorithm, the Lattice Boltzmann Method (LBM). Our results show that these techniques are very efficient and can lead to an increase in occupancy and a drastic improvement in performance compared to non-split version of the programs.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {147--150},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1531743.1531766},
 doi = {http://doi.acm.org/10.1145/1531743.1531766},
 acmid = {1531766},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cuda, gpgpu, optimizations},
} 

@inproceedings{Suri:2009:IPS:1531743.1531768,
 author = {Suri, Tameesh and Aggarwal, Aneesh},
 title = {Improving performance of simple cores by exploiting loop-level parallelism through value prediction and reconfiguration},
 abstract = {There is a growing trend towards designing simpler CPU cores that have considerable area, complexity, and power advantages. These cores are then leveraged in large-scale multicore processors or in SoCs for hand-held devices. The most significant limitation of such simple CPU cores is their lower performance. In this paper, we propose a technique to improve the performance of simple cores with minimal increase in complexity and area. In particular, we integrate a Reconfigurable Hardware Unit (RHU) that exploits loop-level parallelism to increase the core's overall performance. The RHU is reconfigured to execute instructions with highly predictable operand values from the future iterations of loops. Our experiments show that the proposed architecture improves the performance by an average of about 51\% across a wide range of applications, while incurring a area overhead of only about 5.6\%.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {151--160},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1531743.1531768},
 doi = {http://doi.acm.org/10.1145/1531743.1531768},
 acmid = {1531768},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data value prediction, dynamic reconfiguration, loop level parallelism},
} 

@inproceedings{Blagojevic:2009:SDP:1531743.1531769,
 author = {Blagojevic, Filip and Iancu, Costin and Yelick, Katherine and Curtis-Maury, Matthew and Nikolopoulos, Dimitrios S. and Rose, Benjamin},
 title = {Scheduling dynamic parallelism on accelerators},
 abstract = {Resource management on accelerator based systems is complicated by the disjoint nature of the main CPU and accelerator, which involves separate memory hierarhcies, different degrees of parallelism, and relatively high cost of communicating between them. For applications with irregul parallelism, where work is dynamically created based on other computations, the accelerators may both consume and produce work. To maintain load balance, the accelerators hand work back to the CPU to be scheduled. In this paper we consider multiple approaches for such scheduling problems and use the Cell BE system to demonstrate the different schedulers and the trade-offs between them. Our evaluation is done with both microbenchmarks and two bioinformatics applications (PBPI and RAxML). Our baseline approach uses a standard Linux scheduler on the CPU, possibly with more than one process per CPU. We then consider the addition of cooperative scheduling to the Linux kernel and a user-level work-stealing approach. The two cooperative approaches are able to decrease SPE idle time, by 30\% and 70\%, respectively, relative to the baseline scheduler. In both cases we believe the changes required to application level codes, e.g., a program written with MPI processes that use accelerator based compute nodes, is reasonable, although the kernel level approach provides more generality and ease of implementation, but often less performance than work stealing approach.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {161--170},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1531743.1531769},
 doi = {http://doi.acm.org/10.1145/1531743.1531769},
 acmid = {1531769},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cell be, cooperative scheduling},
} 

@inproceedings{Antonelli:2009:PCR:1531743.1531770,
 author = {Antonelli, Dominic A. and Smith, Alan Jay and van de Waerdt, Jan-Willem},
 title = {Power consumption and reduction in a real, commercial multimedia core},
 abstract = {Peak power and total energy consumption are key factors in the design of embedded microprocessors. Many techniques have been shown to provide great reductions in peak power and/or energy consumption. Unfortunately, several unrealistic assumptions are often made in research studies, especially in regards to multimedia processors. This paper focuses on power reduction in real commercial processors, and how that differs from more abstract research studies. We study the power consumption of the TriMedia TM3270, an embedded, synthesized microprocessor used in several commercial products, on both and simulation tools. We find that increased functional unit utilization and memory access density causes significant differences in power consumption between compiler-optimized and carefully hand-optimized code. We also apply some simple techniques for power savings with no performance degradation, though the focus of the paper is the evaluation of such techniques, not the techniques themselves.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {171--174},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1531743.1531770},
 doi = {http://doi.acm.org/10.1145/1531743.1531770},
 acmid = {1531770},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache memory, compiler optimization, embedded processor, energy, multimedia, power, tm3270},
} 

@inproceedings{Cornwall:2009:HSC:1531743.1531772,
 author = {Cornwall, Jay L.T. and Howes, Lee and Kelly, Paul H.J. and Parsonage, Phil and Nicoletti, Bruno},
 title = {High-performance SIMT code generation in an active visual effects library},
 abstract = {SIMT (Single-Instruction Multiple-Thread) is an emerging programming paradigm for high-performance computational accelerators, pioneered in current and next generation GPUs and hybrid CPUs. We present a domain-specific active-library supported approach to SIMT code generation and optimisation in the field of visual effects. Our approach uses high-level metadata and runtime context to guide and to ensure the correctness of optimisation-driven code transformations and to implement runtime-context-sensitive optimisations. Our advanced optimisations require no analysis of the original C++ kernel code and deliver 1.3x to 6.6x speedups over syntax-directed translation on GeForce 8800 GTX and GTX 260 GPUs with two commercial visual effects.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {175--184},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1531743.1531772},
 doi = {http://doi.acm.org/10.1145/1531743.1531772},
 acmid = {1531772},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SIMT, gpu, visual effects},
} 

@inproceedings{Karidis:2009:TVA:1531743.1531773,
 author = {Karidis, John and Moreira, Jose E. and Moreno, Jaime},
 title = {True value: assessing and optimizing the cost of computing at the data center level},
 abstract = {There are five main components to the cost of delivering computing in a data center: (i) the construction of the data center building itself; (ii) the power and cooling infrastructure for the data center; (iii) the acquisition cost of the servers that populate the data center; (iv) the cost of electricity to power (and cool) the servers; and (v) the cost of managing those servers. We first study the fundamental economics of operating such a data center with a model that captures the first four costs. We call these the physical cost, as it does not include the labor cost. We show that it makes economic sense to design data centers for relatively low power densities, and that increasing server utilization is an efficient way to reduce total cost of computation. We then develop a cost/performance model that includes the management cost and allows the evaluation of the optimal server size for consolidation. We show that, for a broad range of operating and cost conditions, servers with 4 to 16 processor sockets result in the lowest total cost of computing.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {185--192},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1531743.1531773},
 doi = {http://doi.acm.org/10.1145/1531743.1531773},
 acmid = {1531773},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data center, servers, system management},
} 

@inproceedings{Herault:2009:HAF:1531743.1531774,
 author = {H\'{e}rault, Thomas and Largillier, Thomas and Peyronnet, Sylvain and Qu\'{e}tier, Benjamin and Cappello, Franck and Jan, Mathieu},
 title = {High accuracy failure injection in parallel and distributed systems using virtualization},
 abstract = {Emulation sits between simulation and experimentation to complete the set of tools available for software designers to evaluate their software and predict behavior under conditions usually unachievable in a laboratory experiment. It consists in running the real application in an emulated environment. Thus, it behaves more realistically than a simulation, but under a controlled and reproducible environment, more suitable for behavior analysis. In this paper, we propose an emulation platform for parallel and distributed systems where both the machines and the network are virtualized at a low level. We demonstrate that the use of virtual machines allows us to test highly accurate failure injection by "destroying" virtual machines. Failure accuracy is a criteria that demonstrates how realistic a fault is. The platform accuracy is evaluated using Pastry, a fault-tolerant distributed hash-table.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {193--196},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1531743.1531774},
 doi = {http://doi.acm.org/10.1145/1531743.1531774},
 acmid = {1531774},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {emulation, fault injection, large-scale applications, virtualization},
} 

@inproceedings{Villa:2009:STC:1531743.1531776,
 author = {Villa, Oreste and Krishnamoorthy, Sriram and Nieplocha, Jarek and Brown,Jr., David M.},
 title = {Scalable transparent checkpoint-restart of global address space applications on virtual machines over infiniband},
 abstract = {Checkpoint-Restart is one of the most used software approaches to achieve fault-tolerance in high-end clusters. While standard techniques typically focus on user-level solutions, the advent of virtualization software has enabled efficient and transparent system-level approaches. In this paper, we present a scalable transparent system-level solution to address fault-tolerance for applications based on global address space (GAS) programming models on Infiniband clusters. In addition to handling communication, the solution addresses transparent checkpoint of user-generated files. We exploit the support for the Infiniband network in the Xen virtual machine environment. We have developed a version of the Aggregate Remote Memory Copy Interface (ARMCI) one-sided communication library capable of suspending and resuming applications. We present efficient and scalable mechanisms to distribute checkpoint requests and to backup virtual machines memory images and file systems. We tested our approach in the context of NWChem, a popular computational chemistry suite. We demonstrated that NWChem can be executed, without any modification to the source code, on a virtualized 8-node cluster with very little overhead (below 3\%). We observe that the total checkpoint time is limited by disk I/O. Finally, we measured system-size depended components of the checkpoint time on up to 1024 cores (128 nodes), demonstrating the scalability of our approach in medium/large-scale systems.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {197--206},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1531743.1531776},
 doi = {http://doi.acm.org/10.1145/1531743.1531776},
 acmid = {1531776},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {checkpoint-restart, global address space, infiniband, virtual machines},
} 

@inproceedings{vanAmesfoort:2009:EMP:1531743.1531777,
 author = {van Amesfoort, Alexander S. and Varbanescu, Ana Lucia and Sips, Henk J. and van Nieuwpoort, Rob V.},
 title = {Evaluating multi-core platforms for HPC data-intensive kernels},
 abstract = {Multi-core platforms have proven themselves able to accelerate numerous HPC applications. But programming data-intensive applications on such platforms is a hard, and not yet solved, problem. Not only do modern processors favor compute-intensive code, they also have diverse architectures and incompatible programming models. And even after making a difficult platform choice, extensive programming effort must be invested with an uncertain performance outcome. By taking the plunge on an irregular, data-intensive application, we present an evaluation of three platform types, namely the generic multi-core CPU, the STI Cell/B.E., and the GPU. We evaluate these platforms in terms of application performance, programming effort and cost. Although we do not select a clear winner, we do provide a list of guidelines to assist in platform choice and development of similar data-intensive applications.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {207--216},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1531743.1531777},
 doi = {http://doi.acm.org/10.1145/1531743.1531777},
 acmid = {1531777},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data-intensive kernels, gpus, memory-bound applications, multi-core processors, the cell processor},
} 

@inproceedings{Bertels:2009:SDM:1531743.1531778,
 author = {Bertels, Peter and Heirman, Wim and Stroobandt, Dirk},
 title = {Strategies for dynamic memory allocation in hybrid architectures},
 abstract = {Hybrid architectures combining the strengths of general-purpose processors with application-specific hardware accelerators can lead to a significant performance improvement. Our hybrid architecture uses a Java Virtual Machine as an abstraction layer to hide the complexity of the hardware/software interface between processor and accelerator from the programmer. The data communication between the accelerator and the processor often incurs a significant cost, which sometimes annihilates the original speedup obtained by the accelerator. This article shows how we minimise this communication cost by dynamically chosing an optimal data layout in the Java heap memory which is distributed over both the accelerator and the processor memory. The proposed self-learning memory allocation strategy finds the optimal location for each Java object's data by means of runtime profiling. The communication cost is effectively reduced by up to 86\% for the benchmarks in the DaCapo suite (51\% on average).},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {217--220},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1531743.1531778},
 doi = {http://doi.acm.org/10.1145/1531743.1531778},
 acmid = {1531778},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hardware acceleration, java, memory management},
} 

@inproceedings{vanderSpek:2009:CPP:1531743.1531779,
 author = {van der Spek, Harmen L.A. and Bakker, Erwin M. and Wijshoff, Harry A.G.},
 title = {Characterizing the performance penalties induced by irregular code using pointer structures and indirection arrays on the intel core 2 architecture},
 abstract = {Irregularity is one of the fundamental causes for performance degradation in applications. Both hardware and software have a hard time coping with irregular memory access patterns and irregularity in flow control. On the hardware side, execution is optimized for regular data accesses and irregular memory access streams cannot be predicted. On the software side, compilers are are not able to reason about memory locations and loop bounds. This prevents many optimizations to be applied. In this paper, we measure and characterize the impact of various facets of irregularity using SPARK00, a set of benchmarks that explicitly targets the measurement of the impact of irregularity, on one of themost commonly used architectures today, the Intel Core 2. The benchmarks consist of kernels that are based on pointers, a notorious cause of irregularity, kernels that use indirection arrays, and kernels that implement regular counterparts of some of the irregular kernels. By employing different data sets and different memory layouts these benchmarks are used to characterize architectural features.},
 booktitle = {Proceedings of the 6th ACM conference on Computing frontiers},
 series = {CF '09},
 year = {2009},
 isbn = {978-1-60558-413-3},
 location = {Ischia, Italy},
 pages = {221--224},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1531743.1531779},
 doi = {http://doi.acm.org/10.1145/1531743.1531779},
 acmid = {1531779},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmarking, irregularity, memory access, spark00},
} 

@inproceedings{Salapura:2008:SUN:1366230.1366232,
 author = {Salapura, Valentina},
 title = {Scaling up next generation supercomputers},
 abstract = {Historically, technology has been the main driver of computer performance. For many system generations, CMOS scaling has been leveraged to increase clock speed and build increasingly complex microarchitectures. As technology-driven performance gains are becoming increasingly harder to achieve, innovative system architecture must step in. In the context of the design of the Blue Gene/P supercomputer chip, we will discuss how we adopted a holistic approach to optimization of the entire hardware and software stack for a range of metrics: performance, power, power/performance, reliability and ease of use. The new Blue Gene/P chip multiprocessor (CMP) scales node performance using a multi-core system-on-a-chip design. While in the past large symmetric multi processor (SMP) designs were sized to handle large amounts of coherence traffic, many modern CMP designs find this cost prohibitive in terms of area, power dissipation, and design complexity. As multi-core processors evolve to larger configurations, the performance loss due to handling coherence traffic must be carefully managed. Thus, to ensure high efficiency of each quad-processor node in Blue Gene/P, taming the cost of coherence of traditional SMP designs was a key requirement. The new Blue Gene/P chip multiprocessor exploits a novel way of reducing coherence cost by filtering useless coherence actions. Each processor core is paired with a snoop filter which identifies and discards unnecessary coherence requests before they can reach the processor cores. Removing unnecessary lookups reduces the interference of invalidate requests with L1 data cache accesses, and reduces power by eliminating expensive tag array accesses. This approach results in improved power and performance characteristics. To optimize application performance, we exploit parallelism at multiple levels: at the process-level, thread-level, data-level, and instruction-level. Hardware supported coherence allows applications to efficiently share data between threads on different processors for thread-level parallelism, while the dual floating point unit and the dual-issue out-of-order PowerPC450 processor core exploit data and instruction level parallelism, respectively. To exploit process-level parallelism, special emphasis was put on efficient communication primitives by including hardware support for the MPI protocol, such as low latency barriers, and five highly optimized communication networks. A new high performance DMA unit supports high throughput data transfers. As the result of this deliberate design for scalability approach, Blue Gene supercomputers offer unprecedented scalability, in some cases by orders of magnitude, to a wide range of scientific applications. A broad range of scientific applications on Blue Gene supercomputers have advanced scientific discovery, which is the real merit and ultimate measure of success of the Blue Gene system family.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {1--2},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1366230.1366232},
 doi = {http://doi.acm.org/10.1145/1366230.1366232},
 acmid = {1366232},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {blue gene, chip multiprocessors (cmp), coherence protocols, multicore, scalability of systems},
} 

@inproceedings{Crawford:2008:ACC:1366230.1366234,
 author = {Crawford, Catherine H. and Henning, Paul and Kistler, Michael and Wright, Cornell},
 title = {Accelerating computing with the cell broadband engine processor},
 abstract = {In this paper, we describe our approach to utilizing the compute power of the Cell Broadband Engine\&#8482; (Cell/B.E.)<sup>1</sup> processor as an accelerator for computationally intensive portions of high performance computing applications. We call this approach "hybrid programming" because it distributes application execution across heterogeneous processors. IBM developed a hardware implementation and software infrastructure that enables this hybrid computing model as part of the Roadrunner project for Los Alamos National Laboratory (LANL). In the hybrid programming model, a process running on a host processor, such as an x86_64 architecture processor, creates an accelerator process on an accelerator processor, such as the IBM\&#174; PowerXCell\&#8482;8i<sup>2</sup>. The PowerXCell8i is a new implementation of the Cell Broadband Engine architecture. The host process then schedules compute intensive operations onto the accelerator process. The host and accelerator process can continue execution concurrently and synchronize when needed to transfer results or schedule new accelerator computation. We describe the Data Communication and Synchronization (DaCS) Library and Accelerated Library Framework (ALF) which are designed to allow applications to create new applications and adapt existing applications to exploit hybrid computing platforms. We also describe our experience in using such frameworks to construct hybrid versions of the familiar Linpack benchmark and an implicit Monte Carlo radiation transport application named Milagro. Performance measurements on prototype hardware are presented that show the performance improvements achieved to date, along with projections of the expected performance on the final Roadrunner system.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {3--12},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366234},
 doi = {http://doi.acm.org/10.1145/1366230.1366234},
 acmid = {1366234},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {accelerators, hybrid programming models},
} 

@inproceedings{Aji:2008:CMS:1366230.1366235,
 author = {Aji, Ashwin M. and Feng, Wu-chun and Blagojevic, Filip and Nikolopoulos, Dimitrios S.},
 title = {Cell-SWat: modeling and scheduling wavefront computations on the cell broadband engine},
 abstract = {This paper presents and evaluates a model and a methodology for implementing parallel wavefront algorithms on the Cell Broadband Engine. Wavefront algorithms are vital in several application areas such as computational biology, particle physics, and systems of linear equations. The model uses blocked data decomposition with pipelined execution of blocks across the synergistic processing elements (SPEs) of the Cell. To evaluate the model, we implement the Smith-Waterman sequence alignment algorithm as a wavefront algorithm and present key optimization techniques that complement the vector processing capabilities of the SPE. Our results show perfect linear speedup for up to 16 SPEs on the QS20 dual-Cell blades, and our model shows that our implementation is highly scalable for more cores, if available. Furthermore, the accuracy of our model is within 3\% of the measured values on average. Lastly, we also test our model in a throughput-oriented experimental setting, where we couple the model with scheduling techniques that exploit parallelism across the simultaneous execution of multiple sequence alignments. Using our model, we improved the throughput of realistic multisequence alignment workloads by up to 8\% compared to FCFS (first-come, first-serve), by trading off parallelism within alignments with parallelism across alignments.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {13--22},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366235},
 doi = {http://doi.acm.org/10.1145/1366230.1366235},
 acmid = {1366235},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cell broadband engine, smith-waterman, wavefront algorithms},
} 

@inproceedings{Rafique:2008:DPI:1366230.1366236,
 author = {Rafique, M. Mustafa and Butt, Ali R. and Nikolopoulos, Dimitrios S.},
 title = {Dma-based prefetching for i/o-intensive workloads on the cell architecture},
 abstract = {Recent advent of the asymmetric multi-core processors such as Cell Broadband Engine (Cell/BE) has popularized the use of heterogeneous architectures. A growing body of research is exploring the use of such architectures, especially in High-End Computing, for supporting scientific applications. However, prior research has focused on use of the available Cell/BE operating systems and runtime environments for supporting compute-intensive jobs. Data and I/O intensive workloads have largely been ignored in this domain. In this paper, we take the first steps in supporting I/O intensive workloads on the Cell/BE and deriving guidelines for optimizing the execution of I/O workloads on heterogeneous architectures. We explore various performance enhancing techniques for such workloads on an actual Cell/BE system. Among the techniques we explore, an asynchronous prefetching-based approach, which uses the PowerPC core of the Cell/BE for file prefetching and decentralized DMAs from the synergistic processing cores (SPE's), improves the performance for I/O workloads that include an encryption/decryption component by 22.2\%, compared to I/O performed na\&#239;vely from the SPE's. Our evaluation shows promising results and lays the foundation for developing more efficient I/O support libraries for multi-core asymmetric architectures.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {23--32},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366236},
 doi = {http://doi.acm.org/10.1145/1366230.1366236},
 acmid = {1366236},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cell broadband engine, high-performance computing, i/o intensive workloads},
} 

@inproceedings{Scarpazza:2008:EMS:1366230.1366237,
 author = {Scarpazza, Daniele Paolo and Villa, Oreste and Petrini, Fabrizio},
 title = {Exact multi-pattern string matching on the cell/b.e. processor},
 abstract = {String searching is the computationally intensive kernel of many security and network applications like search engines, intrusion detection systems, virus scanners and spam filters. The growing size of on-line content and the increasing wire speeds push the need for fast --and often real-time, string searching solutions. Multi-core processors are gaining increasing popularity, thanks to their unprecedented computing power, but they are also bringing new programming challenges. This paper describes a class of high-performance exact string searching solutions that we have optimized for the IBM Cell/B.E. processor using the well known Aho-Corasick algorithm. This class provides several trade-offs between performance and dictionary size. When dictionaries are small enough to fit in the local memories of the processing cores, the throughput reaches 40 Gbps per processor. With larger dictionaries (as many as hundreds of thousands patterns), the typical throughput is between 1.6 and 2.2 Gbps per processor.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {33--42},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366237},
 doi = {http://doi.acm.org/10.1145/1366230.1366237},
 acmid = {1366237},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cell processor, matching, string},
} 

@inproceedings{Yokoyama:2008:PRP:1366230.1366239,
 author = {Yokoyama, Tetsuo and Axelsen, Holger Bock and Gl\"{u}ck, Robert},
 title = {Principles of a reversible programming language},
 abstract = {The principles of reversible programming languages are explicated and illustrated with reference to the design of a high-level imperative language, Janus. The fundamental properties for such languages include backward as well as forward determinism and reversible updates of data. The unique design features of the language include explicit post-condition assertions, direct access to an inverse semantics and the possibility of clean (\ie, garbage-free) computation of injective functions. We suggest the clean simulation of reversible Turing machines as a criterion for computing strength of reversible languages, and demonstrate this for Janus. We show the practicality of the language by implementation of a reversible fast Fourier transform. Our results indicate that the reversible programming paradigm has fundamental properties that are relevant to many different areas of computer science.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {43--54},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1366230.1366239},
 doi = {http://doi.acm.org/10.1145/1366230.1366239},
 acmid = {1366239},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {backward determinism, fast fourier transform, inverse semantics, reversible computing, turing completeness},
} 

@inproceedings{Wen:2008:FPP:1366230.1366240,
 author = {Wen, Xingzhi and Vishkin, Uzi},
 title = {Fpga-based prototype of a pram-on-chip processor},
 abstract = {PRAM (Parallel Random Access Model) has been widely regarded a desirable parallel machine model for many years, but it is also believed to be "impossible in reality." As the new billion-transistor processor era begins, the eXplicit Multi-Threading (XMT) PRAM-On-Chip project is attempting to design an on-chip parallel processor that efficiently supports PRAM algorithms. This paper presents the first prototype of the XMT architecture that incorporates 64 simple in-order processors operating at 75MHz. The microarchitecture of the prototype is described and the performance is studied with respect to some micro-benchmarks. Using cycle accurate emulation, the projected performance of an 800MHz XMT ASIC processor is compared with AMD Opteron 2.6GHz, which uses similar area as would a 64-processor ASIC version of the XMT prototype. The results suggest that an only 800MHz XMT ASIC system outperforms AMD Opteron 2.6GHz, with speedups ranging between 1.57 and 8.56.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {55--66},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1366230.1366240},
 doi = {http://doi.acm.org/10.1145/1366230.1366240},
 acmid = {1366240},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ease-of-programming, explicit multi-threading, on-chip parallel processor, parallel algorithms, pram, xmt},
} 

@inproceedings{Perfumo:2008:LST:1366230.1366241,
 author = {Perfumo, Cristian and S\"{o}nmez, Nehir and Stipic, Srdjan and Unsal, Osman and Cristal, Adri\'{a}n and Harris, Tim and Valero, Mateo},
 title = {The limits of software transactional memory (STM): dissecting Haskell STM applications on a many-core environment},
 abstract = {In this paper, we present a Haskell Transactional Memory benchmark to provide a comprehensive application suite for the use of Software Transactional Memory (STM) researchers. We develop a framework to profile the execution of the benchmark applications and to collect detailed runtime data on their transactional behavior, running them on a 128-core multiprocessor. Using a composite of the collected raw data, we propose new transactional performance metrics. We analyze key statistics related to scalability, atomic sections, transactional events, overall transactional overhead and the relative hardware performance, accordingly drawing conclusions on the results. Our findings advance our comprehension on the STM runtime and the characteristics of different applications under the transactional management of the pure, functional programming language, Haskell.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {67--78},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1366230.1366241},
 doi = {http://doi.acm.org/10.1145/1366230.1366241},
 acmid = {1366241},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {haskell, instrumentation, transactional memory},
} 

@inproceedings{Eleftheriou:2008:SEM:1366230.1366243,
 author = {Eleftheriou, Maria and Fitch, Blake G. and Rayshubskiy, Aleksandr and Ward, T.J. Christopher and Heidelberger, Phillip and Germain, Robert S.},
 title = {A study of the effects of machine geometry and mapping on distributed transpose performance},
 abstract = {This paper describes a parallel strategy to extend the scalability of a small 3D FFT on thousands of Blue Gene/L processors. The approach is to execute the intermediate phases of the 3D FFT on smaller processor subsets. Performance measurements of the standalone 3D FFT on two communication protocols, MPI and BG/L ADE are presented. While the performance of the 3D-FFT with MPI-based and BG/L ADE-based implementations exhibited qualitatively similar behavior, the BG/L ADE-based version has lower communication cost than the MPI based version for small message sizes. Measurements also show that the proposed approach is effective in improving Particle-Mesh-based N-body simulation performance significantly at the limits of scalability.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {79--86},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1366230.1366243},
 doi = {http://doi.acm.org/10.1145/1366230.1366243},
 acmid = {1366243},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {blue gene, fft},
} 

@inproceedings{Kourtis:2008:OSM:1366230.1366244,
 author = {Kourtis, Kornilios and Goumas, Georgios and Koziris, Nectarios},
 title = {Optimizing sparse matrix-vector multiplication using index and value compression},
 abstract = {Previous research work has identified memory bandwidth as the main bottleneck of the ubiquitous Sparse Matrix-Vector Multiplication kernel. To attack this problem, we aim at reducing the overall data volume of the algorithm. Typical sparse matrix representation schemes store only the non-zero elements of the matrix and employ additional indexing information to properly iterate over these elements. In this paper we propose two distinct compression methods targeting index and numerical values respectively. We perform a set of experiments on a large real-world matrix set and demonstrate that the index compression method can be applied successfully to a wide range of matrices. Moreover, the value compression method is able to achieve impressive speedups in a more limited yet important class of sparse matrices that contain a small number of distinct values},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {87--96},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366244},
 doi = {http://doi.acm.org/10.1145/1366230.1366244},
 acmid = {1366244},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data compression, memory bandwidth, sparse matrix},
} 

@inproceedings{Kochte:2008:FSP:1366230.1366245,
 author = {Kochte, Michael A. and Natarajan, Ramesh},
 title = {A framework for scheduling parallel dbms user-defined programs on an attached high-performance computer},
 abstract = {We describe a software framework for deploying, scheduling and executing parallel DBMS user-defined programs on an attached high-performance computer (HPC) platform. This framework is advantageous for many DBMS workloads in the following two aspects. First, the long-running user-defined programs can be speeded up by taking advantage of the greater hardware parallel-ism available on the attached HPC platform. Second, the interac-tive response time of the remaining applications on the database server platform is improved by the off-loading of long-running user-defined programs to the attached HPC platform. Our frame-work provides a new approach for integrating high-performance computing into the workflow of query-oriented, computationally-intensive applications.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {97--104},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1366230.1366245},
 doi = {http://doi.acm.org/10.1145/1366230.1366245},
 acmid = {1366245},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {database accelerators, high-performance computing, parallel user-defined programs},
} 

@inproceedings{Kirischian:2008:MVP:1366230.1366247,
 author = {Kirischian, Valeri and Geurkov, Vadim and Kirischian, Lev},
 title = {A multi-mode video-stream processor with cyclically reconfigurable architecture},
 abstract = {This paper presents an approach for development of cost-effective hardware platform for video/image processing. The approach utilizes the SRAM based reconfigurable logic devices (FPGAs) and, their capability of run-time temporal partitioning of logic resources. We propose the architecture for multi-mode video-stream processor with cyclically reconfigurable structure. The proposed architecture has been analyzed on the basis of experiments conducted on AMIRIX AP1000 development system based on Xilinx Virtex-2Pro FPGA. Multi-mode Adaptive Reconfigurable System has been developed, based on Xilinx Virtex-4 FPGA. This platform is capable of supporting the runtime temporal partitioning of on-chip resources. The main component of the research was the introduction of methodology of design for cyclically reconfigurable processor that uses the temporal partitioning mechanism (TPM). TPM allows reuse of the logic and routing resources of an SRAM based FPGA device by the means of partitioning algorithm in to tasks and execution of these tasks in different time slots. This technique allows the reduction of size requirement for FPGA devices, as well as, increase in cost efficiency, and decrease in power consumption of the system compared to systems with statically configured FPGA devices. Applications associated with stereo-vision algorithms and object tracking have been developed and tested on the platform. Finally, the analysis of the cost-effectiveness of this approach has been conducted. This analysis has demonstrated sufficient increase of efficiency in comparison to statically configured FPGA designs. Work also presents optimal conditions at which the use of the architecture would be most cost effective, and where the use of it would be most beneficial. The experimental tests have been done by the means of development of application that are used in the industry in the area of stereo-vision space-born applications.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {105--106},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1366230.1366247},
 doi = {http://doi.acm.org/10.1145/1366230.1366247},
 acmid = {1366247},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FPGA, computer architecture, cost-performance ratio, dynamic reconfiguration, pre-fetching, reconfigurable computing, temporal partitioning, video-stream processor},
} 

@inproceedings{Du:2008:ERS:1366230.1366248,
 author = {Du, Jing and Yang, Xuejun and Wang, Zhiyuan},
 title = {Effective runtime scalability metric to measure productivity in high performance computing systems},
 abstract = {Current high performance computing systems all rely on parallel processing techniques to achieve high performance. With the parallel computer systems scaling up, the new generation of high performance computers puts more emphasis on "high productivity" [1], rather than "high performance" as in the past. These new systems will not only meet the traditional requirements of computing performance, but also address the ongoing technical challenges in the current high-end computing domain regarding energy consumption, reliability, etc. For energy consumption, with the computer system scaling up, it increases dramatically [2]. High energy consumption means high maintenance cost and low system stability. For example, the peak energy consumption of the Earth Simulator and BlueGene/L is 18MW and 1.6MW respectively. For reliability, with the complexity of a computer system increasing, its meantime-between-failure (MTBF) is becoming significantly shorter than what is required by many current high performance computing applications [3], such as BlueGene/L. Therefore, energy optimization techniques and fault tolerance techniques should be introduced to computer systems to achieve low energy consumption and high reliability. To improve the productivity of high performance computing systems, we need to find a proper way to measure it. Unfortunately, traditional measurement models can not evaluate the system productivity comprehensively and effectively [4]. To address this issue, this paper proposes an effective scalability metric for high performance computing systems based on Gustafson speedup law. The metric makes a good balance among runtime productivity factors including computing performance, energy consumption and reliability. The contribution of our work lies in the following three aspects. First, in order to measure the scalability of an energy-consumption optimized parallel program, we should consider not only whether the program computing performance is scalable, but also whether the energy consumption increases smoothly with the computing performance scaling up. Therefore, we propose an energy-smoothed scalability metric based on a new definition of energy efficiency, which reflects the effect of energy consumption on runtime performance. This metric can be used to measure whether energy consumption increases smoothly with the computer system scaling up. Second, when evaluating the scalability of parallel programs, we should consider the effect of fault tolerance overhead on the program performance. Therefore, we propose a reliability-assured scalability metric based on a new definition of reliable efficiency, which reflects the effect of fault tolerance overhead on runtime performance. This metric can be used to measure whether the performance with the introduction of fault tolerance overhead is scalable as the computer system scales up. Third, based on the analyses above, we propose a synthetic scalability metric, which measures whether the systems are energy-smoothed and reliability-assured when the systems scale up. The metric simultaneously measures the multiple productivity factors regarding computing performance, energy consumption and reliability. The metric is demonstrated by applying it to some well-known energy optimization techniques and fault tolerance techniques. Case studies indicate that using our model, it is more effective to solve the following problems: First, measuring the scalability for high performance computing systems by quantifying the effect of runtime factors including computing performance, energy consumption and reliability on scalability; Second, providing suggestions on how to keep and improve the scalability of high performance computing systems, and guiding the proper selection of energy optimization techniques and fault tolerant techniques to achieve high scalability of high performance computer systems.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {107--108},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1366230.1366248},
 doi = {http://doi.acm.org/10.1145/1366230.1366248},
 acmid = {1366248},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computing performance, energy consumption, high performance computing system, productivity, reliability, scalability metric},
} 

@inproceedings{Trancoso:2008:EGA:1366230.1366249,
 author = {Trancoso, Pedro and Artemiou, Artemakis},
 title = {Exploiting the GPU to accelerate DSS query execution},
 abstract = {},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {109--110},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1366230.1366249},
 doi = {http://doi.acm.org/10.1145/1366230.1366249},
 acmid = {1366249},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {decision support systems, gpgpu, graphics processors},
} 

@inproceedings{Baiardi:2008:VIN:1366230.1366250,
 author = {Baiardi, Fabrizio and Sgandurra, Daniele},
 title = {Virtual interacting network community: exploiting multi-core architectures to increase security},
 abstract = {},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {111--112},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1366230.1366250},
 doi = {http://doi.acm.org/10.1145/1366230.1366250},
 acmid = {1366250},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {critical infrastructures, virtual communities, virtual machines},
} 

@inproceedings{Saini:2008:AEP:1366230.1366251,
 author = {Saini, Subhash and Jespersen, Dennis C. and Talcott, Dale and Djomehri, Jahed and Sandstrom, Timothy},
 title = {Application-based early performance evaluation of SGI altix 4700 systems for SGI systems},
 abstract = {The suitability of the next generation of high performance computing systems for petascale simulations will depend on a balance between various factors such as processor performance, memory performance, local and global network performance, and Input/Output (I/O) performance. As the supercomputing industry develops new technologies for these subsystems, achieving system balance becomes challenging. In this paper we evaluate the performance of newly introduced dual-core based SGI Altix 4700 systems (both Bandwidth and Density models) and we compare their performance with that of a single-core based SGI Altix 3700 Bx2 system. The SGI Altix 4700 Density system installed in October 2007 at NASA Ames Research Center is the largest 2048-processor single system image (SSI) system in the world. We used the High Performance Computing Challenge (HPCC) benchmark, NAS Parallel benchmarks (NPB) and five real-world applications, three from computational fluid dynamics, one from climate modeling and one from nanotechnology. Our study shows that the SGI Altix 4700 Bandwidth system performs slightly better and SGI Altix 4700 Density system performs slightly worse than the SGI Altix 3700 Bx2 up to 128 processors, while the performance of the systems is almost the same beyond 128 processors, when the communication time dominates the compute time.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {113--114},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1366230.1366251},
 doi = {http://doi.acm.org/10.1145/1366230.1366251},
 acmid = {1366251},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmarking, performance analysis, sgi altix 4700, supercomputing},
} 

@inproceedings{Calinescu:2008:MMD:1366230.1366252,
 author = {Calinescu, Radu},
 title = {Methodology for the model-driven development of self-managing systems},
 abstract = {Based on recent advances in autonomic computing, we propose a methodology for the cost-effective development of self-managing systems starting from a model of the resources to be managed and using a general-purpose autonomic architecture.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {115--116},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1366230.1366252},
 doi = {http://doi.acm.org/10.1145/1366230.1366252},
 acmid = {1366252},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {autonomic computing, general-purpose architecture, model-driven development, policy engine},
} 

@inproceedings{Pingali:2008:DAI:1366230.1366254,
 author = {Pingali, Keshav K.},
 title = {Data-parallel abstractions for irregular programs},
 abstract = {Client-side applications running on multicore processors are likely to be irregular programs that deal with complex, pointer-based data structures such as graphs and trees. In her 2007 Turing award lecture, Fran Allen raised an important question about such programs: do irregular programs have data parallelism, and if so, how do we exploit it on multicore processors? In this talk, we demonstrate using concrete examples that irregular programs have a generalized data parallelism that arises from the use of iterative algorithms that manipulate worklists of various sorts. We also argue that optimistic parallelization is required for these programs since compile-time parallelization techniques like points-to and shape analysis cannot expose the parallelism in these programs. We then describe the approach taken in the Galois project to exploit this generalized data-parallelism. There are three main aspects to the Galois system: (1) a small number of syntactic constructs for packaging optimistic parallelism as iteration over ordered and unordered sets, (2) assertions about methods in class libraries, and (3) a runtime scheme for detecting and recovering from potentially unsafe accesses to shared memory made by optimistic computations. We present experimental results that demonstrate that the Galois approach is practical, and discuss ongoing work on this system.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {117--118},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1366230.1366254},
 doi = {http://doi.acm.org/10.1145/1366230.1366254},
 acmid = {1366254},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {irregular programs, multicore programming, optimistic parallelism, set iterators},
} 

@inproceedings{Bhadauria:2008:OTT:1366230.1366256,
 author = {Bhadauria, Major and McKee, Sally A.},
 title = {Optimizing thread throughput for multithreaded workloads on memory constrained CMPs},
 abstract = {Multi-core designs have become the industry imperative, replacing our reliance on increasingly complicated micro-architectural designs and VLSI improvements to deliver increased performance at lower power budgets. Performance of these multi-core chips will be limited by the DRAM memory system: we demonstrate this by modeling a cycle-accurate DDR2 memory controller with SPLASH-2 workloads. Surprisingly, benchmarks that appear to scale well with the number of processors fail to do so when memory is accurately modeled. We frequently find that the most efficient configuration is not the one with the most threads. By choosing the most efficient number of threads for each benchmark, average energy delay efficiency improves by a factor of 3.39, and performance improves by 19.7\%, on average. We also introduce a shadow row of sense amplifiers, an alternative to cached DRAM, to explore potential power/performance impacts. The shadow row works in conjunction with the L2 Cache to leverage temporal and spatial locality across memory accesses, thus attaining average and peak speedups of 13\% and 43\%, respectively, when compared to a state-of-the-art DRAM memory scheduler.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {119--128},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366256},
 doi = {http://doi.acm.org/10.1145/1366230.1366256},
 acmid = {1366256},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {efficiency, memory bandwidth, performance, power},
} 

@inproceedings{Romanescu:2008:RII:1366230.1366257,
 author = {Romanescu, Bogdan F. and Bauer, Michael E. and Ozev, Sule and Sorin, Daniel J.},
 title = {Reducing the impact of intra-core process variability with criticality-based resource allocation and prefetching},
 abstract = {We develop architectural techniques for mitigating the impact of process variability. Our techniques hide the performance effects of slow components-including registers, functional units, and L1I and L1D cache frames-without slowing the clock frequency or pessimistically assuming that all components are slow. Using ideas previously developed for other purposes-criticality-based allocation of resources, prefetching, and prefetch buffering-we allow design engineers to aggressively set the clock frequency without worrying about the subset of components that cannot meet this frequency. Our techniques outperform speed binning, because clock frequency benefits outweigh slight losses in IPC.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {129--138},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366257},
 doi = {http://doi.acm.org/10.1145/1366230.1366257},
 acmid = {1366257},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {microarchitecture, process variability},
} 

@inproceedings{Oliver:2008:CDR:1366230.1366258,
 author = {Oliver, John and Amirtharajah, Rajeevan and Akella, Venkatesh and Chong, Frederic T.},
 title = {Credit-based dynamic reliability management using online wearout detection},
 abstract = {As circuit geometries continue to shrink, and supply voltages remain relatively constant, circuit wearout becomes a concern. We propose that the relative reliability of the circuits of a processor be exposed to the operating system, and be managed by a credit-based wearout monitor. This wearout monitor receives dynamic updates of the reliability of circuits through the use of stability detector circuits that are small enough to be widely deployed. We find that through the combined use of the wearout monitor and stability detectors, we can efficiently and accurately manage the reliability of a processor, and re-coup the performance of a processor that would otherwise be lost when processors are over-provisioned to meet an expected lifetime. We simulate a 16 core DSP with a wearout monitor and stability detectors on a mix of four different media algorithms. Using the wearout monitor and stability detectors, we find that by reducing average performance by only 5\%, we can increase the lifetime of the processor by 46\%.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {139--148},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366258},
 doi = {http://doi.acm.org/10.1145/1366230.1366258},
 acmid = {1366258},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {reliability, wear-out},
} 

@inproceedings{Pratas:2008:LPM:1366230.1366259,
 author = {Pratas, Frederico and Gaydadjiev, Georgi and Berekovic, Mladen and Sousa, Leonel and Kaxiras, Stefanos},
 title = {Low power microarchitecture with instruction reuse},
 abstract = {Power consumption has become a very important metric and challenging research topic in the design of microprocessors in the recent years. The goal of this work is to improve power efficiency of superscalar processors through instruction reuse at the execution stage. This paper proposes a new method for reusing instructions when they compose small loops: the loop's instructions are first buffered in the Reorder Buffer and reused afterwards without the need for dynamically unrolling the loop, as commonly implemented by the traditional instruction reusing techniques. The proposed method is implemented with the introduction of two new auxiliary hardware structures in a typical superscalar microarchitecture: a Finite State Machine</i> (FSM), used to detect the reusable loops; and a Log</i> used to store the renaming data for each instruction when the loop is "unrolled". In order to evaluate the proposed method we modified the sim-outorder</i> tool from Simplescalar v3.0</i> for the PISA, and Wattch v1.02</i> Power Performance simulators. Several different configurations and benchmarks have been used during the simulations. The obtained results show that by implementing this new method in a superscalar microarchitecture, the power efficiency can be improved without significantly affecting neither the performance nor the cost.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {149--158},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366259},
 doi = {http://doi.acm.org/10.1145/1366230.1366259},
 acmid = {1366259},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {loop reusing technique, power reduction, reorder buffer optimization, superscalar processor},
} 

@inproceedings{Loh:2008:MPF:1366230.1366261,
 author = {Loh, Gabriel H.},
 title = {A modular 3d processor for flexible product design and technology migration},
 abstract = {The current methodology used in mass-market processor design is to create a single base microarchitecture (e.g., Intel's ``Core'' or AMD's ``K8'') that is used throughout all of the PC market segments from laptops to servers. To differentiate the products, manufacturers rely on speed binning, different cache sizes, and varying the number of cores. In this paper, we propose using 3D integration to provide a new, but complementary, approach to providing product differentiation. Past research on using 3D to improve performance has focused on the construction of "fully 3D" circuits where functional blocks are partitioned across two or more layers. This approach forces one of two undesirable situations: (1) all products must be implemented in, and therefore pay the cost of, 3D or (2) a 3D-implemented processor is designed for the high-end/high-performance markets and a separate 2D microarchitecture must be designed for the lower-cost markets thereby incurring significant additional design effort and engineering cost. We present a modular processor architecture where 3D can be used to enhance performance within a single unified design and also provides for a more gradual migration path toward fully 3D-integrated designs. To make this work, we describe a generic technique of using "phantom" components where the baseline processor may believe that 3D-stacked resources exist, but are currently unavailable. Simply using 3D to stack more L2 cache provides a 15.1\% average performance benefit, but our proposal increases performance by 25.4\%.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {159--170},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1366230.1366261},
 doi = {http://doi.acm.org/10.1145/1366230.1366261},
 acmid = {1366261},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {3d-integration, modular, superscalar},
} 

@inproceedings{Patil:2008:DSA:1366230.1366262,
 author = {Patil, Shruti R. and Yao, Xiaofeng and Meng, Hao and Wang, Jian-Ping and Lilja, David J.},
 title = {Design of a spintronic arithmetic and logic unit using magnetic tunnel junctions},
 abstract = {Conventional electronics technology uses an electron's charge to store information and a current of electrons to transfer information. Spintronics technology, in contrast, uses an electron's 'spin' in addition to its charge to transfer and store information. Magnetic Tunnel Junctions (MTJ) are spintronic devices that exhibit two distinct resistance states due to the tunneling magnetoresistance (TMR) effect. Their properties can provide significant advantages over conventional electronics in the design of computer systems. We characterize some of the challenges in using spintronic technology in large systems, and describe a novel design technique called 'union with neutralization' to combine individual component designs into multi-functional units. We use this technique to present the design of an arithmetic and logical unit (ALU) using 20 MTJs and two CMOS-based sense amplifiers. The spintronics-based ALU has the potential to offer considerable area, timing, and power advantages over a conventional CMOS-based ALU.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {171--178},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1366230.1366262},
 doi = {http://doi.acm.org/10.1145/1366230.1366262},
 acmid = {1366262},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {alu design, magnetic tunnel junction, spintronic alu design, spintronics},
} 

@inproceedings{Tarau:2008:ECL:1366230.1366263,
 author = {Tarau, Paul and Luderman, Brenda},
 title = {Exact combinational logic synthesis and non-standard circuit design},
 abstract = {Using a new exact synthesizer that automatically induces minimal universal boolean function libraries, we introduce two indicators for comparing their expressiveness: the first based on how many gates are used to synthesize all binary operators, the second based on how many N</i>-variable truth table values are covered by combining up to M</i> gates from the library. By applying the indicators to an exhaustive enumeration of minimal universal libraries, two dual asymmetrical operations, Logic Implication "=\&#61;" and Half XOR "<" are found to consistently outperform their symmetrical counterparts, NAND and NOR. Our expressiveness metrics bring support to the conjecture that asymmetrical operators are significantly more expressive that their well studied symmetric counterparts, omnipresent in various circuit design tools.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {179--188},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366263},
 doi = {http://doi.acm.org/10.1145/1366230.1366263},
 acmid = {1366263},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asymmetrical logic operators, exact combinational circuit synthesis, minimal transistor-count circuits, minimal universal boolean logic libraries},
} 

@inproceedings{Williams:2008:OSJ:1366230.1366265,
 author = {Williams, Kevin and Noll, Albert and Gal, Andreas and Gregg, David},
 title = {Optimization strategies for a java virtual machine interpreter on the cell broadband engine},
 abstract = {Virtual machines (VMs) such as the Java VM are a popular format for running architecture-neutral code in a managed runtime. Such VMs are typically implemented using a combination of interpretation and just-in-time compilation (JIT). A significant challenge for the portability of VM code is the growing popularity of multi-core architectures with specialized processing cores aimed at computation-intensive applications such as media processing. Such cores differ greatly in architectural design compared to traditional desktop processors. One such processing core is the Cell Broadband Engine's (Cell BE) Synergistic Processing Element (SPE). An SPE is a light weight VLIW processor core with a SIMD vector instruction set. In this paper we investigate some popular interpreter optimizations and introduce new optimizations exploiting the special hardware properties offered by the Cell BE's SPE},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {189--198},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366265},
 doi = {http://doi.acm.org/10.1145/1366230.1366265},
 acmid = {1366265},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {interpreter, java, language implementation, virtual machine},
} 

@inproceedings{Nuzman:2008:CIV:1366230.1366266,
 author = {Nuzman, Dorit and Namolaru, Mircea and Zaks, Ayal and Derby, Jeff H.},
 title = {Compiling for an indirect vector register architecture},
 abstract = {The iVMX architecture contains a novel vector register file of up to 4096 vector registers accessed indirectly via a mapping mechanism, providing compatibility with the VMX architecture, and potential for dramatic performance benefits [7]. The large number of vector registers and the unique indirection mechanism pose compilation challenges to be used efficiently: the indirection mechanism emphasizes spatial locality of registers and interaction among destination and source operands during register allocation, and the many vector registers call for aggressive automatic vectorization. This work is a first step in addressing the compilability of iVMX, following the presentation and validation of its architectural aspects [7]. In this paper we present several compilation approaches to deal with the mapping mechanism and an outer-loop vectorization transformation developed to promote the use of many vector registers. We modified an existing register allocator to target all available registers and added a post-pass to rename live-ranges considering spatial locality and interaction among operand types. An FIR filter is used to demonstrate the effectiveness of the techniques developed compared to a version hand-optimized for iVMX. Initial results show that we can reduce the overhead of map management down to 29\% of the total instruction count, compared to 22\% obtained manually, and compared to 49\% obtained using a naive scheme, while outperforming an equivalent VMX implementation by a factor of 2.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {199--208},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366266},
 doi = {http://doi.acm.org/10.1145/1366230.1366266},
 acmid = {1366266},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compiler controlled cache, data reuse, rotating register file, simd, subword parallelism, vectorization, viterbi},
} 

@inproceedings{Rangasamy:2008:CFV:1366230.1366267,
 author = {Rangasamy, Arun and Nagpal, Rahul and Srikant, Y.N.},
 title = {Compiler-directed frequency and voltage scaling for a multiple clock domain microarchitecture},
 abstract = {Multiple Clock Domain processors provide an attractive solution to the increasingly challenging problems of clock distribution and power dissipation. They allow their chips to be partitioned into different clock domains, and each domain's frequency (voltage) to be independently configured. This flexibility adds new dimensions to the Dynamic Voltage and Frequency Scaling problem, while providing better scope for saving energy and meeting performance demands. In this paper, we propose a compiler directed approach for MCD-DVFS. We build a formal petri net based program performance model, parameterized by settings of microarchitectural components and resource configurations, and integrate it with our compiler passes for frequency selection. Our model estimates the performance impact of a frequency setting, unlike the existing best techniques which rely on weaker indicators of domain performance such as queue occupancies (used by online methods) and slack manifestation for a particular frequency setting (software based methods). We evaluate our method with subsets of SPECFP2000, Mediabench and Mibench benchmarks. Our mean energy savings is 60.39\% (versus 33.91\% of the best software technique) in a memory constrained system for cache miss dominated benchmarks, and we meet the performance demands. Our ED2 improves by 22.11\% (versus 18.34\%) for other benchmarks. For a CPU with restricted frequency settings, our energy consumption is within 4.69\% of the optimal.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {209--218},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366267},
 doi = {http://doi.acm.org/10.1145/1366230.1366267},
 acmid = {1366267},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dvs, dynamic energy, energy, multiple clock domains},
} 

@inproceedings{Tripiccione:2008:JRH:1366230.1366269,
 author = {Tripiccione, Raffaele},
 title = {JANUS: reconfigurable high-performance computing for physics},
 abstract = {},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {219--220},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1366230.1366269},
 doi = {http://doi.acm.org/10.1145/1366230.1366269},
 acmid = {1366269},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fpga, monte carlo simulations, reconfigurable systems},
} 

@inproceedings{Komann:2008:EAC:1366230.1366271,
 author = {Komann, Marcus and Kr\"{o}ller, Alexander and Schmidt, Christiane and Fey, Dietmar and Fekete, S\'{a}ndor P.},
 title = {Emergent algorithms for centroid and orientation detection in high-performance embedded cameras},
 abstract = {Due to increasing speed and capabilities of production machines, the need for extremely fast and robust observation, classification, and error handling is vital to industrial image processing. We present an emergent algorithmic computing scheme and a corresponding embedded massively-parallel hardware architecture for these problems. They offer the potential to turn CMOS-camera-chips into intelligent vision devices which carry out tasks without help of a central processor, only based on local interaction of agents crawling on a large field of processing elements. It also constitutes a breakthrough for understanding sensor devices as a decentralized concept, resulting in much faster computation evading communication bottlenecks of classic approaches that become an ever-growing impediment to scalability. Here, in contrast, the number of agents and the field size and thus the computable image resolution is extremely scalable and therefore promises even more benefit with future hardware development. The results are based on novel algorithmic solutions allowing processor elements to compute center points, moments, and orientation of multiple image objects in parallel, which is of central importance to e.g. robotics. We finally present the algorithm's capabilities if realized in state-of-the-art FPGAs and ASICs.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {221--230},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366271},
 doi = {http://doi.acm.org/10.1145/1366230.1366271},
 acmid = {1366271},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {emergent algorithms, image processing, marching pixels, massively-parallel, object detection},
} 

@inproceedings{Strydis:2008:PSA:1366230.1366272,
 author = {Strydis, Christos and Zhu, Di and Gaydadjiev, Georgi N.},
 title = {Profiling of symmetric-encryption algorithms for a novel biomedical-implant architecture},
 abstract = {Starting with the implantable pacemaker, microelectronic implants have been around for more than 50 years. A plethora of commercial and research-oriented devices have been developed so far for a wide range of biomedical applications. In view of an envisioned expanding implant market in the years to come, our ongoing research work is focusing on the specification and design of a novel biomedical microprocessor core, carefully tailored to a large subset of existing and future biomedical applications. Towards this end, we have taken steps in identifying various tasks commonly required by such applications and profiling their behavior and requirements. One such task is decryption of incoming commands to an implant and encryption of outgoing (telemetered) biological data. Secure bidirectional information relaying in implants has been largely overlooked so far although protection of personal (biological) data is very crucial. In this context, we evaluate a large number of symmetric (block) ciphers in terms of various metrics: average and peak power consumption, total energy budget, encryption rate and efficiency, program-code size and security level. For our study we use XTREM, a performance and power simulator for Intel's XScale embedded processor. Findings indicate the best-performing ciphers across most metrics to be MISTY1 (scores high in 5 out of 6 imposed metrics), IDEA and RC6 (both present in 4 out of 6 metrics). Further profiling of MISTY1 indicates a clear dominance of load/store, move and logic-operation instructions which gives us explicit directions for designing the architecture of our novel processor.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {231--240},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366272},
 doi = {http://doi.acm.org/10.1145/1366230.1366272},
 acmid = {1366272},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {implantable devices, microarchitectural profiling, symmetric encryption, ultra-low power},
} 

@inproceedings{Jiang:2008:MIL:1366230.1366273,
 author = {Jiang, Weirong and Prasanna, Viktor K.},
 title = {Multi-terabit ip lookup using parallel bidirectional pipelines},
 abstract = {To meet growing terabit link rates, highly parallel and scalable architectures are needed for IP lookup engines in next generation routers. This paper proposes an SRAM-based multi-pipeline architecture for multi-terabit rate IP lookup. The architecture consists of multiple bidirectional linear pipelines, where each pipeline stores part of a routing table. We address the challenges of realizing such a solution. Two mapping schemes with different granularity are proposed to balance the memory distribution over different pipelines as well as across different stages in each pipeline. Also, IP caching is adopted to facilitate processing multiple packets per clock cycle. Instead of using large reorder buffers and complex logic, a lightweight scheduler and several small output delay queues are developed to preserve the intra-flow packet order. Simulation experiments using real-life data show that the proposed 4-pipeline architecture can store a core routing table with over 200K unique routing prefixes in less than 2 MB of memory, and can achieve a high throughput of up to 18.75 billion packets per second (GPPS), i.e. 6 Tbps for minimum size (40 bytes) packets.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {241--250},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366273},
 doi = {http://doi.acm.org/10.1145/1366230.1366273},
 acmid = {1366273},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bidirectional, ip lookup, pipeline, sram, terabit},
} 

@inproceedings{Zhou:2008:ISP:1366230.1366274,
 author = {Zhou, Peng and \~{O}nder, Soner},
 title = {Improving single-thread performance with fine-grain state maintenance},
 abstract = {We show that a multi-threaded processor that is aware of the processor state in a fine-grain manner can improve single-thread performance significantly by assigning the task of maintaining the correct processor state to an independent thread. We develop fine-grain state maintenance techniques that can be applied in multi-threaded environments and present a fine-grain state application of runahead execution where the data values dependent on a missed load are treated as damaged values. These values are verified and recovered as necessary by an independent thread. We evaluate an SMT-like fine grain state processor and show that it obtains an average of 38.9\% and up to 160.0\% better performance than coarse-grain baseline processors on the SPEC CFP2000 benchmark suite.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {251--260},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366274},
 doi = {http://doi.acm.org/10.1145/1366230.1366274},
 acmid = {1366274},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {checkpoint, processor state, recovery, runahead, simultaneous multi-threading},
} 

@inproceedings{Stone:2008:AAM:1366230.1366276,
 author = {Stone, Samuel S. and Haldar, Justin P. and Tsao, Stephanie C. and Hwu, Wen-mei W. and Liang, Zhi-Pei and Sutton, Bradley P.},
 title = {Accelerating advanced mri reconstructions on gpus},
 abstract = {Computational acceleration on graphics processing units (GPUs) can make advanced magnetic resonance imaging (MRI) reconstruction algorithms attractive in clinical settings, thereby improving the quality of MR images across a broad spectrum of applications. At present, MR imaging is often limited by high noise levels, significant imaging artifacts, and/or long data acquisition (scan) times. Advanced image reconstruction algorithms can mitigate these limitations and improve image quality by simultaneously operating on scan data acquired with arbitrary trajectories and incorporating additional information such as anatomical constraints. However, the improvements in image quality come at the expense of a considerable increase in computation. This paper describes the acceleration of an advanced reconstruction algorithm on NVIDIA's Quadro FX 5600. Optimizations such as register allocating the voxel data, tiling the scan data, and storing the scan data in the Quadro's constant memory dramatically reduce the reconstruction's required bandwidth to on-chip memory. The Quadro's special functional units provide substantial acceleration of the trigonometric computations in the algorithm's inner loops, and experimentally-tuned code transformations increase the reconstruction's performance by an additional 20\%. The reconstruction of a 3D image with 128^3 voxels ultimately achieves 150 GFLOPS and requires less than two minutes on the Quadro, while reconstruction on a quad-core CPU is thirteen times slower. Furthermore, relative to the true image, the error exhibited by the advanced reconstruction is only 12\%, while conventional reconstruction techniques incur error of 42\%. In short, the acceleration afforded by the GPU greatly increases the appeal of the advanced reconstruction for clinical MRI applications.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {261--272},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1366230.1366276},
 doi = {http://doi.acm.org/10.1145/1366230.1366276},
 acmid = {1366276},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cuda, gpgpu, gpu computing, mri, reconstruction},
} 

@inproceedings{Rodrigues:2008:GAC:1366230.1366277,
 author = {Rodrigues, Christopher I. and Hardy, David J. and Stone, John E. and Schulten, Klaus and Hwu, Wen-Mei W.},
 title = {GPU acceleration of cutoff pair potentials for molecular modeling applications},
 abstract = {The advent of systems biology requires the simulation of ever-larger biomolecular systems, demanding a commensurate growth in computational power. This paper examines the use of the NVIDIA Tesla C870 graphics card programmed through the CUDA toolkit to accelerate the calculation of cutoff pair potentials, one of the most prevalent computations required by many different molecular modeling applications. We present algorithms to calculate electrostatic potential maps for cutoff pair potentials. Whereas a straightforward approach for decomposing atom data leads to low compute efficiency, a newer strategy enables fine-grained spatial decomposition of atom data that maps efficiently to the C870's memory system while increasing work-efficiency of atom data traversal by a factor of 5. The memory addressing flexibility exposed through CUDA's SPMD programming model is crucial in enabling this new strategy. An implementation of the new algorithm provides a greater than threefold performance improvement over our previously published implementation and runs 12 to 20 times faster than optimized CPU-only code. The lessons learned are generally applicable to algorithms accelerated by uniform grid spatial decomposition.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {273--282},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366277},
 doi = {http://doi.acm.org/10.1145/1366230.1366277},
 acmid = {1366277},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cuda, gpgpu, graphics processors, molecular dynamics},
} 

@inproceedings{Schellmann:2008:CMI:1366230.1366278,
 author = {Schellmann, Maraike and V\"{o}rding, J\"{u}rgen and Gorlatch, Sergei and Meil\"{a}nder, Dominik},
 title = {Cost-effective medical image reconstruction: from clusters to graphics processing units},
 abstract = {We demonstrate that for modern medical imaging applications, parallel implementations on traditional parallel architectures (clusters and multiprocessor servers) can be outperformed, both in terms of speed and cost-effectiveness, by new implementations on next-generation architectures like GPUs (Graphics Processing Units). Although, compared to clusters and multiprocessor servers, GPUs are rather small and much less expensive, they consist of several SIMD-processors and thus provide a high degree of parallelism. For an iterative image reconstruction algorithm---the list-mode OSEM--- we demonstrate, first, the limitations of parallel reconstructions with this algorithm on the traditional parallel architectures, and second, how the well-analyzed parallel strategies for traditional architectures can be adapted systematically to achieve fast reconstructions on the GPU.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {283--292},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366278},
 doi = {http://doi.acm.org/10.1145/1366230.1366278},
 acmid = {1366278},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {general purpose gpu programming, list-mode osem, medical image reconstruction, parallel programming},
} 

@inproceedings{Smullen:2008:ASR:1366230.1366280,
 author = {Smullen,IV, Clinton Wills and Tarapore, Shahrukh Rohinton and Gurumurthi, Sudhanva and Ranganathan, Parthasarathy and Uysal, Mustafa},
 title = {Active storage revisited: the case for power and performance benefits for unstructured data processing applications},
 abstract = {The proliferation of digital data has resulted in a mushrooming of data-intensive applications, especially in the area of unstructured data processing. Given the growing popularity of unstructured data processing applications (e.g., FlickrTM, Google MapsTM), it is important to rethink system architectures to efficiently run these applications, from both the performance and power viewpoints. In this paper, we revisit active storage, which proposed offloading computation to disk drive processors, as a possible system architecture for these applications. Unlike previous work, we evaluate the microarchitectural aspects of active storage and perform an in-depth examination of the design of the offload processors. Using a set of unstructured data processing benchmarks, we examine two choices along the I/O path where the computation can be offloaded in existing system architectures -- a disk drive processor and a disk array controller. Our evaluation demonstrates that there are interesting tradeoffs in the choice of each location and that microarchitectural enhancements to these processors can provide significant performance boosts. We show that active storage architectures can provide large power savings, by using lower-power processors along the I/O path, while exploiting the data-level parallelism on the storage side of the system.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {293--304},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1366230.1366280},
 doi = {http://doi.acm.org/10.1145/1366230.1366280},
 acmid = {1366280},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {active storage, performance, power, unstructured data},
} 

@inproceedings{Jin:2008:SDI:1366230.1366281,
 author = {Jin, Hai and Tao, Yongcai and Wu, Song and Shi, Xuanhua},
 title = {Scalable dht-based information service for large-scale grids},
 abstract = {Current grid information service is centralized or hierarchical and proves inefficient as grid scale rapidly increases. The introduction of P2P techniques into grids breaks an encouraging path. However, frequent join and departure of resource nodes require strong self-organization capacity of system to maintain their rigid structure. Moreover, arranging identifier space for P2P nodes is knotty and has great impact on system performance. If the identifier space is too large, some nodes will be overloaded. On the contrary, small identifier space will bring the same problem as millennium bug. To address the issues, this paper proposes a scalable DHT-based (Distributed Hash Table) Information Service (DIS) for grid system, which organizes grid resources into a DHT ring based on VO (Virtual Organization). To save the identifier space while retaining the scalability and system performance, only stable VOs can join DIS via a new DHT node, whereas volatile VOs join DIS through being the sub-domain of other VO. Experimental results show that DIS provides rapid resource query, strong scalability and high throughput, meanwhile avoiding the key node failure as well as the bottleneck problem.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {305--312},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1366230.1366281},
 doi = {http://doi.acm.org/10.1145/1366230.1366281},
 acmid = {1366281},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dht, grid computing, information service, p2p},
} 

@inproceedings{Estrada:2008:DEM:1366230.1366282,
 author = {Estrada, Trilce and Fuentes, Olac and Taufer, Michela},
 title = {A distributed evolutionary method to design scheduling policies for volunteer computing},
 abstract = {Volunteer Computing (VC) is a paradigm that uses idle cycles from computing resources donated by volunteers and connected through the Internet to compute large-scale, loosely-coupled simulations. A big challenge in VC projects is the scheduling of work-units across heterogeneous, volatile, and error-prone computers. The design of effective scheduling policies for VC projects involves subjective and time-demanding tuning that is driven by the knowledge of the project designer. VC projects are in need of a faster and project-independent method to automate the scheduling design. To automatically generate a scheduling policy, we must explore the extremely large space of syntactically valid policies. Given the size of this search space, exhaustive search is not feasible. Thus in this paper we propose to solve the problem using an evolutionary method to automatically generate a set of scheduling policies that are project-independent, minimize errors, and maximize throughput in VC projects. Our method includes a genetic algorithm where the representation of individuals, the fitness function, and the genetic operators are specifically tailored to get effective policies in a short time. The effectiveness of our method is evaluated with SimBA, a Simulator of BOINC Applications. Contrary to manually-designed scheduling policies that often perform well only for the specific project they were designed for and require months of tuning, our resulting scheduling policies provide better overall throughput across the different VC projects considered in this work and were generated by our method in a time window of one week.},
 booktitle = {Proceedings of the 5th conference on Computing frontiers},
 series = {CF '08},
 year = {2008},
 isbn = {978-1-60558-077-7},
 location = {Ischia, Italy},
 pages = {313--322},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1366230.1366282},
 doi = {http://doi.acm.org/10.1145/1366230.1366282},
 acmid = {1366282},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed systems, genetic algorithms, global computing, volatile systems},
} 

@inproceedings{Pattnaik:2007:RTT:1242531.1242532,
 author = {Pattnaik, Pratap},
 title = {Recent technological trends and their impact on system design},
 abstract = {Over the past two decades, significant advancements in VLSI technologies (e.g. Moore's Law), Network Bandwidth, and Disk storage capacity have fueled an unprecedented integration of information technologies into the global economy. These advances have enabled the I/T (information technology) community to develop and deploy containerized and composable software stacks, while providing adequate performance to the end users. This model of driving high programmer-productivity with responsiveness in the I/T enterprises is based partly on software containerization, This model also depends significantly on obtaining high performance and robustness from the hardware and the operating systems autonomically. Because programmer-productivity and system responsiveness are key factors for the global economic growth, significant research and development efforts have been invested in continuing the innovations in hardware and operating systems. This talk will discuss the recent technological breakthroughs in VLSI technologies (i.e. high k dielectric) and in disk recording technologies (e.g. perpendicular recording). I will also discuss the impacts of the design decisions in processor designs advancements targeted for the four canonical usage segments, namely, HPC, Commercial computation, Games, and Embedded Systems. I will also speculate on future technological advances that may potentially impact system design.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {1--2},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1242531.1242532},
 doi = {http://doi.acm.org/10.1145/1242531.1242532},
 acmid = {1242532},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {architecture, technology},
} 

@inproceedings{Puzak:2007:AEM:1242531.1242536,
 author = {Puzak, Thomas R. and Hartstein, A. and Emma, P. G. and Srinivasan, V. and Mitchell, Jim},
 title = {An analysis of the effects of miss clustering on the cost of a cache miss},
 abstract = {In this paper we describe a new technique, called pipeline spectroscopy, and use it to measure the cost of each cache miss. The cost of a miss is displayed (graphed) as a histogram, which represents a precise readout showing a detailed visualization of the cost of each cache miss throughout all levels of the memory hierarchy. We call the graphs 'spectrograms' because they reveal certain signature features of the processor's memory hierarchy, the pipeline, and the miss pattern itself. Next we provide two examples that use spectroscopy to optimize the processor's hardware or application's software. The first example demonstrates how a miss spectrogram can aid software designers in analyzing the performance of an application. The second example uses a miss spectrogram to analyze bus queueing. Our experiments show that performance gains of up to 8\% are possible. Detailed analysis of a spectrogram leads to much greater insight in pipeline dynamics, including effects due to miss cluster, miss overlap, prefetching, and miss queueing delays.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {3--12},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1242531.1242536},
 doi = {http://doi.acm.org/10.1145/1242531.1242536},
 acmid = {1242536},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {algorithm, cache, pipeline, spectrogram},
} 

@inproceedings{Dreslinski:2007:AHP:1242531.1242537,
 author = {Dreslinski, Ronald G. and Saidi, Ali G. and Mudge, Trevor and Reinhardt, Steven K.},
 title = {Analysis of hardware prefetching across virtual page boundaries},
 abstract = {Data cache prefetching in the L2 is at the forefront of pre-fetching research. In this paper we analyze the impact of virtual page boundaries on these prefetchers. Conservative measurements on real hardware show that 30-50\% of consecutive virtual pages are mapped to pages which are not consecutive in physical memory. Advanced hardware prefetching techniques that detect access patterns which span virtual page boundaries often end up prefetching data that is from the wrong physical page. Meanwhile, current simulation techniques for evaluating prefetching algorithms assume that all virtual pages are mapped consecutively. We show that not accounting for virtual page boundaries in simulation can lead to overestimates of as much as 29\% (9\% on average). We also show that a simple prefetch filter can improve performance up to 32\% (7\% on average) and recover the overestimated performance. This leads to the conclusion that although previous simulations may not have accounted for virtual page boundaries, the results they demonstrate are still attainable and that it is not necessary to simulate virtual page boundaries to get accurate results. However, actual hardware designers should take care to implement a simple filter or else their hardware may not show the same gains in performance as they did in simulation.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {13--22},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1242531.1242537},
 doi = {http://doi.acm.org/10.1145/1242531.1242537},
 acmid = {1242537},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {prefetching, virtual memory},
} 

@inproceedings{Meixner:2007:UMC:1242531.1242538,
 author = {Meixner, Albert and Sorin, Daniel J.},
 title = {Unified microprocessor core storage},
 abstract = {The organization and management of microprocessor storage structures (e.g., L1 caches, TLBs, etc.) is critical to the performance and energy consumption of the microprocessor. We propose and develop the first microprocessor that can dynamically allocate storage to the structures that need it. First, we replace each existing structure with a dedicated micro-cache (\&#956;cache) that is smaller than is typical for that structure. With the smaller sizes, these structures can be made faster and less energy-hungry than the original full-size versions. Second, we back up all of the \&#956;caches with a single Unified Core Storage (UCS). Storage in the multi-banked UCS is dynamically allocated, which alleviates performance bot-tlenecks. The primary benefits of UCS are a significant reduction of storage structure energy (36\% less on average) and a modest improvement in performance (9.5\% speedup on average).},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {23--34},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1242531.1242538},
 doi = {http://doi.acm.org/10.1145/1242531.1242538},
 acmid = {1242538},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {microarchitecture, power-efficiency, resource allocation, unified caching},
} 

@inproceedings{Shi:2007:AMD:1242531.1242539,
 author = {Shi, Weidong and Lee, Hsien-Hsin S.},
 title = {Accelerating memory decryption and authentication with frequent value prediction},
 abstract = {This paper presents a novel architectural technique to hide fetch latency overhead of hardware encrypted and authenticated memory. A number of recent secure processor designs have used memory block encryption and authentication to protect un-trusted external memory. However, the latency overhead of certain encryption modes or authentication schemes can be intolerably high. This paper proposes novel techniques called frequent value ciphertext speculation and frequent value MAC speculation that synergistically combine value prediction and the inherently pipelined cryptography hardware to address the issue of latency for memory decryption and authentication. Without sacrificing security, frequent value ciphertext speculation can speed up memory decryption or MAC integrity verification by speculatively encrypting predictable memory values and comparing the result ciphertext with the fetched ciphertext. In MAC speculation, a secure processor pre-computes MAC for speculated frequent values and compares the MAC result with the fetched MAC from memory. Using SPEC2000 benchmark suite and detailed architecture simulator, our results show that ciphertext speculation and MAC speculation can significantly improve performance for direct memory encryption modes based on only 8 most frequent 64-bit values. For eight benchmark programs, the speedup is over 10\\% and some benchmark programs achieve more than 20\% speedup. For counter mode encrypted memory, MAC speculation can also significantly reduce the authentication overhead.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {35--46},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1242531.1242539},
 doi = {http://doi.acm.org/10.1145/1242531.1242539},
 acmid = {1242539},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {message authentication, secure processors, value prediction},
} 

@inproceedings{Nieplocha:2007:EPM:1242531.1242541,
 author = {Nieplocha, Jarek and M\'{a}rquez, Andr\`{e}s and Feo, John and Chavarr\'{\i}a-Miranda, Daniel and Chin, George and Scherrer, Chad and Beagley, Nathaniel},
 title = {Evaluating the potential of multithreaded platforms for irregular scientific computations},
 abstract = {The resurgence of current and upcoming multithreaded architectures and programming models led us to conduct a detailed study to understand the potential of these platforms to increase the performance of data-intensive, irregular scientific applications. Our study is based on a power system state estimation application and a novel anomaly detection application applied to network traffic data. We also conducted a detailed evaluation of the platforms using microbenchmarks in order to gain insight into their architectural capabilities and their interaction with programming models and application software. The evaluation was performed on the Cray MTA-2 and the Sun Niagar.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {47--58},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1242531.1242541},
 doi = {http://doi.acm.org/10.1145/1242531.1242541},
 acmid = {1242541},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data-intensive applications, irregular scientific applications, multithreaded architectures},
} 

@inproceedings{Thorsen:2007:PGS:1242531.1242542,
 author = {Thorsen, Oystein and Smith, Brian and Sosa, Carlos P. and Jiang, Karl and Lin, Heshan and Peters, Amanda and Feng, Wu-chun},
 title = {Parallel genomic sequence-search on a massively parallel system},
 abstract = {In the life sciences, genomic databases for sequence search have been growing exponentially in size. As a result, faster sequence-search algorithms to search these databases continue to evolve to cope with algorithmic time complexity. The ubiquitous tool for such search is the Basic Local Alignment Search Tool (BLAST) [1] from the National Center for Biotechnology Information (NCBI). Despite continued algorithmic improvements in BLAST, it cannot keep up with the rate at which the database is exponentially increasing in size. Therefore, parallel implement-ations such as mpiBLAST have emerged to address this problem. The performance of such implementations depends on a myriad of factors including algorithmic, architectural, and mapping of the algorithm to the architecture. This paper describes modifications and extensions to a parallel and distributed-memory version of BLAST called mpiBLAST-PIO and how it maps to a massively parallel system, specifically IBM Blue Gene/L (BG/L). The extensions include a virtual file manager, a "multiple master" run-time model, efficient fragment distribution, and intelligent load balancing. In this study, we have shown that our optimized mpiBLAST-PIO on BG/L using a query with 28014 sequences and the NR and NT databases scales to 8192 nodes (two cores per node). The cases tested here are well suited for a massively parallel system.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {59--68},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1242531.1242542},
 doi = {http://doi.acm.org/10.1145/1242531.1242542},
 acmid = {1242542},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Blue Gene/L, massively parallel computer, mpiBLAST-PIO, multiple masters parallelization},
} 

@inproceedings{Perumalla:2007:STW:1242531.1242543,
 author = {Perumalla, Kalyan S.},
 title = {Scaling time warp-based discrete event execution to 104 processors on a <i>Blue Gene</i> supercomputer},
 abstract = {Lately, important large-scale simulation applications, such as emergency/event planning and response, are emerging that are based on discrete event models. The applications are characterized by their scale (several millions of simulated entities), their fine-grained nature of computation (microseconds per event), and their highly dynamic inter-entity event interactions. The desired scale and speed together call for highly scalable parallel discrete event simulation (PDES) engines. However, few such parallel engines have been designed or tested on platforms with thousands of processors. Here an overview is given of a unique PDES engine that has been designed to support Time Warp-style optimistic parallel execution as well as a more generalized mixed, optimistic-conservative synchronization. The engine is designed to run on massively parallel architectures with minimal overheads. A performance study of the engine is presented, including the first results to date of PDES benchmarks demonstrating scalability to as many as 16,384 processors, on an IBM Blue Gene</i> supercomputer. The results show, for the first time, the promise of effectively sustaining very large scale discrete event execution on up to 104 processors.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {69--76},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1242531.1242543},
 doi = {http://doi.acm.org/10.1145/1242531.1242543},
 acmid = {1242543},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {mixed-mode simulation, parallel discrete event simulation, reverse computation, state saving, time warp},
} 

@inproceedings{Frost-Murphy:2007:GFR:1242531.1242545,
 author = {Frost-Murphy, Sarah E. and DeBenedictis, Erik P. and Kogge, Peter M.},
 title = {General floorplan for reversible quantum-dot cellular automata},
 abstract = {This paper presents the Collapsed Bennett Layout, a general purpose floorplan for reversible quantum-dot cellular automata (QCA) circuits. In order to exploit the full density and speed potential of emerging nanodevices, the principles of reversible computing need to be incorporated into the design of nanoscale circuits and systems. The Collapsed Bennett Layout implements Bennett's algorithm in hardware, allowing any arbitrary logic function to be implemented reversibly in QCA.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {77--82},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1242531.1242545},
 doi = {http://doi.acm.org/10.1145/1242531.1242545},
 acmid = {1242545},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {quantum-dot cellular automata, reversible computing},
} 

@inproceedings{Whitney:2007:AGL:1242531.1242546,
 author = {Whitney, Mark and Isailovic, Nemanja and Patel, Yatish and Kubiatowicz, John},
 title = {Automated generation of layout and control for quantum circuits},
 abstract = {We present a computer-aided design flow for quantum circuits, complete with automatic layout and control logic extraction. To motivate automated layout for quantum circuits, we investigate grid-based layouts and show a performance variance of four times as we vary grid structure and initial qubit placement. We then propose two polynomial-time design heuristics: a greedy</i> algorithm suitable for small, congestion-free quantum circuits and a dataflow-based analysis</i> approach to placement and routing with implicit initial placement of qubits. Finally, we show that our dataflow-based heuristic generates better layouts than the state-of-the-art automated grid-based layout and scheduling mechanism in terms of latency and potential pipelinability, but at the cost of some area.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {83--94},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1242531.1242546},
 doi = {http://doi.acm.org/10.1145/1242531.1242546},
 acmid = {1242546},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CAD, control, ion trap, layout, quantum computing},
} 

@inproceedings{Bilardi:2007:MPH:1242531.1242533,
 author = {Bilardi, Gianfranco},
 title = {Models for parallel and hierarchical computation},
 abstract = {},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {95--96},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1242531.1242533},
 doi = {http://doi.acm.org/10.1145/1242531.1242533},
 acmid = {1242533},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache obliviousness, efficiency, hierarchical algorithms, memory hierarchies, models of computation, network obliviousness, parallel algorithms, parallel architectures, physical constraints on machines, portability},
} 

@inproceedings{Vandierendonck:2007:BOE:1242531.1242548,
 author = {Vandierendonck, Hans and Manet, Philippe and Delavallee, Thibault and Loiselle, Igor and Legat, Jean-Didier},
 title = {By-passing the out-of-order execution pipeline to increase energy-efficiency},
 abstract = {Out-of-order execution significantly increases the performanceof superscalar processors. The out-of-order execution mechanismis, however, energy-inefficient, which inhibits scaling superscalar processorsto high issue widths and large instruction windows. In this paper, we build on the observation that between 19\% and 36\% of the instructions are immediately ready for execution, even before entering the issue queue. Yet, these instructions proceed to the energy-consuming steps ofinstruction wake-up and select and they needlessly occupy space in theissue queue. To save energy, we propose for these instructions to by-pass the out-of-order execution core. Instead, we execute them on an energy-efficient single-issue in-order by-pass pipeline.The by-pass pipeline executes a significant fraction of all instructions,allowing performance-energy trade-offs with respect to the issue width of the out-of-order pipeline and to the issue queue size.By making these trade-offs, we show energy reductions of 53\% for the issue queue, 33\% for the register file and 31\% in the write-back and wake-up logic. Performance remains almost unaffected.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {97--104},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1242531.1242548},
 doi = {http://doi.acm.org/10.1145/1242531.1242548},
 acmid = {1242548},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy-efficiency, instruction scheduling, instruction wake-up, out-of-order execution},
} 

@inproceedings{Aasaraai:2007:CSP:1242531.1242549,
 author = {Aasaraai, Kaveh and Baniasadi, Amirali and Atoofian, Ehsan},
 title = {Computational and storage power optimizations for the O-GEHL branch predictor},
 abstract = {In recent years, highly accurate branch predictors have been proposed primarily for high performance processors. Unfortunately such predictors are extremely energy consuming and in some cases not practical as they come with excessive prediction latency. One example of such predictors is the O-GEHL predictor. To achieve high accuracy, O-GEHL relies on large tables and extensive computations and requires high energy and long prediction delay.In this work we propose power optimization techniques that aim at reducing both computational complexity and storage size for the O-GEHL predictor. We show that by eliminating unnecessary data from computations, we can reduce both predictor's energy consumption and delay. Moreover, we apply information theory findings to remove redundant storage, without any significant accuracy penalty. We reduce the dynamic and static power dissipated in the computational parts of the predictor by up to 74\% and 65\% respectively. Meantime we improve performance by up to 12\% as we make faster prediction possible.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {105--112},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1242531.1242549},
 doi = {http://doi.acm.org/10.1145/1242531.1242549},
 acmid = {1242549},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {O-GEHL, branch prediction, power-aware microarchitectures},
} 

@inproceedings{Cebrian:2007:AVD:1242531.1242550,
 author = {Cebrian, Juan M. and Aragon, Juan L. and Garcia, Jose M and Kaxiras, Stefanos},
 title = {Adaptive VP decay: making value predictors leakage-efficient designs for high performance processors},
 abstract = {Energy-efficient microprocessor designs are one of the major concerns in both high performance and embedded processor domains. Furthermore, as process technology advances toward deep submicron, static power dissipation becomes a new challenge to address, especially for large on-chip array structures such as caches or prediction tables. Value prediction emerged in the recent past as a very effective way of increasing processor performance by overcoming data dependences. The more accurate the value predictor is the more performance is obtained, at the expense of becoming a source of power consumption and a thermal hot spot, and therefore increasing its leakage. Recent techniques, aimed at reducing the leakage power of array structures such as caches, either switch off (non-state preserving) or reduce the voltage level (state-preserving) of unused array portions.In this paper we propose the design of leakage-efficient value predictors by applying adaptive decay techniques in order to disable unused entries in the prediction tables. As value predictors are implemented as non-tagged structures an adaptive decay scheme has no way to precisely determine the induced miss-ratio due to prematurely decaying an entry. This paper explores adaptive decay strategies suited for the particularities of value predictors (Stride, DFCM and FCM) studying the tradeoffs for these prediction structures, that exhibit different pattern access behaviour than caches, in order to reduce their leakage energy efficiently compromising neither VP accuracy nor the speedup provided. Results show average leakage energy reductions of 52\%, 70\% and 80\% for the Stride, DFCM and FCM value predictors of 20 KB respectively.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {113--122},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1242531.1242550},
 doi = {http://doi.acm.org/10.1145/1242531.1242550},
 acmid = {1242550},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache decay, energy efficient architectures, leakage, value prediction},
} 

@inproceedings{Sasaki:2007:IDT:1242531.1242551,
 author = {Sasaki, Hiroshi and Ikeda, Yoshimichi and Kondo, Masaaki and Nakamura, Hiroshi},
 title = {An intra-task dvfs technique based on statistical analysis of hardware events},
 abstract = {The importance and demand for various types of optimization techniques for program execution is growing rapidly. In particular, dynamic optimization techniques are regarded as important. Although conventional techniques usually generated an execution model for dynamic optimization by qualitatively analyzing the behaviors of computer systems in a knowledge-based manner, the proposed technique generates models by statistically analyzing the behaviors from quantitative data of hardware events. In the present paper, a novel dynamic voltage and frequency scaling (DVFS) method based on statistical analysis is proposed. The proposed technique is a hybrid technique in which static information, such as the breakpoint of program phases and, dynamic information, such as the number of cache misses given by the performance counter, are used together. Relationships between the performance and values of performance counters are learned statistically in advance. The compiler then inserts a run-time code for predicting the performance and setting the appropriate frequency/voltage depending on the predicted performance. The proposed technique can greatly reduce the energy consumption while satisfying soft timing constraints.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {123--130},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1242531.1242551},
 doi = {http://doi.acm.org/10.1145/1242531.1242551},
 acmid = {1242551},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DVFS, hardware performance counters, performance estimation, statistical analysis},
} 

@inproceedings{Dubach:2007:FCO:1242531.1242553,
 author = {Dubach, Christophe and Cavazos, John and Franke, Bj\"{o}rn and Fursin, Grigori and O'Boyle, Michael F.P. and Temam, Olivier},
 title = {Fast compiler optimisation evaluation using code-feature based performance prediction},
 abstract = {Performance tuning is an important and time consuming task which may have to be repeated for each new application and platform. Although iterative optimisation can automate this process, it still requires many executions of different versions of the program. As execution time is frequently the limiting factor in the number of versions or transformed programs that can be considered, what is needed is a mechanism that can automatically predict</i> the performance of a modified program without actually having to run it. This paper presents a new machine learning based technique to automatically predict the speedup of a modified program using a performance model based on the code features</i> of the tuned programs. Unlike previous approaches it does not require any prior learning over a benchmark suite. Furthermore, it can be used to predict the performance of any tuning and is not restricted to a prior seen trans-formation space. We show that it can deliver predictions with a high correlation coefficient and can be used to dramatically reduce the cost of search.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {131--142},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1242531.1242553},
 doi = {http://doi.acm.org/10.1145/1242531.1242553},
 acmid = {1242553},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {architecture, artificial neural networks, compiler optimisation, learning, machine, performance modelling},
} 

@inproceedings{Moseley:2007:IPP:1242531.1242554,
 author = {Moseley, Tipp and Connors, Daniel A. and Grunwald, Dirk and Peri, Ramesh},
 title = {Identifying potential parallelism via loop-centric profiling},
 abstract = {The transition to multithreaded, multi-core designs places a greater responsibility on programmers and software for improving performance; thread-level parallelism (TLP) will be increasingly relied upon in addition to instruction-level parallelism (ILP) and increased clock frequency. Deciding where to try to parallelize code is difficult, especially for large, complex applications or those where the original developers have moved on. Outer loops are relatively easy targets for parallelization, but traditional profilers focus primarily on functions and hot inner loops. To aid in programmers' parallelization efforts, we introduce the concept of loop-centric profiling to provide a hierarchical view of how much time is spent in a loop and the loops nested within it.This paper introduces two techniques for loop profiling. First, we describe an instrumentation-based approach that gathers highly detailed and accurate information about loop behavior. Second, we present a sampling approach that achieves similar results with negligible overhead. The paper concludes with a case study evaluating the tool on several SPEC 2000 benchmarks.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {143--152},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1242531.1242554},
 doi = {http://doi.acm.org/10.1145/1242531.1242554},
 acmid = {1242554},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {loop profiling, parallelization},
} 

@inproceedings{Vallee:2007:SMS:1242531.1242555,
 author = {Vallee, Geoffroy and Naughton, Thomas and Scott, Stephen L.},
 title = {System management software for virtual environments},
 abstract = {Recently there has been an increased interest in the use of system-level virtualization using mature solutions such as Xen, QEMU, or VMWare. These virtualization platforms are being used in distributed and parallelenvironments including high performance computing. The use of virtual machines within such environments introduces newchallenges to system management. These include tedious tasks such as deploying para-virtualized host operating systems to support virtual machine execution or virtual overlay networks to connect these virtual machines. Additionally, there is the problem of machine definition and deployment,which is complicated by differentiation in the underlying virtualizationtechnology. This paper discusses tools for the deployment and management of both hostoperating systems and virtual machines in clusters. We begin with an overview of system-level virtualization and move on to a description of tools that we have developed to aid with these environments. These tools extend prior work in the area of cluster installation, configuration and management.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {153--160},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1242531.1242555},
 doi = {http://doi.acm.org/10.1145/1242531.1242555},
 acmid = {1242555},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {OSCAR, configuration management, tools, virtualization},
} 

@inproceedings{Dimitroulakos:2007:UEF:1242531.1242557,
 author = {Dimitroulakos, Gregory and Galanis, Michalis D. and Kostaras, Nikos and Goutis, Costas E.},
 title = {A unified evaluation framework for coarse grained reconfigurable array architectures},
 abstract = {The efficiency of a coarse grained reconfigurable array architecture in terms of performance and hardware cost is hard to be determined. The large number of parameters that define an architecture instance and the mapping complexity makes the evaluation extremely difficult to accomplish without tool assistance. This paper investigates the four factors that are directly related with the efficiency of these architectures namely; the area, the clock frequency, the scheduling efficiency and performance. A unified exploration framework has been build for estimating the values of the 4 aforementioned factors for different architecture alternatives. The exploration framework consists of two parts: a) an existing retargetable compiler from which the mapping efficiency is estimated and b) from the parametric realization of the coarse grained reconfigurable array in hardware description language (VHDL). The latter is used for the estimation of the area and clock frequency of each architecture instance with the realization of the system in the 0.13\&#188;m process of ASIC technology. Also, the experiments refer to different architecture instances in terms of the processing elements. interconnection network, the register files. size, their number of input output ports, and finally the available bandwidth. Totally 72 architecture scenarios have been studied revealing how each characteristic influences performance and area for efficiently make design decisions.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {161--172},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1242531.1242557},
 doi = {http://doi.acm.org/10.1145/1242531.1242557},
 acmid = {1242557},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Modulo scheduling, architectural exploration, coarse-grained reconfigurable arrays, reconfigurable romputing},
} 

@inproceedings{Chavarria-Miranda:2007:APH:1242531.1242558,
 author = {Chavarria-Miranda, Daniel and Marquez, Andres},
 title = {Assessing the potential of hybrid hpc systems for scientific applications: a case study},
 abstract = {We have conducted a detailed study to understand the po-tential of hybrid CPU/FPGA High-Performance Computers for improving the performance of data-intensive, scientific applications. In particular, we have focused on an application in proteomics (Polygraph), which is representative of many types of computational analysis applications in the lifesciences: it focuses on extracting useful information from a large body of experimentally collected data (identifying ob-served peptide spectra collected from a mass spectrometer against a well-known protein database). Our preliminary analysis of Polygraph found that morethan half (51\%) of the computation time was spent in three routines. We have implemented an FPGA version of themost computationally-intensive routine (20\% of the time)on a Cray XD-1 system, and measured the overall speed up achieved in comparison to an optimized software version ofthe routine running on the Cray XD-1's native Opteron processors. We have achieved computational speedups of up to9.16. When we include data movement costs, the overall speedup is reduced to 1.78. We discuss the design and implementation strategies thatled to these results, as well as advantages and limitations we found on the Cray XD-1 platform. We also addressthe advantages and limitations of current development environments, as well as discuss relevant issues we found in our experience as hybrid CPU/FPGA programming model "users".},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {173--182},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1242531.1242558},
 doi = {http://doi.acm.org/10.1145/1242531.1242558},
 acmid = {1242558},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data-intensive applications, hybrid CPU/FPGA systems},
} 

@inproceedings{Kamil:2007:RHI:1242531.1242559,
 author = {Kamil, Shoaib and Pinar, Ali and Gunter, Daniel and Lijewski, Michael and Oliker, Leonid and Shalf, John},
 title = {Reconfigurable hybrid interconnection for static and dynamic scientific applications},
 abstract = {As we enter the era of peta-scale computing, system architects must plan for machines composed of tens or even hundreds of thousands of processors. Although fully connected networks such as fat-tree configurations currently dominate HPC interconnect designs, such approaches are inadequate for ultra-scale concurrencies due to the superlinear growth of component costs. Traditional low-degree interconnect topologies, such as 3D tori, have reemerged as a competitive solution due to the linear scaling of system components relative to the node count; however, such networks are poorly suited for the requirements of many scientific applications at extreme concurrencies. To address these limitations, we propose HFAST, a hybrid switch architecture that uses circuit switches to dynamically reconfigure lower-degree interconnects to suit the topological requirements of a given scientific application. This work presents several new research contributions. We develop an optimization strategy for HFAST mappings and demonstrate that efficiency gains can be attained across a broad range of static numerical computations. Additionally, we conduct an extensive analysis of the communication characteristics of a dynamically adapting mesh calculation and show that the HFAST approach can achieve significant advantages, even when compared with traditional fat-tree configurations. Overall results point to the promising potential of utilizing hybrid reconfigurable networks to interconnect future peta-scale architectures, for both static and dynamically adapting applications.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {183--194},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1242531.1242559},
 doi = {http://doi.acm.org/10.1145/1242531.1242559},
 acmid = {1242559},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive mesh refinement, feasibility, hybrid interconnects, petascale},
} 

@inproceedings{Jorrand:2007:QCC:1242531.1242534,
 author = {Jorrand, Philippe},
 title = {The quantum challenge to computer science},
 abstract = {Information is physical: the laws which govern its encoding, processing and communication are bound by those of its unavoidably physical embodiment. In today's informatics, information obeys the laws of Newton's and Maxwell's classical physics: this assertion holds all the way from commercial computers down to (up to?) their most abstract models, e.g. Turing machines and lambda-calculus. Today's computation and communication are classical. Research in quantum information processing and communication was born some twenty five years ago, as a child of two major scientific achievements of the 20th century, namely quantum physics and information sciences. The driving force of this interdisciplinary research is that of looking for the consequences of having information encoding, computation and communication based upon the laws of quantum physics, i.e. the ultimate knowledge that we have, today, of the foreign world of elementary particles, as described by quantum mechanics. Breakthroughs in cryptography, communications, information theory, algorithmics and, more recently, in abstract computational models, programming languages and semantics frameworks, have shown that this transplantation of information from classical to quantum has far reaching consequences, both quantitative and qualitative, and opens new avenues for research within the foundations of computer science.From a computer scientist's point of view, which is my point of view, I will explain the basics, survey the main achievements, and outline the current hot topics and major challenges of this promising and stimulating research. I will not assume any prior knowledge in quantum mechanics from the audience.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {195--196},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1242531.1242534},
 doi = {http://doi.acm.org/10.1145/1242531.1242534},
 acmid = {1242534},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {quantum information processing and communication},
} 

@inproceedings{Yamagiwa:2007:DIS:1242531.1242561,
 author = {Yamagiwa, Shinichi and Sousa, Leonel},
 title = {Design and implementation of a stream-based distributedcomputing platform using graphics processing units},
 abstract = {Anonymous use of computing resources spread over the world becomes one of the main goals in GRID environments. In GRID-based computing, the security of users or of contributors of computing resources is crucial to execute processes in a safe way. This paper proposes a new method for streambased processing in a distributed environment and also a novel method to solve the security matter under this kind of processing. It also presents the design of the distributed computing platform developed for stream-based processing, including the description of the local and remote execution methods, which are collectively designated by Caravela platform. The proposed flow-model is mapped on the distributed processing resources, connected through a network, by using the Caravela platform. This platform has been developed by the authors of this paper specifically for making use of the Graphics Processing Units available in recent personal computers. The paper also illustrates application of the Caravela platform to different types of processing, namely scientific computing and image/video processing. The presented experimental results show that significant improvements can be achieved with the use of GPUs against the use of general purpose processors.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {197--204},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1242531.1242561},
 doi = {http://doi.acm.org/10.1145/1242531.1242561},
 acmid = {1242561},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPGPU, GPU, GRID, caravela, flow-model, stream-based computing},
} 

@inproceedings{Yamagiwa:2007:DBO:1242531.1242562,
 author = {Yamagiwa, Shinichi and Sousa, Leonel and Ant\~{a}o, Diogo},
 title = {Data buffering optimization methods toward a uniform programming interface for gpu-based applications},
 abstract = {The massive computational power available in off-the shelf Graphics Processing Units (GPUs) can pave the way for its usage in general purpose applications. Current interfaces to program GPU operation are still oriented towards graphics processing. This paper is focused in disparities on those programming interfaces and proposes an extension to of the recently developed Caravela library that supports streambased computation. This extension implements effective methods to counterbalance the disparities and differences in graphics runtime environments. Experimental results show that these methods improve performance of GPU-based applications by more than 50\% and demonstrate that the proposed extended interface can be an effective solution for generalpurpose programming on GPUs.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {205--212},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1242531.1242562},
 doi = {http://doi.acm.org/10.1145/1242531.1242562},
 acmid = {1242562},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DirectX, OpenGL, general purpose processing, graphics processing unit},
} 

@inproceedings{Amamiya:2007:FCM:1242531.1242563,
 author = {Amamiya, Satoshi and Izumi, Masaaki and Matsuzaki, Takanori and Hasegawa, Ryuzo and Amamiya, Makoto},
 title = {Fuce: the continuation-based multithreading processor},
 abstract = {Current trends of research on multithreading processors are the chip multithreading (CMT), which aims to exploit thread level parallelism (TLP) and to improve performance of software built onalltraditional threading components, e.g. pthreads. However, CMT is principally a straight forward extension of conventionalall symmetric multiprocessor (SMP) techniques, and it will suffer from the same limits to scalable multithreaded processing ifallit is built only on the traditional sequential-computation-based framework. Consideringallthese limitations of sequential-processor-basedallmultithreading, we are taking another approach to developing a multithreading processor dedicated to thread level parallelism(TLP). Our processor, named Fuce, is based on continuation-based multithreading. A thread is defined as a block of sequentially ordered instructions which areall executed exclusively. Every execution of a thread is triggered by one or more events, each of which is called continuation. The hardware cost and performance of the Fuce processor areallevaluated by means of a hardware implementation on FPGA and software simulation.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {213--224},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1242531.1242563},
 doi = {http://doi.acm.org/10.1145/1242531.1242563},
 acmid = {1242563},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chip multi-processor, continuation-based multithread programming, multithreading, thread-level parallelism},
} 

@inproceedings{Kusakabe:2007:SCF:1242531.1242564,
 author = {Kusakabe, Shigeru and Aono, Mitsuhiro and Izumi, Masaaki and Amamiya, Satoshi and Nomura, Yoshinari and Taniguchi, Hideo and Amamiya, Makoto},
 title = {Scalability of continuation-based fine-grained multithreading in handling multiple I/O requests on FUCE},
 abstract = {Multi-tasking operating systems not only handle concurrent tasksbut also have concurrency and parallelism of various granularities in themselves. We claim that operating systems need to be developed based on a computation model that can deal with concurrency and parallelism of various granularities. In order to investigate this claim, we have been developing an operating system called CEFOS based on a dataflow-based computation model. A program for CEFOS consists of zero-waitthreads, each of which runs to completion without suspension once started. Synchronization between zero-wait threads is performed in a dataflow manner according to their continuation relations. Handler routines for I/O devices are also realized with zero-wait threads and executed under the continuation-based multithreading mechanism. We can eliminate "interrupts" that interfere with the execution ofinstruction streams in typical conventional approaches, and we can naturally handle concurrency and exploit parallelism in programs even for I/O-centric computation. In this paper, after introducing our model and our operating system based on the model, we discuss implementation issues on Fuce, which is a continuation-based multithreading processor dedicated to fine-grained multithreading. Then we evaluate the scalability of our system with the number of execution units and I/O devices.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {225--236},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1242531.1242564},
 doi = {http://doi.acm.org/10.1145/1242531.1242564},
 acmid = {1242564},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {I/O, fine-grained multithreading, operating systems},
} 

@inproceedings{Tolentino:2007:MPR:1242531.1242566,
 author = {Tolentino, Matthew E. and Turner, Joseph and Cameron, Kirk W.},
 title = {Memory-miser: a performance-constrained runtime system for power-scalable clusters},
 abstract = {Main memory in clusters may dominate total system power. The resulting energy consumption increases system operating cost and the heat produced reduces reliability. Emergent memory technology will provide servers with the ability to dynamically turn-on (online) and turn-off (offline) memory devices at runtime. This technology, coupled with slack in memory demand, offers the potential for significant energy savings in clusters of servers. Enabling power-aware memory and conserving energy in clusters are non-trivial. First, power-aware memory techniques must be scalable to thousands of devices. Second, techniques must not negatively impact the performance of parallel scientific applications. Third, techniques must be transparent to the user to be practical. We propose a Memory Management Infra-Structure for Energy Reduction (Memory MISER). Memory MISER is transparent, performance-neutral, and scalable. It consists of a prototype Linux kernel that manages memory at device granularity and a userspace daemon that monitors memory demand systemically to control devices and implement energy- and performance-constrained policies. Experiments on an 8-node cluster show our control daemon reduces memory energy up to 56.8\% with \&lt;1\% performance degradation for several classes of parallel scientific codes. Our daemon uses a PID controller to conservatively offline memory and aggressively online memory at runtime. For multi-user workloads where memory demand often spikes dramatically, Memory MISER can save up to 67.94\% of memory energy with \&lt;1\% performance degradation. Current IBM eServer systems support up to 2 terabytes of SDRAM per node and 16 processors. For a server-based cluster with 8 90-watt processors and 32 GB of SDRAM per processor, Memory MISER can save about 30\% total system energy for multi-user parallel workloads.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {237--246},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1242531.1242566},
 doi = {http://doi.acm.org/10.1145/1242531.1242566},
 acmid = {1242566},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {control, memory, power management, resource allocation},
} 

@inproceedings{Becchi:2007:PEC:1242531.1242567,
 author = {Becchi, Michela and Franklin, Mark A. and Crowley, Patrick J.},
 title = {Performance/area efficiency in chip multiprocessors with micro-caches},
 abstract = {This paper proposes the use of very small instruction caches, called micro-caches (\&#956;-caches), consisting of tens to hundreds of bytes, at the bottom of the instruction delivery hierarchy in chip-multiprocessors (CMP). Multi-core architectures place a novel emphasis on the performance/area efficiency of processor cores, and we note that traditional instruction cache sizes reflect an emphasis on hit-rate performance rather than efficiency. In brief, \&#188;-caches reduce the area footprint of individual cores, thus allowing additional cores to fit within a given die area. We use commercial design tools and a commercial processor core to evaluate this tradeoff in the context of high-performance networking, where CMP architectures have had their greatest commercial impact to date. Our results suggest that the use of u-caches can yield a 25\% improvement in efficiency relative to traditional hierarchies. In our evaluation, we consider a range of architectural options (cluster organization, non-blocking caches, cache parameters) and justify our conclusions while accounting for the errors inherent in die area estimates.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {247--258},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1242531.1242567},
 doi = {http://doi.acm.org/10.1145/1242531.1242567},
 acmid = {1242567},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache hierarchies, chip multiprocessor, networking workload},
} 

@inproceedings{Atoofian:2007:SSI:1242531.1242568,
 author = {Atoofian, Ehsan and Baniasadi, Amirali and Aasaraai, Kaveh},
 title = {Speculative supplier identification for reducing power of interconnects in snoopy cache coherence protocols},
 abstract = {In this work we reduce interconnect power dissipation in Symmetric Multiprocessors or SMPs. We revisit snoopy cache coherence protocols and reduce unnecessary interconnect activity by speculating nodes expected to provide a missing data. Conventional snoopy cache coherence protocols broadcast requests to all nodes, reducing the latency of cache to cache transfer misses at the expense of increasing interconnect power. We show that it is possible to reduce the associated power dissipation if such requests are broadcasted selectively and only to nodes more likely to provide the missing data. We reduce power as we limit access only to the interconnect components between the requester and the supplier node. We evaluate our technique using shared memory applications and show that it is possible to reduce interconnect power by 21\% in a 4-way multiprocessor without compromising performance. This comes with negligible hardware overhead.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {259--266},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1242531.1242568},
 doi = {http://doi.acm.org/10.1145/1242531.1242568},
 acmid = {1242568},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SMP, cache coherence protocol, interconnect, power},
} 

@inproceedings{Ramdas:2007:CMT:1242531.1242570,
 author = {Ramdas, Tirath and Egan, Gregory K. and Abramson, David and Baldridge, Kim},
 title = {Converting massive TLP to DLP: a special-purpose processor for molecular orbital computations},
 abstract = {We propose an application specific processor for computational quantum chemistry. The kernel of interest is the computation of electron repulsion integrals (ERIs), which vary in control flow with different input data. This lack of uniformity limits the level of data-level parallelism (DLP) inherent in the application, thus apparently rendering a SIMD architecture unfeasible. All ERIs may be computed in parallel, therefore there is much thread-level parallelism (TLP). We observe that it is possible to match threads with certain characteristics in a manner that reveals significant DLP across multiple threads. Our thread matching and scheduling scheme effectively converts TLP to DLP, allowing SIMD processing which was previously unfeasible. We envision that this approach may expose DLP in other applications traditionally considered to be poor candidates for SIMD computation.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {267--276},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1242531.1242570},
 doi = {http://doi.acm.org/10.1145/1242531.1242570},
 acmid = {1242570},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {address generation, content-addressable memory, data-level parallelism, thread-level parallelism, vector processing},
} 

@inproceedings{Marquet:2007:MPP:1242531.1242571,
 author = {Marquet, Philippe and Duquennoy, Simon and Le Beux, S\'{e}bastien and Meftali, Samy and Dekeyser, Jean-Luc},
 title = {Massively parallel processing on a chip},
 abstract = {MppSoC is a SIMD architecture composed of a grid of processors andmemories connected by a X-Net neighbourhood network and a general purpose global router. MppSoC is an evolution of the famous massively parallel systems proposed at the end of the eighties. We claim that today such a machine may be integrated in a single chip. On one side, new design methodologies such as IP reuse and, on the other side, thepossible high level of integration on a chip let us envisage sucha revival. Some improvements of the system architecture are possible because of the high degree of integration: The mppSoC processing elements sharemost of their design with the control processor, the integrated network allows to exchange data between PEs, but also between thecontrol processor and the PE memories, and even to connect the external devices to the system. This paper presents the mppSoC architecture, a cycle-accurate bit-accurate SystemC simulator of this architecture, and a prototype of implementation on FPGA. A complete tool chain and the execution ofsome applications on the simulator and the FPGA implementation validate the modeling choices and show the effectiveness of this design.},
 booktitle = {Proceedings of the 4th international conference on Computing frontiers},
 series = {CF '07},
 year = {2007},
 isbn = {978-1-59593-683-7},
 location = {Ischia, Italy},
 pages = {277--286},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1242531.1242571},
 doi = {http://doi.acm.org/10.1145/1242531.1242571},
 acmid = {1242571},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MasPar, SIMD, SoC, massively parallelism, mppSoC, system-on-a-chip},
} 

@inproceedings{Gschwind:2006:CMC:1128022.1128023,
 author = {Gschwind, Michael},
 title = {Chip multiprocessing and the cell broadband engine},
 abstract = {Chip multiprocessing has become an exciting new direction for system designers to deliver increased performance by exploiting CMOS scaling. We discuss key design decisions facing the system architect of a chip multiprocessor and describe how these choices were made in the design of the Cell Broadband Engine.An important decision is whether to base system performance on thread-level parallelism alone, or to complement thread-level parallelism with other forms of parallelism. Depending on workload characteristics, providing parallelism at the processor core level may increase overall system efficiency.Parallelism is also a key to utilize available memory bandwidth more efficiently, by overlapping and interleaving multiple accesses to system memory. By interleaving the access streams of multiple threads, memory level parallelism can be increased to allow better memory interface utilization. In addition, compute-transfer parallelism (CTP) offers a new form of parallelism to initiate memory transfers under software control without stalling the requesting thread.We describe how the Cell Broadband Enginetmuses parallelism at all levels of the system abstraction to deliver a quantum leap in application performance, and how the Cell Synergistic Memory Flow engine exploits compute-transfer level parallelism by providing efficient block transfer capabilities.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {1--8},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1128022.1128023},
 doi = {http://doi.acm.org/10.1145/1128022.1128023},
 acmid = {1128023},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cell broadband engine, chip multiprocessing, compute-transfer parallelism (CTP), heterogeneous chip multiprocessor, memory-level parallelism (MLP)},
} 

@inproceedings{Williams:2006:PCP:1128022.1128027,
 author = {Williams, Samuel and Shalf, John and Oliker, Leonid and Kamil, Shoaib and Husbands, Parry and Yelick, Katherine},
 title = {The potential of the cell processor for scientific computing},
 abstract = {The slowing pace of commodity microprocessor performance improvements combined with ever-increasing chip power demands has become of utmost concern to computational scientists. As a result, the high performance computing community is examining alternative architectures that address the limitations of modern cache-based designs. In this work, we examine the potential of using the forthcoming STI Cell processor as a building block for future high-end computing systems. Our work contains several novel contributions. First, we introduce a performance model for Cell and apply it to several key scientific computing kernels: dense matrix multiply, sparse matrix vector multiply, stencil computations, and 1D/2D FFTs. The difficulty of programming Cell, which requires assembly level intrinsics for the best performance, makes this model useful as an initial step in algorithm design and evaluation. Next, we validate the accuracy of our model by comparing results against published hardware results, as well as our own implementations on the Cell full system simulator. Additionally, we compare Cell performance to benchmarks run on leading superscalar (AMD Opteron), VLIW (Intel Itanium2), and vector (Cray X1E) architectures. Our work also explores several different mappings of the kernels and demonstrates a simple and effective programming model for Cell's unique architecture. Finally, we propose modest microarchitectural modifications that could significantly increase the efficiency of double-precision calculations. Overall results demonstrate the tremendous potential of the Cell architecture for scientific computations in terms of both raw performance and power efficiency.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {9--20},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1128022.1128027},
 doi = {http://doi.acm.org/10.1145/1128022.1128027},
 acmid = {1128027},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FFT, GEMM, SpMV, cell processor, sparse matrix, stencil, three level memory},
} 

@inproceedings{AlKhatib:2006:MEB:1128022.1128028,
 author = {Al Khatib, Iyad and Bertozzi, Davide and Poletti, Francesco and Benini, Luca and Jantsch, Axel and Bechara, Mohamed and Khalifeh, Hasan and Hajjar, Mazen and Nabiev, Rustam and Jonsson, Sven},
 title = {MPSoC ECG biochip: a multiprocessor system-on-chip for real-time human heart monitoring and analysis},
 abstract = {The interest in high performance chip architectures for biomedical applications is on the rise. Heart diseases remain by far the main cause of death and a challenging problem for biomedical engineers to monitor and analyze. Electrocardiography (ECG) is an essential practice in heart medicine, which faces computational challenges, especially when 12 lead signals are to be analyzed in parallel, in real time, and under increasing sampling frequencies. Another challenge is the analysis of huge amounts of data that may grow to days of recordings. Nowadays, doctors use eyeball monitoring of the 12-lead ECG paper readout, which may seriously impair analysis accuracy. Our solution leverages the advance in multi-processor system-on-chip architectures, and is centered on the parallelization of the ECG computation kernel. It improves upon state-of-the-art mostly for its capability to perform real-time analysis of input data, leveraging the computation horsepower provided by many concurrent DSPs, more accurate diagnosis of cardiac diseases, and prompter reaction to abnormal heart alterations. The design methodology to go from the 12-lead ECG application specification to the final hardware/software architecture, modeling, and simulation is the focus of this paper. Our system model is based on industrial components. The architectural template we employ is scalable and flexible.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {21--28},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1128022.1128028},
 doi = {http://doi.acm.org/10.1145/1128022.1128028},
 acmid = {1128028},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {electrocardiogram algorithms, hardware space exploration, multiprocessor system-on-chip, real-time analysis},
} 

@inproceedings{Becchi:2006:DTA:1128022.1128029,
 author = {Becchi, Michela and Crowley, Patrick},
 title = {Dynamic thread assignment on heterogeneous multiprocessor architectures},
 abstract = {In a multi-programmed computing environment, threads of execution exhibit different runtime characteristics and hardware resource requirements. Not only do the behaviors of distinct threads differ, but each thread may also present diversity in its performance and resource usage over time. A heterogeneous chip multiprocessor (CMP) architecture consists of processor cores and caches of varying size and complexity. Prior work has shown that heterogeneous CMPs can meet the needs of a multi-programmed computing environment better than a homogeneous CMP system. In fact, the use of a combination of cores with different caches and instruction issue widths better accommodates threads with different computational requirements.A central issue in the design and use of heterogeneous systems is to determine an assignment of tasks to processors which better exploits the hardware resources in order to improve performance. In this paper we argue that the benefits of heterogeneous CMPs are bolstered by the usage of a dynamic assignment policy, i.e., a runtime mechanism which observes the behavior of the running threads and exploits thread migration between the cores. We validate our analysis by means of simulation. Specifically, our model assumes a combination of Alpha EV5 and Alpha EV6 processors and of integer and floating point programs from the SPEC2000 benchmark suite. We show that a dynamic assignment can outperform a static one by 20\% to 40\% on average and by as much as 80\% in extreme cases, depending on the degree of multithreading simulated.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {29--40},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1128022.1128029},
 doi = {http://doi.acm.org/10.1145/1128022.1128029},
 acmid = {1128029},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chip multiprocessor, heterogeneous architectures, simulation},
} 

@inproceedings{delCuvillo:2006:LOC:1128022.1128030,
 author = {del Cuvillo, Juan and Zhu, Weirong and Gao, Guang},
 title = {Landing openMP on cyclops-64: an efficient mapping of openMP to a many-core system-on-a-chip},
 abstract = {This paper presents our experience mapping OpenMP parallel programming model to the IBM Cyclops-64 (C64) architecture. The C64 employs a many-core-on-a-chip design that integrates processing logic (160 thread units), embedded memory (5MB) and communication hardware on the same die. Such a unique architecture presents new opportunities for optimization. Specifically, we consider the following three areas: (1) a memory aware runtime library that places frequently used data structures in scratchpad memory; (2) a unique spin lock algorithm for shared memory synchronization based on in-memory atomic instructions and native support for thread level execution; (3) a fast barrier that directly uses C64 hardware support for collective synchronization. All three optimizations together, result in an 80\% overhead reduction for language constructs in OpenMP. We believe that such a drastic reduction in the cost of managing parallelism makes OpenMP more amenable for writing parallel programs on the C64 platform.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {41--50},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1128022.1128030},
 doi = {http://doi.acm.org/10.1145/1128022.1128030},
 acmid = {1128030},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chip multiprocessor, openMP, performance evaluation, run-time system, system-on-a-chip},
} 

@inproceedings{Tsarchopoulos:2006:CFR:1128022.1128024,
 author = {Tsarchopoulos, Panagiotis},
 title = {Current and future research directions in embedded systems: a european perspective},
 abstract = {},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {51--52},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1128022.1128024},
 doi = {http://doi.acm.org/10.1145/1128022.1128024},
 acmid = {1128024},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {European research, computing architectures, embedded systems},
} 

@inproceedings{Bhattacharyya:2006:MAC:1128022.1128032,
 author = {Bhattacharyya, Arnab},
 title = {Morphogenesis as an amorphous computation},
 abstract = {In this paper, we present a programming language viewpoint for morphogenesis, the process of shape formation during embryological development. Specifically, we model morphogenesis as a self-organizing, self-repairing amorphous computation and describe a framework through which we can program large-scale shape formation by giving local instructions to cell-like objects. Then, using this programmatic perspective, we specify some example developmental processes and discuss the characteristics that make them suitable candidates for evolutionary variation and selection. Consistent with the theory of facilitated variation from evolutionary biology, we find that variation in developmental processes can be introduced and conserved due to the hierarchical organization of growth specification.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {53--64},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1128022.1128032},
 doi = {http://doi.acm.org/10.1145/1128022.1128032},
 acmid = {1128032},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {amorphous computing, emergent order, morphogenesis},
} 

@inproceedings{Eshaghian-Wilner:2006:NRM:1128022.1128033,
 author = {Eshaghian-Wilner, Mary M. and Khitun, Alex and Navab, Shiva and Wang, Kang},
 title = {A nano-scale reconfigurable mesh with spin waves},
 abstract = {In this paper, we present a nano-scale reconfigurable mesh that is interconnected with ferromagnetic spin-wave buses. The architecture described here, while requiring the same number of switches and buses as the standard reconfigurable meshes, is capable of simultaneously transmitting N waves on each of the spin-wave buses. Because of this highly parallel feature, very fast and fault-tolerant algorithms can be designed. Furthermore, unlike the traditional spin-based nano structures, which transmit charge, here waves are transmitted. As a result of this, the power consumption of the proposed modules may be low. And using phase logic, simple operations such as AND/OR/NOT can be performed efficiently on the transmitted waves.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {65--70},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1128022.1128033},
 doi = {http://doi.acm.org/10.1145/1128022.1128033},
 acmid = {1128033},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {nano-scale architectures, reconfigurable mesh, spin waves},
} 

@inproceedings{Udrescu:2006:IQG:1128022.1128034,
 author = {Udrescu, Mihai and Prodan, Lucian and Vl\u{a}du\c{t}iu, Mircea},
 title = {Implementing quantum genetic algorithms: a solution based on Grover's algorithm},
 abstract = {This paper presents a new methodology for running Genetic Algorithms on a Quantum Computer. To the best of our knowledge and according to reference [6]there are no feasible solutions for the implementation of the Quantum Genetic Algorithms (QGAs). We present a new perspective on how to build the corresponding QGA architecture. It turns out that the genetic strategy is not particularly helpful in our quantum computation approach; therefore our solution consists of designing a special-purpose oracle that will work with a modified version of an already known algorithm (maximum finding [1]), in order to reduce the QGAs to a Grover search. Quantum computation offers incentives for this approach, due to the fact that the qubit representation of the chromosome can encode the entire population as a superposition of basis-state values.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {71--82},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1128022.1128034},
 doi = {http://doi.acm.org/10.1145/1128022.1128034},
 acmid = {1128034},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {genetic algorithms, quantum computing},
} 

@inproceedings{Oliver:2006:TSS:1128022.1128036,
 author = {Oliver, John and Rao, Ravishankar and Brown, Michael and Mankin, Jennifer and Franklin, Diana and Chong, Frederic T. and Akella, Venkatesh},
 title = {Tile size selection for low-power tile-based architectures},
 abstract = {In this paper, we investigate the power implications of tile size selection for tile-based processors. We refer to this investigation as a tile granularity</i> study. This is accomplished by distilling the architectural cost of tiles with different computational widths into a system metric we call the Granularity Indicator</i> (GI). The GI is then compared against the communications exposed when algorithms are partitioned across multiple tiles. Through this comparison, the tile granularity that best fits a given set of algorithms can be determined, reducing the system power for that set of algorithms. When the GI analysis is applied to the Synchroscalar tile architecture[1], we find that Synchroscalar's already low power consumption can be further reduced by 14\% when customized for execution of the 802.11a reciever. In addition, the GI can also be a used to evaluate tile size when considering multiple applications simultaneously, providing a convenient platform for hardware-software co-design.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {83--94},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1128022.1128036},
 doi = {http://doi.acm.org/10.1145/1128022.1128036},
 acmid = {1128036},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {media processors, multi-core processors},
} 

@inproceedings{Waldman:2006:PCS:1128022.1128037,
 author = {Waldman, Israel and Pinter, Shlomit S.},
 title = {Profile-driven compression scheme for embedded systems},
 abstract = {The extensive usage of embedded systems involves running complex applications that require tightly limited resources such as memory and storage. One efficient way to satisfy the resource requirements is to reduce the code size through code compression. Our work describes a software-based code compression scheme that reduces the storage space of a program, which in turn induces a reduction of access time to off-chip memory in SoC embedded architectures. To select those sections of code that are most advantageous for compression, our scheme utilizes profiling information to evaluate and trade off storage space reduction for future run-time overhead. During run-time, the compressed parts are decompressed as necessary into a run-time buffer for execution. Experimental results on the SPEC CPU2000 and MediaBench suites show reduction in code size averaging 18.5\%, along with reasonable memory consumption overhead averaging 3.8\%, and a reasonable run-time overhead averaging 7.8\%.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {95--104},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1128022.1128037},
 doi = {http://doi.acm.org/10.1145/1128022.1128037},
 acmid = {1128037},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code compression, code size reduction, run-time decompression},
} 

@inproceedings{Son:2006:EDP:1128022.1128038,
 author = {Son, Seung Woo and Kandemir, Mahmut},
 title = {Energy-aware data prefetching for multi-speed disks},
 abstract = {Power consumption of disk based storage systems is becoming an increasingly pressing issue for both commercial and scientific application domains. Prior work proposed several hardware based approaches to reducing disk power consumption by making use of techniques such as spinning down idle disks and rotating them at lower speeds than the maximum speed possible. While such techniques are certainly very important, it is also critical to consider the influence the software can exercise in shaping the power consumption behavior of disk-intensive application programs. Motivated by this, the main goal of this work is to study whether an optimizing compiler can be used for increasing the power benefits that could be obtained from multi-speed disks. Specifically, we propose and experimentally evaluate a compiler-directed energy-aware data prefetching scheme for scientific applications that process disk-resident data sets. This scheme automatically determines the prefetch distance for all disk access instructions, the disk speeds to be employed, and the associated disk layouts (striping parameters) in a unified setting. We implemented the proposed approach within an optimizing compiler framework and conducted experiments with several disk-intensive applications. Our experimental evaluation shows that the proposed approach brings significant reductions in disk energy consumption over a state-of-the-art software-based I/O prefetching mechanism that does not take into account energy consumption explicitly. Our results also show that the energy-aware prefetching scheme does not bring any extra performance penalties and the energy reductions achieved are consistent across a wide spectrum of values of the simulation parameters.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {105--114},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1128022.1128038},
 doi = {http://doi.acm.org/10.1145/1128022.1128038},
 acmid = {1128038},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {low power, multi-speed disks, optimizing compiler, prefetching},
} 

@inproceedings{Mills:2006:ESC:1128022.1128025,
 author = {Mills, Jonathan W. and Parker, Matt and Himebaugh, Bryce and Shue, Craig and Kopecky, Brian and Weilemann, Chris},
 title = {"Empty space" computes: the evolution of an unconventional supercomputer},
 abstract = {Lee A. Rubel defined the extended analog computer to avoid the limitations of Shannon's general purpose analog computer. Partial differential equation solvers were a "quintessential" part of Rubel's theoretical machine. These components have been implemented with "empty space," or VLSI circuits without transistors, as well as conductive plastic. For the past decade research at Indiana University has explored the design and applications of extended analog computers. The machines have become increasingly sophisticated and flexible. The "empty" computational area is devoted to solving partial differential equations. The rest of the space includes fuzzy logic elements, configuration memory and input/output channels. This paper describes the theoretical definition, architecture and implementation of these unconventional computers. Two parallel applications are described in detail. Rubel's model can be viewed as an abstract specification for a distributed supercomputer. We close with a description of an inexpensive 64-node processor that was designed using our current single processor. The next step is to return to VLSI with an improved understanding of the architecture-and seek computation speeds approaching trillions of partial differential equations per second.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {115--126},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1128022.1128025},
 doi = {http://doi.acm.org/10.1145/1128022.1128025},
 acmid = {1128025},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Lukasiewicz logic, extended analog computer, general purpose analog computer, hybrid digital-analog architecture},
} 

@inproceedings{Yardimci:2006:DPM:1128022.1128040,
 author = {Yardimci, Efe and Franz, Michael},
 title = {Dynamic parallelization and mapping of binary executables on hierarchical platforms},
 abstract = {As performance improvements are being increasingly sought via coarse-grained parallelism, established expectations of continued sequential performance increases are not being met. Current trends in computing point towards platforms seeking performance improvements through various degrees of parallelism, with coarse-grained parallelism features becoming commonplace in even entry-level systems.Yet the broad variety of multiprocessor configurations that will be available that differ in the number of processing elements will make it difficult to statically create a single parallel version of a program that performs well on the whole range of such hardware. As a result, there will soon be a vast number of multiprocessor systems that are significantly under-utilized for lack of software that harnesses their power effectively. This problem is exacerbated by the growing inventory of legacy programs in binary executable form with possibly unreachable source code.We present a system that improves the performance of optimized sequential binaries through dynamic recompilation. Leveraging observations made at runtime, a thin software layer recompiles executing code compiled for a uniprocessor and generates parallelized and/or vectorized code segments that exploit available parallel resources. Among the techniques employed are control speculation, loop distribution across several threads, and automatic parallelization of recursive routines.Our solution is entirely software-based and can be ported to existing hardware platforms that have parallel processing capabilities. Our performance results are obtained on real hardware without using simulation.In preliminary benchmarks on only modestly parallel (2-way) hardware, our system already provides speedups of upto 40\% on SpecCPU benchmarks, and near-optimal speedups on more obviously parallelizable benchmarks.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {127--138},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1128022.1128040},
 doi = {http://doi.acm.org/10.1145/1128022.1128040},
 acmid = {1128040},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {continuous optimization, dynamic parallelization},
} 

@inproceedings{Oi:2006:IFH:1128022.1128041,
 author = {Oi, Hitoshi},
 title = {Instruction folding in a hardware-translation based java virtual machine},
 abstract = {Bytecode hardware-translation improves the performance of a Java Virtual Machine (JVM) with small hardware resource and complexity overhead. Instruction folding is a technique to further improve the performance of a JVM by reducing the redundancy in the stack-based instruction execution. However, the variable instruction length of the Java bytecode makes the folding logic complex. In this paper, we propose a folding scheme with reduced hardware complexity and evaluate its performance. For seven benchmark cases, the proposed scheme folded 6.6\% to 37.1\% of the bytecodes which correspond to 84.2\% to 102\% of the PicoJava-II's performance.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {139--146},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1128022.1128041},
 doi = {http://doi.acm.org/10.1145/1128022.1128041},
 acmid = {1128041},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hardware-translation, instruction folding, java virtual machine, performance evaluation},
} 

@inproceedings{Touati:2006:DPO:1128022.1128042,
 author = {Touati, Sid-Ahmed-Ali and Barthou, Denis},
 title = {On the decidability of phase ordering problem in optimizing compilation},
 abstract = {We are interested in the computing frontier around an essential question about compiler construction: having a program P and a set M of non parametric compiler optimization modules (called also phases), is it possible to find a sequence s of these phases such that the performance (execution time for instance) of the final generated program P\&#8242; is "optimal" ? We prove in this article that this problem is undecidable in two general schemes of optimizing compilation: iterative compilation and library optimization/generation. Fortunately, we give some simplified cases when this problem be-comes decidable, and we provide some algorithms (not necessary efficient) that can answer our main question. Another essential question that we are interested in is parame-ters space exploration in optimizing compilation (tuning optimizing compilation parameters). In this case, we assume a fixed sequence of optimization, but each optimization phase is allowed to have a parameter. We try to figure out how to compute the best parameter values for all program transformations when the compilation sequence is given. We also prove that this general problem is undecidable and we provide some simplified decidable instances.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {147--156},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1128022.1128042},
 doi = {http://doi.acm.org/10.1145/1128022.1128042},
 acmid = {1128042},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {input data I, iterative compilation, library generation, optimizing compilation, parameters space exploration, phase ordering},
} 

@inproceedings{Ozturk:2006:MCI:1128022.1128043,
 author = {Ozturk, Ozcan and Chen, Guangyu and Kandemir, Mahmut},
 title = {Multi-compilation: capturing interactions among concurrently-executing applications},
 abstract = {It is well known that while applying a compiler optimization to a large scope of code (e.g., an entire procedure or function) can bring larger benefits in return as compared to smaller scopes (e.g., a nested loop), code analysis and optimization at larger scopes are also more difficult to manage. As of today, the largest scope for a compiler optimization is an entire program source. However, as embedded chip multiprocessor architectures are finding their ways into commercial products, it is becoming important to consider the scenario of multiple applications executing on the same chip multiprocessor. This paper explores a novel technique called multi-compilation where multiple applications that are expected to be executed simultaneously on the same CMP (chip multiprocessor) are compiled together. The benefits of this approach include capturing the interactions amongst applications due to data sharing. While one can think of many potential optimizations that can work in an inter-application fashion exploiting data sharing across applications, we restrict ourselves in this paper to data layout optimization, which is the problem of determining the most suitable memory layout for array data. To demonstrate the impact of our contribution, we implemented our approach and performed a simulation-based study with several embedded applications. Our experimental results show that, by selecting the memory layouts of data arrays considering multiple applications at the same time, we can reduce cache misses by 18.7\% and execution cycles by 13.1\% on average.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {157--170},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1128022.1128043},
 doi = {http://doi.acm.org/10.1145/1128022.1128043},
 acmid = {1128043},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chip multiprocessor, compiler, multi-compilation},
} 

@inproceedings{Folcarelli:2006:ORS:1128022.1128046,
 author = {Folcarelli, Igino and Susu, Alex and Kluter, Ties and De Micheli, Giovanni and Acquaviva, Andrea},
 title = {An opportunistic reconfiguration strategy for environmentally powered devices},
 abstract = {Environmental energy is becoming a feasible alternative to traditional energy sources for ultra low-power devices such as sensor nodes and smart watches. Moreover, the increasing need for flexibility and reconfigurability of such devices makes its energy management even more challenging. As a result, to efficiently exploit the potentially unlimited environmental energy, new adaptation strategies are required. In this paper we present a novel system reconfiguration strategy that exploits the intrinsic unpredictability of environmental energy to opportunistically reconfigure the device. To assess the effectiveness of the proposed reconfiguration strategy we first perform a theoretical evaluation using statistical energy profile distribution and then we evaluate its energy efficiency on a prototype device in the presence of bursty energy profiles that we emulated using a programmable energy source.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {171--176},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1128022.1128046},
 doi = {http://doi.acm.org/10.1145/1128022.1128046},
 acmid = {1128046},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {reconfiguration, scavenging, sensor network},
} 

@inproceedings{Stoyanov:2006:UMC:1128022.1128047,
 author = {Stoyanov, Emil and Wischy, Markus and Roller, Dieter},
 title = {Using managed communication channels in software components},
 abstract = {The paper discusses the potential usage of principles from General System Theory (GST) and Cybernetics for design of Autonomic Software. Motivated by the characteristics of open systems and benefits of software communication management, we introduce the abstraction of Managed Communication Channels and propose general purpose architecture for composition and activation of communication channels. We illustrate examples of their application in different aspects of component oriented design for increase of overall system stability. Prototype of framework for autonomic component communication using the discussed principles is presented.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {177--186},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1128022.1128047},
 doi = {http://doi.acm.org/10.1145/1128022.1128047},
 acmid = {1128047},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {autonomic computing, managed communication, open systems, software evolution, software interoperability},
} 

@inproceedings{Kalte:2006:RTR:1128022.1128045,
 author = {Kalte, Heiko and Porrmann, Mario},
 title = {REPLICA2Pro: task relocation by bitstream manipulation in virtex-II/Pro FPGAs},
 abstract = {One vision of dynamic hardware reconfiguration is to deliver virtually unlimited hardware resources to a set of hardware tasks implementing arbitrary functions. By using partial reconfiguration, these tasks can be allocated and de-allocated on the reconfigurable architecture while others continue to operate. However, the exact placement of each task can only be determined during runtime according to the current resource allocation. This requires relocating each task from its original position after place and route to an area of available resources. The process of relocating tasks can result in a major time overhead. In order to solve this problem we have developed the REPLICA2Pro (Relocation per online Configuration Alteration in Virtex-2/-Pro) filter, which is capable of performing task relocations by manipulating the task's bitstream during the regular allocation process without any extra time overhead. The filter architecture, our reconfigurable system approach as well as our design flow and an experimental system setup are presented in this paper.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {403--412},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1128022.1128045},
 doi = {http://doi.acm.org/10.1145/1128022.1128045},
 acmid = {1128045},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FPGA, bitstream manipulation, reconfigurable computing, task relocation},
} 

@inproceedings{Prodan:2006:DPE:1128022.1128049,
 author = {Prodan, Lucian and Udrescu, Mihai and Vladutiu, Mircea},
 title = {A dependability perspective on emerging technologies},
 abstract = {Emerging technologies are set to provide further provisions for computing in times when the limits of current technology of microelectronics become an ever closer presence. A technology roadmap document lists biologically-inspired computing and quantum computing as two emerging technology vectors for novel computing architectures [43]. But the potential benefits that will come from entering the nanoelectronics era and from exploring novel nanotechnologies are foreseen to come at the cost of increased sensitivity to influences from the surrounding environment. This paper elaborates on a dependability perspective over these two emerging technology vectors from a designer's standpoint. Maintaining or increasing the dependability of unconventional computational processes is discussed in two different contexts: one of a bio-inspired computing architecture (the Embryonics project) and another of a quantum computational architecture (the QUERIST</i> project).},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {187--198},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1128022.1128049},
 doi = {http://doi.acm.org/10.1145/1128022.1128049},
 acmid = {1128049},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bio-inspired computing, bio-inspired digital design, dependability, embryonics, emerging technologies, evolvable hardware, fault-tolerance assessment, quantum computing, reliability},
} 

@inproceedings{Tempesti:2006:SRB:1128022.1128050,
 author = {Tempesti, Gianluca and Mange, Daniel and Mudry, Pierre-Andr\'{e} and Rossier, Jo\"{e}l and Stauffer, Andr\'{e}},
 title = {Self-replication for reliability: bio-inspired hardware and the embryonics project},
 abstract = {The growth and operation of all living beings are directed by the interpretation, in each of their cells, of a chemical program, the DNA string or genome</i>. This process is the source of inspiration for the Embryonics (embryonic electronics) project, whose final objective is the design of highly robust integrated circuits, endowed with properties usually associated with the living world: self-repair (cicatrization) and self-replication. The Embryonics architecture is based on four hierarchical levels of organization: 1) the basic primitive of our system is the molecule</i>, a multiplexer-based element of a novel programmable circuit; 2) a finite set of molecules makes up a cell, essentially a small processor with an associated memory; 3) a finite set of cells makes up an organism</i>, an application-specific multiprocessor system; 4) the organism can itself replicate, giving rise to a population</i> of identical organisms. In this paper, we provide an overview of our latest research in the domain of the self-replication of processing elements within a programmable logic substrate, a key prerequisite for achieving system-level fault tolerance in our bio-inspired approach.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {199--206},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1128022.1128050},
 doi = {http://doi.acm.org/10.1145/1128022.1128050},
 acmid = {1128050},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bio-inspired architectures, embryonic electronics, growth, hierarchical fault tolerance, self-repair, self-replication},
} 

@inproceedings{Tyrrell:2006:DEW:1128022.1128051,
 author = {Tyrrell, A. M.},
 title = {Dependability in an evolving world},
 abstract = {Evolvable hardware offers much for the future of complex system design. Evolutionary techniques not only give the potential of larger solution space, but when implemented on hardware allow system designs to adapt to changes in the environment, including failures in system components.This paper reviews a number of techniques, all based in the bio-inspired camp, that provide varying degrees of dependability over and above standard designs. In particular, three different techniques are considered: using FPGAs and continuous evolution to circumvent faults as and when they occur, using FPGAs and ideas from developmental biology to create designs that possess emergent fault tolerant properties, and finally we consider a novel ASIC, designed and built with bio-inspired systems in-mind, and show how this too can cope with unexpected events during operation.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {207--220},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1128022.1128051},
 doi = {http://doi.acm.org/10.1145/1128022.1128051},
 acmid = {1128051},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {POEtic, evolutionary algorithms, fault-tolerance},
} 

@inproceedings{Sekanina:2006:DFE:1128022.1128052,
 author = {Sekanina, Lukas},
 title = {On dependability of FPGA-based evolvable hardware systems that utilize virtual reconfigurable circuits},
 abstract = {This paper describes experiments conducted to estimate how the use of (area-demanding) virtual reconfigurable circuits (VRC) influences the dependability of FPGA-based evolvable systems. It is shown that these systems are not so sensitive to faults as their area-demanding implementations could evoke. Evolutionary techniques are utilized to design fault tolerant circuits in a virtual reconfigurable circuit and to perform their automatic functional recovery in case of occurence of faults in a configuration memory of FPGA. All the experiments are performed on models of reconfigurable devices. This paper does not claim that the use of the VRC improves the dependability; it shows how the use of VRCs could influence the dependability.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {221--228},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1128022.1128052},
 doi = {http://doi.acm.org/10.1145/1128022.1128052},
 acmid = {1128052},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FPGA, dependability, evolutionary algorithms, evolvable hardware},
} 

@inproceedings{Krishnan:2006:MEP:1128022.1128054,
 author = {Krishnan, Manojkumar and Nieplocha, Jarek},
 title = {Memory efficient parallel matrix multiplication operation for irregular problems},
 abstract = {Regular distributions for storing dense matrices on parallel systems are not always used in practice. In many scientific applicati RUMMA) [1] to handle irregularly distributed matrices. Our approach relies on a distribution independent algorithm that provides dynamic load balancing by exploiting data locality and achieves performance as good as the traditional approach which relies on temporary arrays with regular distribution, data redistribution, and matrix multiplication for regular matrices to handle the irregular case. The proposed algorithm is memory-efficient because temporary matrices are not needed. This feature is critical for systems like the IBM Blue Gene/L that offer very limited amount of memory per node. The experimental results demonstrate very good performance across the range of matrix distributions and problem sizes motivated by real applications.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {229--240},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1128022.1128054},
 doi = {http://doi.acm.org/10.1145/1128022.1128054},
 acmid = {1128054},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SRUMMA, global arrays, irregular distribution, parallel linear algebra, parallel matrix multiplication, parallel programming, remote memory access},
} 

@inproceedings{Garcia:2006:DHA:1128022.1128055,
 author = {Garcia, Philip and Korth, Henry F.},
 title = {Database hash-join algorithms on multithreaded computer architectures},
 abstract = {As the performance gap between main memory and modern processors widens, database algorithms must be adapted to be "architecture-aware" for optimal performance. We address this issue using the computation of hash join, one of the most important operations in database query processing, to study the impact of simultaneous multithreading (SMT) and main-memory latency (cache misses) on performance.Prior work [8] has studied cache misses on a simulation based on the Compaq ES40. Our results are obtained by measuring the performance of actual hardware (Intel Pentium and Xeon, and AMD Opteron) first for the single-threaded version of the hash-join algorithm used in the prior work and a new version designed for multiple threads.We found that hardware prefetching from main-memory data into CPU cache as implemented in the architectures we tested significantly reduces the real-world benefit of software prefetching (contrary to prior work on simulated systems). We found that SMT achieved significant speedup for our thread-aware hash join algorithm when compared with a single-threaded execution on the same single processor. Software prefetching also proved beneficial in this environment.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {241--252},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1128022.1128055},
 doi = {http://doi.acm.org/10.1145/1128022.1128055},
 acmid = {1128055},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SMT, database, hash-join, memory performance, multithreading, performance, software pipelining, software prefetching},
} 

@inproceedings{Shahbahrami:2006:IMB:1128022.1128056,
 author = {Shahbahrami, Asadollah and Juurlink, Ben and Vassiliadis, Stamatis},
 title = {Improving the memory behavior of vertical filtering in the discrete wavelet transform},
 abstract = {The discrete wavelet transform (DWT) is used in several image and video compression standards, in particular JPEG2000. A 2D DWT consists of horizontal filtering along the rows followed by vertical filtering along the columns. It is well-known that a straightforward implementation of vertical filtering (assuming a row-major layout) induces many cache misses, due to lack of spatial locality. This can be avoided by interchanging the loops. This paper shows, however, that the resulting implementation suffers significantly from 64K aliasing, which occurs in the Pentium 4 when two data blocks are accessed that are a multiple of 64K apart, and we propose two techniques to avoid it. In addition, if the filter length is longer than four, the number of ways of the L1 data cache of the Pentium 4 is insufficient to avoid cache conflict misses. Consequently, we propose two methods for reducing conflict misses. Although experimental results have been collected on the Pentium 4, the techniques are general and can be applied to other processors with different cache organizations as well. The proposed techniques improve the performance of vertical filtering compared to already optimized baseline implementations by a factor of 3.11 for the (5,3) lifting scheme, 3.11 for Daubechies' transform of four coefficients, and by a factor of 1.99 for the Cohen, Daubechies, and Feauveau 9/7 transform.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {253--260},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1128022.1128056},
 doi = {http://doi.acm.org/10.1145/1128022.1128056},
 acmid = {1128056},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache, discrete wavelet transform, memory hierarchy, performance},
} 

@inproceedings{Bitonti:2006:DTL:1128022.1128057,
 author = {Bitonti, Luigi and Kiss, Tamas and Terstyanszky, Gabor and Delaitre, Thierry and Winter, Stephen and Kacsuk, Peter},
 title = {Dynamic testing of legacy code resources on the grid},
 abstract = {The Grid Execution Management for Legacy Code Architecture (GEMLCA) enables the exposure of legacy applications as Grid services without code re-engineering and with minimum user effort. GEMLCA is integrated with the P-GRADE Grid portal that provides a user-friendly Web interface to convert legacy applications into Grid services, and to create, submit and monitor the execution of complex Grid workflows composed of legacy and non-legacy components. However, users cannot be sure that the selected resources where the execution of workflow components is mapped are functioning properly. This paper describes how the current GEMLCA P-GRADE portal architecture is extended with resource testing capabilities allowing users to select only tested GEMLCA and Globus resources prior to workflow execution, or when recovering a failed workflow. It is also outlined, how the architecture can be extended in order to enhance the capabilities of current production Grid systems and Grid brokers with reliable and accurate resource availability information.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {261--268},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1128022.1128057},
 doi = {http://doi.acm.org/10.1145/1128022.1128057},
 acmid = {1128057},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {grid computing, grid portal, grid service, legacy code, resource testing/monitoring},
} 

@inproceedings{Ramirez:2006:KPR:1128022.1128059,
 author = {Ram\'{\i}rez, Tanaus\'{u} and Pajuelo, Alex and Santana, Oliverio J. and Valero, Mateo},
 title = {Kilo-instruction processors, runahead and prefetching},
 abstract = {There is a continuous research effort devoted to overcome the memory wall problem. Prefetching is one of the most frequently used techniques. A prefetch mechanism anticipates the processor requests by moving data into the lower levels of the memory hierarchy. Runahead mechanism is another form of prefetching based on speculative execution. This mechanism executes speculative instructions under an L2 miss, preventing the processor from being stalled when the reorder buffer completely fills, and thus allowing the generation of useful prefetches. Another technique to alleviate the memory wall problem provides processors with large instruction windows, avoiding window stalls due to in-order commit and long latency loads. This approach, known as "Kilo-instruction processors", relies on exploiting more instruction level parallelism allowing thousands of in-flight instructions while long latency loads are outstanding in memory.In this work, we present a comparative study of the three above-mentioned approaches, showing their key issues and performance tradeoffs. We show that Runahead execution achieves better performance speedups (30\% on average) than traditional prefetch techniques (21\% on average). Nevertheless, the Kilo-instruction processor performs best (68\% on average). Kilo-instruction processors are not only faster but also generate a lower number of speculative instructions than Runahead. When combining the prefetching mechanism evaluated with Runahead and Kilo-instruction processor, the performance is improved even more in each case (49,5\% and 88,9\% respectively), although Kilo-instruction with prefetch achieves better performance and executes less speculative instructions than Runahead.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {269--278},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1128022.1128059},
 doi = {http://doi.acm.org/10.1145/1128022.1128059},
 acmid = {1128059},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Kilo-instruction processors, memory wall, prefetching, runahead, speculative execution},
} 

@inproceedings{Kumar:2006:ELA:1128022.1128060,
 author = {Kumar, Sailesh and Maschmeyer, John and Crowley, Patrick},
 title = {Exploiting locality to ameliorate packet queue contention and serialization},
 abstract = {Packet processing systems maintain high throughput despite relatively high memory latencies by exploiting the coarse-grained parallelism available between packets. In particular, multiple processors are used to overlap the processing of multiple packets. Packet queuing-the fundamental mechanism enabling packet scheduling, differentiated services, and traffic isolation-requires a read-modify-write operation on a linked list data structure to enqueue and dequeue packets; this operation represents a potential serializing bottleneck. If all packets awaiting service are destined for different queues, these read-modify-write cycles can proceed in parallel. However, if all or many of the incoming packets are destined for the same queue, or for a small number of queues, then system throughput will be serialized by these sequential external memory operations. For this reason, low latency SRAMs are used to implement the queue data structures. This reduces the absolute cost of serialization but does not eliminate it; SRAM latencies determine system throughput.In this paper we observe that the worst-case scenario for packet queuing coincides with the best-case scenario for caches</i>: i.e., when locality exists and the majority of packets are destined for a small number of queues. The main contribution of this work is the queuing cache, which consists of a hardware cache and a closely coupled queuing engine that implements queue operations. The queuing cache improves performance dramatically by moving the bottleneck from external memory onto the packet processor, where clock rates are higher and latencies are lower. We compare the queuing cache to a number of alternatives, specifically, SRAM controllers with: no queuing support, a software-controlled cache plus a queuing engine (like that used on Intel's IXP network processor), and a hardware cache. Relative to these models, we show that a queuing cache improves worst-case throughput by factors of 3.1, 1.5, and 2.1 and the throughput of real-world traffic traces by factors of 2.6, 1.3, and 1.75, respectively. We also show that the queuing cache decreases external memory bandwidth usage, on-chip communication, and the number of queuing instructions executed under best-case, worst-case and real-world traffic workloads. Based on our VHDL models, we conclude that a queuing cache could be implemented at a low cost relative to the resulting performance and efficiency benefits.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {279--290},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1128022.1128060},
 doi = {http://doi.acm.org/10.1145/1128022.1128060},
 acmid = {1128060},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {buffering, cache, packet queuing},
} 

@inproceedings{Alastruey:2006:SER:1128022.1128061,
 author = {Alastruey, Jes\'{u}s and Monreal, Teresa and Vi\~{n}als, V\'{\i}ctor and Valero, Mateo},
 title = {Speculative early register release},
 abstract = {The late release policy of conventional renaming keeps many registers in the register file assigned in spite of containing values that will never be read in the future. In this work, we study the potential of a novel scheme that speculatively releases a physical register as soon as it has been read by a predicted last instruction that references its value. An auxiliary register file placed outside the critical paths of the processor pipeline holds the early released values just in case they are unexpectedly referenced by some instruction. In addition to demonstrate the feasibility of a last-use predictor, this paper also analyzes the auxiliary register file (latency and size) required to support a speculative early release mechanism that uses a perfect predictor. The obtained results set the performance bound that any real speculative early release implementation is able to reach. We show that in a processor with a 64int+64fp register file, a perfect early release supported by an unbounded auxiliary register file has the potential of speeding up computations up to 23\% and 47\% for SPECint2000 and SPECfp2000 benchmarks, respectively. Speculative early release can also be used to reduce register file size without losing performance. For instance, a processor with a conventionally managed 96int+96fp register file could be replaced for equal IPC with a 64int+64fp register file managed with perfect early register release and backed with a 64int+64fp auxiliary register file, this representing a 12\% IPS (Instructions Per Second) increase if the processor frequency were constrained by the register file access time.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {291--302},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1128022.1128061},
 doi = {http://doi.acm.org/10.1145/1128022.1128061},
 acmid = {1128061},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {optimization, physical register release, register file, register renaming},
} 

@inproceedings{Derby:2006:VVI:1128022.1128062,
 author = {Derby, Jeff H. and Montoye, Robert K. and Moreira, Jos\'{e}},
 title = {VICTORIA: VMX indirect compute technology oriented towards in-line acceleration},
 abstract = {There is increasing interest in the use of accelerators in computer systems. Accelerators are processor-attached hardware units that can perform certain functions faster than the conventional general purpose processor. In this paper, we describe the VICTORIA PowerPC architecture, which is based on the iVMX accelerator technology. The iVMX accelerator extends the existing VMX architecture with indirect register addressing. That approach greatly extends the architected space of registers and opens the door for highly optimized vector algorithms that can sustain very high processing rates. The large space of registers is directly controlled by the executing code and offers a sufficiently large storage to hold sizeable intermediate results. This helps reduce the negative effects of limited memory bandwidth and high memory latency. The iVMX accelerator is an example of in-line accelerator; that is, the instructions that drive the accelerator are part of the same stream that drives the main processor. Compared to off-line accelerators, which execute their own instruction stream, in-line accelerators present a much more convenient programming model.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {303--312},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1128022.1128062},
 doi = {http://doi.acm.org/10.1145/1128022.1128062},
 acmid = {1128062},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SIMD, VMX, accelerators, powerPC},
} 

@inproceedings{Hartstein:2006:CMB:1128022.1128064,
 author = {Hartstein, A. and Srinivasan, V. and Puzak, T. R. and Emma, P. G.},
 title = {Cache miss behavior: is it \&\#8730;2?},
 abstract = {It has long been empirically observed that the cache miss rate decreased as a power law of cache size, where the power was approximately -1/2. In this paper, we examine the dependence of the cache miss rate on cache size both theoretically and through simulation. By combining the observed time dependence of the cache reference pattern with a statistical treatment of cache entry replacement, we predict that the cache miss rate should vary with cache size as an inverse power law for a first level cache. The exponent in the power law is directly related to the time dependence of cache references, and lies between -0.3 to -0.7. Results are presented for both direct mapped and set associative caches, and for various levels of the cache hierarchy. Our results demonstrate that the dependence of cache miss rate on cache size arises from the temporal dependence of the cache access pattern.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {313--320},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1128022.1128064},
 doi = {http://doi.acm.org/10.1145/1128022.1128064},
 acmid = {1128064},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache organization, memory hierarchy, performance},
} 

@inproceedings{Ros:2006:ECD:1128022.1128065,
 author = {Ros, Alberto and Acacio, Manuel E. and Garc\'{\i}a, Jos\'{e} M.},
 title = {An efficient cache design for scalable glueless shared-memory multiprocessors},
 abstract = {Traditionally, cache coherence in large-scale shared-memory multiprocessors has been ensured by means of a distributed directory structure stored in main memory. In this way, the access to main memory to recover the sharing status of the block is generally put in the critical path of every cache miss, increasing its latency. Considering the ever-increasing distance to memory, these cache coherence protocols are far from being optimal from the perspective of performance. On the other hand, shared-memory multiprocessors formed by connecting chips that integrate the processor, caches, coherence logic, switch and memory controller through a low-cost, low-latency point-to-point network (glueless shared-memory multiprocessors) are a reality.In this work, we propose a novel design for the L2 cache level, at which coherence has to be maintained, aimed at being used in glueless shared-memory multiprocessors. Our proposal splits the cache structure into two different parts: one for storing data and directory information for the blocks requested by the local processor, and another one for storing only directory information for blocks accessed by remote processors. Using this cache scheme we remove the directory from main memory. Besides saving memory space, our proposal brings very significant reductions in terms of latency of the cache misses (speed-ups of 3.0 on average), which translate into reductions in applications' execution time of 31\% on average.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {321--330},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1128022.1128065},
 doi = {http://doi.acm.org/10.1145/1128022.1128065},
 acmid = {1128065},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {L2, cache, cache coherence, directory structure, glueless shared-memory multiprocessors, memory wall},
} 

@inproceedings{Khunjush:2006:LDT:1128022.1128066,
 author = {Khunjush, Farshad and Dimopoulos, Nikitas J.},
 title = {Lazy direct-to-cache transfer during receive operations in a message passing environment},
 abstract = {The focus of this work is on techniques that promise to reduce the message delivery latency in message passing interface (MPI) environments. The main contributors to message delivery latency in message passing environments are the copying operations needed to transfer and bind a received message to the consuming process/thread. To reduce this copying overhead and to reach toward finer granularity, we introduce architectural extensions comprising of a specialized network cache</i> and instructions to manage the operations of this extension. In this work we study the caching environment and evaluate a new technique called Lazy Direct-to-Cache Transfer (DTCT)</i>. Our simulations show that messages can be bound and kept into a network cache</i> where they persist long enough to be consumed. We also demonstrate that lazy DTCT provides a significant reduction in the access latency for I/O intensive environments such as message passing configurations and SMPs without polluting the data cache.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {331--340},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1128022.1128066},
 doi = {http://doi.acm.org/10.1145/1128022.1128066},
 acmid = {1128066},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MPI, latency hiding, network cache},
} 

@inproceedings{Jeong:2006:SPR:1128022.1128068,
 author = {Jeong, Jaeheon and Stenstr\"{o}m, Per and Dubois, Michel},
 title = {Simple penalty-sensitive replacement policies for caches},
 abstract = {Classic cache replacement policies assume that miss costs are uniform. However, the correlation between miss rate and cache performance is not as straightforward as it used to be. Ultimately, the true cost measure of a miss should be the penalty, i.e. the actual processing bandwidth lost because of the miss. It is known that, contrary to loads, the penalty of stores is mostly hidden in modern processors. To take advantage of this observation, we propose simple schemes to replace load misses by store misses. We extend classic replacement algorithms such as LRU (Least Recently Used) and PLRU (Partial LRU) to reduce the aggregate miss penalty instead of the miss count.One key issue is to predict the next access type to a block, so that higher replacement priority is given to blocks that will be accessed next with a store. We introduce and evaluate various prediction schemes based on instructions, and broadly inspired from branch predictors. To guide the design we run extensive trace-driven simulations on eight Spec95 benchmarks with a wide range of cache configurations and observe that our simple penalty-sensitive policies yield positive load miss improvements over classic algorithms across most the benchmarks and cache configurations. In some cases the improvements are very large.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {341--352},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1128022.1128068},
 doi = {http://doi.acm.org/10.1145/1128022.1128068},
 acmid = {1128068},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache, memory system, penalty, replacement policy},
} 

@inproceedings{Molnos:2006:SCP:1128022.1128069,
 author = {Molnos, A. M. and Cotofana, S. D. and Heijligers, M. J. M. and van Eijndhoven, J. T. J.},
 title = {Static cache partitioning robustness analysis for embedded on-chip multi-processors},
 abstract = {In this paper we analyze the robustness of multi-tasking applications when mapped on an on-chip multiprocessor platform. We assume a multiprocesso structure which embeds a hierarchical cache organization with two levels. The first one is private to the processor cores while the second one is shared among the processors. To enable compositionality, i.e, to be able to evaluate the system's performance out of the individual task's performance, the second level of cache (L2) is partitioned per task basis. Two robustness aspects are relevant in this context: internal (performance deviations are caused by the tasks comprising the application) and external (performance variations are caused by external stimuli). First we introduce two metrics to quantify the robustness. The internal robustness is estimated by a sensitivity function which measures the performance variations induced by the \%private inter-task cache interference. The external robustness is quantified by a stability function which reflects the variations induced by different input data on the partitioned L2 behavior. Subsequently, we exercise our method on two applications (H.264 and picture-in-picture TV) running on a CAKE multi-processor platform. Our experiments indicate that, if the cache is partitioned, the sensitivity is 8\% and 5\% for the H.264 and PiPTV, respectively. For the shared cache scenario the sensitivity is 40\% and 50\% for the H.264 and PiPTV, respectively. The variations induced in the L2 behavior by various input data sets are at most 4\% for the PiPTV application, respectively 9\% for the H.264 decoder. This accounts for a stability of at least 96\%, respectively 91\%, therefore, for the investigated applications, we can conclude that the static cache partitioning is quite robust to input stimuli.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {353--360},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1128022.1128069},
 doi = {http://doi.acm.org/10.1145/1128022.1128069},
 acmid = {1128069},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache partitioning, multi-processors, robustness},
} 

@inproceedings{Benitez:2006:EFC:1128022.1128070,
 author = {Benitez, Domingo and Moure, Juan C. and Rexachs, Dolores I. and Luque, Emilio},
 title = {Evaluation of the field-programmable cache: performance and energy consumption},
 abstract = {Many authors have proposed power management techniques for general-purpose processors at the cost of degraded performance such as lower IPC or longer delay. Some proposals have focused on cache memories because they consume a significant fraction of total microprocessor power. We propose a reconfigurable and adaptive cache microarchitecture based on field-programmable technology that is intended to deliver high performance at low energy consumption. In this paper, we evaluate the performance and energy consumption of a run-time algorithm when used to manage a field-programmable L1 data cache. The adaptation strategy is based on two techniques: a learning process provides the best cache configuration for each program phase, and a recognition process detects program phase changes by using data working-set signatures to activate a low-overhead reconfiguration mechanism. Our proposals achieve performance improvement and cache energy saving at the same time. Considering a design scenario driven by performance constraints, we show that processor execution time and cache energy consumption can be reduced on average by 15.2\% and 9.9\% compared to a non-adaptive high-performance microarchitecture. Alternatively, when energy saving is prioritized and considering a non-adaptive energy-efficient microarchitecture as baseline, cache energy and processor execution time are reduced on average by 46.7\% and 9.4\% respectively. In addition to comparing to conventional microarchitectures, we show that the proposed microarchitecture achieves better performance and more cache energy reduction than other configurable caches.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {361--372},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1128022.1128070},
 doi = {http://doi.acm.org/10.1145/1128022.1128070},
 acmid = {1128070},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive processors, performance evaluation, reconfigurable cache memory, run-time adaptation, static and dynamic energy consumption},
} 

@inproceedings{Beyls:2006:IEC:1128022.1128071,
 author = {Beyls, Kristof and D'Hollander, Erik H.},
 title = {Intermediately executed code is the key to find refactorings that improve temporal data locality},
 abstract = {The growing speed gap between memory and processor makes an efficient use of the cache ever more important to reach high performance. One of the most important ways to improve cache behavior is to increase the data locality. While many cache analysis tools have been developed, most of them only indicate the locations in the code where cache misses occur. Often, optimizing the program, even after pinpointing the cache bottlenecks in the source code, remains hard with these tools.In this paper, we present two related tools that not only pinpoint the locations of cache misses, but also suggest source code refactorings which improve temporal locality and thereby eliminate the majority of the cache misses. In both tools, the key to find the appropriate refactorings is an analysis of the code executed between a data use and the next use of the same data, which we call the Intermediately Executed Code (IEC). The first tool, the Reuse Distance VISualizer (RDVIS), performs a clustering on the IECs, which reduces the amount of work to find required refactorings. The second tool, SLO (short for "Suggestions for Locality Optimizations"), suggests a number of refactorings by analyzing the call graph and loop structure of the IEC. Using these tools, we have pinpointed the most important optimizations for a number of SPEC2000 programs, resulting in an average speedup of 2.3 on a number of different platforms.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {373--382},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1128022.1128071},
 doi = {http://doi.acm.org/10.1145/1128022.1128071},
 acmid = {1128071},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {loop transformations, performance debugger, program analysis, program optimizations, refactoring, temporal data locality},
} 

@inproceedings{Chavarria-Miranda:2006:TTM:1128022.1128073,
 author = {Chavarr\'{\i}a-Miranda, Daniel and Nieplocha, Jarek and Tipparaju, Vinod},
 title = {Topology-aware tile mapping for clusters of SMPs},
 abstract = {We propose a technique to optimize the performance of applications using distributed dense arrays and characterized by a nearest-neighbor communication profile by exploiting the topology of SMP clusters. The topological information is used to map array tiles to processors to reduce network communication and improve utilization of shared memory for inter-process communication. The potential benefits of using the SMP-aware mapping are demonstrated through a simulation, as well as a real application solving a wind-driven ocean circulation model on an IBM SP. On 256 processors, the execution time was reduced by almost 30 percent without any changes to the original application source code. The proposed mapping approach is applicable to multiple programming models and distributed array management systems.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {383--392},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1128022.1128073},
 doi = {http://doi.acm.org/10.1145/1128022.1128073},
 acmid = {1128073},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clusters of SMPs, communication optimization, data layout optimization, topology awareness},
} 

@inproceedings{Welcome:2006:PCA:1128022.1128074,
 author = {Welco Michael and Rendleman, Charles and Oliker, Leonid and Biswas, Rupak},
 title = {Performance characteristics of an adaptive mesh refinement calculation on scalar and vector platforms},
 abstract = {Adaptive mesh refinement (AMR) is a powerful technique that reduces the resources necessary to solve otherwise intractable problems in computational science. The AMR strategy solves the problem on a relatively coarse grid, and dynamically refines it in regions requiring higher resolution. However, AMR codes tend to be far more complicated than their uniform grid counterparts due to the software infrastructure necessary to dynamically manage the hierarchical grid framework. Despite this complexity, it is generally believed that future multi-scale applications will increasingly rely on adaptive methods to study problems at unprecedented scale and resolution. Recently, a new generation of parallel-vector architectures have become available that promise to achieve extremely high sustained performance for a wide range of applications, and are the foundation of many leadership-class computing systems worldwide. It is therefore imperative to understand the tradeoffs between conventional scalar and parallel-vector platforms for solving AMR-based calculations. In this paper, we examine the <b>LibraryHyperCLaw</b> AMR framework to compare and contrast performance on the Cray X1E, IBM Power3 and Power5, and SGI Altix. To the best of our knowledge, this is the first work that investigates and characterizes the performance of an AMR calculation on modern parallel-vector systems.},
 booktitle = {Proceedings of the 3rd conference on Computing frontiers},
 series = {CF '06},
 year = {2006},
 isbn = {1-59593-302-6},
 location = {Ischia, Italy},
 pages = {393--402},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1128022.1128074},
 doi = {http://doi.acm.org/10.1145/1128022.1128074},
 acmid = {1128074},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {IBM power3 and power4, SGI, altix, cray X1E, high end computing, hyperCLaw framework, integrated performance monitoring},
} 

@inproceedings{Fey:2005:MNO:1062261.1062264,
 author = {Fey, Dietmar and Schmidt, Daniel},
 title = {Marching-pixels: a new organic computing paradigm for smart sensor processor arrays},
 abstract = {In this paper we present a new organic computing principle denoted as marching pixels for the architectures of future smart CMOS camera chips. The idea of marching pixels is based on the realization of a massively-parallel fine-grain single-chip processor array. Marching pixels are virtual organic units which are propagating in a pixel processor array, similar to virtual ants in ant algorithms. The task of the marching pixels is to carry out autonomously important image pre-processing tasks, e.g. fast and robust detection of objects and its center points or tracking of moving objects. We are favoring organic computing principles based on virtual life-like objects which are implemented in hardware to realize fast reply times and self-healing properties. This technology is thought for future smart sensor chips which will integrate hundreds of million transistors. The paper presents the basic idea of marching pixels and its functional behavior for different algorithmic tasks. Furthermore a concept for the implementation of marching pixels is shown as well as results of a simulation study which presents a first proof of concept of the effectiveness of the marching pixels idea},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {1--9},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1062261.1062264},
 doi = {http://doi.acm.org/10.1145/1062261.1062264},
 acmid = {1062264},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {image pre-processing, organic computing, self-organization, smart CMOS camera, smart pixels},
} 

@inproceedings{Scholer:2005:FST:1062261.1062265,
 author = {Sch\"{o}ler, Thorsten and M\"{u}ller-Schloer, Christian},
 title = {First steps towards organic computing systems: monitoring an adaptive protocol stack with a fuzzy classifier system},
 abstract = {Protocol stacks for small devices like mobile phones have to fulfill multiple functions in different environments. This has lead to highly complex solutions with alternative stacks. It is more economical to construct new stacks from building blocks on the fly depending on the current requirements. Such an adaptive architecture is an example of an organic computing system. In this paper we show how a proposed monitoring architecture for adaptive protocol stacks fits into the observer/controller pattern of organic computing. The protocol stack framework and the observer/controller pattern are briefly described. Subsequently the observer/controller pattern is mapped onto the framework architecture. Two implementations for monitoring agents are described, a finite state machine solution and a fuzzy classifier approach. The fuzzy classifier system is described in detail. It is shown that a finite state machine and a standard Markovian classifier system can be generalised into a non-Markovian learning classifier system, combining the advantages of both parents},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {10--20},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1062261.1062265},
 doi = {http://doi.acm.org/10.1145/1062261.1062265},
 acmid = {1062265},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {agents, fuzzy learning classifier system, monitoring, observer/controller, organic computing, protocol stack},
} 

@inproceedings{Baniasadi:2005:BCS:1062261.1062267,
 author = {Baniasadi, Amirali},
 title = {Balancing clustering-induced stalls to improve performance in clustered processors},
 abstract = {Clustered processors lose performance as a result of clustering-induced stalls. Such stalls are the result of distributed resources and cluster communication delays. Our performance analysis of clustered architectures shows how previously proposed methods reduce one group of stalls at the expense of the other. Moreover, we extend previous work and present a new class of cluster assignment heuristics for high-performance clustered processors. We affirm that it is possible to improve performance in clustered processors by taking a more balanced approach towards clustering-induced stalls. Our techniques rely on estimating and predicting resource utilization for clustered processors. We show that, on average, our best technique reduces the performance gap between a dual-clustered and a centralized processor down to 6.9\% and 9.2\% for 8-way and 6-way processors and for a representative subset of SPEC2K benchmarks},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {21--27},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1062261.1062267},
 doi = {http://doi.acm.org/10.1145/1062261.1062267},
 acmid = {1062267},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clustered processors, clustering stalls},
} 

@inproceedings{Feo:2005:ELD:1062261.1062268,
 author = {Feo, John and Harper, David and Kahan, Simon and Konecny, Petr},
 title = {ELDORADO},
 abstract = {This paper introduces Eldorado, a third generation multithreaded architecture. Previous Cray multithreaded systems were plagued by unreliable hardware and high costs. Eldorado corrects these problems by using many parts built for other commercial systems. Its compute processor is a 500 MHZ multithreaded processor architecturally similar to the MTA-2 processor; but its interconnection network, I/O subsystem, and service processors are borrowed from other Cray systems. Eldorado retains the programming model, operating system, and tools of the MTA-2. It has the same capability as the MTA-2 to tolerate latencies and achieve high performance on programs that run poorly on SMP clusters. We present several programming examples to illustrate performance and scalability in the presence of high memory and synchronization latencies},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {28--34},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1062261.1062268},
 doi = {http://doi.acm.org/10.1145/1062261.1062268},
 acmid = {1062268},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {heterogeneous architectures, multithreaded architectures, multithreaded processing, performance studies},
} 

@inproceedings{DeVos:2005:RCM:1062261.1062270,
 author = {De Vos, Alexis and Van Rentergem, Yvan},
 title = {Reversible computing: from mathematical group theory to electronical circuit experiment},
 abstract = {Reversible logic gates of a certain logic width w form a group (isomorphic to the symmetric group of order (2 w )!). Study of the subgroups of this group both teaches us a lot about properties of reversible gates and guides us to synthesize particular circuits. After design, circuits are implemented in prototype silicon chips.},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {35--44},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1062261.1062270},
 doi = {http://doi.acm.org/10.1145/1062261.1062270},
 acmid = {1062270},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coset, double coset, group theory, power dissipation, reversible computing, subgroup},
} 

@inproceedings{Miller:2005:TRU:1062261.1062271,
 author = {Miller, Daniel B. and Fredkin, Edward},
 title = {Two-state, reversible, universal cellular automata in three dimensions},
 abstract = {A novel two-state, Reversible Cellular Automata (RCA) is described. This three-dimensional RCA is shown to be capable of universal computation. Additionally, evidence is offered that this RCA is capable of universal construction.},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {45--51},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1062261.1062271},
 doi = {http://doi.acm.org/10.1145/1062261.1062271},
 acmid = {1062271},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adiabatic computing, billiard ball model, cellular automata, massively parallel, nanoscale computing, nanotech, reversible computation, reversible computing, reversible logic},
} 

@inproceedings{Toffoli:2005:SEI:1062261.1062272,
 author = {Toffoli, Tommaso and Levitin, Lev B.},
 title = {Specific ergodicity: an informative indicator for invertible computational media},
 abstract = {Specific ergodicity</i> asks, for an invertible cellular automaton, lattice gas, or similar indefinitely-extended computational medium, what fraction of the information needed to specify an individual state is still missing after one is told the computational trajectory to which that state belongs. While the well-known distinction between "ergodic" and "nonergodic" for a dynamical system is an all-or-nothing</i> classification, specific ergodicity---with range in the [0,1] interval---provides a continuous parameter which may be interpreted as "degree of ergodicity</i>." Moreover, while the property of a system's being ergodic can only refer to the system as a whole</i>, specific ergodicity is an intensive</i> quantity (ie it factors out the size of the system); thus, for a spatially-distributed, homogeneous computational system such as a cellular automaton or a lattice gas, in the limit of infinite system size this quantity reflects an intrinsic property of the material</i> that makes up the computational medium, abstracting from its specific size or shapeWe provide the conceptual background, present theoretical and numerical results, and discuss the relevance of specific ergodicity to a number of concrete research questions.Values of specific ergodicity for a variety of systems of actual interest turn out to be well distributed over its entire range; in this sense, this quantity is an informative indicator</i>. Indeed, besides representing a useful parameter in the classification of distributed computational media, specific ergodicity provides a "sense of direction" in issues such as protection from noise, design of self-organizing media, necessary conditions for the emergence and persistence of life, effectiveness of a parallel architecture as a "programmable medium," and a number of topics in nanotechnology},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {52--58},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1062261.1062272},
 doi = {http://doi.acm.org/10.1145/1062261.1062272},
 acmid = {1062272},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {degree of coupling, distributed computational media, ergodic theory, orbit length distribution, reversible computing, specific ergodicity},
} 

@inproceedings{Therning:2005:JSG:1062261.1062274,
 author = {Therning, Niklas and Bengtsson, Lars},
 title = {Jalapeno: secentralized grid computing using peer-to-peer technology},
 abstract = {This paper presents the Jalapeno grid computing system. Jalapeno is implemented in Java and uses peer-to-peer technology provided by Project JXTA. The Jalapeno system consists of manager, worker and task submitter hosts. The task submitter submits a collection of tasks, a task bundle, to be processed by the system to a randomly chosen manager. The manager splits the bundle into a set of new, smaller bundles which are forwarded to equally many, randomly chosen, managers which repeat the process. Each manager has a small number of connected workers. During task bundle splitting the manager may, depending on its current load, reserve a number of tasks for its workers. Workers return the results to their managers which forward them to the task submitterThe system is self configuring: hosts volunteering their computing power will at first become workers only but will eventually become managers if they can not connect to another manager within a certain time.The major contributions of this project are: an implicit hierarchy of hosts which changes randomly over time and requires no effort to maintain, a framework for applications solving embarrassingly parallel type of problems which automatically partitions the problem into smaller sub-problems and ease of use through the use of Sun's Java Web Start technology.Two applications have been developed for the system to evaluate its performance: an RC5 key cracking application and a 3d ray-tracing application. The entire system is available for download at http://jalapeno.therning.org.},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {59--65},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1062261.1062274},
 doi = {http://doi.acm.org/10.1145/1062261.1062274},
 acmid = {1062274},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GRID, P2P, distributed computing},
} 

@inproceedings{ChunLin:2005:TLM:1062261.1062275,
 author = {ChunLin, Li and Layuan, Li},
 title = {A two level market model for resource allocation optimization in computational grid},
 abstract = {This paper investigates the interactions between agents representing users, services and resources to solve resource allocation optimization in computational grid. In order to reduce the computational complexity, we further decompose the grid resource allocation optimization into subproblems: grid user agent-grid service agent in service market and grid service agent-grid resource agent in resource market. Two-level market converges to its optimal points; a globally optimal point is achieved. Total user benefit of the computational grid is maximized when the equilibrium prices are obtained through the service market level optimization and resource market level optimization. It demonstrates a practical approach to market responsive resource pricing that can benefit grid providers and users alike. The paper presents two-level market grid resource pricing that is an iterative algorithm used to perform optimal resource allocation. The experiment shows that two-level market based resource pricing scheme outperforms one level market scheme in terms of task completion time and resource allocation efficiency},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {66--71},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1062261.1062275},
 doi = {http://doi.acm.org/10.1145/1062261.1062275},
 acmid = {1062275},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computational grid, resource allocation optimization, two level market},
} 

@inproceedings{Calderon:2005:RUS:1062261.1062277,
 author = {Calderon, Humberto and Vassiliadis, Stamatis},
 title = {Reconfigurable universal SAD-multiplier array},
 abstract = {In this paper, we investigate the collapsing of some multi-operand addition related operations into a single array. More specifically we consider multiplication and Sum of Absolute Differences (SAD) and propose an array capable of performing the aforementioned operations for unsigned, signed magnitude, and two's complement notations. The array, called a universal array, is divided into common and controlled logic blocks intended to be reconfigured dynamically. The proposed unit was constructed around three main operational fields, which are feed with the necessary data products or SAD addition terms in order to compute the desired operation. It is estimated that a 66.6\% of the (3:2)counter array is shared by the operations providing an opportunity to reduce reconfiguration times. The synthesis result for a FPGA device, of the new structure, was compared against other multiplier organizations. The obtained results indicate that the proposed unit is capable of processing in 23.9 ns a 16 bit multiplication, and that an 8 input SAD can be computed in 29.8 ns using current FPGA technology. Even though the proposed structure incorporates more operations, the extra delay required over conventional structures is very small in the order of 1\% compared to Baugh\&Wooley multiplier},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {72--76},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/1062261.1062277},
 doi = {http://doi.acm.org/10.1145/1062261.1062277},
 acmid = {1062277},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary multiplication, partial reconfiguration, reconfigurable computing, sum of absolute differences},
} 

@inproceedings{Thuresson:2005:EED:1062261.1062278,
 author = {Thuresson, Martin and Stenstrom, Per},
 title = {Evaluation of extended dictionary-based static code compression schemes},
 abstract = {This paper evaluates how much extended dictionary-based code compression techniques can reduce the static code size. In their simplest form, such methods statically identify identical instruction sequences in the code and replace them by a codeword if they yield a smaller code size based on a heuristic. At run-time, the codeword is replaced by a dictionary entry storing the corresponding instruction sequence.Two previously proposed schemes are evaluated. The first scheme, as used in DISE, provides operand parameters to catch a larger number of identical instruction sequences. The second scheme replaces different instruction sequences with the same dictionary entry if they can be derived from it using a bit mask that can cancel out individual instructions. Additionally, this paper offers a third scheme, namely, to combine the two previously proposed schemes along with an off-line algorithm to compress the program. Our data shows that all schemes in isolation improve the compressibility. However, the most important finding is that the number of operand parameters has a significant effect on the compressibility. In addition, our proposed combined scheme can reduce the size of the dictionary and the number of codewords significantly which can enable efficient implementations of extended dictionary-based code compression techniques},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {77--86},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1062261.1062278},
 doi = {http://doi.acm.org/10.1145/1062261.1062278},
 acmid = {1062278},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code compression, code size reduction, dynamic decompression, memory size reduction},
} 

@inproceedings{Germain-Renaud:2005:GRC:1062261.1062280,
 author = {Germain-Renaud, C\'{e}cile and Monnier-Ragaigne, Dephine},
 title = {Grid result checking},
 abstract = {Result checking is the theory and practice of proving that the result of an execution of a program on an input is correct. Result checking has most often been envisioned in the framework of program testing or property testing, where the issue is the conformity of the program to some a-priori specification. Very large scale distributed computing systems demand to tackle the issue of computation correctness, albeit from hypothesis very different from the program testing ones. The general issues examined in this paper are the following. First, the definition of checking methods adapted to large-scale Monte-Carlo simulations; for these applications, no external criterion can be used to assess the quality of the result. Second, two result checking algorithms which minimize the overall overhead through an adaptive strategy. Finally, a specialization of this framework to a case study, the Auger astrophysics experiment. Our main contributions are: first to focus on checking Monte-Carlo simulations, which have rarely been considered previously; second to define a probabilistic checking strategy including the risk of first kind (false positive) as well as the risk of second kind (false negative) which is usually the only one considered, and which is compatible with Byzantine saboteurs; third, to exploit the probable characteristics of the behaviour of the saboteurs to optimise for the most frequent case. Finally, we show on a case study that the implementation details can be carried out},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {87--96},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1062261.1062280},
 doi = {http://doi.acm.org/10.1145/1062261.1062280},
 acmid = {1062280},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {grids, result checking, simulation},
} 

@inproceedings{Maly:2005:GBF:1062261.1062281,
 author = {Maly, Kurt and Zubair, Mohammad and Chilukamarri, Vamshi and Kothari, Pratik},
 title = {GRID Based Federated Digital Library},
 abstract = {With the growing acceptance of the Open Archive Initiative (OAI) [16] framework, a number of digital libraries are becoming OAI compliant. This is making it feasible to build an effective federated digital library, which harvests metadata from the OAI-compliant libraries and provides a unified search service over the aggregated metadata. Arc [10] is an example of such a federated digital library. Assuming that a rapid increase (e.g., several orders of magnitude) in the adoption of OAI-PMH [16] occurs, we now have a different problem: how to efficiently discover, harvest and index the burgeoning OAI-PMH corpus. In this project, we are working on using Grid and cluster technology to address these performance issues. In this paper, we focus on the use of Grid for parallelizing the harvesting task for an OAI-based federated digital library. We propose a Grid-based architecture for parallel harvesting that supports: dynamic allocation of harvesting nodes, scheduling of harvesting tasks to maximize the performance, and uniform load distribution for the indexing node. We have implemented and evaluated the proposed architecture on a Grid based on the GT3 toolkit},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {97--105},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1062261.1062281},
 doi = {http://doi.acm.org/10.1145/1062261.1062281},
 acmid = {1062281},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {OAI-PMH, grid, high performance, parallel search},
} 

@inproceedings{Cardoso:2005:DLP:1062261.1062283,
 author = {Cardoso, Jo\"{a}o M. P.},
 title = {Dynamic loop pipelining in data-driven architectures},
 abstract = {Data-driven array architectures seem to be important alternatives for coarse-grained reconfigurable computing platforms. Their use has provided performance improvements over microprocessors and shorter programming cycles than FPGA-based platforms. As with other architectures, in data-driven architectures loop pipelining plays an important role to improve performance. Usually this kind of pipelining can be achieved using the dataflow software pipelining technique or other software pipelining approaches. Although performance improvements are achieved, those techniques heavily depend on the insertion of pipelining stages and thus require complex balancing efforts. Furthermore, those techniques statically define the pipelining and do not take fully advantage of the dynamic scheduling attainable by the data-driven concept. This paper presents a novel scheme to pipeline loops in data-driven architectures, orchestrated by a handshaking protocol. Using the new approach, self loop pipelining is naturally achieved. The scheme is based on duplicating cyclic hardware structures, in order they are autonomously executed, with synchronization being achieved by the data flow. It can be applied to nested loops, requires less aggressive pipeline balancing efforts than usual software pipelining techniques, and innermost loops with conditional structures can be pipelined without conservative pipelining implementationsWe show results of using the proposed technique when mapping algorithms in imperative programming languages to the PACT eXtreme Processing Platform (XPP). The results confirm improvements over the use of conventional loop pipelining techniques. Better performance and fewer resources are achieved in a number of cases.},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {106--115},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1062261.1062283},
 doi = {http://doi.acm.org/10.1145/1062261.1062283},
 acmid = {1062283},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compilation, data-driven architectures, dataflow, reconfigurable computing, software pipelining},
} 

@inproceedings{Schulz:2005:ONG:1062261.1062284,
 author = {Schulz, Martin and White, Brian S. and McKee, Sally A. and Lee, Hsien-Hsin S. and Jeitner, J\"{u}rgen},
 title = {Owl: next generation system monitoring},
 abstract = {As microarchitectural and system complexity grows, comprehending system behavior becomes increasingly difficult, and often requires obtaining and sifting through voluminous event traces or coordinating results from multiple, non-localized sources. Owl is a proposed framework that overcomes limitations faced by traditional performance counters and monitoring facilities in dealing with such complexity by pervasively deploying programmable monitoring elements throughout a system. The design exploits reconfigurable or programmable logic to realize hardware monitors located at event sources, such as memory buses. These monitors run and writeback results autonomously with respect to the CPU, mitigating the system impact of interrupt-driven monitoring or the need to communicate irrelevant events to higher levels of the system. The monitors are designed to snoop any kind of system transaction, e.g., within the core, on a bus, across the wire, or within I/O devices},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {116--124},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1062261.1062284},
 doi = {http://doi.acm.org/10.1145/1062261.1062284},
 acmid = {1062284},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {autonomous performance monitoring, performance analysis, reconfiguration},
} 

@inproceedings{Salapura:2005:PPO:1062261.1062262,
 author = {Salapura, Valentina and Bickford, Randy and Blumrich, Matthias and Bright, Arthur A. and Chen, Dong and Coteus, Paul and Gara, Alan and Giampapa, Mark and Gschwind, Michael and Gupta, Manish and Hall, Shawn and Haring, Ruud A. and Heidelberger, Philip and Hoenicke, Dirk and Kopcsay, Gerard V. and Ohmacht, Martin and Rand, Rick A. and Takken, Todd and Vranas, Pavlos},
 title = {Power and performance optimization at the system level},
 abstract = {The BlueGene/L supercomputer has been designed with a focus on power/performance efficiency to achieve high application performance under the thermal constraints of common data centers. To achieve this goal, emphasis was put on system solutions to engineer a power-efficient system. To exploit thread level parallelism, the BlueGene/L system can scale to 64 racks with a total of 65536 computer nodes consisting of a single compute ASIC integrating all system functions with two industry-standard PowerPC microprocessor cores in a chip multiprocessor configuration. Each PowerPC processor exploits data-level parallelism with a high-performance SIMD oating point unitTo support good application scaling on such a massive system, special emphasis was put on efficient communication primitives by including five highly optimized communification networks. After an initial introduction of the Blue-Gene/L system architecture, we analyze power/performance efficiency for the BlueGene system using performance and power characteristics for the overall system performance (as exemplified by peak performance numbers.To understand application scaling behavior, and its impact on performance and power/performance efficiency, we analyze the NAMD molecular dynamics package using the ApoA1 benchmark. We find that even for strong scaling problems, BlueGene/L systems can deliver superior performance scaling and deliver significant power/performance efficiency. Application benchmark power/performance scaling for the voltage-invariant energy delay 2 power/performance metric demonstrates that choosing a power-efficient 700MHz embedded PowerPC processor core and relying on application parallelism was the right decision to build a powerful, and power/performance efficient system},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {125--132},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1062261.1062262},
 doi = {http://doi.acm.org/10.1145/1062261.1062262},
 acmid = {1062262},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BlueGene/L, application performance analysis, application scaling in multiprocessor systems, chip multiprocessors, power/performance efficient systems, power/performance tradeos in systems, supercomputers},
} 

@inproceedings{Udrescu:2005:IQC:1062261.1062286,
 author = {Udrescu, Mihai and Prodan, Lucian and Vl\v{a}du\c{t}iu, Mircea},
 title = {Improving quantum circuit dependability with reconfigurable quantum gate arrays},
 abstract = {The need for error detection and correction techniques is vital in quantum computation, due to the omnipresent nature of quantum errors. No realistic prospect of an operational quantum computational device may be warranted without such mechanisms. Therefore, the fact that error detecting and correcting techniques have been developed has enhanced the feasibility of a potential quantum computer [15] [18]. This paper presents a methodology for improving the fault tolerance of quantum circuits by using the so-called reconfigurable Quantum Gate Arrays</i> (rQGAs). Our solution reduces the problem of stabilizer coding safe recovery to preserving a given quantum configuration state. As shown in this paper's practical example, the configuration register to be protected has a reduced number of qubits, and the overall dependability attribute [2]-- reliability measured by the accuracy threshold [15]-- is drastically improved},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {133--144},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1062261.1062286},
 doi = {http://doi.acm.org/10.1145/1062261.1062286},
 acmid = {1062286},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {accuracy threshold, coding, reconfigurable quantum gate arrays},
} 

@inproceedings{Calidonna:2005:UHC:1062261.1062287,
 author = {Calidonna, Claudia R. and Naddeo, Adele},
 title = {Using a hybrid CA based model for a flexible qualitative qubit simulation: fully frustrated josephson junction ladder (JJL) application},
 abstract = {Quantum Systems modelling and simulation activity is a challenging research area. Several models and applications were proposed in the past with different approaches. In this paper we propose a flexible qualitative model, according to the Cellular Automata Network Model version 2 (CAN2), a hybrid CA based model, for simulating a qubit. The underlying physical model is a fully frustrated JJL. Once basic system components have been individuated a core model, of the JJL device, is obtained. We show that the core model, according to different tunnelling paths, presents the same components in terms of CAN2 model. For each path different transition functions are defined. The equivalence between two different tunnelling paths shows the flexibility of the adopted scheme},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {145--151},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1062261.1062287},
 doi = {http://doi.acm.org/10.1145/1062261.1062287},
 acmid = {1062287},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cellular automata, josepshon junction ladders, programming model, quantum computing, qubit},
} 

@inproceedings{Spjuth:2005:SCL:1062261.1062289,
 author = {Spjuth, Mathias and Karlsson, Martin and Hagersten, Erik},
 title = {Skewed caches from a low-power perspective},
 abstract = {The common approach to reduce cache conflicts is to increase the associativity. From a dynamic power perspective this associativity comes at a high cost. In this paper we present miss ratio performance and a dynamic power comparison for set-associative caches, a skewed cache and also for a new organization proposed,the elbow cache. The elbow cache extends the skewed cache organization with a relocation strategy for conflicting blocks. We show that these skewed designs significantly reduce the conflict problems while consuming p to 56\% less dynamic power than a comparably performing 8-way set associative cache. We believe this to be the strongest case in favor of skewed caches presented so far},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {152--160},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1062261.1062289},
 doi = {http://doi.acm.org/10.1145/1062261.1062289},
 acmid = {1062289},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CAT, elbow, low-power, skewed caches},
} 

@inproceedings{Mohyuddin:2005:CLP:1062261.1062290,
 author = {Mohyuddin, Nasir and Bhatti, Rashed and Dubois, Michel},
 title = {Controlling leakage power with the replacement policy in slumberous caches},
 abstract = {As technology scales down at an exponential rate, leakage power is fast becoming the dominant component of the total power budget. A large share of the total leakage power is dissipated in the cache hierarchy. To reduce cache leakage, individual cache lines can be kept in drowsy mode, a low voltage, low leakage state. Every cache access may then result in dynamic power consumption and performance penalties. A trade-off between the amount of leakage power saved on one hand, and the impact on dynamic power and performance on the other hand must be reachedTo affect this trade-off, we introduce "slumberous caches" in which the power level of cache lines is controlled with the cache replacement policy. In a slumberous cache, cache lines are maintained at different power save modes which we call "tranquility levels", which depend on their order of replacement priorities.We evaluate the trade-offs in the context of PLRU, a common cache replacement algorithm. We explore various assignments of the tranquility levels to lines and compare overall power and performance impacts. As technology scales down, the dynamic power required to energize slumberous cache lines drops drastically while the leakage power savings remain roughly steady. The performance penalty--in cycles-- remains constant with technology scaling for each scheme we evaluate.},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {161--170},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1062261.1062290},
 doi = {http://doi.acm.org/10.1145/1062261.1062290},
 acmid = {1062290},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {drowsy cache, leakage power, replacement policy, tranquility level},
} 

@inproceedings{Shahbahrami:2005:MRF:1062261.1062291,
 author = {Shahbahrami, Asadollah and Juurlink, Ben and Vassiliadis, Stamatis},
 title = {Matrix register file and extended subwords: two techniques for embedded media processors},
 abstract = {In this paper we employ two techniques suitable for embedded media processors. The first technique, extended subwords, uses four extra bits for every byte in a media register. This allows many SIMD operations to be performed without overflow and avoids packing/unpacking conversion overhead because of mismatch between storage and computational formats. The second technique, the Matrix Register File (MRF), allows flexible row-wise as well as column-wise access to the register file. It is useful for many block-based multimedia kernels such as (I)DCT, 2x2 Haar Transform, and pixel padding. In addition, we propose a few new media instructions. We employ Modified MMX (MMMX), MMX with extended subwords, to evaluate these techniques. Our results show that MMMX combined with an MRF reduces the dynamic number of instructions by up to 80\% compared to other multimedia extensions such as MMX},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {171--179},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1062261.1062291},
 doi = {http://doi.acm.org/10.1145/1062261.1062291},
 acmid = {1062291},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {embedded media processors, multimedia kernels, register file, sub-word parallelism},
} 

@inproceedings{Haneda:2005:OGP:1062261.1062293,
 author = {Haneda, M. and Knijnenburg, P. M. W. and Wijshoff, H. A. G.},
 title = {Optimizing general purpose compiler optimization},
 abstract = {The problem of defining optimal optimization strategies for compilers is well known and has been studied extensively over the last years. The problem arises from the fact that the sheer number of possible combinations of optimizations, their order, and their setting creates a search space which cannot adequately be searched. Although it has been shown that compiler settings can be found that outperform standard -Ox switches for a single application, it is not known how to find such settings that work well for sets of applicationsIn this paper, we introduce a statistical technique to derive a methodology which trims down the search space considerably, thereby allowing a feasible and flexible solution for defining high performance optimization strategies. We show that our technique finds a single compiler setting for a collection of programs (SPECint95) that performs better than the standard -Ox settings of gcc</i>. 3.3.1.},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {180--188},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1062261.1062293},
 doi = {http://doi.acm.org/10.1145/1062261.1062293},
 acmid = {1062293},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {back-end optimization, compiler switches, compiler tuning, statistical analysis},
} 

@inproceedings{Vardhan:2005:TAS:1062261.1062294,
 author = {Vardhan, K. Ananda and Srikant, Y. N.},
 title = {Transition aware scheduling: increasing continuous idle-periods in resource units},
 abstract = {Static power consumption has become a significant factor of the total power consumption in a system. Circuit level switching techniques reduce static power consumption by exploiting idle periods in processor components and placing them into low power modes or turning them off completely. In this paper, we propose a modified automaton-based list scheduling technique that augments the circuit level techniques by reducing the number of transitions between power modes, thereby increasing the of idle periods in resource units. Our scheduler uses a global resource usage vector and the usage vector of the last issued instruction(s) to select instruction(s) from the ready list such that resource units common to the last issued instruction and the selected instruction(s) are continuously active without creating a transition into low power mode. We estimate the power consumed in resource units using an energy model parameterized by the number of idle cycles, active cycles and transitions. We have implemented our algorithm in gcc and our simulations for different classes of benchmarks using an ARM simulator, with single and multi issue, indicate an average energy savings of 4\%-15\% in resource units},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {189--198},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1062261.1062294},
 doi = {http://doi.acm.org/10.1145/1062261.1062294},
 acmid = {1062294},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {functional units, idle periods, instruction scheduling, leakage energy consumption, transitions},
} 

@inproceedings{Ghiasi:2005:SHP:1062261.1062295,
 author = {Ghiasi, Soraya and Keller, Tom and Rawson, Freeman},
 title = {Scheduling for heterogeneous processors in server systems},
 abstract = {Applications on today's high-end systems typically make varying load demands over time. A single application may have many different phases during its lifetime, and workload mixes show interleaved phases. Memory-intensive work or phases may exhibit performance saturation at frequencies below the maximum possible for the processors due to the disparity between processor and memory speeds. Performance saturation is a sign of over-provisioning and leads to energy-inefficient systems. Computers using heterogeneous processors, with the same ISA, but different implementation details, have been proposed as a way of reducing power while avoiding or limiting performance degradation. However, using heterogeneous processors effectively is complicated and requires intelligent schedulingThe research reported here explores the use of a heterogeneous system of processors with identical ISAs and implementation details, but with differing voltages and frequencies. The scheduler uses the execution characteristics of each application to predict its future processing needs and then schedule it to a processor which matches those needs if one is available. The predictions are used to minimize the performance loss to the system as a whole rather than that of a single application. The result limits system power while minimizing total performance loss. A prototype implementation on a Power4 four-processor system is presented. The prototype scheduler is validated using both synthetic and real-world benchmarks. The prototype shows reasonable predictor accuracy and significant power savings for memory-bound applications},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {199--210},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1062261.1062295},
 doi = {http://doi.acm.org/10.1145/1062261.1062295},
 acmid = {1062295},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {heterogeneous processors, performance, power, scheduling},
} 

@inproceedings{Moseley:2005:DRA:1062261.1062296,
 author = {Moseley, Tipp and Shye, Alex and Reddi, Vijay Janapa and Iyer, Matthew and Fay, Dan and Hodgdon, David and Kihm, Joshua L. and Settle, Alex and Grunwald, Dirk and Connors, Daniel A.},
 title = {Dynamic run-time architecture techniques for enabling continuous optimization},
 abstract = {Future computer systems will integrate tens of multithreaded processor cores on a single chip die, resulting in hundreds of concurrent program threads sharing system resources. These designs will be the cornerstone of improving throughput in high-performance computing and server environments. However, to date, appropriate systems software (operating system, run-time system, and compiler) technologies for these emerging machines have not been adequately explored. Future processors will require sophisticated hardware monitoring units to continuously feed back resource utilization information to allow the operating system to make optimal thread co-scheduling decisions and also to software that continuously optimizes the program itselfNevertheless, in order to continually and automatically adapt systems resources to program behaviors and application needs, specific run-time information must be collected to adequately enable dynamic code optimization and operating system scheduling. Generally, run-time optimization is limited by the time required to collect profiles, the time required to perform optimization, and the inherent benefits of any optimization or decisions. Initial techniques for effectively utilizing run-time information for dynamic optimization and informed thread scheduling in future multithreaded architectures are presented},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {211--220},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1062261.1062296},
 doi = {http://doi.acm.org/10.1145/1062261.1062296},
 acmid = {1062296},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {multithreading, performance counters, profiling, scheduling},
} 

@inproceedings{Ferrante:2005:QPS:1062261.1062298,
 author = {Ferrante, Alberto and Piuri, Vincenzo and Castanier, Fabien},
 title = {A QoS-enabled packet scheduling algorithm for IPSec multi-accelerator based systems},
 abstract = {IPSec is a suite of protocols that adds security to communications at the IP level. Protocols within the IPSec suite make extensive use of cryptographic algorithms. Since these algorithms are computationally very intensive, some hardware acceleration is needed to support high throughput. In this paper we discuss a scheduling algorithm for distributing IPSec packet processing over the CPU with a software implementation of the cryptographic algorithms considered and multiple cryptographic accelerators. This algorithm also provides support for quality of service. High-level simulations and the related results are provided to show the properties of the algorithm. Some architectural improvements suitable to better exploit this scheduling algorithm are also presented},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {221--229},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1062261.1062298},
 doi = {http://doi.acm.org/10.1145/1062261.1062298},
 acmid = {1062298},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {IPSec, QoS, cryptographic accelerators, quality of service, scheduling},
} 

@inproceedings{Silva:2005:SMS:1062261.1062299,
 author = {Silva, Malik},
 title = {Sparse matrix storage revisited},
 abstract = {In this paper, we consider alternate ways of storing a sparse matrix and their effect on computational speed. They involve keeping both the indices and the non-zero elements in the sparse matrix in a single data structure. These schemes thus help reduce memory system misses that occur when the usual indexing based storage schemes are used to store sparse matrices and give promising performance improvements.},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {230--235},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1062261.1062299},
 doi = {http://doi.acm.org/10.1145/1062261.1062299},
 acmid = {1062299},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {caches, memory wall, sparse matrix computations, sparse matrix storage, spatial and temporal locality},
} 

@inproceedings{Murthy:2005:CIM:1062261.1062301,
 author = {Murthy, V. K. and Krishnamurthy, E. V.},
 title = {Contextual information management using contract: based workflow},
 abstract = {In the Ambient Intelligent Computing Environment (AmI) consisting of peers (clients, servers or agents or other intelligent devices), context-awareness plays an important role to offer intelligent services for various applications, e.g., medical services, robotics, travel planning, security monitoring, and multi-player gaming. Accordingly, context management turns out to be an important issue in manipulating, acquiring information and reacting to the situation. In this paper, we describe a contract-based workflow paradigm to provide transparency and reliability of interactions among the devices and people in the AmI. This paradigm provides for software contract that captures mutual obligations using program constructs such as "require [else]" for precondition and "ensure [then]" for post condition, assertions, invariants needed in the AmI. Such program constructs are essential to deal with the uncertain nature of connectivity of ubiquitous devices and networks, and the trial-error (subjunctive) nature of the processes and the programs used in interactions among devices and people in an unpredictable environment},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {236--245},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1062261.1062301},
 doi = {http://doi.acm.org/10.1145/1062261.1062301},
 acmid = {1062301},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {agents, ambient intelligent computing environment (AmI), chemical-reactivity properties, contextual management, contract-based workflow, intention - context-action protocol},
} 

@inproceedings{Bagci:2005:CSE:1062261.1062302,
 author = {Bagci, Faruk and Schick, Holger and Petzold, Jan and Trumler, Wolfgang and Ungerer, Theo},
 title = {Communication and security extensions for a ubiquitous mobile agent system (UbiMAS)},
 abstract = {Future computers will be integrated in objects of everyday life. The number of processors in the environment will increase and data will be distributed over different nodes. New classes of information and devices will appear, i.e. data will be catched from environmental sensors and will be used for context extraction. The amount of new devices and services makes an efficient use by centralized systems very difficult.Mobile agents provide an eminent method by virtualizing the user and performing actions on her behalf. They offer a possibility to encapsulate information of a person and her preferences, likings, and habits and perform location-based services of the ubiquitous system in the name of the user. This paper presents a mobile agent system application, where a mobile user-agent follows the user while she moves in the physical space. The user-agent knows personal information and communicates with service-agents in the name of its user to perform special tasks. Because of the personal data security and privacy are major concerns of such an agent system. The paper describes the communication of agents and agent nodes and focuses on the security extensions required by this approach},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {246--251},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1062261.1062302},
 doi = {http://doi.acm.org/10.1145/1062261.1062302},
 acmid = {1062302},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {flexible office, mobile agents, privacy, ubiquitous system},
} 

@inproceedings{Carr:2005:CWM:1062261.1062304,
 author = {Carr, Steve and \"{O}nder, Soner},
 title = {A case for a working-set-based memory hierarchy},
 abstract = {Modern microprocessor designs continue to obtain impressive performance gains through increasing clock rates and advances in the parallelism obtained via micro-architecture design. Unfortunately, corresponding improvements in memory design technology have not been realized, resulting in latencies of over 100 cycles between processors and main memory. This ever-increasing gap in speed has pushed the current memory-hierarchy approach to its limit.Traditional approaches to memory-hierarchy management have not yielded satisfactory results. Hardware solutions require more power and energy than desired and do not scale well. Compiler solutions tend to miss too many optimization opportunities because of limited compile-time knowledge of run-time behavior. This paper explores a different approach that combines both approaches by making use of the static knowledge obtained by the compiler in the dynamic decision making of the micro-architecture. We propose a memory-hierarchy design based on working sets that uses compile-time annotations regarding the working set of memory operations to guide cache placement decisionsOur experiments show that a working-set-based memory hierarchy can significantly reduce the miss rate for memory-intensive tiled kernels by limiting cross interference. The working-set-based memory hierarchy allows the compiler to tile many loops without concern for cross interference in the cache, making tile size choice easier. In addition, the compiler can more easily tailor tile choices to the separate needs of different working sets.},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {252--261},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1062261.1062304},
 doi = {http://doi.acm.org/10.1145/1062261.1062304},
 acmid = {1062304},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache design, loop tiling},
} 

@inproceedings{Nieplocha:2005:EPG:1062261.1062305,
 author = {Nieplocha, Jarek and Krishnan, Manoj and Palmer, Bruce and Tipparaju, Vinod and Zhang, Yeliang},
 title = {Exploiting processor groups to extend scalability of the GA shared memory programming model},
 abstract = {Exploiting processor groups is becoming increasingly important for programming next-generation high-end systems composed of tens or hundreds of thousands of processors. This paper discusses the requirements, functionality and development of multilevel-parallelism based on processor groups in the context of the Global Array (GA) shared memory programming model. The main effort involves management of shared data, rather than interprocessor communication. Experimental results for the NAS NPB Conjugate Gradient benchmark and a molecular dynamics (MD) application are presented for a Linux cluster with Myrinet and illustrate the value of the proposed approach for improving scalability. While the original GA version of the CG benchmark lagged MPI, the processor-group version outperforms MPI in all cases, except for a few points on the smallest problem size. Similarly, processor groups were very effective in improving scalability of a Molecular Dynamics application},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {262--272},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1062261.1062305},
 doi = {http://doi.acm.org/10.1145/1062261.1062305},
 acmid = {1062305},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {extreme scalability, global arrays, multi-level parallelism, processor groups},
} 

@inproceedings{Fredkin:2005:CAP:1062261.1062307,
 author = {Fredkin, Edward},
 title = {A computing architecture for physics},
 abstract = {In this paper, we show how a computing architecture, called "Salt", might be able to implement the workings of a particular Discrete Space-Time-State model of Physics (DSTSP). While what is presented is certainly not a correct model of fundamental processes in physics, it illustrates how such models could give us insights into new methods of modeling some attributes of microscopic physical systems. The main novelty of the approach is an unusual definition of a discrete space-time where time itself is made more complex in order to allow for simple representations of quantities that have properties in common with QM spin, charge and fractional charge, QCD color and reversibility with CPT symmetry.},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {273--279},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1062261.1062307},
 doi = {http://doi.acm.org/10.1145/1062261.1062307},
 acmid = {1062307},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cellular-automata, discrete-physics, fundamental-processes, models},
} 

@inproceedings{Platte:2005:CHS:1062261.1062308,
 author = {Platte, J\"{u}rg and Naroska, Edwin},
 title = {A combined hardware and software architecture for secure computing},
 abstract = {Remote code execution becomes more and more important as can be seen by Grid computing or distributed computing projects like SETI@home. However, executing programs on foreign computers leads to security risks if the program contains sensitive data or algorithms. Current operating systems can protect user programs from other malicious programs running on the same host. But this does not prevent attacks from a system administrator or a malicious operating system. Further, even if the operating system is trusted it is possible to physically intercept communication between main memory and processor to gather information about the executed programs. As a result, these security risks prevent the execution of sensitive algorithms or programs computing on sensitive data on not trustworthy remote systemsIn this paper we present a combined hardware and software architecture to provide a secure and tamper resistant computing environment without relying on trusted system administrators and a fully trusted operating system. Our proposed architecture provides a security enhancement implemented on top of a standard processor. Compared to external co-processor solutions, our architecture does not suffer from memory, functionality and performance limitations. Furthermore, normal and protected programs can be run concurrently in a multitasking environment},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {280--288},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1062261.1062308},
 doi = {http://doi.acm.org/10.1145/1062261.1062308},
 acmid = {1062308},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {certified execution, encrypted programs, secure processors},
} 

@inproceedings{Warg:2005:RMO:1062261.1062310,
 author = {Warg, Fredrik and Stenstrom, Per},
 title = {Reducing misspeculation overhead for module-level speculative execution},
 abstract = {Thread-level speculative execution is a technique that makes it possible for a wider range of single-threaded applications to make use of the processing resources in a chip multiprocessor.We consider module-level speculation, i.e., speculative threads executing the code after a module (i.e., a procedure, function, or method) call. Unfortunately, previous studies have shown that indiscriminate module-level speculation results in significant overheads, mainly due to frequent misspeculations. In addition to hurting performance, excessive overhead is harmful from a resource usage and energy efficiency standpoint. We show that the overhead when spawning speculative threads for all module continuations is on average three times as big as the time spent on useful execution on our baseline 8-way chip multiprocessorIn this paper, we present and make a detailed evaluation of a technique that aims at reducing the overhead associated with misspeculations. History-based prediction is used in an attempt to prevent speculative threads from being spawned when they are expected to cause misspeculations. We find that the overhead can be reduced with a factor of six on average compared to indiscriminate speculation. The impact on speedup is small for most applications, but in several cases speedup is slightly improved.},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {289--298},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1062261.1062310},
 doi = {http://doi.acm.org/10.1145/1062261.1062310},
 acmid = {1062310},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {chip multiprocessors, misspeculation prediction, module-level parallelism, performance evaluation, thread-level speculation},
} 

@inproceedings{Thomason:2005:POE:1062261.1062311,
 author = {Thomason, Braxton and Chase, Craig},
 title = {Partially ordered epochs for thread-level speculation},
 abstract = {<b>T</b>hread-<b>L</b>evel <b>S</b>peculation <b>TLS</b>) can be used to exploit parallelism in programs where static analysis fails. When a dependence violation is dynamically detected, the violating thread is rolled-back and restarted. However, we believe that for many applications, the restrictions imposed by referencing the sequential execution model to interpret dependence ordering will unnecessarily throttle parallelism. In this paper, we consider the possibility that the application requires that threads merely be partially ordered. We analyze the sufficient conditions for TLS to produce semantically correct results with this relaxed constraint. We apply our methodology, which we call <b>P</b>artially-<b>O</b>rdered <b>T</b>hread-level <b>S</b>peculation <b>POTS</b>), to priority-based scheduling and graph search algorithms and demonstrate that such a methodology still yields semantically correct execution while yielding substantially more parallelism than TLS},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {299--306},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1062261.1062311},
 doi = {http://doi.acm.org/10.1145/1062261.1062311},
 acmid = {1062311},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {automatic parallelization, partially-ordered threads, thread-level speculation},
} 

@inproceedings{Rochange:2005:TEM:1062261.1062312,
 author = {Rochange, Christine and Sainrat, Pascal},
 title = {A time-predictable execution mode for superscalar pipelines with instruction prescheduling},
 abstract = {The time predictability of the components of a real-time system is required whenever it must be guaranteed that deadlines will be met. Research on techniques to evaluate the Worst-Case Execution Time (WCET) of programs has received much attention these last years but current high-performance processors prove to be hard to model both safely and tightly. We acknowledge the difficulty of taking into account more and more dynamic mechanisms within static analysis and this motivates our approach that consists in making the processor fit WCET estimation techniques. We focus on out-of-order superscalar pipelines and we propose to regulate the instruction flow so that subsequent basic blocks execute independently one of each other. This would allow any WCET estimation tool to limit the measurement to individual basic blocks.},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {307--314},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1062261.1062312},
 doi = {http://doi.acm.org/10.1145/1062261.1062312},
 acmid = {1062312},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {WCET, pipeline, processor architecture, real-time},
} 

@inproceedings{Schaelicke:2005:SSN:1062261.1062314,
 author = {Schaelicke, Lambert and Wheeler, Kyle and Freeland, Curt},
 title = {SPANIDS: a scalable network intrusion detection loadbalancer},
 abstract = {Network intrusion detection systems (NIDS) are becoming an increasingly important security measure. With rapidly increasing network speeds, the capacity of the NIDS sensor can limit the ability of the system to detect intrusions. The SPANIDS parallel NIDS architecture overcomes this limitation by distributing network traffic load over an array of sensor nodes. Based on a custom hardware load balancer and cost-effective off-the-shelf sensors, the system employs novel stateless load balancing heuristics to thwart scalability limitations. It also uses dynamic feedback from the sensor nodes to adapt to changes in network traffic. This paper describes the overall system architecture, discusses some of the critical design decisions and presents experimental results that demonstrate the performance advantage of this approach},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {315--322},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1062261.1062314},
 doi = {http://doi.acm.org/10.1145/1062261.1062314},
 acmid = {1062314},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {load balancer, network intrusion detection, parallel intrusion detection},
} 

@inproceedings{Prodan:2005:RAE:1062261.1062315,
 author = {Prodan, Lucian and Udrescu, Mihai and Vladutiu, Mircea},
 title = {Reliability assessment in embryonics inspired by fault-tolerant quantum computation},
 abstract = {The Embryonics (embryonic electronics) project aims at implementing Nature's structural redundancy mechanisms in digital electronics in order to attain superior reliability in aggressive, critical environments. It offers a hierarchically reconfigurable framework, whose effectiveness was assessed only for some particular cases [8]. This paper proposes a complete and original approach to the reliability analysis for Embryonics, by adopting the accuracy threshold measure, taken from fault-tolerant quantum computing theory, as the main parameter for our qualitative evaluation. We also start a plea for the concatenated coding technique, which is suitable for the multiple-level organization in Embryonics, and preserves an arbitrary long fault-tolerant computation},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {323--333},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1062261.1062315},
 doi = {http://doi.acm.org/10.1145/1062261.1062315},
 acmid = {1062315},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bio-inspired computing, bio-inspired digital design, computation accuracy threshold, embryonics, fault-tolerance assessment, quantum computing, reliability},
} 

@inproceedings{Lai:2005:IBP:1062261.1062316,
 author = {Lai, Chunrong and Lu, Shih-Lien and Chen, Yurong and Chen, Trista},
 title = {Improving branch prediction accuracy with parallel conservative correctors},
 abstract = {In this paper we discuss how a confidence mechanism is integrated into the branch prediction correctors to improve the accuracy of a primary predictor in the prediction process. A corrector reverses the decision of the primary predictor when it is confident that the primary predictor may be wrong. Using correctors to improve prediction accuracy is efficient because correctors only need to remember the information on branches that the primary predictor has problems with. Each corrector maintains its own "confidence" value. In our proposed design, these confidence values are judged conservatively so that a corrector will only reverse the predicted direction of the primary predictor when it is confident about the reversal. Moreover, to further increase the accuracy of the prediction, we use multiple correctors working together, each having a particular "expertise". In this paper we present three correctors: the interval corrector, the bimodal corrector, and the two-level corrector. Simulation experiments on a common evaluation framework show that correctors with an additional 2 KB (i.e., 8 KB totally) of storage can reduce mis-predicted branches by 18\% over a primary predictor that has only 6 KB of storage. The total prediction accuracy of this 8 KB predictor is 28.4\% better than a Gshare predictor with the same size. We also discuss some design issues such as the critical path and show how correctors can be easily implemented into these designs},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {334--341},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1062261.1062316},
 doi = {http://doi.acm.org/10.1145/1062261.1062316},
 acmid = {1062316},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {branch prediction, confidence mechanism, corrector},
} 

@inproceedings{Puzak:2005:PIP:1062261.1062317,
 author = {Puzak, Thomas R. and Hartstein, A. and Emma, P. G. and Srinivasan, V.},
 title = {When prefetching improves/degrades performance},
 abstract = {We formulate a new method for evaluating any prefetching algorithm (real or hypothetical). This method allows researchers to analyze the potential improvements prefetching can bring to an application independent of any known prefetching algorithm. We characterize prefetching with the metrics: timeliness, coverage, and accuracy. We demonstrate the usefulness of this method using a Markov prefetch algorithm. Under ideal conditions, prefetching can remove nearly all of the pipeline stalls associated with a cache miss. However, in today's processors, we show that nearly all of the performance benefits derived from prefetching are eroded and, in many cases, prefetching loses performance. We do quantitative analysis of these trade-offs, and show that there are linear relationships between overall performance and coverage, accuracy, and bandwidth},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {342--352},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1062261.1062317},
 doi = {http://doi.acm.org/10.1145/1062261.1062317},
 acmid = {1062317},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {accuracy, cache, coverage, prefetch, prefetching algorithm, timeliness},
} 

@inproceedings{Hsiao:2005:EWD:1062261.1062319,
 author = {Hsiao, Kuo-Su and Chen, Chung-Ho},
 title = {An efficient wakeup design for energy reduction in high-performance superscalar processors},
 abstract = {In modern superscalar processors, the complex instruction scheduler could form the critical path of the pipeline stages and limit the clock cycle time. In addition, complex scheduling logic results in the formation of a hot spot on the processor chip. Consequently, the latency and power consumption of the dynamic scheduler are two of the most crucial design issues when developing a high-performance microprocessor. We propose an instruction wakeup scheme that remedies the speed and power issues faced with conventional designs. This is achieved by a new design that separates RAM cells from the match circuits. This separated design is such that the advantages of the CAM and bit-map RAM schemes are retained, while their respective disadvantages are eliminated. Specifically, the proposed design retains the moderate area advantage of the CAM scheme and the low power and low latency advantages of the bit-map RAM schemeThe experimental results show that the proposed design saves power consumption by 80\% compared to the traditional CAM-based design and 18\% to the bit-map RAM design, respectively. In speed, the proposed design reduces an average of 77\% in the wakeup latency compared to the conventional CAM-based design and an average of 33\% reduction of the latency of the bit-map RAM design. For an 8-issue superscalar processor, the proposed design reduces the power consumption of the conventional wakeup logic by 80\%, while simultaneously increasing the Instruction Count per nano-second (IPns) by a factor of approximately 2.5 times with a moderate area cost},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {353--360},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1062261.1062319},
 doi = {http://doi.acm.org/10.1145/1062261.1062319},
 acmid = {1062319},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {high performance, issue window, low power, wakeup logic},
} 

@inproceedings{Sam:2005:ESH:1062261.1062320,
 author = {Sam, Nana B. and Burtscher, Martin},
 title = {On the energy-efficiency of speculative hardware},
 abstract = {Microprocessor trends are moving towards wider architectures and more aggressive speculation. With the increasing transistor budgets, energy consumption has become a critical design constraint. To address this problem, several researchers have proposed and evaluated energy-efficient variants of speculation mechanisms. However, such hardware is typically evaluated in isolation and its impact on the energy consumption of the rest of the processor, for example, due to wrong-path executions, is ignored. Moreover, the available metrics that would provide a thorough evaluation of an architectural optimization employ somewhat complicated formulas with hard-to-measure parametersIn this paper, we introduce a simple method to accurately compare the energy-efficiency of speculative architectures. Our metric is based on runtime analysis of the entire processor chip and thus captures the energy consumption due to the positive as well as the negative activities that arise from the speculation activities. We demonstrate the usefulness of our metric on the example of value speculation, where we found some proposed value predictors, including low-power designs, not to be energy-efficient},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {361--370},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1062261.1062320},
 doi = {http://doi.acm.org/10.1145/1062261.1062320},
 acmid = {1062320},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy-efficiency, energy-performance metric, speculation},
} 

@inproceedings{Petit:2005:ETL:1062261.1062321,
 author = {Petit, Salvador and Sahuquillo, Julio and Such, Jose M. and Kaeli, David},
 title = {Exploiting temporal locality in drowsy cache policies},
 abstract = {Technology projections indicate that static power will become a major concern in future generations of high-performance microprocessors. Caches represent a significant percentage of the overall microprocessor die area. Therefore, recent research has concentrated on the reduction of leakage current dissipated by caches. The variety of techniques to control current leakage can be classified as non-state preserving or state preserving. Non-state preserving techniques power off selected cache lines while state preserving place selected lines into a low-power state. Drowsy caches are a recently proposed state-preserving technique. In order to introduce low performance overhead, drowsy caches must be very selective on which cache lines are moved to a drowsy statePast research on cache organization has focused on how best to exploit the temporal locality present in the data stream. In this paper we propose a novel drowsy cache policy called Reuse Most Recently used On (RMRO), which makes use of reuse information to trade off performance versus energy consumption. Our proposal improves the hit ratio for drowsy lines by about 67\%, while reducing the power consumption by about 11.7\% (assuming 70nm technology) with respect to previously proposed drowsy cache policies.},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {371--377},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1062261.1062321},
 doi = {http://doi.acm.org/10.1145/1062261.1062321},
 acmid = {1062321},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {drowsy cache policies, low-power, reuse information, set-associative caches, temporal locality},
} 

@inproceedings{Geiger:2005:DRC:1062261.1062322,
 author = {Geiger, Michael J. and McKee, Sally A. and Tyson, Gary S.},
 title = {Drowsy region-based caches: minimizing both dynamic and static power dissipation},
 abstract = {Power consumption within the memory hierarchy grows in importance as on-chip data caches occupy increasingly greater die area. Among dynamic power conservation schemes, horizontal partitioning reduces average power per data access by employing multiple smaller structures or using cache subbanks. For instance, region-based caching</i> places small caches dedicated to stack and global accesses next to the L1 data cache. With respect to static power dissipation, leakage power may be addressed at both circuit and architectural levels. Drowsy caches</i> reduce leakage power by keeping inactive lines in a low-power mode. Here we merge drowsy and region-based caching to reduce overall cache power consumption, showing that the combination yields more benefits than either alone. Applications from the MiBench suite exhibit power reductions in the cache system of up to 68-71\%, depending on memory configuration, with a small increase in execution time},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {378--384},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1062261.1062322},
 doi = {http://doi.acm.org/10.1145/1062261.1062322},
 acmid = {1062322},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {drowsy caches, energy-aware design, region-based caches},
} 

@inproceedings{Frank:2005:IRC:1062261.1062324,
 author = {Frank, Michael P.},
 title = {Introduction to reversible computing: motivation, progress, and challenges},
 abstract = {Reversible computing is motivated by the von Neumann-Landauer (VNL) principle, a theorem of modern physics telling us that ordinary irreversible logic operations (which destructively overwrite previous outputs) incur a fundamental minimum energy cost. Such operations typically dissipate roughly the logic signal energy, itself irreducible due to thermal noise. This fact threatens to end improvements in practical computer performance within the next few decades. However, computers based mainly on reversible</i> logic operations can reuse a fraction of the signal energy that theoretically can approach arbitrarily near to 100\% as the quality of the hardware is improved, reopening the door to arbitrarily high computer performance at a given level of power dissipation. In the 32 years since the theoretical possibility of this approach was first shown by Bennett, our understanding of how to design and engineer practical machines based on reversible logic has improved dramatically, but a number of significant research challenges remain, e.g</i>., (1) the development of fast and cheap switching devices with adiabatic energy coefficients well below those of transistors, (2) and of clocking systems that are themselves of very high reversible quality; and (3) the design of highly-optimized reversible logic circuits and algorithms. Finally, the field faces an uphill social battle in overcoming the enormous inertia of the established semiconductor industry, with its extreme resistance to revolutionary change. A more evolutionary strategy that aims to introduce reversible computing concepts only very gradually might well turn out to be more successful. This talk explains these basic issues, to set the stage for the rest of the workshop, which aims to address them in more detail},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {385--390},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1062261.1062324},
 doi = {http://doi.acm.org/10.1145/1062261.1062324},
 acmid = {1062324},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {VLSI, computer architecture, digital logic technologies, field-effect devices, high-performance computing, limits of computing, low-power computing, power management, reversible computing, reversible logic, unconventional computing},
} 

@inproceedings{DeBenedictis:2005:RLS:1062261.1062325,
 author = {DeBenedictis, Erik P.},
 title = {Reversible logic for supercomputing},
 abstract = {This paper is about making reversible logic a reality for supercomputing. Reversible logic offers a way to exceed certain basic limits on the performance of computers, yet a powerful case will have to be made to justify its substantial development expense. This paper explores the limits of current, irreversible logic for supercomputers, thus forming a threshold above which reversible logic is the only solution. Problems above this threshold are discussed, with the science and mitigation of global warming being discussed in detail. To further develop the idea of using reversible logic in supercomputing, a design for a 1 Zettaflops supercomputer as required for addressing global climate warming is presented. However, to create such a design requires deviations from the mainstream of both the software for climate simulation and research directions of reversible logic. These deviations provide direction on how to make reversible logic practical},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {391--402},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1062261.1062325},
 doi = {http://doi.acm.org/10.1145/1062261.1062325},
 acmid = {1062325},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {applications modeling, climate change global warming, computer architecture, quantum dot cellular automata, reversible logic, supercomputing},
} 

@inproceedings{Lent:2005:RCQ:1062261.1062327,
 author = {Lent, Craig S. and Frost, Sarah E. and Kogge, Peter M.},
 title = {Reversible computation with quantum-dot cellular automata (QCA)},
 abstract = {Quantum-dot cellular automata (QCA) is a strategy in which binary data is represented by charge configuration within a multi-dot cell. Data is transmitted to nearest neighbors by the Coulombic interaction. An electric field acts as a clock and imposes directionality on circuits. We have explored the connection between logical reversibility and physical reversibility in the context of a QCA system, explicitly calculating the energy dissipated by performing an erasure as a function of the time over which it is performed [1]. Further, we present a Bennett-style clocking scheme to implement reversible computation that is natural to the circuits to minimize the amount of information that is erased. Molecular QCA may provide a practical implementation of reversible computing},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {403--403},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1062261.1062327},
 doi = {http://doi.acm.org/10.1145/1062261.1062327},
 acmid = {1062327},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {molecular electronics, quantum-dot cellular automata, reversible computing},
} 

@inproceedings{Forsberg:2005:EWY:1062261.1062328,
 author = {Forsberg, Erik},
 title = {The electron waveguide y-branch switch: a review and arguments for its use as a base for reversible logic},
 abstract = {},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {404--406},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/1062261.1062328},
 doi = {http://doi.acm.org/10.1145/1062261.1062328},
 acmid = {1062328},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {electron waveguide devices, reversible computing, y-branch switch},
} 

@inproceedings{Sathe:2005:FER:1062261.1062330,
 author = {Sathe, Visvesh and Chueh, Juang-Ying and Kim, Joohee and Ziesler, Conrad H. and Kim, Suhwan and Papaefthymiou, Marios C.},
 title = {Fast, efficient, recovering, and irreversible},
 abstract = {Recent advances in CMOS VLSI design have taken us to real working chips that rely on controlled charge recovery to operate at sub-stantially lower power dissipation levels than their conventional counterparts. In this paper, we present two such chips that were designed in our research group and highlight some of the promising charge-recovery techniques in practice. Although their origins can be traced back to the early adiabatic circuits, these techniques approach energy recycling from a more practical angle, shedding reversibility to achieve operating frequencies in excess of 1GHz with relatively low overheads},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {407--413},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1062261.1062330},
 doi = {http://doi.acm.org/10.1145/1062261.1062330},
 acmid = {1062330},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adiabatic computing, charge-recovery circuits, resonant systems, reversible logic},
} 

@inproceedings{Henzler:2005:MAC:1062261.1062331,
 author = {Henzler, Stephan and Nirschl, Thomas and Eireiner, Matthias and Amirante, Ettore and Schmitt-Landsiedel, Doris},
 title = {Making adiabatic circuits attractive for todays VLSI industry by multi-mode operation-adiabatic mode circuits},
 abstract = {Quasi adiabatic circuits like the efficient charge recovery logic (ECRL) are known to reduce dynamic power dissipation of digital CMOS circuits significantly. The possible operation frequencies have been continuously increased due to technology scaling. Anyway, the field of operation is limited to medium performance applications. If it was possible to operate a given adiabatic circuit also at extremely high frequencies there would be many new applications: A circuit working at a medium frequency most of the time and at high frequencies only for some burst mode operations could be implemented in adiabatic logic. This paper presents a new perspective of adiabatic circuits called adiabatic mode circuits. These circuits can be operated in a quasi adiabatic low-power mode but also in a high frequency domino mode if high speed data processing is required. Based on the 3-transistor DRAM cell a novel 3-transistor memory cell capable for adiabatic and conventional operation is presented. Thus new systems with a small total power consumption but temporarily high performance can be constructed.},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {414--420},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1062261.1062331},
 doi = {http://doi.acm.org/10.1145/1062261.1062331},
 acmid = {1062331},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adiabatic logic, adiabatic mode logic, cross coupled domino, dynamic power reduction, low-power design styles},
} 

@inproceedings{Kim:2005:ISM:1062261.1062332,
 author = {Kim, Seokkee and Chae, Soo-Ik},
 title = {Implementation of a simple 8-bit microprocessor with reversible energy recovery logic},
 abstract = {We describe a simple 8-bit adiabatic microprocessor implemented with nMOS reversible energy recovery logic (nRERL) [1]. The implemented adiabatic microprocessor supports only a subset of the DLX instruction set architecture [15] in order to be fitted into a limited silicon area, and is integrated with an energy-efficient 6-phase clocked power generator (CPG). Phase scheduling was employed to reduce the number of the buffers required in the adiabatic microprocessor. Furthermore, reversibility breaking with self-energy recovery circuits (SERCs) was also employed to reduce energy consumption as well as circuit complexity by. The 8-bit microprocessor core and its on-chip 6-phase CPG were implemented in 0.18-mm CMOS technology. The former and the latter occupied 2.62 x 2.03 mm2 and 1.0 x 0.6 mm2, respectively. From the measurements, we have found that its minimum power consumption is 7.5\&#956;W at V<inf>dd</inf> =1.8V and f</i>=880kHz},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {421--426},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1062261.1062332},
 doi = {http://doi.acm.org/10.1145/1062261.1062332},
 acmid = {1062332},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clocked power generator (CPG), microprocessor, nMOS reversible energy recovery logic (nRERL), phase scheduling, reversibility breaking},
} 

@inproceedings{Fischer:2005:STA:1062261.1062333,
 author = {Fischer, Juergen and Teichmann, Philip and Schmitt-Landsiedel, Doris},
 title = {Scaling trends in adiabatic logic},
 abstract = {Adiabatic circuits which are able to dissipate less energy than the fundamental limit of static CMOS are promising candidates for low-power circuits in the frequency range in which signals are digitally processed. This paper shows the main sources of the energy dissipation in adiabatic circuits. It will be presented that with state-of-the-art transistors the distinction between quasi- and fully adiabatic circuits has become obsolete. With the shrinking of the transistor dimensions, new leakage mechanisms like gate leakage occur. As the adiabatic circuits work with an oscillating power supply, leakage currents flow only a part of the period. Without any further effort adiabatic circuits save about 30\% of energy dissipation caused by leakage. As in static CMOS, adiabatic circuits benefit from voltage scaling. The Efficient Charge Recovery Logic scales linearly down to supply voltages near the threshold voltage. Simulations with a sinusoidal power supply showed no significant difference to a trapezoidal supply at most frequencies. For overall dissipation accounting also for generator efficiency and attenuation on the wiring, the sinusoidal supply voltage should be preferred},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {427--434},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1062261.1062333},
 doi = {http://doi.acm.org/10.1145/1062261.1062333},
 acmid = {1062333},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adiabatic computing, energy recovery, low power},
} 

@inproceedings{Vitanyi:2005:TSE:1062261.1062335,
 author = {Vit\'{a}nyi, Paul},
 title = {Ti space, and energy in reversible computing},
 abstract = {We survey some results of a quarter century of work on general reversible computation about energy-, time- and space requirements.},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {435--444},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1062261.1062335},
 doi = {http://doi.acm.org/10.1145/1062261.1062335},
 acmid = {1062335},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adiabatic computing, computational complexity, energy dissipation complexity, low-energy computing, reversible computing, reversible simulation, space complexity, time complexity, tradeoffs},
} 

@inproceedings{Levitin:2005:TCR:1062261.1062336,
 author = {Levitin, Lev B. and Toffoli, Tommaso},
 title = {Thermodynamical cost of reversible computing},
 abstract = {Since reversible computing requires preservation of information throughout the entire computational process, it implies that all the errors that appear as a result of the interaction of the information-carrying system with uncontrolled degrees of freedom must be corrected. This can only be done at the expense of an increase in the entropy of the environment corresponding to the dissipation of the "noisy" part of the energy of the system in the form of heat.This paper gives an expression of that energy in terms of the effective noise temperature, and calculates the relationship between the energy dissipation rate and the rate of computation.},
 booktitle = {Proceedings of the 2nd conference on Computing frontiers},
 series = {CF '05},
 year = {2005},
 isbn = {1-59593-019-1},
 location = {Ischia, Italy},
 pages = {445--446},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1062261.1062336},
 doi = {http://doi.acm.org/10.1145/1062261.1062336},
 acmid = {1062336},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy in computation, entropy in computation, physics of computation, quantum computing, reversible computing, thermodynamics of computation},
} 

@inproceedings{Marinescu:2004:QPE:977091.977092,
 author = {Marinescu, Dan C.},
 title = {Quantum parallelism and the exact simulation of physical systems},
 abstract = {Classical mechanics and physics limit our ability to compute faster and cheaper and suggests that we need to consider a revolutionary rather than an evolutionary approach to computing and communication. Quantum properties such as uncertainty, interference, and entanglement form the foundation of a new brand of theory where computational and communication processes rest upon fundamental physics.In early 1980s Richard Feynman argued that in traditional numerical simulations such as weather forecasting or aerodynamic calculations, computers model physical reality only approximately. He advanced the idea that physics was computational and that a computer could do an exact simulation of a physical system</i>, even of a quantum system. Starting from basic principles of thermodynamics and quantum mechanics, Feynman suggested that problems for which polynomial time algorithms do not exist could be solved; computations for which polynomial algorithms exist could be speeded up considerably and even made reversible.In quantum systems the amount of parallelism increases exponentially with the size of the system; in other words, an exponential increase in parallelism requires only a linear increase in the amount of space needed. The major difficulty lies in the fact that access to the results of a quantum computation is restricted, access to the results disturbs the quantum state.In this presentation we discuss basic concepts in quantum computing and quantum information theory and present some of our work on computational structural biology.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/977091.977092},
 doi = {http://doi.acm.org/10.1145/977091.977092},
 acmid = {977092},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Rossi:2004:JDP:977091.977094,
 author = {Rossi, G. and Poleo, E.},
 title = {javaset: declarative programming in Java with sets},
 abstract = {In this paper we present a Java library---called JSetL---that offers a number of facilities to support declarative programming like those usually found in logic or functional declarative languages: logical variables, list and set data structures (possibly partially specified), unification and constraint solving over sets, nondeterminism. The paper describes the main features of JSetL and it shows, through a number of simple examples, how these features can be exploited to support a real declarative programming style in Java.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {2--11},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/977091.977094},
 doi = {http://doi.acm.org/10.1145/977091.977094},
 acmid = {977094},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, constraint programming, declarative programming, nondeterminism},
} 

@inproceedings{Katagiri:2004:EAU:977091.977095,
 author = {Katagiri, Takahiro and Kise, Kenji and Honda, Hiroki and Yuba, Toshitsugu},
 title = {Effect of auto-tuning with user's knowledge for numerical software},
 abstract = {This paper evaluates the effect of an auto-tuning facility with the user's knowledge for numerical software. We proposed a new software architecture framework, named FIBER, to generalize auto-tuning facilities and obtain highly accurate estimated parameters. The FIBER framework also provides a loop-unrolling function and an algorithm selection function to support code development by library developers needing code generation and parameter registration processes. FIBER offers three kinds of parameter optimization layers---install-time, before execute-time, and run-time. The user's knowledge is needed in the before execute-time optimization layer. In this paper, eigensolver parameters that apply the FIBER framework are described and evaluated in three kinds of parallel computers: the HITACHI SR8000/MPP, Fujitsu VPP800/63, and Pentium4 PC cluster. Our evaluation of the application of the before execute-time layer indicated a maximum speed increase of 3.4 times for eigensolver parameters, and a maximum increase of 17.1 times for the algorithm selection of orthogonalization in the computation kernel of the eigensolver.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {12--25},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/977091.977095},
 doi = {http://doi.acm.org/10.1145/977091.977095},
 acmid = {977095},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {auto-tuning, eigensolver, numerical library, parameter optimization, performance modeling},
} 

@inproceedings{Malek:2004:INN:977091.977097,
 author = {Malek, Miroslaw},
 title = {Introduction to NOMADS: networks of mobile adaptive dependable systems},
 abstract = {A vision of the NOMADS Republic, the largest nation on earth, is introduced. The NOMADS (Networks of Mobile Adaptive Dependable Systems) infrastructure is aimed at supporting the functionality of such nation. It consists of all systems that satisfy certain minimum requirements, namely, the ability to communicate and to discover, provide and/or use NOMADS services. The goal of NOMADS infrastructure is to deliver low cost, dependable and adaptive connectivity in intelligent and highly semantic manner, making it possible to enter the age of "subdued computing", where computing and communication converge, but stay out of the way.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {26--27},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/977091.977097},
 doi = {http://doi.acm.org/10.1145/977091.977097},
 acmid = {977097},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptivity, dependability, interoperability, mobility, networking, nomadic computing, services, subdued computing},
} 

@inproceedings{Casimiro:2004:AFM:977091.977098,
 author = {Casimiro, Ant\'{o}nio and Kaiser, J\"{o}rg and Ver\'{\i}ssimo, Paulo},
 title = {An architectural framework and a middleware for cooperating smart components},
 abstract = {In a future networked physical world, a myriad of smart sensors and actuators assess and control aspects of their environments and autonomously act in response to it. Examples range in telematics, traffic management, team robotics or home automation to name a few. To a large extent, such systems operate proactively and independently of direct human control driven by the perception of the environment and the ability to organize respective computations dynamically. The challenging characteristics of these applications include sentience and autonomy of components, issues of responsiveness and safety criticality, geographical dispersion, mobility and evolution. A crucial design decision is the choice of the appropriate abstractions and interaction mechanisms. Looking to the basic building blocks of such systems we may find components which comprise mechanical components, hardware and software and a network interface, thus these components have different characteristics compared to pure software components. They are able to spontaneously disseminate information in response to events observed in the physical environment or to events received from other component via the network interface. Larger autonomous components may be composed recursively from these building block.The paper describes an architectural framework and a middleware supporting a component-based system and an integrated view on events-based communication comprising the real world events and the events generated in the system. It starts by an outline of the component-based system construction. The generic event architecture GEAR is introduced which describes the event-based interaction between the components via a generic event layer. The generic event layer hides the different communication channels including the interactions through the environment. An appropriate middleware is presented which reflects these needs and allows to specify events which have quality attributes to express temporal constraints. This is complemented by the notion of event channels which are abstractions of the underlying network and allow to enforce quality attributes. They are established prior to interaction to reserve the needed computational and network resources for highly predictable event dissemination.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {28--39},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/977091.977098},
 doi = {http://doi.acm.org/10.1145/977091.977098},
 acmid = {977098},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {component-based systems, event-based communication, middleware architectures, sentient computing},
} 

@inproceedings{Nett:2004:ASC:977091.977099,
 author = {Nett, Edgar and Schemmer, Stefan},
 title = {An architecture to support cooperating mobile embedded systems},
 abstract = {There is a sustained trend to embed computer systems in all kinds of intelligent products. Increasing emphasis is given to enhance the functionality of such systems beyond the provision of easy-of-use and comfort to more safety-critical tasks where they exert direct control over the intelligent product. Examples of such systems can be exploited in many domains like team robotics, factory automation, transport systems, and intelligent traffic control. To master the inherent complexity, we present a layered system architecture that partly integrates already existing components. It has to cope with two major challenges imposed by the considered type of applications. First, the architecture has to resolve the conflict between the autonomy of the individual systems and the demand to cooperate. We think that coordination is the key to the required effective cooperation. Coordination means that the individual systems can take actions under local control, based on a sufficiently consistent view of the common system environment and on the application of a set of common rules. To enable this, the proposed architecture provides the necessary communication services. Secondly, system control must be prepared to cope with a dynamic and unpredictable system environment. Still guaranteeing QoS properties, especially real-time behavior, necessitates innovative solution concepts. Our approach is to exploit system -- and application -- inherent redundancies across different layers of the architecture.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {40--50},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/977091.977099},
 doi = {http://doi.acm.org/10.1145/977091.977099},
 acmid = {977099},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {mobile embedded systems, mobility and adaptivity, modeling of complex systems, service-based architectures, wireless ad-hoc networks},
} 

@inproceedings{Porcarelli:2004:MER:977091.977100,
 author = {Porcarelli, Stefano and Di Giandomenico, Felicita and Bondavalli, Andrea and Lollini, Paolo},
 title = {Model-based evaluation of a radio resource management system for wireless networks},
 abstract = {This paper focuses on dependability analysis of an interoperable platform for radio resource management and mobility support in multiple radio environments. The emphasis is on reliability and availability issues, which unavoidably need to be addressed to some extent to cope with malfunctions in such complex environment. With reference to the European project CAUTION++, which aims to build a capacity and network management platform for increased utilization of present and future wireless systems, means to help system design and verification w.r.t dependability requirements are here presented. For this purpose, we introduce and apply a modelling technique based on Petri nets in order to compare different architectural fault tolerant solutions. This evaluation is carried out in terms of the probability of correct/incorrect emission and correct/incorrect omission of a reconfiguration action decided by the CAUTION++ system.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {51--59},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/977091.977100},
 doi = {http://doi.acm.org/10.1145/977091.977100},
 acmid = {977100},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {modeling, reliability, resource management system, stochastic activity networks, wireless networks},
} 

@inproceedings{Berhe:2004:MSM:977091.977102,
 author = {Berhe, Girma and Brunie, Lionel and Pierson, Jean-Marc},
 title = {Modeling service-based multimedia content adaptation in pervasive computing},
 abstract = {Pervasive computing applications allow users to access information from anywhere while traveling and using variety of devices. Heterogeneity and limitation of resources involved in this application demand adaptation of content according to the current context (device, user, network etc.). The dynamic nature of adaptation mechanisms together with emerging opportunities of Web Service technology provides new approach of adaptation which is service-based. While this approach would provide a valuable service for the end customer, the service provider, and the content provider, it is important to have an architectural framework which is simple, scalable, flexible and interoperable. Moreover, in order to provide a complete service-based content negotiation and adaptation solution, we must have a model, or a tool, that allows defining environmental constraints, mapping them to appropriate adaptation service requirements and finding an optimal service configuration.In this paper, we present service-based content adaptation architecture, enabling the use of third-party adaptation services and a novel content negotiation and adaptation model. The proposed architectural framework is validated through a prototype.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {60--69},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/977091.977102},
 doi = {http://doi.acm.org/10.1145/977091.977102},
 acmid = {977102},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {content adaptation services, media transformation, multimedia content delivery, pervasive computing},
} 

@inproceedings{Kalapriya:2004:FRD:977091.977103,
 author = {Kalapriya, K. and Nandy, S. K. and Srinivasan, Deepti and Uma Maheshwari, R. and Satish, V.},
 title = {A framework for resource discovery in pervasive computing for mobile aware task execution},
 abstract = {Aimed to provide computation ubiquitously, pervasive computing is perceived as a means to provide a user the transparency of anywhere, anyplace, anytime computing. Pervasive computing is characterized by execution of high-level user tasks in heterogeneous environments that use invisible and ubiquitously distributed computational devices. Resource discovery is an integral part of pervasive computing. Due to the limited computing capacities of the mobile entities in the pervasive space it becomes important for these entities to discover equivalent peers to execute complex tasks. Also requirements of tasks in pervasive space are diverse ranging from static resources like printers to dynamically varying resources like network bandwidth. This requires seamless aggregation of resources/services required for the execution of the task. This is further complicated by frequent associations and disassociation of mobile elements with hotspots which are highly variable in performance and availability. We believe that predicting variability of resources would make the task mobile aware rather than mobility oblivious. We propose a framework for estimation of future resource requirements, which would allow the mobile applications to adapt to wearing (due to disassociations and reassociations) of resources. We also show through case analysis that proactive systems benefit from our architecture.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {70--77},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/977091.977103},
 doi = {http://doi.acm.org/10.1145/977091.977103},
 acmid = {977103},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {framework for resource discovery, pervasive computing},
} 

@inproceedings{Negri:2004:APM:977091.977104,
 author = {Negri, Luca and Barretta, Domenico and Fornaciari, William},
 title = {Application-level power management in pervasive computing systems: a case study},
 abstract = {In this paper we propose an application-level power consumption modeling and optimization technique for mobile devices. The application being considered is modeled as a FSM and the power consumption figures are associated with it through current measurements on selected states, followed by the application of a linear functional model. The FSM model is then used, together with a power management policy, to extend battery lifetime while guaranteeing the execution of essential states in the application. In this paper, the methodology is applied to a specific case study, namely the fruition of multimedia content in an E-Learning scenario.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {78--88},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/977091.977104},
 doi = {http://doi.acm.org/10.1145/977091.977104},
 acmid = {977104},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {pervasive computing, power management, virtual campus},
} 

@inproceedings{Imre:2004:QDN:977091.977106,
 author = {Imre, S\'{a}ndor and Abronits, P\'{e}ter and Darabos, D\'{a}niel},
 title = {Quantum designer and network simulator},
 abstract = {In this paper we introduce our new quantum circuit design tool. Based on quantum mechanical models a universal discrete-time quantum-network designer and simulator was implemented. The graphical user interface allows the user to design complex quantum networks efficiently. The component-based architecture enables independent researchers to use our simulator API (written in C), while we continue to expand and refine the C# based user interface. Future plans include components for distributed simulation and automated circuit design.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {89--95},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/977091.977106},
 doi = {http://doi.acm.org/10.1145/977091.977106},
 acmid = {977106},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {quantum algorithms and circuits, quantum computing, simulation},
} 

@inproceedings{Udrescu:2004:UHD:977091.977107,
 author = {Udrescu, Mihai and Prodan, Lucian and Vl\v{a}dutiu, Mircea},
 title = {Using HDLs for describing quantum circuits: a framework for efficient quantum algorithm simulation},
 abstract = {The quantum algorithms could efficiently solve problems having exponential classical solutions [8]. The circuit model is considered as the most feasible implementation of the quantum algorithms [17]. This paper tries to find common ground between classical circuit design techniques and quantum computation, by identifying quantum circuit specification and simulation tools under the form of Hardware Description Languages (HDLs). The HDL-based simulation approach could reduce the complexity of quantum circuit simulation, by considering entanglement as the main source of gate-level simulation complexity, and isolating it in an automated manner. This is possible by taking advantage of the HDL feature of describing a circuit with both structural and functional architectures. We also performed an analysis of our methodology effectiveness, for the arithmetic circuits involved in Shor's algorithm and the circuits implementing Grover's algorithm. Our method is further improved by an entangled representation avoidance technique called 'bubble logic insertion'.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {96--110},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/977091.977107},
 doi = {http://doi.acm.org/10.1145/977091.977107},
 acmid = {977107},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bubble logic, entanglement, hardware description languages, quantum algorithms, quantum circuits, simulation, views},
} 

@inproceedings{Jorrand:2004:TQP:977091.977108,
 author = {Jorrand, Philippe and Lalire, Marie},
 title = {Toward a quantum process algebra},
 abstract = {Quantum computations operate in the quantum world. For their results to be useful in any way, there is an intrinsic necessity of cooperation and communication controlled by the classical world. As a consequence, full formal descriptions of algorithms making use of quantum principles must take into account both quantum and classical computing components and assemble them so that they communicate and cooperate. This paper aims at defining a high level language allowing the description of classical and quantum programming, and their cooperation. Since process algebras provide a framework to model cooperating computations and have well defined semantics, they have been chosen as a basis for this language. Starting with a classical process algebra, this paper explains how to transform it for including quantum computation. The result is a quantum process algebra with its operational semantics, which can be used to fully describe quantum algorithms in their classical context.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {111--119},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/977091.977108},
 doi = {http://doi.acm.org/10.1145/977091.977108},
 acmid = {977108},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {probabilistic process algebra, quantum computing, semantics of quantum measurement},
} 

@inproceedings{Ceruti:2004:SMI:977091.977110,
 author = {Ceruti, Marion G.},
 title = {States of matter, information organization and dimensions of expressiveness},
 abstract = {This paper expands the concept of "states of information" as analogous to states of matter. It introduces new ideas on the expressiveness of information systems and how information is organized in these systems. By taking advantage of the isomorphism that exists between states of matter and states of information, we can begin to understand new ways to characterize and measure information systems. This paper is the second in a series of papers in the newly emerging and interdisciplinary field of "infodynamics.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {120--124},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/977091.977110},
 doi = {http://doi.acm.org/10.1145/977091.977110},
 acmid = {977110},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {database, expressiveness, infodynamics, knowledge base, model base, states of matter},
} 

@inproceedings{Gruau:2004:BC:977091.977111,
 author = {Gruau, Fr\'{e}d\'{e}ric and Lhuillier, Yves and Reitz, Philippe and Temam, Olivier},
 title = {BLOB computing},
 abstract = {Current processor and multiprocessor architectures are almost all based on the Von Neumann paradigm. Based on this paradigm, one can build a general-purpose computer using very few transistors, e.g., 2250 transistors in the first Intel 4004 microprocessor. In other terms, the notion that on-chip space is a scarce resource is at the root of this paradigm which trades on-chip space for program execution time. Today, technology considerably relaxed this space constraint. Still, few research works question this paradigm as the most adequate basis for high-performance computers, even though the paradigm was not</i> initially designed to scale with technology and space.In this article, we propose a different computing model, defining both an architecture and a language, that is intrinsically designed to exploit space</i>; we then investigate the implementation issues of a computer based on this model, and we provide simulation results for small programs and a simplified architecture as a first proof of concept. Through this model, we also want to outline that revisiting some of the principles of today's computing paradigm has the potential of overcoming major limitations of current architectures.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {125--139},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/977091.977111},
 doi = {http://doi.acm.org/10.1145/977091.977111},
 acmid = {977111},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bio-inspiration, cellular automata, scalable architectures},
} 

@inproceedings{Krishnamurthy:2004:BIR:977091.977112,
 author = {Krishnamurthy, E. V. and Murthy, V. K. and Krishnamurthy, Vikram},
 title = {Biologically inspired rule-based multiset programming paradigm for soft-computing},
 abstract = {This paper describes a rule-based multiset programming paradigm, as a unifying theme for biological, chemical, DNA, physical and molecular computations. The computations are interpreted as the outcome arising out of deterministic, nondeterministic or stochastic interaction among elements in a multiset object space which includes the environment. These interactions are like chemical reactions and the evolution of the multiset can mimic the biological evolution. Since the reaction rules are inherently parallel, any number of actions can be performed cooperatively or competitively among the subsets of elements, so that the elements evolve toward an equilibrium or an emergent state. Hence, this paradigm is widely applicable; e.g., to conventional algorithms, evolutionary algorithms, Markov chain Monte Carlo based Bayesian inference, genetic algorithms, self-organized criticality and active walker models (swarm and ant intelligence), DNA and molecular computing. Practical realisation of this paradigm is achieved through a coordination programming language using Multiset and transactions. This paradigm permits carrying out parts or all of the computations independently on distinct processors and is eminently suitable for cluster and grid computing.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {140--149},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/977091.977112},
 doi = {http://doi.acm.org/10.1145/977091.977112},
 acmid = {977112},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DNA, biologically-inspired paradigm, closed and open systems, first and second order logic, genetic and molecular computing, probabilistic rule based paradigm, soft computing},
} 

@inproceedings{Cojocaru:2004:WAP:977091.977113,
 author = {Cojocaru, Liliana},
 title = {Watson-Crick automata and PCFAS with two components: a computational power analogy},
 abstract = {Watson-Crick Automata</i> are a counterpart of finite automata working on a Watson-Crick tape</i> composed of double stran-ded sequences of symbols linked by a complementarity relation like DNA molecules. Consequently, these devices have as input double strands of strings of symbols arranged in a pairwise affinity similar to Watson-Crick complementarity given by the pairs of nucleotides (A, T) and (C, G) of the DNA alphabet. They have been created in order to investigate the behaviour of DNA molecules.Parallel Communicating Finite Automata Systems</i> are accepting devices composed of two or more finite automata that work in parallel in a synchronized manner, communicating states to each other by request. The protocol of collaboration is controlled by the so-called query symbols</i>. They impose restrictions in the communication process (they specify which automaton has to provide its current state to the automaton that asked for the information). Depending on this protocol several variants of parallel communicating finite automata systems have been introduced in the literature, e.g. returning or not returning, centralized or not.In this paper we prove that Watson-Crick automata and parallel communicating finite automata systems with two components are equivalent from a computational point of view. This statement does not depend on the type of the system protocol. We use this result in solving two open problems that concern the characterization of recursively enumerable languages with parallel communicating devices. Several examples, given at the end of the paper, illustrate possible application of the above systems in DNA computing, more exactly in the process of gene assembly in ciliates</i>.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {150--161},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/977091.977113},
 doi = {http://doi.acm.org/10.1145/977091.977113},
 acmid = {977113},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DNA molecule, Szilard languages, Watson-Crick automata, ciliates, formal language theory, gene assembly, parallel communicating finite automata systems, parallel communicating finite transducer systems, theory of computation},
} 

@inproceedings{McKee:2004:RMW:977091.977115,
 author = {McKee, Sally A.},
 title = {Reflections on the memory wall},
 abstract = {This paper looks at the evolution of the "Memory Wall" problem over the past decade. It begins by reviewing the short Computer Architecture News note that coined the phrase, including the motivation behind the note, the context in which it was written, and the controversy it sparked. What has changed over the years? Are we hitting the Memory Wall? And if so, for what types of applications?},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {162--},
 url = {http://doi.acm.org/10.1145/977091.977115},
 doi = {http://doi.acm.org/10.1145/977091.977115},
 acmid = {977115},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {memory performance, system balance},
} 

@inproceedings{Dubois:2004:FMW:977091.977116,
 author = {Dubois, Michel},
 title = {Fighting the memory wall with assisted execution},
 abstract = {Assisted execution is a form of simultaneous multithreading in which a set of auxiliary "assistant" threads, called nanothreads</i>, is attached to each thread of an application. Nanothreads are lightweight threads which run on the same processor as the main (application) thread and help execute the main thread as fast as possible. Nanothreads exploit resources that are idled in the processor because of hazards due to program dependencies and memory access delays.Assisted execution has the potential to alter the current trade-offs between static and dynamic execution mechanisms. Nanothreads can monitor and reconfigure the underlying hardware, can emulate hardware and can profile applications with little or no interference to improve the program on-line or off-line.We demonstrate the power of assisted execution with an important application, namely data prefetching to fight the memory wall problem. Simulation results on several SPEC95 benchmarks show that sequential and stride prefetching implemented with nanothreads performs just as well as ideal hardware prefetchers.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {168--180},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/977091.977116},
 doi = {http://doi.acm.org/10.1145/977091.977116},
 acmid = {977116},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache memories, latency tolerance, prefetching, simultaneous multithreading, superscalar processors},
} 

@inproceedings{Kampe:2004:SLR:977091.977117,
 author = {Kampe, Martin and Stenstrom, Per and Dubois, Michel},
 title = {Self-correcting LRU replacement policies},
 abstract = {L1 caches must be fast and have a good hit rate at the same time. To be fast, they must remain small. To have a good hit rate, they must be set-associative. With wider associativity the replacement algorithm becomes critical. The wide performance gap between OPT, the optimum off-line algorithm, and LRU suggests that LRU still makes too many mistakes. One way to improve L1 cache behavior is to manage actively the replacement policy to correct these mistakes on the fly.We introduce Self-correcting LRU (SCLRU) which is based on LRU augmented with a feedback loop to constantly monitor and correct replacement mistakes. It relies on several mechanisms to detect, predict, and correct bad replacement decisions. We identify three types of mistakes made by LRU and associate them with memory-access instructions. Our goal is to prevent a mistake to occur more than once. Based on evaluations using a set of seven SPEC95 benchmarks, we show that our approach achieves significant and reliable miss rate improvements, sometimes close to that of OPT, for 2-way and 4-way L1 caches and can do this at a low implementation cost and without affecting the hit cycle time.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {181--191},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/977091.977117},
 doi = {http://doi.acm.org/10.1145/977091.977117},
 acmid = {977117},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {LRU algorithms, mistake prediction, shadow directories},
} 

@inproceedings{Juurlink:2004:DTR:977091.977118,
 author = {Juurlink, Ben and de Langen, Pepijn},
 title = {Dynamic techniques to reduce memory traffic in embedded systems},
 abstract = {Memory transfers, in particular from/to off-chip memories, consume a significant amount of power. In order to reduce the amount of off-chip memory traffic, one or more levels of cache can be employed, located on the same die as the processor core. For performance, energy, and cost reasons, it is expedient that the on-chip cache is small and direct-mapped. Small, direct-mapped caches, however, generally produce much more traffic than needed. The purpose of this paper is two-fold. First, to measure how much traffic is generated by small, direct-mapped caches and what the minimal amount of traffic is. This yields an upper bound on the amount of traffic that can be saved by utilizing the on-chip memory more effectively. Second, we survey some techniques that can be deployed to reduce the amount of traffic produced by direct-mapped caches and present results for some of these techniques.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {192--201},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/977091.977118},
 doi = {http://doi.acm.org/10.1145/977091.977118},
 acmid = {977118},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {caches, embedded processors, memory traffic, power consumption},
} 

@inproceedings{Papanikolaou:2004:OMW:977091.977119,
 author = {Papanikolaou, Antonis and Miranda, Miguel and Catthoor, Francky},
 title = {Overcoming the "Memory Wall" by improved system design exploration and a link to process technology options},
 abstract = {Data transfer and storage issues "take the centre stage" in information and communication systems because of the increasing complexity and data dominance typically associated to them. In this paper, we summarise a systematic methodology for the power critical storage modules such as local SRAMs. We focus on their memory organisation (access schedule and data assignment) together with an exploration of the effect that the interconnect technology may have on the energy consumed by these local memory organisations in deep sub-micron technologies.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {202--211},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/977091.977119},
 doi = {http://doi.acm.org/10.1145/977091.977119},
 acmid = {977119},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {combined system design and process technology exploration, optimal energy/delay trade-off exploration in memories},
} 

@inproceedings{Galluzzi:2004:FGK:977091.977120,
 author = {Galluzzi, Marco and Puente, Valent\'{\i}n and Cristal, Adri\'{a}n and Beivide, Ram\'{o}n and Gregorio, Jos\'{e}-\'{A}ngel and Valero, Mateo},
 title = {A first glance at Kilo-instruction based multiprocessors},
 abstract = {The ever increasing gap between processor and memory speed, sometimes referred to as the Memory Wall</i> problem [42], has a very negative impact on performance. This mismatch will be more severe in future processor's generation. Modern cache organizations and prefetching techniques will not be able to solve this problem. A very novel and promising technique to deal with the Memory Wall</i> consists on designing processors able to maintain thousands of in-flight instructions. An example of this kind of processors has been denoted as Kilo-instruction processors</i> [8]. When running numerical applications, Kilo-instruction processors</i> have demonstrated its ability to effectively maintain high values of IPC while increasing memory latencies.In this paper, we will study for the first time, the influence of Kilo-instruction processors</i> on the performance of small-scale CC-NUMA multiprocessors. Our first results, using an ideal network, show the enormous potential of the Kilo-instruction processors</i>, when using them as computing nodes, not only for hiding local DRAM latencies but also for the remote ones. A deeper analysis, using realistic networks, reveals the existence of heavy demands on packet throughput required by each node, since larger re-order buffers translate on higher density of remote accesses. Next, we show that current interconnection networks cannot cope with this high traffic levels, so newer and faster networks have to be designed. In short, our results show dramatic performance gains over multiprocessors based on current microprocessors and dictate a possible way to build future shared-memory multiprocessor systems.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {212--221},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/977091.977120},
 doi = {http://doi.acm.org/10.1145/977091.977120},
 acmid = {977120},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CC-NUMA, Kilo-instruction processors, ROB, in-flight instructions, instruction window, memory wall, shared-memory multiprocessors},
} 

@inproceedings{Wu:2004:ADC:977091.977122,
 author = {Wu, Keqiang and Chuang, Peng-fei and Lilja, David J.},
 title = {An active data-aware cache consistency protocol for highly-scalable data-shipping DBMS architectures},
 abstract = {In a data-shipping database system, data items are retrieved from the server machines, cached and processed at the client machines, and then shipped back to the server. Current cache consistency approaches typically rely on a centralized server or servers to enforce the necessary concurrency control actions. This centralized server imposes a limitation on the scalability and performance of these systems. This paper presents a new consistency protocol, Active Data-aware Cache Consistency (ADCC), that allows clients to be aware of the global state of their cached data via a two-tier directory. Using parallel communication with simultaneous client-server and client-client messages, ADCC reduces the network latency for detecting data conflicts by 50\%, while increasing message overhead by about 8\% only. In addition, ADCC improves scalability by partially offloading the concurrency control function from the server to the clients. An optimization, Lazy Update, is introduced to reduce the message overhead for maintaining client directory consistency. We implement ADCC in a page server DBMS architecture and compare it with the leading cache consistency algorithm, Callback Locking (CBL), which is the most widely implemented algorithm in commercial DBMSs. Our performance study shows that ADCC has a similar or lower abort rate, higher throughput, and better scalability for important workloads and system configurations. Both the simulation results and the analytic study indicate that the message overhead is low and that ADCC produces better behavior compared to the traditional server-based communication under high contention workloads.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {222--234},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/977091.977122},
 doi = {http://doi.acm.org/10.1145/977091.977122},
 acmid = {977122},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DBMS, active control, cache consistency, data-shipping, parallel communication},
} 

@inproceedings{deLangen:2004:RTG:977091.977123,
 author = {de Langen, Pepijn J. and Juurlink, Ben},
 title = {Reducing traffic generated by conflict misses in caches},
 abstract = {Off-chip memory accesses are a major source of power consumption in embedded processors. In order to reduce the amount of traffic between the processor and the off-chip memory as well as to hide the memory latency, nearly all embedded processors have a cache on the same die as the processor core. Because small caches dissipate less power and are cheaper than large caches, a small cache is preferable to a large cache. Furthermore, because set-associative caches consume more power than direct-mapped caches, a direct-mapped cache is preferable to a set-associative one. Small, direct-mapped caches generally incur many conflict misses, however. In this paper we propose and evaluate a structure called the Conflict Detection Table</i> (CDT). This table can be used to determine if a memory access is expected to hit the cache. If a hit is expected and a miss occurs, then a conflict is detected and appropriate action can be taken. In addition, we propose two cache structures that employ this technique: the Bypass in Case of Conflict</i> (BCC) cache and the Sub-block in Case of Conflict</i> (SCC) cache. The BCC cache bypasses the cache when a conflict is detected, whereas the SCC cache fetches a sub-block of the missing cache block in such a case. Experimental results using several embedded workloads show that the BCC and SCC cache reduce the amount of traffic significantly in many cases. Furthermore, overall they incur the same number of cache misses as the direct-mapped cache. This shows that the BCC and SCC cache reduce the amount of power consumed with a negligible reduction in performance.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {235--239},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/977091.977123},
 doi = {http://doi.acm.org/10.1145/977091.977123},
 acmid = {977123},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {caches, conflict misses, embedded processors, power reduction},
} 

@inproceedings{Chheda:2004:CCR:977091.977125,
 author = {Chheda, Saurabh and Unsal, Osman and Koren, Israel and Krishna, C. Mani and Moritz, Csaba Andras},
 title = {Combining compiler and runtime IPC predictions to reduce energy in next generation architectures},
 abstract = {Next generation architectures will require innovative solutions to reduce energy consumption. One of the trends we expect is more extensive utilization of compiler information directly targeting energy optimizations. As we show in this paper, static information provides some unique benefits, not available with runtime hardware-based techniques alone. To achieve energy reduction, we use IPC information at various granularities, to adaptively adjust voltage and speed, and to throttle the fetch rate in response to changes in ILP. We evaluate schemes that are based on static IPC, runtime IPC and also combined, hybrid approaches.We show that IPC-based adaptive voltage scaling schemes can reduce energy consumption significantly, but the approach that also uses static IPC information in combination with runtime IPC, better captures program ILP burstiness and helps meet applications' target performance: an important criterion in the real-time domain. We have found that static IPC-based fetch-throttling works very well, in most cases performing similarly or better than hardware-only runtime IPC-based schemes. Overall, static IPC based resource throttling alone can save up to 14\% energy in the processor with less than 5\\% IPC degradation. The hybrid scheme saves somewhat more energy but at the expense of higher performance degradation than the static-only approach. In fact, we obtain the lowest IPC degradation with the static IPC-based scheme.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {240--254},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/977091.977125},
 doi = {http://doi.acm.org/10.1145/977091.977125},
 acmid = {977125},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive voltage scaling, compiler architecture interaction, fetch throttling, instruction level parallelism, low power design},
} 

@inproceedings{LaRosa:2004:DSA:977091.977126,
 author = {LaRosa, Christopher R. and Bailey, Mark W.},
 title = {A docked-aware storage architecture for mobile computing},
 abstract = {We explore how the power-abundant docked state of mobile devices can be exploited to reduce power consumption during mobile operation and expand the capabilities of portable devices. We propose a storage hierarchy, which includes a hard disk, a large low-power cache, and a docked-aware file system that lowers the average power cost of file access from the disk while retaining the storage capacity of the disk. We investigate how hoarding files in low-power memory during a power-abundant docked state can drastically reduce the power consumption of the hard disk during mobile operation. Using traced-based simulation, we determine the effects on battery run-time of adding the storage architecture to a modern palmtop device--effectively adding mass storage capability to the device. We experiment in the palmtop environment because palmtops are frequently and easily docked, and epitomize the battery and power constraints that face compact, portable devices. Our trace-based simulation shows that up to 86\% of power used by the storage subsystem can be recovered using simple hoarding algorithms that cache data during the docked state. This power savings translates into simulated run-times 86\%--97\% as long as the run-time of a diskless palmtop.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {255--262},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/977091.977126},
 doi = {http://doi.acm.org/10.1145/977091.977126},
 acmid = {977126},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {battery life, caching, docked, energy, file system, handheld, hoarding, palmtop, power},
} 

@inproceedings{Nemeth:2004:KGI:977091.977128,
 author = {N\'{e}meth, G\'{a}bor},
 title = {Knowledge-based generic intelligent network model},
 abstract = {The next generation of intelligent networks provides information processing, retrieval and transfer services for an arbitrary number of users in a transparent way. The structure of large networks is dynamically changing because of failures and modifications. Besides, it must support user mobility as well. Such a system cannot be handled by the classical models. The architecture proposed is based on the demand-driven information-processing model. The task allocation can be both static or dynamic over inhomogeneous or homogeneous resource types and network structures. The processing nodes may employ any mix of the four basic information-processing models (control-, data-, demand- and information-driven models). Such an intelligent network can be modelled as having logical, temporal and topological structures together with operations on these structures. We introduce a universal relational scheme of parameterized sets of objects of arbitrary types, taking also variables and topological structures into account, to represent precise and imprecise (vague) knowledge. By operations applied to the scheme further knowledge can be deduced and by functional queries facts can be interrogated. These concepts are then applied to define a knowledge module. By composition of knowledge modules and intelligent switches we obtain the generic intelligent network. Introducing a model time as a partially ordered set of events, an evolutionary system can be considered, both in its capabilities and active structure. The services of the intelligent network are provided by intelligent mobile agents.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {263--267},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/977091.977128},
 doi = {http://doi.acm.org/10.1145/977091.977128},
 acmid = {977128},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed systems, information processing models, intelligent networks},
} 

@inproceedings{Georgiev:2004:IRM:977091.977129,
 author = {Georgiev, Iliyak and Georgiev, Ivo I.},
 title = {An information-interconnectivity-based retrieval method for network attached storage},
 abstract = {Network attached disk storage is characterized by independent network attachment and embedded intelligence. For Internet applications, it provides the key functionality of geographical replication and intelligent retrieval of data objects.The paper describes a latency reducing method based on the relative interconnectivity between data objects. We follow the locality-of-reference principle to partition interrelated data objects on close disk areas or network storage nodes. The method incorporates a clustering algorithm to support smarter placement of related objects and read-ahead group caching. Objects that are associated together are clustered in the same group and can be read from disk and cached together. The proposed clustering and cache algorithms do not use floating point, allowing direct and fast implementation on a variety of disk controllers.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {268--275},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/977091.977129},
 doi = {http://doi.acm.org/10.1145/977091.977129},
 acmid = {977129},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {interconnectivity-based retrieval, network attached storage, storage objects clustering},
} 

@inproceedings{Kuhnemann:2004:IET:977091.977130,
 author = {K\"{u}hnemann, Matthias and Rauber, Thomas and R\"{u}nger, Gudula},
 title = {Improving the execution time of global communication operations},
 abstract = {Many parallel applications from scientific computing use MPI global communication operations to collect or distribute data. Since the execution times of these communication operations increase with the number of participating processors, scalability problems might occur. In this article, we show for different MPI implementations how the execution time of global communication operations can be significantly improved by a restructuring based on orthogonal processor structures. As platform, we consider a dual Xeon cluster, a Beowulf cluster and a Cray T3E with different MPI implementations. We show that the execution time of operations like MPI_Bcast()</i> or MPI_Allgather()</i> can be reduced by 40\% and 70\% on the dual Xeon cluster and the Beowulf cluster. But also on a Cray T3E a significant improvement can be obtained by a careful selection of the processor groups. We demonstrate that the optimized communication operations can be used to reduce the execution time of data parallel implementations of complex application programs without any other reordering of the computation and communication structure.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {276--287},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/977091.977130},
 doi = {http://doi.acm.org/10.1145/977091.977130},
 acmid = {977130},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MPI, global communication operations, orthogonal processor groups, parallel programs},
} 

@inproceedings{Wagner:2004:MAB:977091.977131,
 author = {Wagner, Ambrus},
 title = {Mobile agent: based module distribution in heterogeneous networks},
 abstract = {The effective use of computing resources in a heterogeneous network of computers is a field of intensive research. In this paper a mobile agent-based system is described for distributing modules in such a network. Modules in this paper mean either an entire task or the modules of a task or submodules created by a segmentation algorithm. Methods for module distribution and result collection are given, along with a routing algorithm that allows the easy navigation of agents throughout the system. Some solutions for describing the resource requirements of modules and the resources available on the network are also introduced.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {288--293},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/977091.977131},
 doi = {http://doi.acm.org/10.1145/977091.977131},
 acmid = {977131},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed processing, mobile agents},
} 

@inproceedings{Spigarolo:2004:BVB:977091.977133,
 author = {Spigarolo, Micaela and Davoli, Renzo},
 title = {Berserkr: a virtual beowulf cluster for fast prototyping and teaching},
 abstract = {Parallel programming is developing very fast and it is already part of several aspects of our everyday life. Furthermore, the fall of prices for hardware along with an increase in terms of reliability and performance, the wider and wider availability of free software and the wide usage and need for parallel computing and processing have created the natural environment for a development (an almost spontaneous evolution) that has lead to Beowulf, a technology to create a parallel supercomputer out of a cluster of Linux boxes. Nowadays it is possible to create parallel computers effectively at a reasonable price by using off-the-shelf technology. In fact it is possible to build a parallel computer at home by interconnecting components that can be found at an electronic shop. Moreover, we have to deal with the application rather than the specific hardware which carries it out. In fact, universal, general purpose parallel machines simply do not exist. The application must be designed before the cluster architecture. Berserkr represents the solution for this set of problems: it aims to be a tool to test and compare different possible implementations (both in hardware and in software) on several different architectures by building the hardware structure just in the virtual world and not physically.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {294--301},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/977091.977133},
 doi = {http://doi.acm.org/10.1145/977091.977133},
 acmid = {977133},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {parallel programming teaching, parallel programs prototyping, performance evaluation, virtual parallel machine, virtual parallelized program},
} 

@inproceedings{Kouril:2004:PBF:977091.977134,
 author = {Kouril, Michal and Paul, Jerome L.},
 title = {A parallel backtracking framework (BkFr) for single and multiple clusters},
 abstract = {There is a wide class of important problems, typified by the NP-complete SAT problem, for which the best known solutions are obtained using the backtracking strategy and heuristics to bound the search of the associated state-space tree. The backtracking strategy is well suited to parallelization, since different portions of the state-space tree can be assigned to different processes. The resulting strategy is often referred to in the literature as parallel depth-first search or parallel backtracking. In this paper we introduce Backtracking Framework (BkFr) that simplifies implementation of the parallel backtracking paradigm in the single cluster environment, and supports the extension of computations over multiple clusters. BkFr uses two communication methods -- MPI for intra-cluster communication and ICI (Inter-Cluster Interface) for inter-cluster communication. What differentiates ICI from MPI-2, Globus and other libraries that support inter-cluster message passing is its lightweight (requiring only Unix socket API and little or no systems administration support), and its fault-tolerant capabilities. The latter capability was one of our principal motivations for developing ICI, since it allows new clusters to join an ongoing computation anytime without affecting the integrity of the whole system. Also, a sudden failure of a cluster that is part of the computation will result in a reassignment of its work to another resource. There are a number of other parallel frameworks for parallel state-space tree searching, such as the ZRAM framework, but they do not support the multi-cluster fault-tolerant option. In addition to its ease of implementation, BkFr offers a simple interface (less than 10 functions) for new compute modules, making it relatively simple for the user to alter existing code to a form suitable for the BkFr framework. To further simplify the utilization of BkFr for solving user-defined problems, we have developed our own Shared Variable Emulation layer, which hides MPI and ICI communication functions, and allows the user to send information (messages) as variables. Because of its lightweight features, our framework does not incur significant additional overhead, and performance data shows good speedup on problems ranging from a simply implemented version of the sum of subsets problem, to complicated implementations of SAT solvers such as SBSAT and zChaff. While the current implementations of ICI and BkFr are developed in the context of MPI, they can readily be adapted to other message passing environments.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {302--312},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/977091.977134},
 doi = {http://doi.acm.org/10.1145/977091.977134},
 acmid = {977134},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ICI, MPI, SAT, backtracking, inter-cluster communication, intra-cluster communication},
} 

@inproceedings{Juurlink:2004:AOR:977091.977136,
 author = {Juurlink, Ben},
 title = {Approximating the optimal replacement algorithm},
 abstract = {LRU is the de facto standard page replacement strategy. It is well-known, however, that there are many situations where LRU behaves far from optimal. We present a replacement policy that approximates the optimal algorithm OPT more closely by predicting</i> the time each page will be referenced again and by evicting the page that has the largest predicted time of next reference. Experiments using several benchmarks from the SPEC 2000 benchmark suite show that our algorithm is superior to LRU, in some cases by as much as 25\%-30\% and in one case by more than 100\%.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {313--319},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/977091.977136},
 doi = {http://doi.acm.org/10.1145/977091.977136},
 acmid = {977136},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {LRU, OPT, paging, replacement strategy, virtual memory},
} 

@inproceedings{Francia:2004:PSO:977091.977137,
 author = {Francia, M. and Panizzi, E. and Petricola, A. and Visconti, G.},
 title = {Parallel simulation of orography influence on large-scale atmosphere motion on APEmille},
 abstract = {The experience described in this paper relates to the implementation on the parallel computer APEmille of a model for large-scale atmosphere motion, originally developed in Fortran for a conventional architecture. The most critical aspects of this work are described: the mapping of a bidimensional problem on the tridimensional thoroidal architecture of the parallel machine, the choice of a data distribution strategy that minimizes the internode communication needs, the definition of an algorithm for internode communication that minimizes communication costs by performing only first neighbour communications, and the implementation of machine dependant optimizations that allowed to exploit the pipelined architecture of the APEmille processing node and the large register file. An analysis of the performances is reported, compared to both the APEmille peak performance and to the performance on other conventional sequential architectures. Finally, a comparison with the original physical results is presented.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {320--325},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/977091.977137},
 doi = {http://doi.acm.org/10.1145/977091.977137},
 acmid = {977137},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {APEmille, atmospheric physics, data distribution, porting},
} 

@inproceedings{Bera:2004:NTC:977091.977138,
 author = {Bera, Marco and Danese, Giovanni and Leporati, Francesco and Spelgatti, Alvaro},
 title = {A new technique to calculate dipolar energy and its implementation onto an application specific processor},
 abstract = {In this paper we present a new technique which allows an efficient implementation of the energy evaluation, in a Montecarlo-Metropolis simulation, onto an application specific processor, exploiting FPGA technology and purposely designed to accelerate floating point operations. The technique has been implemented and simulated using a proper software development environment designed and realised in our laboratory, while expecting the furnishing of a FPGA Stratix based board from Altera. Results compared with those obtained on last generation workstations show a good acceleration in the processing.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {326--334},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/977091.977138},
 doi = {http://doi.acm.org/10.1145/977091.977138},
 acmid = {977138},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FPGA, Montecarlo simulations, acceleration, application specific processors},
} 

@inproceedings{Park:2004:RRA:977091.977139,
 author = {Park, Yong-Joon and Lee, Gyungho},
 title = {Repairing return address stack for buffer overflow protection},
 abstract = {Although many defense mechanisms against buffer overflow attacks have been proposed, buffer overflow vulnerability in software is still one of the most prevalent vulnerabilities exploited. This paper proposes a micro-architecture based defense mechanism against buffer overflow attacks. As buffer overflow attack leads to a compromised return address, our approach is to provide a software transparent micro-architectural support for return address integrity checking. By keeping an uncompromised copy of the return address separate from the activation record in run-time stack, the return address compromised by a buffer overflow attack can be detected at run time. Since extra copies of return addresses are already found in the return address stack (RAS) for return address prediction in most high-performance microprocessors, this paper considers augmenting the RAS in speculative superscalar processors for return address integrity checking. The new mechanism provides 100\% accurate return address prediction as well as integrity checking for return addresses. Hence, it enhances system performance in addition to preventing a buffer overflow attack.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {335--342},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/977091.977139},
 doi = {http://doi.acm.org/10.1145/977091.977139},
 acmid = {977139},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {buffer overflow, computer architecture, computer security, intrusion tolerance},
} 

@inproceedings{Tabrizi:2004:MMR:977091.977141,
 author = {Tabrizi, Nozar and Bagherzadeh, Nader and Kamalizad, Amir H. and Du, Haitao},
 title = {MaRS: a macro-pipelined reconfigurable system},
 abstract = {We introduce MaRS, a reconfigurable, parallel computing engine with special emphasis on scalability, lending itself to the computation-/data-intensive multimedia data processing and wireless communication. Global communication between the processing elements (PEs) in MaRS is performed through a 2D-mesh deadlock-free network, avoiding any concerns due to non-scalable bus-based communication. Additionally, we have developed a second layer of inter-PE connection realized by distributed shared register files and conditional operands, to enhance the performance of MaRS for those applications demanding a tightly coupled PE array. We have modeled and verified a major part of MaRS. The promising results of our preliminary analyses show that MaRS can efficiently be tailored to different applications offering flexible data communication, and high performance.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {343--349},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/977091.977141},
 doi = {http://doi.acm.org/10.1145/977091.977141},
 acmid = {977141},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {2D-mesh network, MIMD, computer graphics, multimedia, reconfigurable architectures, wireless communication},
} 

@inproceedings{Feng:2004:FTC:977091.977142,
 author = {Feng, T. and Jin, B. and Wang, J. and Park, N. and Kim, Y. B. and Lombardi, F.},
 title = {Fault tolerant clockless wave pipeline design},
 abstract = {This paper presents a fault tolerant design technique for clockless wave pipeline. The specific architectural model investigated in this paper is the two-phase clockless wave pipeline [12] which is ideally supposed to yield the theoretical maximum performance. Request signal is the most critical component for the clockless control of the wave pipelined processing of data. In practice, the request signal is very sensitive and vulnerable to electronic crosstalk noise, referred to as glitch</i>, and this problem has become extremely stringent in the ultra-high density integrated circuits today. Electronic crosstalk noise may devastate the operational confidence level of the clockless wave pipeline. In this context, this paper characterizes the yield and reliability properties of the two-phase clockless asynchronous pipeline with respect to glitch. Based on the yield and reliability characterization, a simple yet effective fault tolerant architecture by using redundant request signals is proposed. The reliability model evaluates the impact of the request signal glitch on the overall reliability, and can be used to maneuver the proposed fault tolerant architecture. An experimental simulation is conducted to demonstrate the efficiency and effectiveness of the proposed fault tolerant technique.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {350--356},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/977091.977142},
 doi = {http://doi.acm.org/10.1145/977091.977142},
 acmid = {977142},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clockless wave pipeline, fault tolerance, inter-wave fault, intra-wave fault, reliability},
} 

@inproceedings{Hartenstein:2004:DDC:977091.977144,
 author = {Hartenstein, Reiner},
 title = {The digital divide of computing},
 abstract = {This presentation urges for creating more awareness of the impact of configware engineering onto embedded system development and examines the requirements of overdue CSE curricular upgrades. Because of the impact of reconfigurable computing, configware engineering has proceeded from niche to mainstream. Morphware has become an essential and indispensable ingredient in SoC (System on a Chip) design and beyond. It turns embedded system design from hardware / software co-design into configware / software co-design [1] [2] [3] [4]. This hot development, supported by the fastest growing segment of the semiconductor market [5], provides morphware [6] [7] as an alternative RAM-based "programmable" (more precisely called: "reconfigurable") platform for parallelism avoiding the limitations of classical high performance computation [8] [9] [10] caused by the von Neumann paradigm [11] [12] [13]. The digital divide of computing determines who is qualified to take off toward new horizons in high performance computing, and, who is not. Currently the typical CS graduate is not.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {357--362},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/977091.977144},
 doi = {http://doi.acm.org/10.1145/977091.977144},
 acmid = {977144},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SoC, morphware, performance, reconfigurable computing},
} 

@inproceedings{Verbauwhede:2004:HMA:977091.977145,
 author = {Verbauwhede, Ingrid and Schaumont, Patrick},
 title = {The happy marriage of architecture and application in next-generation reconfigurable systems},
 abstract = {New applications and standards are first conceived only for functional correctness and without concerns for the target architecture. The next challenge is to map them onto an architecture. Embedding such applications in a portable, low-energy context is the art of molding it onto an energy-efficient target architecture combined with an energy efficient execution. With a reconfigurable architecture, this task becomes a two-way process where the architecture adapts to the application and vice-versa. This leads to the idea of a marriage between architecture and application.These next generation reconfigurable systems consist of a heterogeneous collection of domain-specific processing units. Communication between processors occurs over a reconfigurable interconnect scheme. Global control is provided by one or more embedded micro-controllers, which operate at a low frequency since they don't run compute intensive functions. Because of the domain specific features, this architecture is low power, yet at the same time reconfigurable.In this paper, we will describe the RINGS (Reconfigurable interconnect for next generation systems) architecture and the associated design environment, GEZEL. We will describe how applications are mapped onto RINGS architectures and how they can be modeled and simulated in the GEZEL environment.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {363--376},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/977091.977145},
 doi = {http://doi.acm.org/10.1145/977091.977145},
 acmid = {977145},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {embedded, real-time systems},
} 

@inproceedings{Glesner:2004:RPU:977091.977146,
 author = {Glesner, Manfred and Hollstein, Thomas and Indrusiak, Leandro Soares and Zipf, Peter and Pionteck, Thilo and Petrov, Mihail and Zimmer, Heiko and Murgan, Tudor},
 title = {Reconfigurable platforms for ubiquitous computing},
 abstract = {Ubiquitous computing requires flexibilty. Melting distributed electronic devices into everyday's life implies the need to adapt to evolving standards and dynamic environments. Furthermore, to gain user acceptance, such devices should be able to adapt to different usage patterns and user profiles. Scalability is also an important issue, allowing functional enhancements to already deployed systems. In this work we address these issues applying the concept of reconfigurability on different abstraction layers. Concerning the physical layer we discuss multistandard and standard update capabilities, dynamic power management and functional optimisation. Within this context we consider important design issues related to on-chip communication networks. Furthermore, dynamic reconfigurable processor architectures are analyzed with respect to applications in communications. Finally, in the service layer, we address device location, interconnection and clustering.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {377--389},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/977091.977146},
 doi = {http://doi.acm.org/10.1145/977091.977146},
 acmid = {977146},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {communication, dynamic power management, networks-on-chip, reconfigurable hardware, reconfigurable processors, reconfiguration, ubiquitous computing},
} 

@inproceedings{Reis:2004:PDM:977091.977147,
 author = {Reis, Ricardo and Kastensmidt, Fernanda Lima and G\"{u}ntzel, Jos\'{e} Lu\'{\i}s},
 title = {Physical design methodologies for performance predictability and manufacturability},
 abstract = {The Physical Design Methodology of Integrated Systems is increasing its relevance in deep submicron technologies due to the appearance of new problems related to electrical behavior and performance predictability. This paper presents some techniques to improve reliability and manufacturability by the use of some layout strategies. One main approach is the search of regular solutions as the use of a layout composed by a matrix of cells. It is discussed the effects of layout strategies in the design of reconfigurable systems.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {390--397},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/977091.977147},
 doi = {http://doi.acm.org/10.1145/977091.977147},
 acmid = {977147},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DFM, design methodologies, layout, physical design, regularity},
} 

@inproceedings{Koch:2004:PMP:977091.977148,
 author = {Koch, Dirk and Teich, J\"{u}rgen},
 title = {Platform-independent methodology for partial reconfiguration},
 abstract = {In this paper we present a novel methodology for partial (re-)configuration that can be used for most bitstream configured hardware (HW). In particular low priced and not for partial reconfiguration designed devices can be used by our technique. Furthermore, the methodology is platform independent and requires neither specialized HW synthesis tools nor a documented bitstream. We manage configuration data by extracting and compressing (sub-)module data from complete bitstreams. Experiments demonstrated that this extracted bitstreams could be compressed down by orders of the primary bitstream size for some submodules.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {398--403},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/977091.977148},
 doi = {http://doi.acm.org/10.1145/977091.977148},
 acmid = {977148},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bitstream extraction, configuration compression, partial reconfiguration},
} 

@inproceedings{Murgan:2004:AAO:977091.977149,
 author = {Murgan, Tudor and Petrov, Mihail and Majer, Mateusz and Zipf, Peter and Glesner, Manfred and Heinkel, Ulrich and Pleickhardt, Joerg and Bleisteiner, Bernd},
 title = {Adaptive architectures for an OTN processor: reducing design costs through reconfigurability and multiprocessing},
 abstract = {The standardisation process of Optical Transport Networks generally spans a long period of time. For providers intending to be present early on the market, this implies costly design re-spins if the wrong "flavour" of the protocol standard has been implemented. Extending a protocol processing device through application specific reconfigurable elements or multiprocessor units augment its flexibility. Thus, the architecture can be upgraded to standard updates or changes not even considered at design time. This paper discusses several flexible architectural extensions for a complex ASIC for network processing device developed by Lucent Technologies Network Systems GmbH in N\&#252;rnberg. The device is a multi-rate multiple forward error correction code device which supports 40/43 Gbit/s applications. The proposed architectural enhancements provide adaptable overhead processing and include solutions based on reconfigurable elements, single processing units and multiprocessors.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {404--418},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/977091.977149},
 doi = {http://doi.acm.org/10.1145/977091.977149},
 acmid = {977149},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ITU-T G.709, multiprocessor and reconfigurable architectures, optical transport networks, standard upgrades},
} 

@inproceedings{Kastensmidt:2004:DTF:977091.977150,
 author = {Kastensmidt, Fernanda Lima and Neuberger, Gustavo and Carro, Luigi and Reis, Ricardo},
 title = {Designing and testing fault-tolerant techniques for SRAM-based FPGAs},
 abstract = {This paper discusses fault-tolerant techniques for SRAM-based FPGAs. These techniques can be based on circuit level modifications, with obvious modifications in the programmable architecture, or they can be implemented at the high-level description, without modification in the FPGA architecture. The high-level method presented in this work is based on Triple Modular Redundancy (TMR) and a combination of Duplication Modular Redundancy (DMR) with Concurrent Error Detection (CED) techniques, which are able to cope with upsets in the combinational and in the sequential logic. The methodology was validated by fault injection experiments in an emulation board. Results have been analyzed in terms of reliability, input and output pin count, area and power dissipation.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {419--432},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/977091.977150},
 doi = {http://doi.acm.org/10.1145/977091.977150},
 acmid = {977150},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FPGA, fault-tolerance},
} 

@inproceedings{Cazorla:2004:PPS:977091.977152,
 author = {Cazorla, Francisco J. and Knijnenburg, Peter M.W. and Sakellariou, Rizos and Fern\'{a}ndez, Enrique and Ramirez, Alex and Valero, Mateo},
 title = {Predictable performance in SMT processors},
 abstract = {Current instruction fetch policies in SMT processors are oriented towards optimization of overall throughput and/or fairness. However, they provide no control over how individual threads are executed, leading to performance unpredictability, since the IPC of a thread depends on the workload it is executed in and on the fetch policy used.From the point of view of the Operating System (OS), it is the job scheduler that determines how jobs are executed. However, when the OS runs on an SMT processor, the job scheduler cannot guarantee execution time constraints of any job due to this performance unpredictability.In this paper we propose a novel kind of collaboration between the OS and the SMT hardware that enables the OS to enforce that a high priority thread runs at a specific fraction of its full speed. We present an extensive evaluation using many different workloads, that shows that this mechanism gives the required performance in more than 97\% of all cases considered, and even more than 99\% for the less extreme cases. At the same time, our mechanism does not need to trade off predictability against overall throughput, as it maximizes the IPC of the remaining low priority threads, giving 94\% on average (and 97.5\% on average for the less extreme cases) of the throughput obtained using instruction fetch policies oriented toward throughput maximization, such as icount</i>.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {433--443},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/977091.977152},
 doi = {http://doi.acm.org/10.1145/977091.977152},
 acmid = {977152},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ILP, SMT, multithreading, operating systems, performance predictability, real time, thread-level parallelism},
} 

@inproceedings{Metra:2004:FSN:977091.977153,
 author = {Metra, C. and Mak, T. M. and Oma\~{n}a, M.},
 title = {Fault secureness need for next generation high performance microprocessor design for testability structures},
 abstract = {We analyze the risks associated with faults affecting some Design For Testability (DFT) features employed within todays' high performance microprocessors. We will show that, because of the occurrence of internal faults, some of these structures may become useless, with consequent dramatic impact on test effectiveness and product quality. We borrow the Fault Secure property, that is well known for Self-Checking Circuits, for DFT structures. We will show that it guarantees that no escapes or false acceptance of faulty products may occur because of faults affecting the employed DFT structures. We will discuss the Fault Secureness of the considered DFT structures. We will provide some examples of how the non Fault Secure ones can be modified to meet the Fault Secure goal, thus avoiding their prospected detrimental effect on next generation high performance microprocessors test effectiveness and quality.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {444--450},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/977091.977153},
 doi = {http://doi.acm.org/10.1145/977091.977153},
 acmid = {977153},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {built in self test, comparator, design for testability, fault secureness, microprocessor},
} 

@inproceedings{Xu:2004:HPC:977091.977154,
 author = {Xu, X. H. and Clarke, C. T. and Jones, S. R.},
 title = {High performance code compression architecture for the embedded ARM/THUMB processor},
 abstract = {The use of code compression in embedded systems based on standard RISC instruction set architectures (ISA) has been shown in the past to be of benefit in reducing overall system cost. The 16-bit THUMB ISA from ARM Ltd has a significantly higher density than the original 32-bits ARM ISA. Our proposed memory compression architecture has showed a further size reduction of 15\% to 20\% on the THUMB code. In this paper we propose to use a high-speed data lossless hardware decompressor to improve the timing performance of the architecture. We simulated the architecture on the SimpleScalar platform and show that for some applications, the time overheads are limited within 5\% of the original application.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {451--456},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/977091.977154},
 doi = {http://doi.acm.org/10.1145/977091.977154},
 acmid = {977154},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code compression, embedded systems, high-speed hardware decompressor, performance},
} 

@inproceedings{Nagpal:2004:ITS:977091.977155,
 author = {Nagpal, Rahul and Srikant, Y. N.},
 title = {Integrated temporal and spatial scheduling for extended operand clustered VLIW processors},
 abstract = {Centralized register file architectures scale poorly in terms of clock rate, chip area, and power consumption and are thus not suitable for consumer electronic devices. The consequence is the emergence of architectures having many interconnected clusters each with a separate register file and a few functional units. Among the many inter-cluster communication models proposed, the extended operand model extends some of operand fields of instruction with a cluster specifier and allows an instruction to read some of the operands from other clusters without any extra cost.Scheduling for clustered processors involves spatial concerns (where to schedule) as well as temporal concerns (when to schedule). A scheduler is responsible for resolving the conflicting requirements of aggressively exploiting the parallelism offered by hardware and limiting the communication among clusters to available slots. This paper proposes an integrated spatial and temporal scheduling algorithm for extended operand clustered VLIW processors and evaluates its effectiveness in improving the run time performance of the code without code size penalty.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {457--470},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/977091.977155},
 doi = {http://doi.acm.org/10.1145/977091.977155},
 acmid = {977155},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clustered VLIW processors, spatial scheduling, temporal scheduling},
} 

@inproceedings{Groen:2004:ASR:977091.977157,
 author = {Groen, Peter and H\"{a}m\"{a}l\"{a}inen, Panu and Juurlink, Ben and H\"{a}m\"{a}l\"{a}inen, Timo},
 title = {Accelerating the secure remote password protocol using reconfigurable hardware},
 abstract = {The Secure Remote Password (SRP) protocol is an authentication and key-exchange protocol suitable for secure password verification and session key generation over insecure communication channels. The modular exponentiations involved, however, are very time-consuming, causing slow log-on procedures. This work presents the design of a hardware accelerator that performs modular exponentiation of very wide integers. The experimental platform is <sc>tutwlan</sc>, a Wireless Local Area Network (<sc>wlan</sc>) being developed at Tampere University of Technology. It runs on the Altera Excalibur development board that contains a microprocessor and a chip with programmable hardware. The results show that a full modular exponentiation with 1023-bit inputs can be performed in less than 40 ms using less than 10,000 logic elements, each consisting of a 4-input lookup table and a register. By using the implemented hardware accelerator in the authentication protocol, the execution time is reduced by a factor of 4. In addition, proposals to improve the implemented modular exponentiation architecture are presented. An additional factor of 5 improvement (totaling a factor of 20) can be achieved by implementing the fastest design.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {471--480},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/977091.977157},
 doi = {http://doi.acm.org/10.1145/977091.977157},
 acmid = {977157},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {WLAN, authentication, hardware acceleration, modular exponentiation, reconfigurable hardware, secure remote password protocol},
} 

@inproceedings{Kosaka:2004:SDO:977091.977158,
 author = {Kosaka, Atsushi and Yamaguchi, Satoshi and Okuhata, Hiroyuki and Onoye, Takao and Shirakawa, Isao},
 title = {SoC design of Ogg Vorbis decoder using embedded processor},
 abstract = {This paper presents an ARM-based SoC architecture for the Ogg Vorbis audio decoder. A trivial software-based implementation incurs high computational cost and requires high operation frequency. In order to achieve realtime processing and efficient bus interface design for our target system, the load of an embedded processor is reduced through the use of specific hardware for a functional block that has higher computational complexity than other blocks of Ogg Vorbis decoding process. Based on computational cost analysis of whole decoding process, IMDCT (Inverse Modified Discrete Cosine Transform) is detected as the most computation-intensive functional block. As a result of FPGA (Field Programmable Gate Array) implementation, a 48\% improvement in execution cycle is achieved by the specific hardware with 3,749 slices.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {481--487},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/977091.977158},
 doi = {http://doi.acm.org/10.1145/977091.977158},
 acmid = {977158},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Tosic:2004:PFM:977091.977160,
 author = {Tosic, Predrag T.},
 title = {A perspective on the future of massively parallel computing: fine-grain vs. coarse-grain parallel models comparison \& contrast},
 abstract = {Models, architectures and languages for parallel computation</i> have been of utmost research interest in computer science and engineering for several decades. A great variety of parallel computation models has been proposed and studied, and different parallel and distributed architectures designed as some possible ways of harnessing parallelism and improving performance of the general purpose computers.Massively parallel connectionist models</i> such as artificial neural networks</i> (<b>ANNs</b>) and cellular automata</i> (<b>CA</b>) have been primarily studied in domain-specific contexts, namely, learning</i> and complex dynamics</i>, respectively. However, they can also be viewed as generic abstract models of massively parallel computers that are in many respects fundamentally different from the "main stream" parallel and distributed computation models.We compare and contrast herewith the parallel computers as they have been built by the engineers with those built by Nature. We subsequently venture onto a high-level discussion of the properties and potential advantages of the proposed massively parallel computers of the future that would be based on the fine-grained connectionist parallel models, rather than on either various multiprocessor architectures, or networked distributed systems, which are the two main architecture paradigms in building parallel computers of the late 20th and early 21st centuries. The comparisons and contrasts herein are focusing on the fundamental conceptual characteristics of various models rather than any particular engineering idiosyncrasies, and are carried out at both structural and functional levels. The fundamental distinctions between the fine-grain connectionist parallel models and their "classical" coarse-grain counterparts are discussed, and some important expected advantages of the hypothetical massively parallel computers based on the connectionist paradigms conjectured.We conclude with some brief remarks on the role that the paradigms, concepts, and design ideas originating from the connectionist models have already had in the existing parallel design, and what further role the connectionist models may have in the foreseeable future of parallel and distributed computing.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {488--502},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/977091.977160},
 doi = {http://doi.acm.org/10.1145/977091.977160},
 acmid = {977160},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cellular automata, distributed systems, massively parallel computing, multiprocessor computers, neural networks, parallel computation models},
} 

@inproceedings{Wang:2004:OCA:977091.977161,
 author = {Wang, Teng and Qi, Zhenghua and Moritz, Csaba Andras},
 title = {Opportunities and challenges in application-tuned circuits and architectures based on nanodevices},
 abstract = {Nanoelectronics research has primarily focused on devices. By contrast, not much has been published on innovations at higher layers: we know little about how to construct circuits or architectures out of such devices. In this paper, we focus on the currently most promising nanodevice technologies, such as arrays of semiconductor nanowires (NWs) and arrays of crossed carbon nanotubes (CNTs). In contrast to general-purpose programmable fabrics (such as PLAs), we investigate nano-fabrics that, while also programmable and hierarchical, are more tuned towards an application domain (in this sense they resemble ASIC). Our goal is to achieve denser designs with better fabric utilization, efficient cascading of circuits, and routing of signals. We demonstrate detailed designs of several circuits and processor data-paths, and highlight associated challenges and opportunities for optimization.},
 booktitle = {Proceedings of the 1st conference on Computing frontiers},
 series = {CF '04},
 year = {2004},
 isbn = {1-58113-741-9},
 location = {Ischia, Italy},
 pages = {503--511},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/977091.977161},
 doi = {http://doi.acm.org/10.1145/977091.977161},
 acmid = {977161},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FET, NASIC, microwire, nanowire, tile},
} 

