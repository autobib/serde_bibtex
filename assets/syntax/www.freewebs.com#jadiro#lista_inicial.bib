% This file was created with JabRef 2.4.
% Encoding: Cp1252

@INPROCEEDINGS{aberg_automatic_2003,
  author = {R.A. Aberg and J.L. Lawall and M. Sudholt and G. Muller and A.-F.
	Le Meur},
  title = {On the automatic evolution of an OS kernel using temporal logic and
	AOP},
  booktitle = {Automated Software Engineering, 2003. Proceedings. 18th IEEE International
	Conference on},
  year = {2003},
  pages = {196â€•204},
  abstract = {Automating software evolution requires both identifying precisely
	the affected program points and selecting the appropriate modification
	at each point. This task is particularly complicated when considering
	a large program, even when the modifications appear to be systematic.
	We illustrate this situation in the context of evolving the Linux
	kernel to support Bossa, an event-based framework for process-scheduler
	development. To support Bossa, events must be added at points scattered
	throughout the kernel. In each case, the choice of event depends
	on properties of one or a sequence of instructions. To describe precisely
	the choice of event, we propose to guide the event insertion by using
	a set of rules, amounting to an aspect that describes the control-flow
	contexts in which each event should be generated. In this paper,
	we present our approach and describe the set of rules that allows
	proper event insertion. These rules use temporal logic to describe
	sequences of instructions that require events to be inserted. We
	also give an overview of an implementation that we have developed
	to automatically perform this evolution.},
  doi = {10.1109/ASE.2003.1240307},
  isbn = {1527-1366 },
  keywords = {AOP,automatic evolution,automatic programming,Bossa,control-flow contexts,instruction
	sequences,Linux kernel,object-oriented programming,operating system,operating
	system kernels,OS kernel,process-scheduler development,software evolution,software
	maintenance,temporal logic,Unix},
  owner = {user},
  timestamp = {2008.10.19}
}

@INBOOK{abraham_software_2007,
  pages = {82â€•91},
  title = {Software Component Selection Algorithm Using Intelligent Agents},
  year = {2007},
  author = {Blanca Abraham and Jose Aguilar},
  abstract = {We have developed one stochastic model for intelligent selection of
	software components, in Internet. Components can be physically located
	in different repositories, and the selection is done using a XML
	file which is associated to each component. This file contains the
	most relevant characteristics of the component, with one extra field
	stored to be used by this algorithm; this field is called â€œpheromoneâ€?,
	which is a concept taken from collective intelligence theory that
	has been the main inspiration of this work. Swarm intelligence is
	based on each agent capacity to work individually in order to achieve
	a collective goal; intelligent agents interact not only with each
	other but also with their environment. This model can be used not
	only for component selection but also for services, resources, etc.
	This is because it is general enough for been replicated with different
	types of requirements. },
  journal = {Agent and Multi-Agent Systems: Technologies and Applications},
  owner = {user},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1007/978-3-540-72830-6\_9}
}

@INPROCEEDINGS{abrahao_web-enabling_1999,
  author = {S.M. Abrahao and A.F. do Prado},
  title = {Web-enabling legacy systems through software transformations},
  booktitle = {Advance Issues of E-Commerce and Web-Based Information Systems, WECWIS,
	1999. International Conference on},
  year = {1999},
  pages = {87â€•90},
  abstract = {Reengineering applications so that they can be Web-enabled is a problem
	faced by many software development teams nowadays. In this research
	work, we present a semi-automated approach to help tackling this
	kind of problem. By using automated software transformation techniques,
	we combine the Fusion/RE reverse-engineering approach and the Draco-PUC
	transformational workbench in order to reengineer legacy procedure-oriented
	Clipper systems into object-oriented Java Web-enabled applications.
	We demonstrate our approach through a sample system having 20 K lines
	of code},
  doi = {10.1109/ASWEC.1998.730920},
  keywords = {application reengineering,automated software transformation techniques,Draco-PUC
	transformational workbench,Fusion/RE reverse-engineering approach,Internet,Java,legacy
	procedure-oriented Clipper system reengineering,legacy system Web
	enabling,object-oriented Java Web-enabled applications,object-oriented
	programming,reverse engineering,semi-automated approach,software
	development,software maintenance,systems re-engineering},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{abramson_debugging_1996,
  author = {D. Abramson and R. Sosic},
  title = {A debugging and testing tool for supporting software evolution},
  journal = {Automated Software Engineering},
  year = {1996},
  volume = {3},
  pages = {369â€•390},
  number = {3},
  abstract = {This paper describes a tool for debugging programs which develop faults
	after they have been modified or are ported to other computer systems.
	The tool enhances the traditional debugging approach by automating
	the comparison of data structures between two running programs. Using
	this technique, it is possible to use early versions of a program
	which are known to operate correctly to generate values for comparison
	with the new program under development. The tool allows the reference
	code and the program being developed to execute on different computer
	systems by using open distributed systems techniques. A data visualisation
	facility allows the user to view the differences in data structures.
	By using the data flow of the code, it is possible to locate faulty
	sections of code rapidly. An evaluation is performed by using three
	case studies to illustrate the power of the technique.},
  doi = {10.1007/BF00132573},
  owner = {user},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1007/BF00132573}
}

@INPROCEEDINGS{adamek_perspectives_2008,
  author = {Jiri Adamek and Petr Hnetynka},
  title = {Perspectives in component-based software engineering},
  booktitle = {Proceedings of the 2008 international workshop on Software Engineering
	in east and south europe},
  year = {2008},
  pages = {35â€•42},
  address = {Leipzig, Germany},
  publisher = {ACM},
  abstract = {Component-based software engineering (CBSE) has become a commonly
	used development technique. Using it, applications are composed of
	reusable components with well defined interfaces and behavior.},
  doi = {10.1145/1370868.1370875},
  isbn = {978-1-60558-076-0},
  keywords = {components,formal verification,research cooperation,software architecture},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1370868.1370875\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{Agatonovic-Kustrin1999,
  author = {S. Agatonovic-Kustrin and I. G. Tucker and D. Schmierer},
  title = {Solid state assay of ranitidine HCl as a bulk drug and as active
	ingredient in tablets using DRIFT spectroscopy with artificial neural
	networks.},
  journal = {Pharm Res},
  year = {1999},
  volume = {16},
  pages = {1477--1482},
  number = {9},
  month = {Sep},
  abstract = {PURPOSE: A new, simple, sensitive and rapid method was developed to
	analyse the polymorphic purity of crystalline ranitidine-HCI as a
	bulk drug and from a tablet formulation. METHODS: Diffuse reflectance
	infrared Fourier transform (DRIFT) spectroscopy was combined with
	Artificial Neural Networks (ANNs) as a data modelling tool. A standard
	feed-forward network, with backpropagation rule and with single hidden
	layer architecture was chosen. Reduction and transformation of the
	spectral data enhanced the ANN performance and reduced the complexity
	of the ANNs model. Spectral intensities from 1738 wavenumbers were
	reduced into 173 averaged spectral values. These 173 values were
	used as inputs for the ANN. Following a sensitivity analysis the
	number of inputs was reduced to 30, or 35, these being the input
	windows which had most effect on the output of the ANN. RESULTS:
	For the bulk drug assay, the ANN model had 30 inputs selected from
	a sensitivity analysis, one hidden layer, and two output neurons,
	one for the percentage of each ranitidine hydrochloride crystal form.
	The model could simultaneously distinguish between crystal forms
	and quantify them enabling the physical purity of the bulk drug to
	be checked. For the tablet assay, the ANN model had 173 averaged
	spectral values as the inputs, one hidden layer and five output neurons,
	two for the percentage of the two ranitidine hydrochloride crystal
	forms and three more outputs for tablet excipients and additives.
	The ANN was able to solve the problem of overlapping peaks and it
	successfully identified and quantified all components in tablet formulation
	with reasonable accuracy. CONCLUSIONS: Some of the advantages over
	conventional analytical methods include simplicity, speed and good
	selectivity. The results from DRIFT spectral quantification study
	show the benefits of the neural network approach in analysing spectral
	data.},
  institution = {School of Pharmacy, University of Otago, Dunedin, New Zealand. nena.kustrin@stonebow.otago.ac.nz},
  keywords = {Anti-Ulcer Agents, analysis/standards; Calibration; Crystallography;
	Drug Industry, methods/standards; Fourier Analysis; Microscopy, Electron,
	Scanning; Neural Networks (Computer); Ranitidine, analysis/standards;
	Sensitivity and Specificity; Software; Spectrum Analysis, instrumentation/methods;
	Tablets, chemistry/standards},
  owner = {user},
  pmid = {10496668},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{agrawal_open_2004,
  author = {M. Agrawal and S. Cooper and L. Graba and V. Thomas},
  title = {An open software architecture for high-integrity and high-availability
	avionics},
  booktitle = {Digital Avionics Systems Conference, 2004. DASC 04. The 23rd},
  year = {2004},
  volume = {2},
  pages = {8.C.2â€•81--11 Vol.2},
  abstract = {We describe a software architecture that can greatly reduce re-certification
	costs associated with the re-hosting of avionics applications from
	one platform to another. This is achieved by (1) enabling the development
	of core application components independent of platform specific concerns
	related to I/O and fault-tolerance, (2) defining abstractions of
	platform I/O and fault-tolerance strategies for use by application
	components, and (3) providing transforms that enable system integrators
	to build a system with its specific I/O and fault-tolerance requirements
	using platform-independent application components. Application component
	and transform source code (and in many cases, binaries) can be moved
	from one platform to another without the need for modification. The
	system configuration and any new transforms developed still need
	to be recertified. The I/O abstractions defined by the architecture
	are key to enable the development of platform independent application
	components. Inputs to components are simple values (signals) with
	attributes such as refresh rate and units. On different platforms,
	these values may be generated at different rates, in different units,
	and in different ways (by combining values from multiple sources,
	produced by a fail-stop source, etc.). Transforms mask these platform
	differences from application components. Similarly, differences in
	component output attributes and those required by the platform are
	handled by transforms. The architecture makes provision for application
	specific built-in-tests, fault-detectors, and reconfiguration strategies.
	Again, these are specified and implemented independent of core application
	functionality, allowing application components to be moved across
	platforms with different fault-tolerance strategies. A software framework
	based on this architecture has been implemented and demonstrated
	using an FMS-like application. Core application functionality was
	implemented as components and packaged as shared libraries. Multiple
	I/O and redundancy schemes were then constructed using these application
	modules by changing only the configuration. This demonstrated the
	feasibility of developing application components in a platform independent
	manner and configuring them for different platforms.},
  doi = {10.1109/SIMSYM.2000.844926},
  keywords = {application specific built-in tests,avionics,avionics applications,built-in
	self test,component output attributes,core application functionality,fault
	detectors,fault tolerance,I/O abstractions,open software architecture,platform
	independent application components,reconfiguration strategies,software
	architecture,software fault tolerance,source coding,transform source
	code},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{al-kady_uml_2008,
  author = {M. Al-Kady and R. Bahgat and A. Fahmy},
  title = {A UML Heavyweight Extension for MAS Modeling},
  booktitle = {Quality Software, 2008. QSIC '08. The Eighth International Conference
	on},
  year = {2008},
  pages = {435--440},
  abstract = {Unified Modeling Language (UML) is the standard notation technique
	used for modeling object-oriented (OO) systems. Previous efforts
	exist to extend UML for representing multi-agent systems (MAS). Due
	to the UML widespread, the resulting extensions are therefore easy
	to use and to adopt by software engineers. Unfortunately, most of
	these attempts are based on applying stereotypes for the object-oriented
	entities. However, heavyweight extension is a more adequate choice
	to handle different capabilities of the MAS over the OO systems.
	In this work a heavyweight extension to UML metamodel is proposed
	for modeling MAS according to our unified MAS conceptual model. It
	is not limited to a specific application domain or specific agent
	architecture. A proposed MAS-UML tool is built based on the extended
	metamodel.},
  doi = {10.1109/IAT.2006.44},
  isbn = {1550-6002},
  keywords = {Agent Oriented Software Engineering,Metamodeling,multiagent systems,object-oriented
	systems,specific agent architecture,specific application domain,UML
	extension,UML metamodel,unified modeling language},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{alvaro_component_2007,
  author = {Alexandre Alvaro and Eduardo Santana de Almeida and Silvio Romero
	de Lemos Meira},
  title = {A component quality assurance process},
  booktitle = {Fourth international workshop on Software quality assurance: in conjunction
	with the 6th ESEC/FSE joint meeting},
  year = {2007},
  pages = {94â€•101},
  address = {Dubrovnik, Croatia},
  publisher = {ACM},
  abstract = {One of the major problem with Component-Based Software Engineering
	(CBSE) is the quality of the components used in a system. The reliability
	of a component-based software system depends on the reliability of
	the components that is made of. In CBSE, the proper search, selection
	and evaluation process of components is considered the cornerstone
	for the development of any effective component-based system. So far
	the software industry was concentrated on the functional aspects
	of components, leaving aside the difficult task of assessing their
	quality. In this way, we propose a component certification process
	to evaluate the quality of software components in an efficient way.},
  doi = {10.1145/1295074.1295093},
  isbn = {978-1-59593-724-7},
  keywords = {component certification,component quality evaluation and certification
	process,software component quality},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1295074.1295093\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{rym_ameur_interactive_2006,
  author = {Rym Ameur and Jean-Claude Heudin},
  title = {Interactive Intelligent Agent Architecture},
  booktitle = {Web Intelligence and Intelligent Agent Technology Workshops, 2006.
	WI-IAT 2006 Workshops. 2006 IEEE/WIC/ACM International Conference
	on},
  year = {2006},
  pages = {331--334},
  abstract = {This paper proposes a conversational creature model. It is based on
	multi-agent subsumption architecture. Behaviour agents are represented
	by classifiers. We use an ethological hierarchy of agents from reflexive
	agent to cognitive agent. A strategic agent is also developed to
	control the whole dialogue, into a proactive architecture. To make
	this conversational creature evolve, we use a classifier system based
	on lisp-like s-expressions},
  doi = {10.1109/CISDA.2007.368134},
  keywords = {artificial life.,behaviour agents,classifier system,classifier systems,cognitive
	agent,conversational agent,conversational creature model,ethological
	hierarchy,interactive intelligent agent architecture,lisp-like s-expressions,multi-agent
	subsumption architecture,natural language interfaces,proactive architecture,reflexive
	agent,strategic agent,subsumption},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Amigoni2003,
  author = {Francesco Amigoni and Marco Dini and Nicola Gatti and Marco Somalvico},
  title = {Anthropic agency: a multiagent system for physiological processes.},
  journal = {Artif Intell Med},
  year = {2003},
  volume = {27},
  pages = {305--334},
  number = {3},
  month = {Mar},
  abstract = {Multiagent systems are powerful and flexible tools for modelling and
	regulating complex phenomena. In fact, a way to manage the complexity
	of a phenomenon is to decompose it in such a way that each agent
	embeds the control model for a portion of the phenomenon. In this
	perspective, the cooperative interaction among the agents results
	in the controller for the whole phenomenon. Since the portions in
	which the phenomenon is decomposed may overlap, the actions the single
	agents undertake to regulate these portions may conflict; hence a
	balanced negotiation is required. A class of complex phenomena that
	present several difficulties in their satisfactory modelling and
	controlling is the class of physiological processes. The purpose
	of this paper is to introduce a general multiagent architecture,
	called anthropic agency, for the modelling and the regulation of
	complex physiological phenomena.},
  institution = {Artificial Intelligence and Robotics Project, Dipartimento di Elettronica
	e Informazione, Politecnico di Milano, Piazza Leonardo da Vinci 32,
	I-20133 Milano, Italy. amigoni@elet.polimi.it},
  keywords = {Animals; Artificial Intelligence; Computer Simulation; Glucose, metabolism;
	Hypoglycemic Agents, pharmacology; Insulin, pharmacology; Medical
	Informatics; Models, Theoretical; Physiology; Software},
  owner = {user},
  pii = {S0933365703000083},
  pmid = {12667741},
  timestamp = {2008.10.19}
}

@ARTICLE{Anchin1994,
  author = {J. M. Anchin and C. Mandal and C. Culberson and S. Subramaniam and
	D. S. Linthicum},
  title = {Computer-aided molecular modeling of the binding site architecture
	for eight monoclonal antibodies that bind a high potency guanidinium
	sweetener.},
  journal = {J Mol Graph},
  year = {1994},
  volume = {12},
  pages = {257--66, 289-90},
  number = {4},
  month = {Dec},
  abstract = {Computer-aided molecular modeling of the antibody binding site of
	eight different monoclonal antibodies (mAb) that bind the intense
	sweetener ligand (N-(p-cyanophenyl)-N'-diphenylmethyl) guanidine
	acetic acid was completed using canonical loop structures and framework
	regions from known immunoglobulins as "parent structures" for the
	molecular scaffoldings. The models of the fragment variable (Fv)
	region of the mAb were analyzed for the presence and location of
	residues predicted to be involved in ligand binding. Several binding
	site tryptophan residues in these models were located in positions
	that support previous flurospectroscopic observations of the mAb-ligand
	complexation. Computer-aided renderings of the electrostatic potential
	at the van der Waals surface of the Fv region were compared and found
	to be consistent with the ligand binding specificity profiles for
	the different mAb. The Fv model of mAb NC6.8 was consistent with
	the binding site features determined in the Fab structure recently
	solved by X-ray diffraction techniques. These Fv models should provide
	an adequate basis for site-directed mutagenesis experiments in order
	to characterize interactive motifs in the mAb binding site.},
  institution = {M University, College Station 77843.},
  keywords = {Acetic Acids, immunology/metabolism; Algorithms; Amino Acid Sequence;
	Antibodies, Monoclonal, chemistry/immunology; Binding Sites, Antibody;
	Computer-Aided Design; Drug Design; Guanidines, immunology/metabolism;
	Immunoglobulin Variable Region, chemistry; Models, Molecular; Molecular
	Sequence Data; Molecular Structure; Protein Structure, Tertiary;
	Sequence Alignment; Software; Sweetening Agents, metabolism},
  owner = {user},
  pmid = {7696216},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{archuleta_maintainable_2007,
  author = {J. Archuleta and E. Tilevich and Wu-chun Feng},
  title = {A Maintainable Software Architecture for Fast and Modular Bioinformatics
	Sequence Search},
  booktitle = {Software Maintenance, 2007. ICSM 2007. IEEE International Conference
	on},
  year = {2007},
  pages = {144--153},
  abstract = {Bioinformaticists use the Basic Local Alignment Search Tool (BLAST)
	to characterize an unknown sequence by comparing it against a database
	of known sequences, thus detecting evolutionary relationships and
	biological properties. mpiBLAST is a widely-used, high-performance,
	open-source parallelization of BLAST that runs on a computer cluster
	delivering super-linear speedups. However, the Achilles heel of mpiBLAST
	is its lack of modularity, thus adversely affecting maintainability
	and extensibility. Alleviating this shortcoming requires an architectural
	refactoring to improve maintenance and extensibility while preserving
	high performance. Toward that end, this paper evaluates five different
	software architectures and details how each satisfies our design
	objectives. In addition, we introduce a novel approach to using mixin
	layers to enable mixing-and-matching of modules in constructing sequence-search
	applications for a variety of high-performance computing systems.
	Our design, which we call "mixin layers with refined roles", utilizes
	mixin layers to separate functionality into complementary modules
	and the refined roles in each layer improve the inherently modular
	design by precipitating flexible and structured parallel development,
	a necessity for an open-source application. We believe that this
	new software architecture for mpiBLAST-2.0 will benefit both the
	users and developers of the package and that our evaluation of different
	software architectures will be of value to other software engineers
	faced with the challenges of creating maintainable and extensible,
	high-performance, bioinformatics software.},
  doi = {10.1109/ICMLC.2007.4370372},
  isbn = {1063-6773},
  keywords = {basic local alignment search tool,bioinformatics sequence search,computer
	cluster,maintainable software architecture,modular design,open-source
	parallelization,parallel processing,sequences,software refactoring},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{aretxandieta_component_2008,
  author = {X. Aretxandieta and X. Elkorobarrutia and F. Barbier},
  title = {Component Adaptation for Correctness in Composite Systems},
  booktitle = {Composition-Based Software Systems, 2008. ICCBSS 2008. Seventh International
	Conference on},
  year = {2008},
  pages = {130â€•137},
  abstract = {When creating systems by composing individual components, even if
	those ones behave as promised by its developer, unexpected situation
	may arise due to the composition operation itself. These situations,
	that may cause system failures or unsatisfied requirements occur
	because component providers have not foreseen the whole environments
	in which their own components may be deployed. They also do not know
	which third-party components their own components will be combined
	with. Here we present a framework for component development which
	promotes a system constructor (integrator) to adapt components at
	runtime, i.e., to adjust their behaviors to the needs of a component
	assembly. Components are equipped with appropriate mechanisms to
	adjust its behavior to assembly requirements and constraints without
	accessing to its source code. Only the component's behavioral model
	is reconsidered and thus, adapted so that it operates in conformance
	with the rest of the designed assembly.},
  keywords = {behavior,component adaptation,component assembly requirement,component
	behavioral model,component development framework,composite system,Composition,correctness,object-oriented
	programming,program assemblers,software engineering,source code,statecharts,third-party
	component},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{arora_software_2006,
  author = {D. Arora and A. Raghunathan and S. Ravi and M. Sankaradass and N.K.
	Jha and S.T. Chakradhar},
  title = {Software architecture exploration for high-performance security processing
	on a multiprocessor mobile SoC},
  booktitle = {Design Automation Conference, 2006 43rd ACM/IEEE},
  year = {2006},
  pages = {496--501},
  abstract = {We present a systematic methodology for exploring the security processing
	software architecture for a commercial heterogeneous multiprocessor
	system-on-chip (SoC) for mobile devices. The SoC contains multiple
	host processors executing applications and a dedicated programmable
	security processing engine. We developed an exploration methodology
	to map the code and data of security software libraries onto the
	platform, with the objective of maximizing the overall application-visible
	performance. The salient features of the methodology include (i)
	the use of real performance measurements from a prototyping board
	that contains the target platform to drive the exploration, (ii)
	a new data structure access profiling framework that allows us to
	accurately model the communication overheads involved in offloading
	a given set; of functions to the security processor, and (iii) an
	exact branch-and-bound based design space exploration algorithm that
	determines the best mapping of security library functions and data
	structures to the host and security processors. We used the proposed
	framework to map a commercial security library to the target mobile
	application SoC. The resulting optimized software architecture outperformed
	several manually-designed software architectures, resulting in up
	to 12.5Ã— speedup for individual cryptographic operations (encryption,
	hashing) and 2.2Ã—-6.2Ã— speedup for applications such as a digital
	rights management (DRM) agent and secure sockets layer (SSL) client.
	We also demonstrate the applicability of our framework to software
	architecture exploration in other multiprocessor scenarios.},
  doi = {10.1109/ASWEC.2005.9},
  isbn = {0738-100X },
  keywords = {commercial system-on-chip,communication overheads,computation offloading,cryptography,digital
	rights management agent,DRM agent,encryption,hashing,heterogeneous
	system-on-chip,high-performance security processing,mobile devices,mobile
	SoC,multiprocessor system-on-chip,Performance,secure sockets layer
	client,Security,security library functions,software architecture
	exploration,software partitioning,Software partitioning,SSL client,system-on-chip},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{avritzer_coordination_2008,
  author = {A. Avritzer and D. Paulish and Yuanfang Cai},
  title = {Coordination Implications of Software Architecture in a Global Software
	Development Project},
  booktitle = {Software Architecture, 2008. WICSA 2008. Seventh Working IEEE/IFIP
	Conference on},
  year = {2008},
  pages = {107--116},
  abstract = {In this paper, we report on our experience assessing the relationship
	between the dependency structure of a software architecture and the
	coordination needs among distributed development teams. We use as
	a case study for global software development the Global Studio Project
	Version 3.0, where matrix models were used to represent both architectural
	dependencies and the coordination structure among the team members.
	Analysis of data gathered during the Global Studio Project Version
	3.0 revealed that design structure matrix (DSM) models representing
	the modular structure of the software architecture are highly consistent
	with the social network matrix models that represent the actual coordination
	structure. We conclude that DSM modeling can help guide the task
	assignments in global software development projects.},
  doi = {10.1109/WICSA.2008.16},
  keywords = {architectural dependencies,coordination structure,dependency structure,design
	structure matrix,Design Structure Matrix,global software development
	project,Global Studio Project Version 3.0,modular structure,Social
	Network Analysis,social network matrix models,Software Architecture,task
	assignments},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{babar_evaluating_2007,
  author = {M.A. Babar},
  title = {Evaluating Product Line Architectures: Methods and Techniques},
  booktitle = {Software Engineering Conference, 2007. APSEC 2007. 14th Asia-Pacific},
  year = {2007},
  pages = {13},
  abstract = {Summary form only given. Good software architecture is one of the
	key factors in successfully developing and evolving a system or a
	family of systems. Software architecture provides the key framework
	for the earliest design decisions taken to achieve functional and
	quality requirements. In addition, it has a profound influence on
	project organizations' functioning and structure. Poor architecture
	usually results in project inefficiencies, poor communication, and
	poor decision making. Software architecture for a family of systems
	also helps identify the commonality among different systems and explicitly
	document variability. Since software architecture plays a significant
	role in the life of a system, it is important to evaluate a system's
	architecture as early as possible. Architecture evaluation is considered
	one of the most important and effective techniques of addressing
	quality related issues at the software architecture level and mitigating
	architectural risks. Moreover, architecture evaluation sessions are
	an effective means of sharing and capturing architecture design rationale,
	reasoning behind architecture design decisions. This tutorial highlights
	the benefits and challenges in evaluating software architectures.
	It discusses theoretical and practical concepts underpinning some
	of the well-known scenario-based architecture evaluation methods
	and various approaches to characterize quality attributes using scenarios.
	The use of the presented methods, techniques, and tools will be demonstrated
	with a case study based on an industrial project.},
  doi = {10.1109/ASPEC.2007.10},
  isbn = {1530-1362},
  keywords = {decision making,design decision making,functional requirements,quality
	requirements,software architecture evaluation},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{lenin_babu_archvoc--towardsontology_2007,
  author = {T. Lenin Babu and M. Seetha Ramaiah and T.V. Prabhakar and D. Rambabu},
  title = {ArchVoc--Towards an Ontology for Software Architecture},
  booktitle = {Sharing and Reusing Architectural Knowledge - Architecture, Rationale,
	and Design Intent, 2007. SHARK/ADI '07: ICSE Workshops 2007. Second
	Workshop on},
  year = {2007},
  pages = {5},
  abstract = {Knowledge management of any domain requires controlled vocabularies,
	taxonomies, thesauri, ontologies, concept maps and other such artifacts.
	This paper describes an effort to identify the major concepts in
	software architecture that can go into such meta knowledge. The concept
	terms are identified through two different techniques (1) manually,
	through back-of-the-book index of some of the major texts in Software
	Architecture (2) through a semi-automatic technique by parsing the
	Wikipedia pages. Only generic architecture knowledge is considered.
	Apart from identifying the important concepts of software architecture,
	we could also see gaps in the software architecture content in the
	Wikipedia.},
  doi = {10.1109/AERO.2007.352807},
  keywords = {architecture knowledge,ArchVoc,back-of-the-book index,grammars,knowledge
	management,meta knowledge,parsing,Wikipedia page},
  owner = {user},
  timestamp = {2008.10.04}
}

@ARTICLE{bach_beyond_2003,
  author = {Francis R. Bach and Michael I. Jordan},
  title = {Beyond independent components: trees and clusters},
  journal = {J. Mach. Learn. Res.},
  year = {2003},
  volume = {4},
  pages = {1205â€•1233},
  abstract = {We present a generalization of independent component analysis (ICA),
	where instead of looking for a linear transform that makes the data
	components independent, we look for a transform that makes the data
	components well fit by a tree-structured graphical model. This tree-dependent
	component analysis (TCA) provides a tractable and flexible approach
	to weakening the assumption of independence in ICA. In particular,
	TCA allows the underlying graph to have multiple connected components,
	and thus the method is able to find "clusters" of components such
	that components are dependent within a cluster and independent between
	clusters. Finally, we make use of a notion of graphical models for
	time series due to Brillinger (1996) to extend these ideas to the
	temporal setting. In particular, we are able to fit models that incorporate
	tree-structured dependencies among multiple time series.},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=945365.964303\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{bagheri_injecting_2007,
  author = {H. Bagheri and S.-H. Mirian-Hosseinabadi},
  title = {Injecting security as aspectable NFR into Software Architecture},
  booktitle = {Software Engineering Conference, 2007. APSEC 2007. 14th Asia-Pacific},
  year = {2007},
  pages = {310--317},
  abstract = {Complexity of the software development process is often increased
	by actuality of crosscutting concerns in software requirements; moreover,
	software security as a particular non-functional requirement of software
	systems is often addressed late in the software development process.
	Modeling and analyzing of these concerns and especially security
	in the software architecture facilitate detecting architectural vulnerabilities,
	decrease costs of the software maintenance, and reduce finding tangled
	and complex components in the ultimate design. Aspect oriented ADLs
	have emerged to overcome this problem; however, imposing radical
	changes to existing architectural modeling methods is not easily
	acceptable by architects. In this paper, we present a method to enhance
	conventional software architecture description languages through
	utilization of aspect features with special focuses on security.
	To achieve the goal, aspectable NFRs have been clarified; then, for
	their description in the software architecture, an extension to xADL
	2.0 [E.M. Dashofy, 2005] has been proposed; finally, we illustrate
	this material along with a case study.},
  doi = {10.1109/ASPEC.2007.34},
  isbn = {1530-1362},
  keywords = {aspectable nonfunctional requirement,software architecture description
	language,software development process complexity,software requirement},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{bagheri_evaluation_2008,
  author = {H. Bagheri and V. Montaghami and G. Safi and S.-H. Mirian-Hosseinabadi},
  title = {An evaluation method for aspectual modeling of distributed software
	architectures},
  booktitle = {Computer Systems and Applications, 2008. AICCSA 2008. IEEE/ACS International
	Conference on},
  year = {2008},
  pages = {903--908},
  abstract = {Dealing with crosscutting requirements in software development usually
	makes the process more complex. Modeling and analyzing of these requirements
	in the software architecture facilitate detecting architectural risks
	early. Distributed systems have more complexity and so these facilities
	are much useful in development of such systems. Aspect oriented Architectural
	Description Languages (ADD) have emerged to represent solutions for
	discussed problems; nevertheless, imposing radical changes to existing
	architectural modeling methods is not easily acceptable by architects.
	Software architecture analysis methods, furthermore, intend to verify
	that the quality requirements have been addressed properly. In this
	paper, we enhance ArchC\# through utilization of aspect features
	with an especial focus on Non-Functional Requirements (NFR). ArchC\#
	is mainly focused on describing architecture of distributed systems;
	in addition, it unifies software architecture with an object- oriented
	implementation to make executable architectures. Moreover, in this
	paper, a comparative analysis method is presented for evaluation
	of the result. All of these materials are illustrated along with
	a case study.},
  doi = {10.1109/AICCSA.2008.4493639},
  keywords = {ArchC\# language,aspect oriented architectural description languages,aspectual
	modeling evaluation,distributed software architectures,high level
	languages,nonfunctional requirements,object-oriented implementation,quality
	requirement verification,software architecture analysis},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{bahreini_new_2008,
  author = {K. Bahreini and A. Elci},
  title = {A New Software Architecture for J2EE Enterprise Environments via
	Semantic Access to Web Sources for Web Mining by Distributed Intelligent
	Software Agents},
  booktitle = {Computer Software and Applications, 2008. COMPSAC '08. 32nd Annual
	IEEE International},
  year = {2008},
  pages = {902--907},
  abstract = {Web mining treats the World Wide Web as the ultimate data source.
	This area of research is more interesting than data mining involving
	extracting data from in- house corporate databases or data warehouses.
	Gathering information from Web sources via distributed intelligent
	software agents in enterprise architecture can be implemented securely,
	platform independent, reusable, and would be better modeled in n-tier
	or enterprise-tier architecture. The purpose of this paper is to
	develop a novel distributed software agent architecture capable of
	automatically locating and extracting user specified data from the
	web, and to dynamically populate semantic databases with the data
	for later access and retrieval. Implementation of this project not
	only enables the software to access the semantic information which
	has been created by a typical Web source, but also it is capable
	of producing a meta database for executing the queries in semantic
	database. On the other hand, it is too difficult for the current
	users to interact with distributed intelligent agents over n-tier
	architecture on the Internet and gather information for Web mining
	issues which have been produced by those agents.},
  doi = {10.1109/COMPSAC.2008.102},
  isbn = {0730-3157},
  keywords = {corporate database,data warehouse,data warehouses,distributed intelligent
	software agent architecture,Distributed Software Agent,Enterprise
	Architecture,enterprise-tier architecture,J2EE,J2EE enterprise environment,semantic
	database,Semantic WEB,Web mining,Web Mining},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{banker_automating_1994,
  author = {R.D. Banker and R.J. Kauffman and C. Wright and D. Zweig},
  title = {Automating output size and reuse metrics in a repository-based computer-aided
	software engineering (CASE) environment},
  journal = {Software Engineering, IEEE Transactions on},
  year = {1994},
  volume = {20},
  pages = {169â€•187},
  number = {3},
  abstract = {Measurement of software development productivity is needed in order
	to control software costs, but it is discouragingly labor-intensive
	and expensive. Computer-aided software engineering (CASE) technologies-especially
	repository-based, integrated CASE-have the potential to support the
	automation of this measurement. We discuss the conceptual basis for
	the development of automated analyzers for function point and software
	reuse measurement for object-based CASE. Both analyzers take advantage
	of the existence of a representation of the application system that
	is stored within an object repository, and that contains the necessary
	information about the application system. We also discuss metrics
	for software reuse measurement, including reuse leverage, reuse value,
	and reuse classification that are motivated by managerial requirements
	and the efforts, within industry and the IEEE, to standardize measurement.
	The functionality and the analytical capabilities of state-of-the-art
	automated software metrics analyzers are illustrated in the context
	of an investment banking industry application that is similar to
	systems deployed at the New York City-based investment bank where
	these tools were developed and tested },
  doi = {10.1109/51.940044},
  issn = {0098-5589},
  keywords = {automated analyzers,bank data processing,CASE environment,conceptual
	basis,function point,integrated CASE,investment banking industry
	application,managerial requirements,object repository,object-based
	CASE,object-oriented programming,output size,repository-based computer-aided
	software engineering environment,reuse classification,reuse leverage,reuse
	metrics,software cost control,software development productivity,software
	metrics,software reusability,software reuse measurement,software
	tools,state-of-the-art automated software metrics analyzers},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{barais_framework_2005,
  author = {O. Barais and L. Duchien and A.-F. Le Meur},
  title = {A framework to specify incremental software architecture transformations},
  booktitle = {Software Engineering and Advanced Applications, 2005. 31st EUROMICRO
	Conference on},
  year = {2005},
  pages = {62--69},
  abstract = {A software architecture description facilitates the comprehension,
	analysis and prototyping of a piece of software. However, such a
	description is often monolithic and difficult to evolve. This paper
	proposes a framework, named TranSAT (transformations for software
	architecture), for incrementally integrating new concerns into a
	software architecture. The structural and behavioral properties of
	a new concern are represented by a self-sufficient component assembly
	description, called an architecture plan. TranSAT proposes a software
	architecture pattern as a means of integrating business and technical
	plans. Such a pattern includes not only the plan to integrate but
	also the preconditions that the target architecture must satisfy,
	and the modifications to perform on this architecture. Consequently,
	from a set of patterns, TranSAT allows a software architect to incrementally
	build complex architectures.},
  doi = {10.1109/EUROMICRO.2005.5},
  keywords = {incremental software architecture transformation,self-sufficient component
	assembly description,software analysis,software architecture pattern,TranSAT},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{barbier_component_2003,
  author = {F. Barbier and N. Belloir},
  title = {Component behavior prediction and monitoring through built-in test},
  booktitle = {Engineering of Computer-Based Systems, 2003. Proceedings. 10th IEEE
	International Conference and Workshop on the},
  year = {2003},
  pages = {17â€•22},
  abstract = {Real-time systems or safety-critical applications require high-confidence
	software components. Component behavior prediction refers to the
	ability to check, even certify, component specification conformance
	at development time. Complementarily, odd and varied execution contexts
	linked to the idea of deployment impose extra checking when components
	are deployed. This paper proposes incorporation of the test into
	components. Component states and complex dependencies between these
	states are methodically specified using UML statechart diagrams.
	Code is next derived in order to verify at development time component
	functioning in relation to specification. At deployment time, facilities
	are offered for (re-)configuring components to carefully fit specific
	runtime environments. Built-in test (BIT) material may thus be optionally
	generated in components to capture execution conditions and to make
	possible component behavior adjustments. All of these principles
	are powered by means of the BIT/J dedicated Java library that is
	presented and illustrated in the paper.},
  doi = {10.1109/ASWEC.2001.948505},
  keywords = {BIT/J dedicated Java library,built-in self test,built-in test,component
	behavior monitoring,component behavior prediction,component specification
	conformance,execution conditions,formal specification,high-confidence
	software components,Java,object-oriented programming,program testing,real-time
	systems,runtime environments,safety-critical applications,safety-critical
	software,software libraries,system monitoring,UML statechart diagrams},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{barbosa_type-level_2007,
  author = {LuÃ­s Barbosa and JÃ¡come Cunha and Joost Visser},
  title = {A type-level approach to component prototyping},
  booktitle = {International workshop on Synthesis and analysis of component connectors:
	in conjunction with the 6th ESEC/FSE joint meeting},
  year = {2007},
  pages = {23â€•36},
  address = {Dubrovnik, Croatia},
  publisher = {ACM},
  abstract = {Algebraic theories for modeling components and their interactions
	offer abstraction over the specifics of component states and interfaces.
	For example, such theories deal with forms of sequential composition
	of two components in a manner independent of the type of data stored
	in the states of the components, and independent of the number and
	types of methods offered by the interfaces of the combinators. General
	purpose programming languages do not offer this level of abstraction,
	which implies that a gap must be bridged when turning component models
	into implementations.},
  doi = {10.1145/1294917.1294920},
  isbn = {978-1-59593-720-X},
  keywords = {coalgebra,combinator library,haskell,mealy machine,type-level programming},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1294917.1294920\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{barbosa_refinement_2005,
  author = {Marco Antonio Barbosa},
  title = {A refinement calculus for software components and architectures},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {2005},
  volume = {30},
  pages = {377â€•380},
  number = {5},
  abstract = {The complexity and ubiquity achieved by software in the present world
	makes it imperative, more than ever, the availability of both technologies
	and sound methods to drive its development. Programming 'inâ€•theâ€•large',
	componentâ€•based programming and software architecture are popular
	expressions which embody this concern and correspond to driving forces
	in current software engineering. In such a context, this paper reports
	on the research, which constitutes the PhD project of the author,
	on a formal calculus for reasoning about and transforming componentâ€•based
	architectures.},
  doi = {10.1145/1095430.1081767},
  keywords = {refinement calculus,software architectures},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1095430.1081767\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{barella_agent_2007,
  author = {A. Barella and C. Carrascosa and V. Botti},
  title = {Agent Architectures for Intelligent Virtual Environments},
  booktitle = {Intelligent Agent Technology, 2007. IAT '07. IEEE/WIC/ACM International
	Conference on},
  year = {2007},
  pages = {532--535},
  abstract = {This paper presents a new approach to integrate Artificial Intelligence
	in Virtual Environments. The system presented deals in a separated
	way the visualization and intelligence modules, applying in this
	last case a distributed approach (multi-agent systems) so that scalable
	applications may be built. Therefore, it is necessary to define agent
	architectures that allow agents to be integrated in the VW. Thus,
	a designer is abstracted from the peculiarities of interacting with
	a Virtual Environment. There is a first prototype of the framework
	using JADE as the supporting multiagent systems platform.},
  doi = {10.1109/IAT.2007.10},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{barros_representing_2007,
  author = {F.J. Barros},
  title = {Representing Hierarchical Mobility in Software Architectures},
  booktitle = {Software Engineering for Adaptive and Self-Managing Systems, 2007.
	ICSE Workshops SEAMS '07. International Workshop on},
  year = {2007},
  pages = {5},
  abstract = {The ability to move components from one hierarchical model to another
	becomes necessary to support arbitrary changes in software topologies.
	Mobility allows the access to the hidden interface of a hierarchical
	component without breaking encapsulation, keeping the architecture
	modular. Mobility permits also the introduction of new functionally
	in the application without requiring changes in the architecture,
	enabling its adaptation to dynamic requirements. To illustrate hierarchical
	mobility we employ the connecton software architecture (CSA). CSA
	combines a modular and hierarchical software construction with the
	object- oriented architectural style to achieve an innovative software
	architecture that eliminates the limitations of the classical object-oriented
	technology in supporting modular components. The simulation of a
	queuing system whose clients are represented by mobile components
	is provided.},
  doi = {10.1109/SEAMS.2007.16},
  keywords = {connection software architecture,hierarchical component,hierarchical
	mobility,hierarchical software construction,innovative software architecture,object-oriented
	architectural style,object-oriented technology,queuing system,software
	topologies},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{barrow_study_2005,
  author = {R. Barrow and K. Frampton and M. Hamilton and B. Crossman},
  title = {A study of the in-practice application of a commercial software architecture},
  booktitle = {Software Engineering Conference, 2005. Proceedings. 2005 Australian},
  year = {2005},
  pages = {292--301},
  abstract = {In recent years, the size and complexity of information systems have
	grown, with a resulting increased emphasis on using a software development
	methodology in the belief that by doing so, the experience of other,
	more experienced, developers can be re-used. Recent studies however,
	have found that developers do not adhere rigidly to a methodology,
	but usually adapt it to their specific needs. Also, the extent to
	which individual software architects apply a specific software architecture
	development method in practice is largely unexplored. This paper
	reports on the in practice use of a proprietary software architecture
	development method, the IBM Custom Application Design. It describes
	the current state of understanding of application of software architecture
	methods, explains the research approach applied, reports on the results
	of the survey and workshops, and describes possible future work.
	We found that software architects, like software and requirements
	engineers, do not adhere rigidly to a method. Furthermore, we have
	identified that there are multiple levels at which the modification
	to a method can be undertaken. These are "Tailoring", "Adapting"
	and "Customizing". We also found that a unique version of a method
	is created for each project and IBM IT architects rely extensively
	on tools and techniques for visual communication as one of the key
	ways in which they work. These are important findings for commercial
	organizations which develop and/or use methods, and for academic
	institutions which teach software architecture.},
  isbn = {1530-0803 },
  keywords = {commercial software architecture,IBM Custom Application Design,requirement
	engineering,software development methodology,software techniques,visual
	communication},
  owner = {user},
  timestamp = {2008.10.04}
}

@INBOOK{bartenstein_software_2003,
  pages = {23â€•35},
  title = {Software Components for Internet Based Self-service Consulting Systems},
  year = {2003},
  author = {Oskar Bartenstein},
  abstract = {This industrial report introduces the web language â€œ*.mssâ€? for
	the operation of commercial web sites, developed at IF Computer for
	content management to help authors and for decision support to help
	users. A web server isolates users from authors in time and space
	and makes their interaction explicit. *.mss extends this and also
	separates the tasks of authors into cooperating but independently
	manageable workload units, e.g. along the responsibilities within
	an organization. Automatic integration into complete web sites and
	combination with existing solvers and logic programming is briefly
	discussed.. An example illustrates the application for a car dealership,
	shown is cooperation of the people in charge of inventory updates,
	merchandising, IT services and visual impact. A reference section
	gives all defined constructs of *.mss and a comparison with other
	methods for programmed web services. },
  journal = {Web Knowledge Management and Decision Support},
  owner = {user},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1007/3-540-36524-9\_3}
}

@INPROCEEDINGS{bartholet_computational_2005,
  author = {Robert G. Bartholet and David C. Brogan and Jr Paul F. Reynolds},
  title = {The computational complexity of component selection in simulation
	reuse},
  booktitle = {Proceedings of the 37th conference on Winter simulation},
  year = {2005},
  pages = {2472â€•2481},
  address = {Orlando, Florida},
  publisher = {Winter Simulation Conference},
  abstract = {Simulation composability has been much more difficult to realize than
	some initially imagined. We believe that success lies in explicit
	considerations for the adaptability of components. In this paper
	we show that the complexity of optimal component selection for adaptable
	components is NP-complete. However, our approach allows for the efficient
	adaptation of components to construct a complex simulation in the
	most flexible manner while allowing the greatest opportunity to meet
	all requirements, all the while reducing time and costs. We demonstrate
	that complexity can vary from polynomial, to NP, and even to exponential
	as a function of seemingly simple decisions made about the nature
	of dependencies among components. We generalize these results to
	show that regardless of the types or reasons for dependencies in
	component selection, just their mere existence makes this problem
	very difficult to solve optimally.},
  isbn = {0-7803-9519-0},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1162708.1163174\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{Bartocci2007,
  author = {Ezio Bartocci and Diletta Cacciagrano and Nicola Cannata and Flavio
	Corradini and Emanuela Merelli and Luciano Milanesi and Paolo Romano},
  title = {An agent-based multilayer architecture for bioinformatics grids.},
  journal = {IEEE Trans Nanobioscience},
  year = {2007},
  volume = {6},
  pages = {142--148},
  number = {2},
  month = {Jun},
  abstract = {Due to the huge volume and complexity of biological data available
	today, a fundamental component of biomedical research is now in silico
	analysis. This includes modelling and simulation of biological systems
	and processes, as well as automated bioinformatics analysis of high-throughput
	data. The quest for bioinformatics resources (including databases,
	tools, and knowledge) becomes therefore of extreme importance. Bioinformatics
	itself is in rapid evolution and dedicated Grid cyberinfrastructures
	already offer easier access and sharing of resources. Furthermore,
	the concept of the Grid is progressively interleaving with those
	of Web Services, semantics, and software agents. Agent-based systems
	can play a key role in learning, planning, interaction, and coordination.
	Agents constitute also a natural paradigm to engineer simulations
	of complex systems like the molecular ones. We present here an agent-based,
	multilayer architecture for bioinformatics Grids. It is intended
	to support both the execution of complex in silico experiments and
	the simulation of biological systems. In the architecture a pivotal
	role is assigned to an "alive" semantic index of resources, which
	is also expected to facilitate users' awareness of the bioinformatics
	domain.},
  institution = {Department of Mathematics and Computer Science, University of Camerino,
	1-62032 Camerino, Italy.},
  keywords = {Computational Biology, methods; Database Management Systems; Databases,
	Factual; Information Storage and Retrieval, methods; Internet; Molecular
	Biology, methods; User-Computer Interface},
  owner = {user},
  pmid = {17695749},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{bass_evaluatingsoftware_2008,
  author = {L. Bass and P. Clements and R. Kazman and M. Klein},
  title = {Evaluating the Software Architecture Competence of Organizations},
  booktitle = {Software Architecture, 2008. WICSA 2008. Seventh Working IEEE/IFIP
	Conference on},
  year = {2008},
  pages = {249--252},
  abstract = {An organization is architecturally competent if it has the ability
	to acquire, use and sustain the skills and knowledge necessary to
	carry out architecture-related practices that lead to systems that
	serve the organization's business goals. This paper presents some
	principles of architecture competence, based on four models that
	aid in explaining, measuring, and improving the architecture competence
	of an individual or an organization with respect to these principles,
	The principles are based on a set of fundamental beliefs about software
	architecture.},
  doi = {10.1109/WICSA.2008.12},
  keywords = {organizational coordination theory,organizational software architecture
	competence,software architecture competence},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{bastarrica_integrated_2004,
  author = {M.C. Bastarrica and S.F. Ochoa and P.O. Rossel},
  title = {Integrated notation for software architecture specifications},
  booktitle = {Computer Science Society, 2004. SCCC 2004. 24th International Conference
	of the Chilean},
  year = {2004},
  pages = {26--34},
  abstract = {Currently, there are many notations to specify software architectures,
	which address a wide range of formality and completeness. Completely
	formal notations produce accurate and analyzable software architecture
	specifications, but the most formal and complete notations are also
	the most difficult to use and understand. Conversely, informal notations
	are easier to use and understand, but several design aspects may
	remain underspecified. This paper presents an integrated notation
	for specifying software architecture that reduces the complexity
	to use completely formal notations without resigning the formality
	required by software architecture specifications. The integrated
	notation proposes an architectural specification in three levels
	of abstraction: a graphical box-and-line diagram to specify the structure,
	a behavioral specification using input/output automata, and a basis
	of Larch traits describing the domain specific abstract data types.
	The proposed integrated notation has been used to specify the architecture
	of a complex mesh management tool and part of the specification is
	presented. Although more experimentation is required, the obtained
	results are encouraging.},
  doi = {10.1109/QEST.2004.12},
  keywords = {abstract data types,architectural specification,architecture definition
	languages,Architecture definition languages,automata theory,behavioral
	specification,complete notations,complex mesh management tool,domain
	specific abstract data types,formal notations,graphical box-and-line
	diagram,input-output automata,integrated notation,Larch traits,software
	architecture specifications},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{bastide_adapting_2006,
  author = {Gautier Bastide and Abdelhak Seriai and Mourad Oussalah},
  title = {Adapting software components by structure fragmentation},
  booktitle = {Proceedings of the 2006 ACM symposium on Applied computing},
  year = {2006},
  pages = {1751â€•1758},
  address = {Dijon, France},
  publisher = {ACM},
  abstract = {We present in this paper an approach aiming at adapting software components.
	It focuses on adapting component structures instead of adapting component
	services. Among the motivations of this kind of adaptation, we note
	its possible application to permit flexible deployment of software
	components and flexible loading of component code according to the
	available resources (CPU, memory). Our adaptation process is based
	on the analysis and the instrumentation of component codes. It respects
	the black-box property when it is implemented as a service provided
	by the component to be adapted. To support this structural adaptation
	technique, we developed an adaptation process which we have experimented
	using the Java framework of the Fractal component model.},
  doi = {10.1145/1141277.1141691},
  isbn = {1-59593-108-2},
  keywords = {object-oriented,refactoring,software component,structural adaptation},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1141277.1141691\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{baude_component_2007,
  author = {FranÃ§oise Baude and Ludovic Henrio and Paul Naoumenko},
  title = {A component platform for experimenting with autonomic composition},
  booktitle = {Proceedings of the 1st international conference on Autonomic computing
	and communication systems},
  year = {2007},
  pages = {1â€•9},
  address = {Rome, Italy},
  publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications
	Engineering)},
  abstract = {In this paper, we propose a component-oriented framework that can
	support autonomic computing and in particular bio-inspired approaches.
	Starting from the Grid Component Model, a component model targeting
	at Grid computing and already featuring some autonomicity, we show
	how such a model can be used in a general autonomic computing context.
	Indeed the model provides hierarchical structure and reconfiguration
	for both functional and non-functional levels. This should ease the
	development of self-* and in particular, self-evolving applications.
	With our approach, even the autonomic strategies themselves can evolve.
	We consider this model and its implementation as powerful tools for
	easily experimenting autonomic behaviours.},
  isbn = {978-963-9799-09-7},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1365562.1365573\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{becker_model-based_2007,
  author = {Steffen Becker and Heiko Koziolek and Ralf Reussner},
  title = {Model-Based performance prediction with the palladio component model},
  booktitle = {Proceedings of the 6th international workshop on Software and performance},
  year = {2007},
  pages = {54â€•65},
  address = {Buenes Aires, Argentina},
  publisher = {ACM},
  abstract = {One aim of component-based software engineering (CBSE) is to enable
	the prediction of extra-functional properties, such as performance
	and reliability, utilising a well-defined composition theory. Nowadays,
	such theories and their accompanying prediction methods are still
	in a maturation stage. Several factors influencing extra-functional
	properties need additional research to be understood. A special problem
	in CBSE stems from its specific development process: Software components
	should be specified and implemented independent from their later
	context to enable reuse. Thus, extra-functional properties of components
	need to be specified in a parametric way to take different influence
	factors like the hardware platform or the usage profile into account.
	In our approach, we use the Palladio Component Model (PCM) to specify
	component-based software architectures in a parametric way. This
	model offers direct support of the CBSE development process by dividing
	the model creation among the developer roles. In this paper, we present
	our model and a simulation tool based on it, which is capable of
	making performance predictions. Within a case study, we show that
	the resulting prediction accuracy can be sufficient to support the
	evaluation of architectural design decisions.},
  doi = {10.1145/1216993.1217006},
  isbn = {1-59593-297-6},
  keywords = {component-based software engineering,performance prediction,software
	architecture},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1216993.1217006\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{bell_automated_2008,
  author = {D.G. Bell and G.P. Brat},
  title = {Automated Software Verification \& Validation: An Emerging Approach
	for Ground Operations},
  booktitle = {Aerospace Conference, 2008 IEEE},
  year = {2008},
  pages = {1â€•8},
  abstract = {Software is an increasingly critical component in aerospace systems,
	and automated software testing technologies that use formal methods
	are emerging as a new type of ground data system for verifying and
	validating software changes. With each change made to software for
	use in operational settings, there is the opportunity for new bugs
	to be introduced that can cause potentially catastrophic results.
	This research has evolved two formal method approaches for automated
	software verification and validation model checking and static analysis,
	with extensions of the approaches for multiple languages including
	Java, C and C++. The research is removing the barriers to routine
	use of formal methods to gain high assurance for human-rated space
	missions. These methods have been used on control software for a
	variety of mission critical systems including guidance, navigation
	and control (GN\&C) subsystems, and for various projects such as
	the NASA Crew Exploration Vehicle (CEV).},
  doi = {10.1109/ASE.1998.732559},
  isbn = {1095-323X},
  keywords = {aerospace computing,automated software verification,formal methods,ground
	data system,ground operation,ground support systems,human-rated space
	missions,program verification,software validation},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{bernholdt_component_2007,
  author = {David E. Bernholdt},
  title = {Component architectures in the next generation of ultrascale scientific
	computing: challenges and opportunities},
  booktitle = {Proceedings of the 2007 symposium on Component and framework technology
	in high-performance and scientific computing},
  year = {2007},
  pages = {1â€•10},
  address = {Montreal, Quebec, Canada},
  publisher = {ACM},
  abstract = {Component architectures for high-end scientific computing are still
	a relatively new idea, and their most effective use and benefits
	are active areas of research for both developers and users of such
	approaches. At the same time, however, the scale of both scientific
	simulations and the computer hardware on which they're run has been
	growing rapidly, generating an increasing desire for more complex
	software systems (such as coupled simulations), new programming models
	and languages, and more complex hardware environments. This paper
	presents the author's analysis of how these trends might interact
	with component-based software engineering (CBSE) for scientific computing,
	in some cases exacerbating existing challenges or posing new ones,
	or in other cases offering opportunities in which the component environment
	might be leveraged to give software developers new capabilities or
	simplify challenges they face.},
  doi = {10.1145/1297385.1297387},
  isbn = {978-1-59593-867-1},
  keywords = {component-based software engineering,high-performance computing,parallel
	computing,scientific computing,simulation},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1297385.1297387\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{bertolino_framework_2003,
  author = {Antonia Bertolino and Andrea Polini},
  title = {A framework for component deployment testing},
  booktitle = {Proceedings of the 25th International Conference on Software Engineering},
  year = {2003},
  pages = {221â€•231},
  address = {Portland, Oregon},
  publisher = {IEEE Computer Society},
  abstract = {Component-based development is the emerging paradigm in software production,
	though several challenges still slow down its full taking up. In
	particular, the "component trust problem" refers to how adequate
	guarantees and documentation about a component' s behaviour can be
	transferred from the component developer to its potential users.
	The capability to test a component when deployed within the target
	application environment can help establish the compliance of a candidate
	component to the customer's expectations and certainly contributes
	to "increase trust". To this purpose, we propose the CDT framework
	for Component Deployment Testing. CDT provides the customer with
	both a technique to early specify a deployment test suite and an
	environment for running and reusing the specified tests on any component
	implementation. The framework can also be used to deliver the component
	developer's test suite and to later re-execute it. The central feature
	of CDT is the complete decoupling between the specification of the
	tests and the component implementation.},
  isbn = {0-7695-1877-X},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=776816.776843\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{bessam_enhancing_2008,
  author = {A. Bessam and M.T. Kimour},
  title = {Enhancing Software Architecture Behavior Understanding and Extensibility
	by Multi-view Metamodeling},
  booktitle = {Information and Communication Technologies: From Theory to Applications,
	2008. ICTTA 2008. 3rd International Conference on},
  year = {2008},
  pages = {1--6},
  abstract = {Component based development is recognized now as a powerful tool to
	manage actual systems' technological complexity. The success key
	factor of this discipline is the high level abstracting of systems'
	structural and behavioral constituents. On the other hand, enhancing
	software architectures simplicity and clarity by separating several
	concerns is a useful technique to manage complexity. In order to
	have a complete system specification, a rigorous behavior description
	is needed. Behavioral concepts and their use in architectural specification
	are in a fast evolution and have become so numerous, so it becomes
	difficult to elicit and manage them. For these purposes, we present
	in this paper, a generalized metamodel of behavioral aspects, that
	indexes the various architectural behavior concepts in classes, in
	a generic way. To enable more sophisticated and consistent analysis
	of architecture behavior we have separated behavioral concepts into
	packages basing on four functional perspectives: interface, static
	behavior, dynamic behavior, and interaction protocols. We show that
	our proposed metamodel allows having a general, a unified and an
	adaptable view of behavioral concepts required in software architecture
	description from all functional viewpoints.},
  doi = {10.1109/ICTTA.2008.4530354},
  keywords = {architectural specification,architecture description languages,component-based
	software architecture,extensibility,high level abstracting,Multi-view
	behavior metamodeling,multiview metamodeling,Software architecture
	functional views,system specification},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{beydeda_merging_2003,
  author = {S. Beydeda and V. Gruhn},
  title = {Merging components and testing tools: the self-testing COTS components
	(STECC) strategy},
  booktitle = {Euromicro Conference, 2003. Proceedings. 29th},
  year = {2003},
  pages = {107â€•114},
  abstract = {Development of a software system from existing components can surely
	have various benefits, but can also entail a series of problems.
	One type of problem is caused by a limited exchange of information
	between the developer and user of a component. A limited exchange
	and thereby a lack of information can have various consequences,
	among them the requirement to test a component prior to its integration
	into a software system. A lack of information cannot only make test
	prior to integration necessary, it can also complicate this tasks.
	We propose a new strategy to testing components and making components
	testable. The basic idea of the strategy is to merge components and
	testing tools in order to make components capable of testing their
	own methods. Such components allow their thorough testing without
	disclosing detailed information, such as source code. This strategy
	thereby fulfills the needs of both the developer and user of a component.},
  doi = {10.1109/APSEC.2003.1254401},
  isbn = {1089-6503 },
  keywords = {integrated software,object-oriented programming,program testing,self-testing
	COTS component strategy,software component testing,software integration,software
	system development,software tools},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{bhansali_automated_1998,
  author = {S. Bhansali and T.J. Hoar},
  title = {Automated software synthesis: an application in mechanical CAD},
  journal = {Software Engineering, IEEE Transactions on},
  year = {1998},
  volume = {24},
  pages = {848â€•862},
  number = {10},
  abstract = {Automated program synthesis has not gained widespread acceptance among
	software practitioners despite considerable efforts by several researchers.
	We outline some of the difficulties in applying program synthesis
	for practical problems and argue that a careful analysis of the cost
	vs. benefit tradeoff is essential when considering such an approach.
	We describe a successful application of automated program generation
	for synthesizing geometric constraint satisfaction routines in the
	domain of mechanical CAD. We present a general framework for modeling
	and solving the problem, illustrate the framework using examples
	from the geometric constraint satisfaction domain, and describe experimental
	results on productivity increase using this approach. We also discuss
	characteristics of the problem domain and our approach that were
	critical for success},
  doi = {10.1109/32.729684},
  issn = {0098-5589},
  keywords = {automated program generation,automated program synthesis,automated
	software synthesis,automatic programming,CAD,constraint handling,geometric
	constraint satisfaction domain,geometric constraint satisfaction
	routines,mechanical CAD,mechanical engineering computing,productivity
	increase,software practitioners},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{bigot_enabling_2007,
  author = {Julien Bigot and Christian Perez},
  title = {Enabling collective communications between components},
  booktitle = {Proceedings of the 2007 symposium on Component and framework technology
	in high-performance and scientific computing},
  year = {2007},
  pages = {121â€•130},
  address = {Montreal, Quebec, Canada},
  publisher = {ACM},
  abstract = {Existing high performance component models mainly focus on the efficiency
	of the composition of two components, tackling also the special case
	of parallel components that enable N x M communications. The implementation
	of parallel components is usually assumed to be done thanks to some
	external communication paradigms like MPI. However, as of today,
	collective communication operations like broadcast, reduction, gather,
	etc. are not supported by component models. Programmers should develop
	such operations on top of point-to-point communication operations
	provided by component models. This paper studies how collective operations
	between components can be provided from an user and developer point
	of view. The result is an abstract component model that allows the
	implementation of collective communications. Software components
	are then able to use collective communications between several instances.
	To be effective on hierarchical resources such as grids, the model
	is hierarchical and relies on the concept of replicating component
	implementation. Last, the paper deals with the projection of such
	an abstract model onto existing models. It is validated through some
	very preliminary experiments.},
  doi = {10.1145/1297385.1297406},
  isbn = {978-1-59593-867-1},
  keywords = {collective communications,component model},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1297385.1297406\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{Billard2004,
  author = {Edward A Billard},
  title = {Patterns of agent interaction scenarios as use case maps.},
  journal = {IEEE Trans Syst Man Cybern B Cybern},
  year = {2004},
  volume = {34},
  pages = {1933--1939},
  number = {4},
  month = {Aug},
  abstract = {A use case map (UCM) presents, in general, an abstract description
	of a complex system and, as such, is a good candidate for representing
	scenarios of autonomous agents interacting with other autonomous
	agents. The "gang of four" design patterns are intended for object-oriented
	software development but at least eight of the patterns illustrate
	structure, or architecture, that is appropriate for interacting agents,
	independent of software development. This study presents these particular
	patterns in the form of UCMs to describe abstract scenarios of agent
	interaction. Seven of the patterns attempt to balance the decentralized
	nature of interacting agents with an organized structure that makes
	for better, cleaner interactions. An example performance analysis
	is provided for one of the patterns, illustrating the benefit of
	an early abstraction of complex agent behavior. The original contribution
	here is a UCM presentation of the causal paths in agent behavior
	as suggested by software design patterns.},
  institution = {Department of Math and Computer Science, California State University,
	Hayward, CA 94542, USA. billard@csuhayward.edu},
  keywords = {Algorithms; Artificial Intelligence; Equipment Design, methods; Models,
	Theoretical; Numerical Analysis, Computer-Assisted; Pattern Recognition,
	Automated},
  owner = {user},
  pmid = {15462458},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{bobeff_component_2004,
  author = {Gustavo Bobeff and Jacques NoyÃ©},
  title = {Component specialization},
  booktitle = {Proceedings of the 2004 ACM SIGPLAN symposium on Partial evaluation
	and semantics-based program manipulation},
  year = {2004},
  pages = {39â€•50},
  address = {Verona, Italy},
  publisher = {ACM},
  abstract = {Component-Based Software Development (CBSD)is an attractive way to
	deliver generic executable pieces of program, ready to be reused
	in many different contexts. Component reuse is based on a black-box
	model that frees component consumers from diving into implementation
	details. Adapting a generic component to a particular context of
	use is then based on a parameterized interface that becomes a specific
	component wrapper at runtime. This shallow adaptation, which keeps
	the component implementation unchanged, is a major source of inefficiency.
	By building on top of well-known specialization techniques, it is
	possible to take advantage of the genericity of components and adapt
	their implementation to their usage context without breaking the
	black-box model. We illustrate these ideas on a simple component
	model, considering dual specialization techniques, partial evaluation
	and slicing. A key to not breaking encapsulation is to use specialization
	scenarios extended with assumptions on the required services and
	to package components as component generators.},
  doi = {10.1145/1014007.1014012},
  isbn = {1-58113-835-0},
  keywords = {component generator,component-based software development,partial evaluation,program
	slicing},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1014007.1014012\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{g._booch_quantitative_2005,
  author = {G. Booch},
  title = {Quantitative Observation and Theoretical Construction in Software
	Architecture},
  booktitle = {Software Architecture, 2005. WICSA 2005. 5th Working IEEE/IFIP Conference
	on},
  year = {2005},
  pages = {3},
  abstract = {Classical science advances via the dance between quantitative observation
	and theoretical construction. It has been ten years since the first
	International Software Architecture Workshop, and since that time
	there has been a steady increase in the number of people who call
	themselves "software architect" and a similar growth in the value
	that organizations place in software architecture. It is a sign of
	maturity for any given engineering discipline when we can name, study,
	and apply the patterns relevant to that domain but, unfortunately,
	no such reference yet exists for software-intensive systems. We've
	architected and deployed many systems, but have studied their patterns
	of success and failure only a little; we've explored theoretical
	frameworks for describing software architectures and processes to
	build them, but we've done only a modest job in validating them in
	the real world. In this presentation, I'll summarize the things we
	know and the things we donÂ’t know (and speculate on the things we
	don't know we know) about software architecture. I'll then examine
	an effort to create a handbook of software architecture to help fill
	this gap between observation and construction.},
  doi = {10.1109/WICSA.2005.57},
  owner = {user},
  timestamp = {2008.10.04}
}

@ARTICLE{bowie_automated_1988,
  author = {L.A. Bowie and M.A. Hickox and A.B. Sripad},
  title = {An automated field repair tracking system [telecommunication products]},
  journal = {Selected Areas in Communications, IEEE Journal on},
  year = {1988},
  volume = {6},
  pages = {1350â€•1358},
  number = {8},
  abstract = {The purpose of repair tracking (RT) is to provide ongoing feedback
	of product reliability performance in the customer environment. The
	authors present an RT technique and provide an overview of its automated
	software implementation, which has been used to track several transmission
	system products. The RT implementation, referred to as the repair
	equipment analysis program (REAP), tracks the field performance of
	electronic equipment at system, subsystem, circuit pack, and component
	levels. The features of REAP include monthly reliability performance
	reports, alarms when degradation in performance is detected, model
	sensitivity analysis, problem areas isolation for initiating corrective
	actions, and specific customer performance studies. The approach
	encompasses the entire population during the product life using existing
	sales and repair data as main inputs. Further, it does not require
	customer involvement or product serialization},
  doi = {10.1109/CMPCON.1989.301930},
  issn = {0733-8716},
  keywords = {alarms,automated field repair tracking system,automated software implementation,corrective
	actions,customer environment,customer performance studies,electronic
	equipment,feedback,model sensitivity analysis,performance degradation
	detection,product life,product reliability performance,reliability,repair
	equipment analysis program,sensitivity analysis,telecommunication
	equipment,telecommunication products,telecommunications computing},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{braga_use_2001,
  author = {Regina M. M. Braga and Marta Mattoso and ClÃ¡udia M. L. Werner},
  title = {The use of mediation and ontology technologies for software component
	information retrieval},
  booktitle = {Proceedings of the 2001 symposium on Software reusability: putting
	software reuse in context},
  year = {2001},
  pages = {19â€•28},
  address = {Toronto, Ontario, Canada},
  publisher = {ACM},
  abstract = {Component Based Developed aims at constructing software through the
	inter-relationship between pre-existing components. However, these
	components should be bound to a specific application domain in order
	to be effectively reused. Reusable domain components and Their related
	documentation are usually stored in a great variety of data sources.
	Thus, a possible solution for accessing this information is to use
	a software layer that integrates different component information
	sources. We present a component information integration data layer,
	based on mediators. Through mediators, domain ontology acts as a
	technique/formalism for specifying ontological commitments or agreements
	between component users and providers, enabling more accurate software
	component information search.},
  doi = {10.1145/375212.375229},
  isbn = {1-58113-358-8},
  keywords = {component based engineering,component repositories,domain engineering,software
	classification and identification},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=375212.375229\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{maricela_c._bravo_ontology_2006,
  author = {Maricela C. Bravo and Jose C. Velazquez and Azucena Montes},
  title = {An Ontology Solution for Language Interoperability between Agents},
  booktitle = {Electronics, Robotics and Automotive Mechanics Conference, 2006},
  year = {2006},
  volume = {1},
  pages = {143--148},
  abstract = {Traditional negotiation systems have been implemented using agent
	architectures, where agents communicate exchanging negotiation primitives
	generated by each system, based on particular language definitions
	implicitly encoded, giving different syntax and semantics to their
	messages. In this paper we address the problem of communicating heterogeneous
	negotiation agents in a Web-based environment, considering differences
	in their language implementations. Our research is based in the development
	of an ontology solution for describing and sharing negotiation primitives;
	and a translator module which is executed only when a misunderstanding
	occurs. We executed experiments in an electronic marketplace architecture,
	where heterogeneous agents participate in negotiation processes.
	The results of experiments show that the proposed solution improves
	the communication between negotiation agents},
  doi = {10.1109/WI-IATW.2006.83},
  keywords = {agent architecture,agent language interoperability,agent negotiation
	system,electronic marketplace architecture,heterogeneous agent communication,knowledge
	representation languages,ontology solution,translator module,Web-based
	environment},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{brereton_software_2002,
  author = {P. Brereton and S. Linkman and N. Thomas and J. Boegh and S. De Panfilis},
  title = {Software components - enabling a mass market},
  booktitle = {Software Technology and Engineering Practice, 2002. STEP 2002. Proceedings.
	10th International Workshop on},
  year = {2002},
  pages = {169â€•176},
  abstract = {Component based software engineering, the building of software systems
	from reusable parts, offers the potential to radically improve the
	way in which software is developed. It promises increased reuse leading
	to higher quality and reduced time to market. In addition, if component
	parts are available 'off the shelf' then we can expect to see the
	emergence of a thriving market in new ever-better components. This
	should enable those who integrate components into whole systems to
	adapt and improve such systems rapidly and predictably by replacing
	old components with new and better ones. This paper reports on work
	undertaken within CLARiFi, a European-funded project, which aimed
	to provide a supportive infrastructure to enable such a component
	marketplace. The infrastructure is manifested through the design,
	development and evaluation of a series of pre-industrial component
	broker prototypes. The underlying research areas include the component
	classification scheme, ranking and selection of components, component
	certification and visualization in the ranking and selection process.
	The paper concludes with a discussion of the lessons learned from
	the prototyping and evaluation activities carried out throughout
	the project.},
  keywords = {CLARiFi,Clear and Reliable Information for Integration,component based
	software engineering,component behavior,component certification,component
	classification,component integration,component marketplace,component
	ranking,component selection,European-funded project,function reuse,interface
	reuse,interoperability,mass market,object-oriented programming,off
	the shelf component part,preindustrial component broker prototype,process
	visualization,program visualisation,reusable part,software architecture,software
	components,software development,software development management,software
	process improvement,software prototyping,software quality,software
	reusability,software system building,supportive infrastructure,time
	to market reduction,user friendliness},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{briand_automated_2006,
  author = {Lionel C. Briand and Yvan Labiche and Michal M. SÃ³wka},
  title = {Automated, contract-based user testing of commercial-off-the-shelf
	components},
  booktitle = {Proceedings of the 28th international conference on Software engineering},
  year = {2006},
  pages = {92â€•101},
  address = {Shanghai, China},
  publisher = {ACM},
  abstract = {Commercial-off-the-Shelf (COTS) components provide a means to construct
	software (component-based) systems in reduced time and cost. In a
	COTS component software market there exist component vendors (original
	developers of the component) and component users (developers of the
	component-based systems). The former provide the component to the
	user without source code or design documentation, and as a result
	it is difficult for the latter to adequately test the component when
	deployed in their system. In this article we propose a framework
	that clarifies the roles and responsibilities of both parties so
	that the user can adequately test the component in a deployment environment
	and the vendor does not need to release proprietary details. Then,
	based on this framework we combine and adapt two specification-based
	testing techniques and describe (and implement) a method for the
	automated generation of adequate test sets. An evaluation of our
	approach on a case study demonstrates that it is possible to automatically
	generate cost effective test sequences and that these test sequences
	are effective at detecting complex errors.},
  doi = {10.1145/1134285.1134300},
  isbn = {1-59593-375-1},
  keywords = {adequacy criteria,component,cots,uml},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1134285.1134300\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{Brogle2006,
  author = {Kevin C Brogle and Cindy Lin and Paul R Blake},
  title = {Interactive tools for risk reduction and efficiency improvements
	in medicinal chemistry.},
  journal = {Comb Chem High Throughput Screen},
  year = {2006},
  volume = {9},
  pages = {131--145},
  number = {2},
  month = {Feb},
  abstract = {There are many decisions and risks associated with the design and
	development of new pharmaceutical agents. To help improve decision-making,
	and reduce the associated risks--prior to synthesis, we have developed
	interactive web-browser tools for: (i) tracking, searching, clustering
	and categorizing (by reactive moieties) chemical reactants, (ii)
	interactively assessing risks, either synthetic--based on prior experience,
	absorption following oral administration--based on rules of 5, or
	diversity, and (iii) a complete architecture for enumerating, analyzing,
	submitting and plating large combinatorial or small biased libraries.
	We believe the implementation of this highly interactive system has
	given our scientists a competitive advantage by maintaining their
	focus on the lowest risk, highest quality molecules throughout the
	research process.},
  institution = {Purdue Pharma, L.P., Department of Computational, Combinatorial and
	Medicinal Chemistry, 6 Cedar Brook Drive, Cranbury, NJ 08512, USA.},
  keywords = {Administration, Oral; Biological Availability; Chemistry, Pharmaceutical,
	methods; Combinatorial Chemistry Techniques; Computing Methodologies;
	Quantitative Structure-Activity Relationship; Risk Assessment; Software},
  owner = {user},
  pmid = {16475971},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{bucchiarone_towardsarchitectural_2006,
  author = {Antonio Bucchiarone and Andrea Polini and Patrizio Pelliccione and
	Massimo Tivoli},
  title = {Towards an architectural approach for the dynamic and automatic composition
	of software components},
  booktitle = {Proceedings of the ISSTA 2006 workshop on Role of software architecture
	for testing and analysis},
  year = {2006},
  pages = {12â€•21},
  address = {Portland, Maine},
  publisher = {ACM},
  abstract = {In a component-based software system the components are specified,
	designed and implemented with the intention to be reused, and are
	assembled in various contexts in order to produce a multitude of
	software systems. However, this ideal scenario is not always the
	case, e.g., the integration with legacy components. In this context,
	one main problem in component assembly arises. It is related to the
	ability to automatically and efficiently (i.e., by reducing the state-explosion
	phenomenon) synthesize an assembly code for a set of, possibly incompatible,
	software components. Moreover, this assembly should be able to evolve
	when things change and to be correct-by-construction, i.e., despite
	the changes, it always ensures a set of properties of interest. In
	this paper we propose a Software Architecture (SA) based approach
	in which architectural analysis and code synthesis are combined together
	in order to efficiently and correctly assemble a system out of a
	set of already implemented components. The approach can be equally
	applied to efficiently manage the whole re-factoring of the system
	when one or more components needs to be substituted, still maintaining
	the required properties. The specified and validated system SA is
	used as starting point for the derivation of adaptors required to
	correctly replace components in the composed system. The approach
	is applied and validated over an explanatory example concerning with
	a "cooling water pipe" system.},
  doi = {10.1145/1147249.1147251},
  isbn = {1-59593-459-6},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1147249.1147251\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{bui_diagnosis_2007,
  author = {Thi Quynh Bui and Oum-El-Kheir Aktouf},
  title = {Diagnosis service for embedded software component based systems},
  booktitle = {Proceedings of the 2007 workshop on Engineering fault tolerant systems},
  year = {2007},
  pages = {3},
  address = {Dubrovnik, Croatia},
  publisher = {ACM},
  abstract = {This paper studies the fault diagnosis of component-based applications,
	especially embedded ones. The principle of the proposed diagnosis
	technique is to implement inter-component tests in order to detect
	and locate faulty components without component replication. A diagnosis
	service for embedded software component based systems is developed.
	Its advantages are application autonomy, cost-effectiveness and better
	usage of system resources. Such advantages are very important for
	embedded systems.},
  doi = {10.1145/1316550.1316553},
  isbn = {978-1-59593-725-4},
  keywords = {dependability,diagnosis,embedded systems,fault tolerance},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1316550.1316553\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{bundell_software_2000,
  author = {G.A. Bundell and G. Lee and J. Morris and K. Parker and Peng Lam},
  title = {A software component verification tool},
  booktitle = {Software Methods and Tools, 2000. SMT 2000. Proceedings. International
	Conference on},
  year = {2000},
  pages = {137â€•146},
  abstract = {Component based software engineering depends on reliable, robust components,
	since it may omit a unit test phase wholly or partially from the
	development cycle. The paper describes a tool that allows a component
	developer to design and run verification tests. In developing components
	for our library, we found it necessary to provide multiple mechanisms
	for identifying and capturing tests to overcome the limitations of
	any single mechanism. Once specified, test specifications and test
	results are stored in XML documents, providing a standard, portable
	form of storing, retrieving and updating test histories. One module
	of our component test bench, the test pattern verifier, has been
	designed to be general, lightweight and portable, so that it can
	be packaged with a component and its test specifications. This allows
	a component user to verify a component's compliance with specifications
	in a target environment},
  doi = {10.1109/SWMT.2000.890429},
  keywords = {component based software engineering,component compliance,component
	developer,component test bench,component user,development cycle,formal
	specification,hypermedia markup languages,object-oriented programming,program
	verification,reliable robust components,software component verification
	tool,software libraries,software reliability,specifications,standard
	portable form,target environment,test histories,test pattern verifier,test
	results,test specifications,unit test phase,verification tests,XML
	documents},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{cai_adaptive_2005,
  author = {Kai-Yuan Cai and T. Y. Chen and Yong-Chao Li and Wei-Yi Ning and
	Y. T. Yu},
  title = {Adaptive testing of software components},
  booktitle = {Proceedings of the 2005 ACM symposium on Applied computing},
  year = {2005},
  pages = {1463â€•1469},
  address = {Santa Fe, New Mexico},
  publisher = {ACM},
  abstract = {Software components are popular in nowadays software industries. However,
	how to test software components is a problem since the source code
	of the software component under test may not be available for the
	third-party user. In this paper we show that the software component
	should be tested in an adaptive manner in the sense that the software
	defect detection rates are estimated on-line by using testing data
	collected during testing to improve test case selections. In doing
	so, we use a recursive least squares estimation method to do on-line
	parameter estimations. This paper further justifies the advantages
	of the controlled Markov chain (CMC) approach to software testing
	in particular, and the practicality of the idea of software cybermetics
	in general.},
  doi = {10.1145/1066677.1067011},
  isbn = {1-58113-964-0},
  keywords = {adaptive testing,controlled markov chain,software component,software
	cybernetics,software testing},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1066677.1067011\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{cai_experiences_2004,
  author = {Yuhong Cai and J. Grundy and J. Hosking},
  title = {Experiences integrating and scaling a performance test bed generator
	with an open source CASE tool},
  booktitle = {Automated Software Engineering, 2004. Proceedings. 19th International
	Conference on},
  year = {2004},
  pages = {36â€•45},
  abstract = {We report on our experiences developing a performance test-bed generator
	for industrial usage by extending an open-source UML CASE tool. This
	tool generates client and server code, database configuration and
	deployment scripts from a high-level software architecture description.
	It automates the code generation, compilation, deployment and performance
	metric result collection processes. We identify a range of problems
	that arose from our previous research on performance test-bed generation
	that needed to be addressed to scale this automated software engineering
	technique. We describe a range of approaches we used to solve these
	problems in our new tool. We then report on industrial deployment
	and evaluation of our new tool and discuss the effectiveness of these
	solutions},
  doi = {10.1109/ASE.2004.1342764},
  isbn = {1068-3062},
  keywords = {architecture analysis,automated software engineering,client-and-server
	code,client-server systems,code compilation,code deployment,code
	generation,computer aided software engineering,database configuration,deployment
	scripts,high-level software architecture description,industrial deployment,industrial
	evaluation,industrial usage,open source CASE tool,performance metric
	result collection,performance test bed generator,program compilers,program
	testing,public domain software,software architecture,software performance
	testing,software tool extension,software tools,UML CASE tool,Unified
	Modeling Language},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{calabretto_agent_2002,
  author = {J.-P. Calabretto and D. Couper and B. Mulley and M. Nissen and S.
	Siow and J. Tuck and J. Warren},
  title = {Agent support for patients and community pharmacists},
  booktitle = {System Sciences, 2002. HICSS. Proceedings of the 35th Annual Hawaii
	International Conference on},
  year = {2002},
  pages = {2003--2012},
  abstract = {This research explores agent technology to support the information
	needs of patients and community pharmacists toward a beneficial outcome
	for the patient. As patients make more use of over the counter (OTC)
	medications to manage their own conditions, they must achieve effective
	communication in dealings with community pharmacists to avoid drug
	interactions and for guidance to seek consultation with physicians
	when appropriate. We describe an agent architecture that utilizes
	implicit and explicit sources of information for patient profiling.
	Collected profiles are used to provide a range of agent-based functions
	- including question formulation aides, Internet search utilities,
	and to support navigation of treatment guidelines - for both patients
	and pharmacists. The agent services are implemented in the context
	of the "Winston" Internet Medicine Cabinet, an online personal medication
	history service. We illustrate the value of the agent architecture
	and profiling in terms of an Internet search function.},
  doi = {10.1109/WCICA.2000.859947},
  keywords = {agent architecture,agent technology,community pharmacists,medical
	information systems,over the counter medications,patients,personal
	medication history,Winston Internet Medicine Cabinet},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{canuto_personality-based_2005,
  author = {A.M.P. Canuto and A.M.C. Campos and J.C. Alchiere and E.C.M. de Moura
	and A.M. Santos and E.B. dos Santos and R.G. Soares},
  title = {A personality-based model of agents for representing individuals
	in working organizations},
  booktitle = {Intelligent Agent Technology, IEEE/WIC/ACM International Conference
	on},
  year = {2005},
  pages = {65--71},
  abstract = {This paper proposes an agent architecture which can be used to represent
	individuals within a working organization. The proposed architecture
	has been based on the theory of human personality and its working
	relationship from Theodore Milton. The main aim of this paper is
	to describe a suitable representation of individual behaviors which
	is able to be mapped to collective patterns of a human organization.
	The proposed architecture has been used in the SimOrg project, which
	aims to apply a multi-agent simulation in human organizations.},
  doi = {10.1109/IAT.2005.3},
  keywords = {agent architecture,behavioural sciences computing,human personality,human
	resource management,individual behaviors,multiagent simulation,personality-based
	model,psychology,Theodore Milton,working organization},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{mu-kun_cao_designing_2005,
  author = {Mu-Kun Cao and Yu-Qiang Feng and Chun-Yan Wang},
  title = {Designing Intelligent Agent for E-Business Oriented Multi-Agent Automated
	Negotiation},
  booktitle = {Machine Learning and Cybernetics, 2005. Proceedings of 2005 International
	Conference on},
  year = {2005},
  volume = {1},
  pages = {328--333},
  abstract = {This paper studies automated negotiation from a new point, proposes
	a novel negotiating agent architecture, which can supports both goal-directed
	reasoning and reactive response, and a communication model, in which
	the negotiation language used by agents is defined. The communication
	model and the language are defined in a way general enough to support
	a wide variety of market mechanisms, thus being particularly suitable
	for flexible applications such as electronic business. The paper
	describes the negotiating agent architecture and the communication
	modelâ€™s frame, analyze the content of the negotiation language;
	finally, discusses the design and expression of the negotiation ontology.
	},
  doi = {10.1109/SEW.2005.38},
  keywords = {agent,Automated negotiation,BDI model,KQML},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{carbo_bdi_2001,
  author = {J. Carbo and J.M. Molina and J. Davila},
  title = {A BDI agent architecture for reasoning about reputation},
  booktitle = {Systems, Man, and Cybernetics, 2001 IEEE International Conference
	on},
  year = {2001},
  volume = {2},
  pages = {817--822 vol.2},
  abstract = {Agents acting on behalf of human users should cooperate with others
	and reason about their expected behavior in order to avoid deceptions
	and frauds. This knowledge about others will be used as a means to
	judge their reputation, and it involves how the services were provided,
	and whether they suited the particular expectations of the human
	user represented by the agent. This paper outlines an application
	of the most popular (and theoretically sound) agent architecture
	to that problem. This is the so called BDI architecture. Our research
	describes the beliefs, desires, intentions, and the relationships
	among them relevant to the given dominion of an agent reasoning about
	reputation. Due to the subjective nature of such reputations, we
	use fuzzy sets to represent them},
  doi = {10.1109/ANZIIS.1996.573880},
  keywords = {BDI agent architecture,belief maintenance,beliefs,fuzzy logic,fuzzy
	sets,reasoning,reputation},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{carvalho-junior_design_2007,
  author = {Francisco Heron Carvalho-Junior and Rafael Dueire Lins and Ricardo
	Cordeiro CorrÃªs and Gisele Azevedo AraÃºjo and Jefferson Carvalho
	Silva},
  title = {On the design of abstract binding connectors for high performance
	computing component models},
  booktitle = {Proceedings of the 2007 symposium on Component and framework technology
	in high-performance and scientific computing},
  year = {2007},
  pages = {67â€•76},
  address = {Montreal, Quebec, Canada},
  publisher = {ACM},
  abstract = {Some requirements of high-performance computing (HPC), mainly regarding
	parallel synchronization, are not met by the service connectors of
	standard commercial component models. Thus, this paper analyzes the
	usual extensions to the notion of service connector and provides
	alternative solutions based on type system theory, achieving a higher-level
	of abstraction. A notion of "abstract connector", adopted by HPE
	(the \# Programming Environment), is presented.},
  doi = {10.1145/1297385.1297397},
  isbn = {978-1-59593-867-1},
  keywords = {high performance computing,parallel programming,software components},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1297385.1297397\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{Casher2006,
  author = {Omer Casher and Henry S Rzepa},
  title = {SemanticEye: a semantic web application to rationalize and enhance
	chemical electronic publishing.},
  journal = {J Chem Inf Model},
  year = {2006},
  volume = {46},
  pages = {2396--2411},
  number = {6},
  abstract = {SemanticEye, an ontology with associated tools, improves the classification
	and open accessibility of chemical information in electronic publishing.
	In a manner analogous to digital music management, RDF metadata encoded
	as Adobe XMP can be extracted from a variety of document formats,
	such as PDF, and managed in an RDF repository called Sesame. Users
	upload electronic documents containing XMP to a central server by
	"dropping" them into WebDAV folders. The documents can then be navigated
	in a Web browser via their metadata, and multiple documents containing
	identical metadata can then be aggregated. SemanticEye does not actually
	store any documents. By including unique identifiers within the XMP,
	such as the DOI, associated documents can be retrieved from the Web
	with the help of resolving agents. The power of this metadata driven
	approach is illustrated by including, within the XMP, InChI identifiers
	for molecular structures and finding relationships between articles
	based on their InChIs. SemanticEye will become increasingly more
	comprehensive as usage becomes more widespread. Furthermore, following
	the Semantic Web architecture enables the reuse of open software
	tools, provides a "semantically intuitive" alternative to search
	engines, and fosters a greater sense of trust in Web-based scientific
	information.},
  doi = {10.1021/ci060139e},
  institution = {Clinical Imaging Centre, GlaxoSmithKline, Harlow CM19 5AW, UK.},
  owner = {user},
  pmid = {17125182},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1021/ci060139e}
}

@INPROCEEDINGS{catelani_novel_2008,
  author = {M. Catelani and L. Ciani and V.L. Scarano and A. Bacioccola},
  title = {A Novel Approach To Automated Testing To Increase Software Reliability},
  booktitle = {Instrumentation and Measurement Technology Conference Proceedings,
	2008. IMTC 2008. IEEE},
  year = {2008},
  pages = {1499â€•1502},
  abstract = {Software plays an increasingly important role in equipment and systems,
	both in terms of technical relevance and of development cost (often
	higher than 50\% even for small systems). Unlike HW, SW does not
	go through a production phase. Also, SW cannot break or wear out.
	However, it can fail to satisfy its required function because of
	defects which manifest themselves while the system is operating (dynamic
	defects). A fault in the SW is thus caused by a defect, even if appears
	randomly in time, and SW problems are basically quality problems
	which have to be solved with quality assurance tools (configuration
	management, testing, and quality data reporting systems). In order
	to grant a software high quality level against a reasonable cost,
	the testing planning phase has to be study in detail. In order to
	do so, an adequate coverage of the product functionality has to be
	supplied to reduce the test time. The aim of the paper is suggesting
	an automated software testing as a solution to the problem of having
	to maximize the test plan coverage within the available time and
	to increase software reliability and quality in use.},
  doi = {10.1109/ISoLA.2006.67},
  isbn = {1091-5281},
  keywords = {automated software testing,configuration management,dynamic defects,memory
	leaks,program testing,quality assurance tools,quality data reporting
	systems,regression test,software quality,software reliability,software
	testing,testing planning phase},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{cervantes_autonomous_2004,
  author = {Humberto Cervantes and Richard S. Hall},
  title = {Autonomous Adaptation to Dynamic Availability Using a Service-Oriented
	Component Model},
  booktitle = {Proceedings of the 26th International Conference on Software Engineering},
  year = {2004},
  pages = {614â€•623},
  publisher = {IEEE Computer Society},
  abstract = {This paper describes a project, called Gravity, that defines a component
	model, where components provide and require services (i.e., functionality)
	and all component interaction occurs via services. This approach
	introduces service-oriented concepts into a component model and execution
	environment. The goal is to support the construction and execution
	of component-based applications that are capable of autonomously
	adapting at run time due to the dynamic availability of the services
	provided by constituent components. In this component model the execution
	environment manages an application that is described as an abstract
	composition that can adapt and evolve at run time depending on available
	functionality. The motivation of Gravity is to simplify the construction
	of applications where dynamic availability arises, ranging from modern
	extensible systems to novel computing approaches, such as contrext-aware
	applications.},
  isbn = {0-7695-2163-0},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=998675.999465\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{cha_design_2001,
  author = {Jung-Eun Cha and Young-Jung Yang and Mun-Sub Song and Hang-Gon Kim},
  title = {Design and implementation of component repository for supporting
	the component based development process},
  booktitle = {Systems, Man, and Cybernetics, 2001 IEEE International Conference
	on},
  year = {2001},
  volume = {2},
  pages = {735â€•740 vol.2},
  abstract = {CBD (component based software development) has become an interesting
	field in the development of business applications. Because CBD represents
	a new development paradigm for composing applications from software
	components, increasing requirements for productivity of flexible
	system development can be solved using CBD technologies. The component
	repository is the most important part in the CBD process. In practice,
	to reuse components in CBD deployment, we must store and manage the
	related work-products from each step of component development as
	well as the component itself using a component repository. In this
	paper, we try to clarify CBD-related theories as practical techniques
	to be applied to real systems. So, we suggested individual technique
	theory for repository construction to support and realize the CBD
	process. CRPS (component repository prototyping system) was also
	constructed for supporting the CBD process based on architecture.
	CRPS supports the CBD process by managing the variety of component
	products classified by each step of the component life cycle and
	business domain based on component architecture},
  doi = {10.1109/ICEBE.2006.20},
  keywords = {business application,component architecture,component based software
	development,component repository,component repository prototyping
	system,component reuse,software libraries,software reusability},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{chahal_metrics_2008,
  author = {Kuljit Kaur Chahal and Hardeep Singh},
  title = {A Metrics Based Approach to Evaluate Design of Software Components},
  booktitle = {Global Software Engineering, 2008. ICGSE 2008. IEEE International
	Conference on},
  year = {2008},
  pages = {269â€•272},
  abstract = {Component based software development approach makes use of already
	existing software components to build new applications. Software
	components may be available in-house or acquired from the global
	market. One of the most critical activities in this reuse based process
	is the selection of appropriate components. Component evaluation
	is the core of the component selection process. Component quality
	models have been proposed to decide upon a criterion against which
	candidate components can be evaluated and then compared. But none
	is complete enough to carry out the evaluation. It is advocated that
	component users need not bother about the internal details of the
	components. But we believe that complexity of the internal structure
	of the component can help estimating the effort related to evolution
	of the component. In our ongoing research, we are focusing on quality
	of internal design of a software component and its relationship to
	the external quality attributes of the component.},
  doi = {10.1109/ICSE.2000.870393},
  keywords = {component based software development,component design,metrics,software
	components},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Chamjangali2007,
  author = {M. Arab Chamjangali and M. Beglari and G. Bagherian},
  title = {Prediction of cytotoxicity data (CC(50)) of anti-HIV 5-phenyl-1-phenylamino-1H-imidazole
	derivatives by artificial neural network trained with Levenberg-Marquardt
	algorithm.},
  journal = {J Mol Graph Model},
  year = {2007},
  volume = {26},
  pages = {360--367},
  number = {1},
  month = {Jul},
  abstract = {A Levenberg-Marquardt algorithm trained feed-forward artificial neural
	network in quantitative structure-activity relationship (QSAR) was
	developed for modeling of cytotoxicity data for anti-HIV 5-phenyl-1-phenylamino-1H-imidazole
	derivatives. A large number of descriptors were calculated with Dragon
	software and a subset of calculated descriptors was selected with
	a stepwise regression as a feature selection technique. The 28 molecular
	descriptors selected by stepwise regression, as the most feasible
	descriptors, were used as inputs for feed-forward neural network.
	The neural network architecture and its parameters were optimized.
	The data were randomly divided into 31 training and 11 validation
	sets. The prediction ability of the model was evaluated using validation
	data set and "one-leave-out" cross validation method. The root mean
	square errors (RMSE) and mean absolute errors for the validation
	data set were 0.042 and 0.024, respectively. The prediction ability
	of ANN model was also statistically compared with results of linear
	free energy related model. The obtained results show the validity
	of proposed model in the prediction of cytotoxicity data of corresponding
	anti-HIV drugs.},
  doi = {10.1016/j.jmgm.2007.01.005},
  institution = {College of Chemistry, Shahrood University of Technology, Shahrood,
	P.O. Box 36155-316, Iran. marab@shahroodut.ac.ir},
  keywords = {Algorithms; Anti-HIV Agents, chemistry/toxicity; Humans; Imidazoles,
	chemistry/toxicity; Least-Squares Analysis; Linear Models; Neural
	Networks (Computer); Quantitative Structure-Activity Relationship;
	Regression Analysis; Software; Toxicity Tests, statistics /&/ numerical
	data},
  owner = {user},
  pii = {S1093-3263(07)00006-X},
  pmid = {17350867},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1016/j.jmgm.2007.01.005}
}

@ARTICLE{chan_variational_2003,
  author = {Kwokleung Chan and Te-Won Lee and Terrence J. Sejnowski},
  title = {Variational learning of clusters of undercomplete nonsymmetric independent
	components},
  journal = {J. Mach. Learn. Res.},
  year = {2003},
  volume = {3},
  pages = {99â€•114},
  abstract = {We apply a variational method to automatically determine the number
	of mixtures of independent components in high-dimensional datasets,
	in which the sources may be nonsymmetrically distributed. The data
	are modeled by clusters where each cluster is described as a linear
	mixture of independent factors. The variational Bayesian method yields
	an accurate density model for the observed data without overfitting
	problems. This allows the dimensionality of the data to be identified
	for each cluster. The new method was successfully applied to a difficult
	real-world medical dataset for diagnosing glaucoma.},
  keywords = {bayesian learning,density estimations,ica,mixture models},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=944919.944924\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{zhiming_chang_towardsformal_2008,
  author = {Zhiming Chang and Xinjun Mao and Zhichang Qi},
  title = {Towards a Formal Model for Reconfigurable Software Architectures
	by Bigraphs},
  booktitle = {Software Architecture, 2008. WICSA 2008. Seventh Working IEEE/IFIP
	Conference on},
  year = {2008},
  pages = {331--334},
  abstract = {With the spread of the Internet and software evolution in complex
	intensive systems, software architecture often need be reconfigured
	during runtime to adapt variable environments and design objectives.
	To deal with reconfigurable software architectures, the formal method
	should be presented to describe software architectures and express
	their changes so that these changes on the evolutions of software
	architectures could be reasoned about. However, current formal methods
	for reconfigurable software architectures are difficult to represent
	hierarchy and model context-aware systems. In this paper, we use
	and extend bigraph as a formal method to describe reconfigurable
	software architecture. By providing graphic elements and term languages,
	extended bigraphs can survey static and dynamic architectures easily.
	Then we represent basic architectural operations based on extended
	bigraphs, through a case describe reconfigurations with constraints
	and context-aware information by reaction rules, and illustrate how
	to check the properties to satisfy design requirements by BiLog.},
  doi = {10.1109/WICSA.2008.17},
  keywords = {architectural operation,bigraph,Bigraph,BiLog,context-aware information,design
	requirement,dynamic architecture,formal languages,graphic elements,reconfigurable
	software architecture,Reconfiguration,Software Architecture,static
	architecture,term languages},
  owner = {user},
  timestamp = {2008.10.04}
}

@ARTICLE{han-chieh_chao_micro-mobility_2003,
  author = {Han-Chieh Chao and Ching-Yang Huang},
  title = {Micro-mobility mechanism for smooth handoffs in an integrated ad-hoc
	and cellular IPv6 network under high-speed movement},
  journal = {Vehicular Technology, IEEE Transactions on},
  year = {2003},
  volume = {52},
  pages = {1576--1593},
  number = {6},
  abstract = {The success of the Internet has attracted more people to take part
	in network navigation. Numerous wireless-communication devices have
	rapidly evolved in the past decade. The demand for mobile communications
	is increasing and packet data services through Internet protocol
	(IP) networks have become a trend. To supply more IP addresses to
	network devices and improve network performance, a new IP version
	6 (IPv6) was developed by the Internet Engineering Task Force in
	1994. IPv6 supports certain features that make mobility management
	more efficient in mobile IP. A cellular architecture is needed to
	improve the communications quality and to reduce power consumption,
	both at the base and mobile stations. In a cellular environment,
	handoffs occur frequently. Reducing the defects caused by handoffs
	is extremely important in the mobile network environment. This is
	especially important for high-speed moving devices. In this paper,
	a handoff strategy called neighbor-assisted agent architecture, which
	takes advantage of the ad-hoc network to improve handoff performance,
	is proposed. Timing analytical and simulation results show that the
	proposed mechanism can provide a better solution than mobile IP for
	handoff breaks during high-speed movement.},
  doi = {10.1109/ROBOT.2008.4543343},
  issn = {0018-9545},
  keywords = {ad hoc networks,cellular architecture,cellular radio,communications
	quality,high-speed movement,integrated ad-hoc network,Internet protocol,IPv6,micro-mobility
	mechanism,mobile communications,mobility management,neighbor-assisted
	agent architecture,packet data services,power consumption,power consumption
	reduction,routing protocols,smooth handoffs},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{chardigny_extraction_2008,
  author = {S. Chardigny and A. Seriai and M. Oussalah and D. Tamzalit},
  title = {Extraction of Component-Based Architecture from Object-Oriented Systems},
  booktitle = {Software Architecture, 2008. WICSA 2008. Seventh Working IEEE/IFIP
	Conference on},
  year = {2008},
  pages = {285--288},
  abstract = {Software architecture modeling and representation became a main phase
	of the development process of complex systems. In fact, software
	architecture representation provides many advantages during all phases
	of software life cycle. Nevertheless, for many systems, like legacy
	or eroded ones, there is no available representation of their architectures.
	In order to benefit from this representation, we propose, in this
	paper, an approach called ROMANTIC which focuses on extracting a
	component-based architecture of an existing object-oriented system.
	The main idea of this approach is to propose a quasi-automatic process
	of architecture recovery based on semantic and structural characteristics
	of software architecture concepts.},
  doi = {10.1109/WICSA.2008.44},
  keywords = {architecture recovery,Architecture recovery,clustering,component-based
	architecture,object-oriented system,ROMANTIC approach},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{chen_se4sc:specific_2004,
  author = {Hao Chen and Shi Ying and Jin Liu and Wei Wang},
  title = {SE4SC: a specific search engine for software components},
  booktitle = {Computer and Information Technology, 2004. CIT '04. The Fourth International
	Conference on},
  year = {2004},
  pages = {863â€•868},
  abstract = {Currently component-based software engineering is increasingly being
	adopted for software development. This approach relies on using reusable
	components as the building blocks for constructing software systems.
	As the growth in the popularity of Internet, component providers
	can publish the components easily on the Internet. Therefore, the
	major problem facing the component reusers is to find suitable components
	on the Internet. The solution to this problem lies in applying search
	engine technology. However, general-purpose search engines are inappropriate
	to search for software components. Some people developed the specific
	search engines for software components. But those engines have some
	limitations. This paper proposes a specific search engine for software
	components, which provides convenient support for component reusers
	to search for software components on the Internet.},
  doi = {10.1109/CIT.2004.1357303},
  keywords = {component reuse,component-based software engineering,Internet,object-oriented
	programming,SE4SC,search engine,search engines,software components,software
	development,software engineering,software systems},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{po-chun_chen_boosting-based_2006,
  author = {Po-Chun Chen and Xiaocong Fan and Shizhuo Zhu and J. Yen},
  title = {Boosting-Based Learning Agents for Experience Classification},
  booktitle = {Intelligent Agent Technology, 2006. IAT '06. IEEE/WIC/ACM International
	Conference on},
  year = {2006},
  pages = {385--388},
  abstract = {The capability of learning from experience is of critical importance
	in developing multi-agent systems supporting dynamic group decision
	making. In this paper, we introduce a hierarchical learning approach,
	aiming to support hierarchical group decision making where the decision
	makers at lower levels only have partial view of the whole picture.
	To further understand such a hierarchical learning concept, we implemented
	a learning component within the R-CAST agent architecture, with lower-level
	learners using the LogitBoost algorithm with decision stumps. The
	boosting-based learning agents were then used in our experiments
	to classify experience instances. The results indicate that hierarchical
	learning can largely improve decision accuracy when lower-level decision
	makers only have limited information accessibility.},
  keywords = {boosting-based learning agents,decision making,decision stumps,dynamic
	group decision making,experience classification,hierarchical learning
	approach,hierarchical learning concept,information accessibility,LogitBoost
	algorithm,multiagent systems,R-CAST agent architecture},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{yuh-ming_cheng_applications_2007,
  author = {Yuh-Ming Cheng and Lih-Shyang Chen and Sheng-Feng Weng and Yong-Guo
	Chen and Chyi-Her Lin},
  title = {Applications of a Pedagogical Agent Mechanism in a Web-based Clinical
	Simulation System for Medical Education},
  booktitle = {Intelligent Information Hiding and Multimedia Signal Processing,
	2007. IIHMSP 2007. Third International Conference on},
  year = {2007},
  volume = {1},
  pages = {635--638},
  abstract = {In education, computer learning systems make wide use of pedagogical
	agents to simulate a tutor and/or mimic tutoring interaction, as
	well as offering just-in-time and adaptive feedback. Although the
	theoretical aspect of the pedagogical agents has been well-documented
	in the literature, relatively fewer efforts have been made on how
	a pedagogical agent should be implemented in a real multimedia computerized
	simulation learning environment. In this paper, we propose a pedagogical
	agent architecture and implement it in the multimedia medical simulation
	web-based learning system call HINTS to further facilitate students'
	learning and thereby make the HINTS a more effective educational
	tool. A preliminary students' performance evaluation result is reported.
	Some experiments have been conducted and the results show that the
	pedagogical agent indeed help the students in their learning process.},
  doi = {10.1109/IIHMSP.2007.4457628},
  keywords = {adaptive feedback,biomedical education,computer aided instruction,computer
	learning systems,HINTS,just-in-time feedback,medical education,multimedia
	computerized simulation learning environment,multimedia computing,multimedia
	medical simulation,pedagogical agent architecture,pedagogical agent
	mechanism,tutoring interaction,Web-based clinical simulation system,Web-based
	learning system},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{chien_automated_2000,
  author = {S. Chien and F. Fisher and T. Estlin},
  title = {Automated software module reconfiguration through the use of artificial
	intelligence planning techniques},
  journal = {Software, IEE Proceedings -},
  year = {2000},
  volume = {147},
  pages = {186â€•192},
  number = {5},
  abstract = {One important approach to enhancing software re-use is through the
	creation of large-scale software libraries. By modularising functionality,
	many complex specialised applications can be built up from smaller
	reusable general-purpose libraries. Consequently, many large software
	libraries have been formed for applications such as image processing
	and data analysis. However, knowing the requirements and formats
	of each of these routines requires considerable expertise thus limiting
	the usage of these libraries to experts. An approach is described
	to enable novices to use complex software libraries. In this approach,
	the interactions between, and requirements of, the software modules
	are represented in a declarative language based on artificial intelligence
	(AI) planning techniques. The user is then able to specify their
	goals in terms of this language-designating what they want accomplished
	instead of how to do it. The AI planning system then uses this model
	of the available subroutines to compose a domain specific script
	to fulfil the user request. Three such systems developed by the Artificial
	Intelligence Group of the Jet Propulsion Laboratory and described.
	The multimission VICAR planner (MVP) was deployed in 1994 and used
	to support image processing for science product generation for the
	Galileo mission. MVP reduced the time for filling certain classes
	of requests from 4 h to 15 min. The automated SAR image processing
	system (ASIP) was deployed in 1996 to the Department of Geology at
	Arizona State University to support aeolian science analysis of synthetic
	aperture radar images. ASIP reduces the number of manual inputs in
	science product generation tenfold. Finally, the DPLAN system reconfigures
	software modules that control complex antenna hardware in configuring
	antennas to support a wide range of tracks for NASA's Deep Space
	Network of communications and radio science antennas},
  doi = {10.1049/ip-sen:20000899},
  issn = {1462-5970},
  keywords = {aeolian science analysis,artificial intelligence planning techniques,ASIP,automated
	SAR image processing system,automated software module reconfiguration,complex
	antenna hardware,configuration management,data analysis,declarative
	language,deductive databases,Deep Space Network,domain specific script,DPLAN
	system,Galileo mission,image processing,knowledge based systems,large-scale
	software libraries,multimission VICAR planner,planning (artificial
	intelligence),radio science antennas,science product generation,software
	libraries,software modules,software re-use,synthetic aperture radar
	images,user request},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{choi_practical_2008,
  author = {Yoonjung Choi and Sungwook Lee and Houp Song and Jingoo Park and
	SunHee Kim},
  title = {Practical S/W Component Quality Evaluation Model},
  booktitle = {Advanced Communication Technology, 2008. ICACT 2008. 10th International
	Conference on},
  year = {2008},
  volume = {1},
  pages = {259â€•264},
  abstract = {Software component has been developed lively. However, the measurement
	of software component quality is relatively less researched against
	conventional software quality in practice. This paper introduces
	an in-house component quality model which includes metrics for component
	quality evaluation, tailoring guidelines for evaluations, and reporting
	formats of evaluations. Also, we present the examples of quality
	evaluation with it. Recently, to formalize and visualize the quality
	of S/W components in a quantitative way we have applied this component
	quality model to embedded system development projects so that component
	developers can control component quality to build reusable and quality
	components while enabling cost reduction and quality improvement.
	In addition, we've improved component quality model for the proper
	tailoring when applied in embedded system domain.},
  doi = {10.1109/ICACT.2008.4493757},
  isbn = {1738-9445},
  keywords = {cost reduction,embedded system development projects,embedded systems,quality
	improvement,reporting formats,Software Component,software component
	quality,Software Component Quality,software engineering,software
	quality,tailoring guidelines},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{chow_digital_2005,
  author = {K.P. Chow and C.F. Chong and K.Y. Lai and L.C.K. Hui and K.H. Pun
	and W.W. Tsang and H.W. Chan},
  title = {Digital evidence search kit},
  booktitle = {Systematic Approaches to Digital Forensic Engineering, 2005. First
	International Workshop on},
  year = {2005},
  pages = {187â€•194},
  abstract = {With the rapid development of electronic commerce and Internet technology,
	cyber crimes have become more and more common. There is a great need
	for automated software systems that can assist law enforcement agencies
	in cyber crime evidence collection. This paper describes a cyber
	crime evidence collection tool called DESK (digital evidence search
	kit), which is the product of several years of cumulative efforts
	of our center together with the Hong Kong Police Force and several
	other law enforcement agencies of the Hong Kong Special Administrative
	Region. We use DESK to illustrate some of the desirable features
	of an effective cyber crime evidence collection tool.},
  doi = {10.1109/SWSTE.2005.2},
  keywords = {automated software systems,computer crime,cyber crime evidence collection
	tool,DESK,digital evidence search kit,electronic commerce,Hong Kong
	Police Force,Hong Kong Special Administrative Region,Internet,Internet
	technology,law enforcement agencies,police data processing},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{min_chee_choy_cooperative_2003,
  author = {Min Chee Choy and D. Srinivasan and R.L. Cheu},
  title = {Cooperative, hybrid agent architecture for real-time traffic signal
	control},
  journal = {Systems, Man and Cybernetics, Part A, IEEE Transactions on},
  year = {2003},
  volume = {33},
  pages = {597--607},
  number = {5},
  abstract = {This paper presents a new hybrid, synergistic approach in applying
	computational intelligence concepts to implement a cooperative, hierarchical,
	multiagent system for real-time traffic signal control of a complex
	traffic network. The large-scale traffic signal control problem is
	divided into various subproblems, and each subproblem is handled
	by an intelligent agent with a fuzzy neural decision-making module.
	The decisions made by lower-level agents are mediated by their respective
	higher-level agents. Through adopting a cooperative distributed problem
	solving approach, coordinated control by the agents is achieved.
	In order for the multiagent architecture to adapt itself continuously
	to the dynamically changing problem domain, a multistage online learning
	process for each agent is implemented involving reinforcement learning,
	learning rate and weight adjustment as well as dynamic update of
	fuzzy relations using an evolutionary algorithm. The test bed used
	for this research is a section of the Central Business District of
	Singapore. The performance of the proposed multiagent architecture
	is evaluated against the set of signal plans used by the current
	real-time adaptive traffic control system. The multiagent architecture
	produces significant improvements in the conditions of the traffic
	network, reducing the total mean delay by 40\% and total vehicle
	stoppage time by 50\%.},
  doi = {10.1109/64.585097},
  issn = {1083-4427},
  keywords = {Central Business District of Singapore,complex traffic network,computational
	intelligence concepts,continuous adaptation,cooperative distributed
	problem solving approach,cooperative hierarchical multiagent system,cooperative
	hybrid agent architecture,coordinated control,dynamically changing
	problem domain,evolutionary algorithm,evolutionary computation,feedforward
	neural nets,fuzzy neural decision-making module,fuzzy neural nets,hybrid
	synergistic approach,large-scale traffic signal control problem,learning
	rate,multiagent architecture,multistage online learning process,real-time
	traffic signal control,reinforcement learning,road traffic,signalling,total
	mean delay,total vehicle stoppage time,traffic control,traffic network
	conditions,weight adjustment},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{zhou_chuan-sheng_research_2007,
  author = {Zhou Chuan-Sheng and Liu Jie},
  title = {Research and Design of Software Bus Based Generic Software Agent
	Architecture},
  booktitle = {Intelligent Information Hiding and Multimedia Signal Processing,
	2007. IIHMSP 2007. Third International Conference on},
  year = {2007},
  volume = {2},
  pages = {174--178},
  abstract = {Today alone with the agent technologies popularization, software agent
	technology has being applied into many information systems, like
	ERP system, CRM system, SCM system and OA system, etc. But till now,
	there are not any standards to follow for software agent design.
	In this paper, by research on agent characteristics and its working
	principle, we introduce software bus based design of generic software
	agent architecture and with a certain ERP workflow to describe its
	real application in an ERP system.},
  keywords = {ERP workflow,generic software agent architecture,software bus design},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{coelho_meta-agency_2005,
  author = {F. Coelho},
  title = {Meta-agency and individual-power: an experimental approach},
  booktitle = {Intelligent Agent Technology, IEEE/WIC/ACM International Conference
	on},
  year = {2005},
  pages = {414--420},
  abstract = {What if an agent mental space does constitute the environment for
	a second agent? Even the simplest agent architectures are composites
	of entities that, together, define the agent's behavior. Now, suppose
	that one can use a second agent to percept and manipulate those entities.
	Why should one want to do so? Are there any advantages? And what
	are the disadvantages? This paper reports our initial investigation
	into the advantages of such scenario. We conducted a simple experiment
	based on Steels' Mars Explorer subsumption agent: the list of reaction-pairs
	is initially shuffled and a meta-agent can promote or demote those
	pairs in response to individual-power local variations. After a while
	the observed list of reaction-pairs and the implied agent behavior
	are very close to the original, and intended, design.},
  doi = {10.1109/IAT.2005.97},
  keywords = {agent architecture,agent behavior,agent mental space,meta-agent,Steel
	Mars Explorer subsumption agent},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{collofello_assessingprocess_1999,
  author = {J.S. Collofello and Chi Heng Ng},
  title = {Assessing the process maturity utilized in software engineering team
	project courses},
  booktitle = {Frontiers in Education Conference, 1999. FIE '99. 29th Annual},
  year = {1999},
  volume = {1},
  pages = {12A9/5â€•12A9/9 vol.1},
  abstract = {As part of their curriculum, many computer science departments offer
	an introductory software engineering course. This course normally
	provides an introduction to software engineering topics in conjunction
	with a semester long team project. The typical goals of this project
	are to provide students with a team based realistic software development
	project experience. To ensure students acquire the correct lessons
	from this project experience, it is essential that the teams utilize
	well-defined software development processes similar to those practised
	by leading software development organizations. Since its inception,
	the Software Engineering Institute Capability Maturity Model (CMM)
	has served as a guide for organizations seeking to improve their
	development practices. The CMM identifies five levels of maturity,
	each of which exemplifies the utilization of key software engineering
	practices. Organizations can utilize the CMM to assess the maturity
	of their development processes via a self-assessment questionnaire,
	Although most organizations would like to believe their processes
	are mature, most are at the first two levels of maturity. In an effort
	to assess the maturity of development practices utilized in software
	engineering courses, an â€œacademicâ€? version of the CMM questionnaire
	was developed. This questionnaire was distributed to a sample of
	software engineering instructors in an effort to assess the maturity
	of academic software engineering course projects. The questionnaire
	and the survey results are presented and discussed. The goal of this
	effort is to provide an assessment of current software engineering
	course project education. As in the commercial world, this assessment
	will help pinpoint areas of improvement. It will also provide a challenge
	to software engineering instructors to â€œpractice what they preach
	â€? and strive for higher levels of process maturity},
  keywords = {computer science curriculum,computer science education,development
	practices,educational courses,instructors,process maturity assessment,software
	engineering,software engineering course,Software Engineering Institute
	Capability Maturity Model,software engineering team project courses},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{coppit_multiple_2000,
  author = {David Coppit and Kevin J. Sullivan},
  title = {Multiple mass-market applications as components},
  booktitle = {Proceedings of the 22nd international conference on Software engineering},
  year = {2000},
  pages = {273â€•282},
  address = {Limerick, Ireland},
  publisher = {ACM},
  abstract = {Truly successful models for component-based software development continue
	to prove elusive. One of the few is the use of operating system,
	database and similar programs in many systems. We address three related
	problems in this paper. First, we lack needed models. Second, we
	do not know the conditions under which such models can succeed. In
	particular, it is unclear whether the notable success with operating
	systems can be replicated. Third, we do not know whether certain
	specific models can succeed. We are addressing these problems by
	evaluating a particular model that shares important characteristics
	with the successful operating system example: using compatible PC
	packages as components. Our approach to evaluating such a model is
	to engage in a case study that aims to build an industrially successful
	system representative of an important class of systems. We report
	on our use of the model to develop a computational tool for reliability
	engineering. We draw two conclusions. First, this kind of model has
	the potential to succeed. Second, even today, the model can produce
	significant returns, but it clearly carries considerable risks.},
  doi = {10.1145/337180.337210},
  isbn = {1-58113-206-9},
  keywords = {component-based software,package-oriented programming},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=337180.337210\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{cortellessa_drivingselection_2007,
  author = {Vittorio Cortellessa and Ivica Crnkovic and Fabrizio Marinelli and
	Pasqualina Potena},
  title = {Driving the selection of cots components on the basis of system requirements},
  booktitle = {Proceedings of the twenty-second IEEE/ACM international conference
	on Automated software engineering},
  year = {2007},
  pages = {413â€•416},
  address = {Atlanta, Georgia, USA},
  publisher = {ACM},
  abstract = {In a component-based development process the selection of components
	is an activity that takes place over multiple lifecycle phases that
	span from requirement specifications through design to implementation-integration.
	Automated tool support for component selection would be very helpful
	in each phase. In this paper we introduce a framework that supports
	the selection of COTS components in the requirements phase. The framework
	lays on a tool that builds and solves an optimization model, whose
	solution provides the optimal COTS component selection. The selection
	criterion is based on cost minimization of the whole system while
	assuring a certain degree of satisfaction of the system requirements.
	The output of the model solution indicates the optimal combination
	of single COTS components and assemblies of COTS that satisfy the
	requirements while minimizing costs},
  doi = {10.1145/1321631.1321697},
  isbn = {978-1-59593-882-4},
  keywords = {cots selection,optimization model,software requirements},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1321631.1321697\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{Costa2006,
  author = {Suzete Costa and Cristina Santos and João Silveira},
  title = {Community pharmacy services in Portugal.},
  journal = {Ann Pharmacother},
  year = {2006},
  volume = {40},
  pages = {2228--2234},
  number = {12},
  month = {Dec},
  abstract = {OBJECTIVE: To describe the status and outline the trends of community
	pharmacy services in the Portuguese healthcare system. FINDINGS:
	The legal framework for Portuguese pharmacies includes geographic
	and demographic criteria for opening pharmacies and restricting ownership
	to pharmacists. Since 1975, pharmacy owners have been organized under
	one association, the Portuguese National Pharmacy Association, which
	has dealt with pharmacy development over the past 20 years. Securing
	the economic sustainability of pharmacies was the first milestone.
	Investing in information software and communication technologies
	and modernizing the architecture of pharmacies was the next step.
	Then, professional services such as point-of-care measurements, drug
	waste management, and needle exchange services were developed. The
	methadone substitution program was introduced at a later date. Finally,
	major comprehensive services, including disease management/pharmaceutical
	care programs, were implemented. DISCUSSION: All Portuguese pharmacies
	are independent, which means that the decision to provide different
	services and the extent to which they are attained vary from pharmacy
	to pharmacy. Recent political changes seem to reinforce the need
	to pursue the strategies already defined-to expand the traditional
	scope of pharmacy business to a wider range of health services. CONCLUSIONS:
	Achievements that have been accomplished and strategies that are
	in place reflect a high level of organization and unity among Portuguese
	pharmacies. From a global perspective, pharmacies are reorganizing
	to be able to compete against other possible agents and are preparing
	to expand their scope of intervention.},
  doi = {10.1345/aph.1H129},
  institution = {Department of Pharmacy-based Disease Management Programs, Portuguese
	National Association of Pharmacies, Lisbon, Portugal. suzete.costa@anf.pt},
  keywords = {Community Pharmacy Services, trends; Delivery of Health Care, trends;
	Humans; Portugal},
  owner = {user},
  pii = {aph.1H129},
  pmid = {17148652},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1345/aph.1H129}
}

@ARTICLE{coulson_generic_2008,
  author = {Geoff Coulson and Gordon Blair and Paul Grace and Francois Taiani
	and Ackbar Joolia and Kevin Lee and Jo Ueyama and Thirunavukkarasu
	Sivaharan},
  title = {A generic component model for building systems software},
  journal = {ACM Trans. Comput. Syst.},
  year = {2008},
  volume = {26},
  pages = {1â€•42},
  number = {1},
  abstract = {Component-based software structuring principles are now commonplace
	at the application level; but componentization is far less established
	when it comes to building low-level systems software. Although there
	have been pioneering efforts in applying componentization to systems-building,
	these efforts have tended to target specific application domains
	(e.g., embedded systems, operating systems, communications systems,
	programmable networking environments, or middleware platforms). They
	also tend to be targeted at specific deployment environments (e.g.,
	standard personal computer (PC) environments, network processors,
	or microcontrollers). The disadvantage of this narrow targeting is
	that it fails to maximize the genericity and abstraction potential
	of the component approach. In this article, we argue for the benefits
	and feasibility of a generic yet tailorable approach to component-based
	systems-building that offers a uniform programming model that is
	applicable in a wide range of systems-oriented target domains and
	deployment environments. The component model, called OpenCom, is
	supported by a reflective runtime architecture that is itself built
	from components. After describing OpenCom and evaluating its performance
	and overhead characteristics, we present and evaluate two case studies
	of systems we have built using OpenCom technology, thus illustrating
	its benefits and its general applicability.},
  doi = {10.1145/1328671.1328672},
  keywords = {component-based software,computer systems implementation},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1328671.1328672\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{craig_component_2007,
  author = {D.C. Craig and W.M. Zuberek},
  title = {Component Compatibility and Its Verification},
  booktitle = {Digital Telecommunications, 2007. ICDT '07. Second International
	Conference on},
  year = {2007},
  pages = {26},
  abstract = {An approach to verification of component compatibility is proposed
	in which each component's behaviour (at its interfaces) is represented
	by a labeled Petri net in such a way that the sequences of services
	(provided or requested) correspond to sequences of labels assigned
	to occurring transitions. The behaviour of a component can thus be
	defined as the language of its modeling net. Two interacting components
	are compatible if and only if all possible sequences of services
	requested by one of these two components can be satisfied by the
	other component; in other words, two components are compatible if
	the language of the requesting component is a subset of the language
	of the component providing the services. Verification of this simple
	relation depends upon the class of languages defining the behaviours
	of the components. If the languages are regular, the verification
	of compatibility is straightforward. For non-regular languages, a
	more elaborate approach is needed in which a net model composed of
	the interacting components is checked for the absence of deadlocks.
	Some applications of the proposed approach are also discussed.},
  doi = {10.1109/ASE.2003.1240302},
  keywords = {component compatibility verification,formal specification,formal verification,interoperability,labeled
	Petri net,object-oriented programming,open systems,Petri nets,software
	architecture},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{crnkovic_specification_2002,
  author = {Ivica Crnkovic and Brahim Hnich and Torsten Jonsson and Zeynep Kiziltan},
  title = {Specification, implementation, and deployment of components},
  journal = {Commun. ACM},
  year = {2002},
  volume = {45},
  pages = {35â€•40},
  number = {10},
  abstract = {Clarifying common terminology and exploring component-based relationships.},
  doi = {10.1145/570907.570928},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=570907.570928\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{crnkovic_case_2000,
  author = {I. Crnkovic and M. Larsson},
  title = {A case study: demands on component-based development},
  booktitle = {Software Engineering, 2000. Proceedings of the 2000 International
	Conference on},
  year = {2000},
  pages = {23â€•31},
  abstract = {Building software systems with reusable components brings many advantages.
	The development becomes more efficient, the reliability of the products
	is enhanced, and the maintenance requirement is significantly reduced.
	Designing, developing and maintaining components for reuse is, however,
	a very complex process which places high requirements not only on
	the component functionality and flexibility, but also on the development
	organization. The authors discuss the different levels of component
	reuse, and certain aspects of component development, such as component
	generality and efficiency, compatibility problems, the demands on
	development environment, maintenance, etc. The evolution of requirements
	for products generates new requirements for components, if components
	are not general and mature enough. This dynamism determines the component
	life cycle where the component first reaches its stability and later
	degenerates in an asset that is difficult to use, difficult to adapt
	and maintain. When reaching this stage, the component becomes an
	obstacle for efficient reuse and should be replaced. Questions related
	to use of standard and de-facto standard components are addressed
	specifically. As an illustration of reuse issues, we present a successful
	implementation of a component based system which is widely used for
	industrial process control},
  doi = {10.1109/SYSTEMS.2008.4519027},
  keywords = {case study,compatibility problems,component based development,component
	based system,component functionality,component generality,component
	life cycle,component reuse,de-facto standard components,development
	environment,development organization,industrial process control,maintenance
	requirement,object-oriented programming,process control,reusable
	components,reuse issues,software maintenance,software reusability,software
	standards,software systems design,very complex process},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{dam_agent-oriented_2006,
  author = {K.H. Dam and M. Winikoff and L. Padgham},
  title = {An agent-oriented approach to change propagation in software evolution},
  booktitle = {Software Engineering Conference, 2006. Australian},
  year = {2006},
  pages = {10 pp.},
  abstract = {Software maintenance and evolution are inevitable activities since
	almost all software that is useful and successful stimulates user-generated
	requests for change and improvements. One of the most critical problems
	in software maintenance and evolution is to maintain consistency
	between software artefacts by propagating changes correctly. Although
	many approaches have been proposed, automated change propagation
	is still a significant technical challenge in software engineering.
	In this paper we present a novel, agent-oriented approach to deal
	with change propagation in evolving software systems that are developed
	using the Prometheus methodology. A meta-model with a set of the
	object constraint language (OCL) rules forms the basis of the proposed
	framework. The underlying change propagation mechanism of our framework
	is based on the well-known Belief-Desire-Intention (BDI) agent architecture.
	Traceability information and design heuristics are also incorporated
	into the framework to facilitate the change propagation process.},
  doi = {10.1109/ICPADS.2005.159},
  isbn = {1530-0803 },
  keywords = {agent-oriented approach,Belief-Desire-Intention agent architecture,meta-model,Prometheus
	methodology},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{damiani_corrigenda:hierarchy-aware_1999,
  author = {E. Damiani and M. G. Fugini and C. Bellettini},
  title = {Corrigenda: a hierarchy-aware approach to faceted classification
	of object-oriented components},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  year = {1999},
  volume = {8},
  pages = {425â€•472},
  number = {4},
  abstract = {This article presents a hierarchy-aware classification schema for
	object-oriented code, where software components are classified according
	to their behavioral characteristics, such as provided services, employed
	algorithms, and needed data. In the case of reusable application
	frameworks, these characteristics are constructed from their model,
	i.e., from the description of the abstract classes specifying both
	the framework structure and purpose. In conventional object libraries,
	the characteristics are extracted semiautomatically from class interfaces.
	Characteristics are term pairs, weighted to represent "how well"
	they describe component behavior. The set of characteristics associated
	with a given component forms its software descriptor. A descriptor
	base is presented where descriptors are organized on the basis of
	structured relationships, such as similarity and composition. The
	classification is supported by a thesaurus acting as a language-independent
	unified lexicon. The descriptor base is conceived for developers
	who, besides conventionally browsing the descriptors hierarchy, can
	query the system, specifying a set of desired functionalities and
	getting a ranked set of adaptable candidates. User feedback is taken
	into account in order to progressively ameliorate the quality of
	the descriptors according to the views of the user community. Feedback
	is made dependent of the user typology through a user profile. Experimental
	results in terms of recall and precision of the retrieval mechanism
	against a sample code base are reported.},
  doi = {10.1145/322993.322997},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=322993.322997\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{damiani_hierarchy-aware_1999,
  author = {E. Damiani and M. G. Fugini and C. Bellettini},
  title = {A hierarchy-aware approach to faceted classification of objected-oriented
	components},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  year = {1999},
  volume = {8},
  pages = {215â€•262},
  number = {3},
  abstract = {This article presents a hierarchy-aware classification schema for
	obje ct-oriented code, where software components are classified according
	to their behavioral characteristics, such as provided services, employed
	algorithms, and needed data. In the case of reusable application
	frameworks, these characteristics are constructured from their model,
	i.e., from the description of the abstract classes specifying both
	the framework structure and purpose. In conventional object libraries,
	the characteristics are extracted semiautomatically from class interfaces.
	Characteristics are term pairs, weighted to represent â€œhow wellâ€?
	they describe component behavior. The set of characteristics associated
	with a given component forms its software descriptor. A descriptor
	base is presented where descriptors are organized on the basis of
	structured relationships, such as similarity and composition. The
	classification is supported by a thesaurus acting as a language-independent
	unified lexicon. The descriptor base is conceived for developers
	who, besides conventionally browsing the descriptors hierarchy, can
	query the system, specifying a set of desired functionalities and
	getting a ranked set of adaptable candidates. User feedback is taken
	into account in order to progressively ameliorate the quality of
	the descriptors according to the views of the user community. Feedback
	is made dependent of the user typology through a user profile. Experimental
	results in terms of recall and precision of the retrieval mechanism
	against a sample code base are reported.},
  doi = {10.1145/310663.310665},
  keywords = {code analysis,component repositories,component retrieval,software
	reuse,user feedback},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=310663.310665\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{dasgupta_mobile_2001,
  author = {D. Dasgupta and H. Brian},
  title = {Mobile security agents for network traffic analysis},
  booktitle = {DARPA Information Survivability Conference \& Exposition II, 2001.
	DISCEX '01. Proceedings},
  year = {2001},
  volume = {2},
  pages = {332--340 vol.2},
  abstract = {This paper describes the implementation of a distributed agent architecture
	for intrusion detection and response in networked computers. Unlike
	conventional intrusion detection systems (IDS), this security system
	attempts to emulate mechanisms of the natural immune system using
	Java-based mobile software agents. These security agents monitor
	multiple levels (packet process, system, and user) of networked computers
	to determine correlation among the observed anomalous patterns, reporting
	such abnormal behavior to the network administrator and/or possibly
	taking some action to counter a suspected security violation. The
	paper focuses on the design aspects of such an intrusion detection
	system by integrating different artificial intelligence techniques
	and a mobile agent architecture. Specifically, IBM's Aglets (TM)
	Software Development Kit (ASDK) is used as the base agent architecture,
	along with adaptive resonance theory (ART-2) neural networks for
	network pattern classification, and a fuzzy logic controller for
	decision/action resolution. The feasibility and implementation of
	the mobile security agent system is demonstrated and some preliminary
	results are reported},
  doi = {10.1109/ECBS.2004.1316739},
  keywords = {abnormal behavior,adaptive resonance theory neural networks,Aglets,ART
	neural nets,ART-2 neural nets,distributed agent architecture,fuzzy
	control,fuzzy logic controller,intrusion detection,mobile security
	agents,mobile software agents,natural immune system,network pattern
	classification,network traffic analysis,networked computers,security
	violation,Software Development Kit,telecommunication traffic},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{davis_case_1989,
  author = {M.J. Davis},
  title = {Case study: applying expert systems technology to testing phase of
	software life cycle},
  booktitle = {AI Systems in Government Conference, 1989.,Proceedings of the Annual},
  year = {1989},
  pages = {108â€•112},
  abstract = {Describes the use of an expert-systems approach to automation of systems
	and integration testing for validation of complex, real-time communications
	software. The approach permits a `state'-based rather than path-
	or branch-based testing style. States can be associated with high-level
	system requirements to give a measure of test coverage. The benefits
	and weaknesses realized from using an embeddable expert-system shell
	with a custom relational database interface to construct an automated
	software verification tool supporting this approach are discussed,
	and a brief summary of the utility of applying expert systems technology
	in this software engineering area is presented. The effectiveness
	of the prototype automated software verification analysis was tested
	against an AWACS (Airborne Warning and Control System) baseline known
	to be faulty, and both documented and undocumented errors were identified},
  doi = {10.1109/ASE.1997.632848},
  keywords = {Airborne Warning and Control System,automated software verification
	tool,AWACS,custom relational database interface,embeddable expert-system
	shell,expert systems,expert systems technology,high-level system
	requirements,military computing,program verification,real-time communications
	software,software engineering,software life cycle,software testing,software
	tools,telecommunications computing,test coverage,undocumented errors},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{debenham_intelligent_2006,
  author = {J. Debenham and S. Simoff},
  title = {Intelligent Agents that Span the Process Management Spectrum},
  booktitle = {Intelligent Systems, 2006 3rd International IEEE Conference on},
  year = {2006},
  pages = {386--389},
  abstract = {The process management spectrum extends from conventional workflow
	processes to emergent processes. Three categories of process are
	identified. Activity-driven processes that are managed by a single
	reactive agent architecture. Goal-driven processes that are managed
	by a multiagent system of deliberative agents. Knowledge-driven processes
	that are managed by augmenting the multiagent system from the goal-driven
	system with an approach based on task types. The idea behind task
	types is that if the system knows what sort of task is being worked
	on by the (human) users then appropriate support may be provided.
	Three general purpose agent architectures are described, one for
	each category of process. The business of process management is generally
	limited to the management of the processes themselves - this is appropriate
	for production workflows. Goal-driven and knowledge-driven processes
	both rely on the management of the collaboration between the human
	players. Collaboration management is seen here to be an important
	component of process management, and an agent architecture, founded
	on concepts from information theory, is described for it},
  doi = {10.1109/IAT.2005.43},
  keywords = {activity-driven process,business process management,business process
	re-engineering,collaboration management,deliberative agent,goal-driven
	system,intelligent agent,Intelligent agents,knowledge-driven process,multiagent
	system,process management,process management spectrum,production
	workflow,single reactive agent architecture,workflow management software,workflow
	process},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{debnath_testing_2005,
  author = {N.C. Debnath and H.K. Lee and M. Burgin and E. Thiemann},
  title = {A testing and analysis tool for certain 3-variable functions},
  booktitle = {Information Reuse and Integration, Conf, 2005. IRI -2005 IEEE International
	Conference on.},
  year = {2005},
  pages = {560â€•565},
  abstract = {The object oriented design and implementation of an automated software
	testing tool is presented. The tool mechanically generates the test
	cases using the specifications of certain 3-variable functions and
	automatically run the test cases on a specified software to evaluate
	correctness. The tool can be reused for other related functions.},
  doi = {10.1109/SADFE.2005.10},
  keywords = {3-variable functions,automated software testing tool,formal specification,object
	oriented design,object-oriented programming,program testing,program
	verification,software reusability,software tools,test cases generation},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{dechow_common_2007,
  author = {Douglas R. Dechow and Boyana Norris and James Amundson},
  title = {The common component architecture for particle accelerator simulations},
  booktitle = {Proceedings of the 2007 symposium on Component and framework technology
	in high-performance and scientific computing},
  year = {2007},
  pages = {111â€•120},
  address = {Montreal, Quebec, Canada},
  publisher = {ACM},
  abstract = {Synergia2 is a beam dynamics modeling and simulation application for
	high-energy accelerators such as the Tevatron at Fermilab and the
	International Linear Collider, which is now under planning and development.
	Synergia2 is a hybrid, multilanguage software package comprised of
	two separate accelerator physics packages (Synergia and MaryLie/Impact)
	and one high-performance computer science package (PETSc). We describe
	our approach to producing a set of beam dynamics-specific software
	components based on the Common Component Architecture specification.
	Among other topics, we describe particular experiences with the following
	tasks: using Python steering to guide the creation of interfaces
	and to prototype components; working with legacy Fortran codes; and
	an example component-based, beamdynamics simulation.},
  doi = {10.1145/1297385.1297404},
  isbn = {978-1-59593-867-1},
  keywords = {accelerator simulations,cca,common component architecture,components,marylie/impact,synergia2},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1297385.1297404\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{Decruyenaere2003,
  author = {J. Decruyenaere and F. De Turck and S. Vanhastel and F. Vandermeulen
	and P. Demeester and G. de Moor},
  title = {On the design of a generic and scalable multilayer software architecture
	for data flow management in the intensive care unit.},
  journal = {Methods Inf Med},
  year = {2003},
  volume = {42},
  pages = {79--88},
  number = {1},
  abstract = {OBJECTIVES: The current Intensive Care Information Systems (IC-ISs)
	collect and store monitoring data in on automated way and can replace
	all paper forms by an electronic equivalent, resulting in a paperless
	ICU. Future development of IC-ISs will now have to focus on bedside
	clinical decision support. The current IC-ISs are data-driven systems,
	with a two-layer software architecture. This software architecture
	is hardly maintainable and probably not the most optimal architecture
	to make the transition towards future systems with-decision support.
	The aim of this research was to address the design of an alternative
	software architecture based on new paradigms. METHODS: State-of-the
	art component, middleware and agent technology were deployed to design
	and implement a software architecture for ICU data flow management.
	RESULTS: An advanced multi-layer architecture for efficient data
	flow management in the ICU has been designed. The architecture is
	both generic and scalable, which means that it neither depends on
	a particular ICU nor on the deployed monitoring devices. Automatic
	device detection and Graphical User Interface generation are taken
	into account. Furthermore, a demonstrator has been developed as a
	proof that the proposed conceptual software architecture is feasible
	in practice. The core of the new architecture consists of Bed Decision
	Agents (BDAs). The introduction of BDAs, who perform specific dedicated
	tasks, improves the adaptability and maintainability of the future
	very complex IC-ISs. CONCLUSIONS: A software architecture, based
	on component, middleware and agent technology, is feasible and offers
	important advantages over the currently used two-layer software architecture.},
  doi = {10.1267/METH03010079},
  institution = {Department of Intensive Care, Ghent University Hospital, Belgium.
	johan.decruyenaere@rug.ac.be},
  keywords = {Computer Systems; Hospital Information Systems; Intensive Care Units;
	Software Design},
  owner = {user},
  pii = {03010079},
  pmid = {12695799},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1267/METH03010079}
}

@INPROCEEDINGS{delanote_using_2008,
  author = {D. Delanote and S. Van Baelen and W. Joosen and Y. Berbers},
  title = {Using AADL to Model a Protocol Stack},
  booktitle = {Engineering of Complex Computer Systems, 2008. ICECCS 2008. 13th
	IEEE International Conference on},
  year = {2008},
  pages = {277--281},
  abstract = {In recent trends, the Architecture Analysis and Design Language (AADL)
	has received increasing attention from safety-critical software development
	industries. Specific about the AADL is its strong syntactic and semantic
	support for the description of both hardware and software architectures.
	Considering the existing range of software architectures, we study
	the support AADL offers for the description of software architecture.
	As a case study we use an implementation of a UDP/IP protocol stack.
	Based on our experiences, our position is that a number of abstract
	concepts, e.g. a generic component concept, are missing in the AADL
	to make it well-suited for the high- level description of software
	architecture.},
  doi = {10.1109/ICECCS.2008.12},
  keywords = {AADL,ADL,Architecture Analysis and Design Language,generic component
	concept,protocol stack,UDP/IP protocol stack},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{demillo_progress_1991,
  author = {R.A. DeMillo},
  title = {Progress toward automated software testing},
  booktitle = {Software Engineering, 1991. Proceedings., 13th International Conference
	on},
  year = {1991},
  pages = {180â€•183},
  abstract = {Mothra is an integrated environment for automated software validation.
	Using Mothra, a tester can create and execute test cases, measure
	test case adequacy, determine input-output correctness, locate and
	remove faults or bugs, and control and document the test. There are
	no size constraints built into the design of the environment. A goal
	of the research has been to exploit computational opportunities in
	each of the major subtasks of software validation. The author has
	attempted to reduce test case generation, measurement of the effectiveness
	of test cases, input-output correctness checking, and debugging to
	one or more computational metaphors and to design the appropriate
	algorithms. The result is a system where there is a high degree of
	control over the apparent cost of testing},
  doi = {10.1109/CSCWD.2006.253060},
  keywords = {automated software testing,automated software validation,bug location,bug
	removal,fault location,fault removal,input-output correctness,integrated
	environment,Mothra,program testing,program verification,programming
	environments,test cases,test documentation},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{lei_deng_iscf:semantic_2008,
  author = {Lei Deng and Jian Wu and Zhengguo Hu},
  title = {ISCF: A Semantic Web Service Composition Framework Based on OAA},
  booktitle = {Grid and Pervasive Computing Workshops, 2008. GPC Workshops '08.
	The 3rd International Conference on},
  year = {2008},
  pages = {250--255},
  abstract = {Through providing an execution environment for semantic web services,
	SWSF (Semantic Web Service Framework) can facilitate automatic web
	service delivery, discovery, selection, invoking, composition and
	interoperation. However, each of those ongoing approaches does not
	satisfy all basic requirements of SWSF, e.g. OWL-S (Ontology Language
	of Web Service) does not offer the mechanism for automatic web service
	tasks' execution and WSMO can not describe the composition of web
	services in a structural way yet. So, in order to provide a fully-
	fledged SWSF, we present the ISCF (Integrated Service Composition
	Framework) which is based on the OAA (Open Agent Architecture ).
	We describe the technical characters and design objectives of ISCF
	from global and detailed view. Comparing with other SWSFs, ISCF has
	more extensible and autonomous distributed architecture, and has
	more complete and effective web service automatization. In addition,
	ISCF provide some useful tools to help users design semantic web
	services and manage ISCF system.},
  doi = {10.1109/NCM.2008.37},
  keywords = {integrated service composition framework,ISCF,OAA,ontology language
	of Web service,open agent architecture,OWL-S,semantic Web service
	composition framework,Semantic Web Service Framework,Semantic Web
	Services,Web Service Automatization},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{van_deursen_symphony:_2004,
  author = {A. van Deursen and C. Hofmeister and R. Koschke and L. Moonen and
	C. Riva},
  title = {Symphony: view-driven software architecture reconstruction},
  booktitle = {Software Architecture, 2004. WICSA 2004. Proceedings. Fourth Working
	IEEE/IFIP Conference on},
  year = {2004},
  pages = {122--132},
  abstract = {Authentic descriptions of a software architecture are required as
	a reliable foundation for any but trivial changes to a system. Far
	too often, architecture descriptions of existing systems are out
	of sync with the implementation. If they are, they must be reconstructed.
	There are many existing techniques for reconstructing individual
	architecture views, but no information about how to select views
	for reconstruction, or about process aspects of architecture reconstruction
	in general. In this paper we describe view-driven process for reconstructing
	software architecture that fills this gap. To describe Symphony,
	we present and compare different case studies, thus serving a secondary
	goal of sharing real-life reconstruction experience. The Symphony
	process incorporates the state of the practice, where reconstruction
	is problem-driven and uses a rich set of architecture views. Symphony
	provides a common framework for reporting reconstruction experiences
	and for comparing reconstruction approaches. Finally, it is a vehicle
	for exposing and demarcating research problems in software architecture
	reconstruction.},
  keywords = {Symphony,view-driven software architecture reconstruction},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{devanbu_research_1997,
  author = {P. Devanbu and S. Stubblebine},
  title = {Research directions for automated software verification: using trusted
	hardware},
  booktitle = {Automated Software Engineering, 1997. Proceedings., 12th IEEE International
	Conference},
  year = {1997},
  pages = {274â€•279},
  abstract = {Service providers hosting software on servers at the request of content
	providers need assurance that the hosted software has no undesirable
	properties. This problem applies to browsers which host applets,
	networked software which can host software agents, etc. The hosted
	software's properties are currently verified by testing and/or verification
	processes by the hosting computer. This increases cost, causes delay,
	and leads to difficulties in version control. By furnishing content
	providers with a physically secure computing device with an embedded
	certified private key, such properties can be verified and/or enforced
	by the secure computing device at the content provider's site; the
	secure device can verify such properties, statically whenever possible,
	and by inserting checks into the executable binary when necessary.
	The resulting binary is attested by a trusted signature, and can
	be hosted with confidence. The position paper is a preliminary report
	that outlines scientific and engineering goals in this project },
  doi = {10.1109/AISIG.1989.47312},
  keywords = {applets,automated software verification,browsers,checks,computer networks,configuration
	management,content providers,embedded certified private key,engineering
	goals,executable binary,hosted software,hosting computer,networked
	software,physically secure computing device,program testing,program
	verification,scientific goals,security of data,servers,service providers,software
	agents,testing,trusted hardware,trusted signature,version control},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{dhavachelvan_complexity_2005,
  author = {P. Dhavachelvan and G.V. Uma},
  title = {Complexity measures for software systems : towards multi-agent based
	software testing},
  booktitle = {Intelligent Sensing and Information Processing, 2005. Proceedings
	of 2005 International Conference on},
  year = {2005},
  pages = {359â€•364},
  abstract = {Bringing together agents and other fields of software engineering
	might be difficult, as the advantages of agent technology are still
	not widely recognized. Effectiveness claims of agent-oriented software
	engineering are based upon the strategies for addressing complex
	systems. Agent technologies facilitate the automated software testing
	by virtue of their high-level decomposition, independency and parallel
	activation. The informal interpretations of qualitative agent theories
	are not sufficient to distinguish agent-based approaches from other
	approaches in software testing. In this paper, we do not just described
	the agent-based approach in software testing, also developed an evaluation
	framework for agent-oriented approach in software testing and proposed
	a multi-agent system for software testing. This paper therefore provides
	a timely summary and enhancement of agent theory in software testing,
	which motivates recent efforts in adapting concepts and methodologies
	for agent-oriented software testing (AOST) to complex systems, which
	has not previously done. The 'multi-modal' approach proposed here
	is to offer a definition for encompassing to cover the software testing
	phenomena, based on agents, at the preliminary level, yet sufficiently
	tight that it can rule out complex systems that are clearly not agent-based.},
  doi = {10.1109/ISDPE.2007.104},
  keywords = {agent-oriented software testing,automated software testing,multiagent
	based software testing,multi-agent systems,multimodal approach,object-oriented
	programming,program testing,software engineering,software metrics,software
	system complexity measures},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{dhungana_integrated_2006,
  author = {Deepak Dhungana},
  title = {Integrated Variability Modeling of Features and Architecture in Software
	Product Line Engineering},
  booktitle = {Automated Software Engineering, 2006. ASE '06. 21st IEEE/ACM International
	Conference on},
  year = {2006},
  pages = {327â€•330},
  abstract = {Existing methods and tools supporting product line variability management
	typically emphasize either the feature or the architecture level.
	There have been attempts to combine these aspects, but no widely
	accepted method is available so far. This paper reports ongoing research
	in designing and implementing product line variability models, where
	the focus lies in treating features and architectural elements as
	parts of an integrated model. The research is carried together with
	our industry partner Siemens VAI},
  doi = {10.1109/ASE.2006.42},
  isbn = {1527-1366},
  keywords = {formal specification,integrated variability modeling,product line
	variability management,software architecture,software product line
	engineering,software reusability},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{diaz_automated_2003,
  author = {E. Diaz and J. Tuya and R. Blanco},
  title = {Automated software testing using a metaheuristic technique based
	on Tabu search},
  booktitle = {Automated Software Engineering, 2003. Proceedings. 18th IEEE International
	Conference on},
  year = {2003},
  pages = {310â€•313},
  abstract = {The use of techniques for automating the generation of software test
	cases is very important as it can reduce the time and cost of this
	process. The latest methods for automatic generation of tests use
	metaheuristic search techniques, i.e. genetic algorithms and simulated
	annealing. There is a great deal of research into the use of genetic
	algorithms to obtain a specific coverage in software testing but
	there is none using the metaheuristic Tabu search technique. In this
	paper, we explain how we have created an efficient testing technique
	that combines Tabu search with Korel chaining approach. Our technique
	automatically generates test data in order to obtain branch coverage
	in software testing.},
  doi = {10.1109/ASE.1999.802086},
  isbn = {1527-1366 },
  keywords = {automated software testing,automatic test data generation,automatic
	test generation,branch coverage,genetic algorithms,heuristic programming,Korel
	chaining approach,metaheuristic search techniques,metaheuristic technique,program
	testing,search problems,simulated annealing,software engineering,Tabu
	search},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{dinger_semantic_2006,
  author = {Ulrich Dinger and Roy Oberhauser and Christian Reichel},
  title = {A Semantic Web Services Approach Towards Automated Software Engineering},
  booktitle = {Web Services, 2006. ICWS '06. International Conference on},
  year = {2006},
  pages = {935â€•938},
  abstract = {The growing complexity of software, combined with demands for greater
	productivity and shorter cycles, creates an increasing demand for
	more automation and integration within the software engineering (SE)
	domain. When viewed holistically, the heterogeneous nature, implicit
	feature cross-dependencies, and manual administration of the toolchain
	infrastructure results in unnecessary complexity, inefficiencies,
	and reduced reliability for the SE process. A common infrastructure
	is missing that provides an interoperable and distributed tool environment,
	addresses feature dependency selection, and automates toolchain workflow
	composition and execution. To address these challenges, this paper
	explores the practicality of a unifying semantic Web services approach
	towards automated software engineering (SWS-ASE)},
  doi = {10.1109/ICWS.2006.12},
  keywords = {automated software engineering,automates toolchain workflow composition,distributed
	tool environment,feature dependency selection,semantic Web,semantic
	Web services,software reliability,toolchain infrastructure,Web services},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{dix_automated_2002,
  author = {M. Dix and H.D. Hofmann},
  title = {Automated software robustness testing - static and adaptive test
	case design methods},
  booktitle = {Euromicro Conference, 2002. Proceedings. 28th},
  year = {2002},
  pages = {62â€•66},
  abstract = {Testing is essential in the development of any software system. Testing
	is required to assess a system's functionality and quality of operation
	in its final environment. This is especially of importance for systems
	being assembled from many self-contained software components. In
	this article, we focus on automated testing of software component
	robustness, which is a component's ability to handle invalid input
	data or environmental conditions. We describe how large numbers of
	test cases can effectively and automatically be generated from small
	sets of test values. However, there is a great demand on ways to
	efficiently reduce this mass of test cases as actually executing
	them on a data processing machine would be too time consuming and
	expensive. We discuss static analytic methods for test case reduction
	and some of the disadvantages they bring. Finally a more intelligent
	and efficient approach is introduced, the Adaptive Test Procedure
	for Software Robustness Testing developed at ABB Corporate Research
	in Ladenburg. Along with these discussions the need for intelligent
	test approaches is illustrated by the Ballista methodology for automated
	robustness testing of software component interfaces. An object-oriented
	approach based on parameter data types rather than component functionality
	essentially eliminates the need for function-specific test scaffolding.},
  doi = {10.1109/EURMIC.2002.1046134},
  isbn = {1089-6503 },
  keywords = {ABB Corporate Research,adaptive test case design,Adaptive Test Procedure
	for Software Robustness Testing,automated software robustness testing,automatic
	testing,Ballista methodology,data structures,object-oriented approach,object-oriented
	programming,parameter data types,program diagnostics,program testing,self-contained
	software components,software component interfaces,static analytic
	methods,static test case design,test case reduction},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{dong_cots_2005,
  author = {Jing Dong and Sheng Yang and Lawrence Chung and Paulo Alencar and
	Donald Cowan},
  title = {A COTS architectural component specification stencil for selection
	and reasoning},
  booktitle = {Proceedings of the second international workshop on Models and processes
	for the evaluation of off-the-shelf components},
  year = {2005},
  pages = {1â€•4},
  address = {St. Louis, Missouri},
  publisher = {ACM},
  abstract = {Reusing commercial-off-the-shelf (COTS) components may reduce cost
	and time-to-market. It may significantly improve software productivity.
	However, the selection and assessment of COTS components are still
	a challenge task. It is hard to find the right components that exactly
	fit into the requirements. The selection processes are in general
	ad-hoc. Wrong choice of COTS components may compromise the benefits
	from reusing these components since the chosen component may mismatch
	with other components and the environment. In this position paper,
	we advocate a more detailed architectural specification stencil which
	may help on the component selection and mismatch detection. The architectural
	specification of a COTS component is encoded in XML so that searching
	components can be automated. In addition, inconsistencies and mismatches
	among components can be detected.},
  doi = {10.1145/1082948.1082959},
  isbn = {1-59593-129-5},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1082948.1082959\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{du_passive_1998,
  author = {W.Y. Du and S.L. Dickerson},
  title = {Passive component inspection using machine vision},
  booktitle = {Multichip Modules and High Density Packaging, 1998. Proceedings.
	1998 International Conference on},
  year = {1998},
  pages = {74â€•79},
  abstract = {Passive electronic components now make up 50 to 90\% of all components
	on circuit boards and represent more than one-fifth of the board
	cost. Many of these passive components have been 100\% inspected
	by the manufacturers for electrical properties, but not for mechanical
	properties. Most manufacturers still measure small samples with micrometers
	and calipers to verify overall dimensions, although machine vision
	is beginning to be used for mechanical inspection. In this paper,
	we investigate machine vision as a technique to enable 100\% inspection
	of such components including properties of size, edge straightness
	and smoothness, and surface defects for both the component body and
	terminations. In addition, machine vision is used to derive the component
	orientation and location. This may have application to the bulk feeding
	of such components, which is a subject of current research },
  doi = {10.1109/ICMCM.1998.670758},
  keywords = {assembling,automatic optical inspection,board cost,bulk component
	feeding,calipers,capacitors,circuit boards,component body,component
	edge straightness,component location,component orientation,component
	size,component smoothness,component surface defects,component terminations,computer
	vision,electrical properties,fault location,inductors,inspection,machine
	vision,mechanical inspection,mechanical properties,micrometers,passive
	component inspection,passive components,passive electronic components,position
	measurement,printed circuit manufacture,printed circuit testing,production
	testing,resistors,surface topography},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{dulz_matelo_2003,
  author = {W. Dulz and Fenhua Zhen},
  title = {MaTeLo - statistical usage testing by annotated sequence diagrams,
	Markov chains and TTCN-3},
  booktitle = {Quality Software, 2003. Proceedings. Third International Conference
	on},
  year = {2003},
  pages = {336â€•342},
  abstract = {In this paper, we present a general framework for testing time-critical
	systems and software, as it is proposed in the European IST project
	MaTeLo. The main focus is on automatically generating a MCUM (Markov
	chain usage model) starting from an FDT (formal description technique)
	description in order to derive TTCN-3 (testing and test control notation
	version 3) compatible test case definitions. Our approach is a combination
	of statistical usage testing based on a given MCUM and specification-based
	testing that is using FDT inputs. Within MaTeLo, special attention
	is given to international standardized FDT notations, specifically
	ITU-T MSC (message sequence chart). In addition, we make use of annotations
	to specify selected non-functional requirements to support the automated
	software testing of the real time systems. We also defined an XML-based
	representation format called MCML (Markov chain Markup Language)
	to build a common interface between various parts of the MaTeLo tool
	set.},
  doi = {10.1109/QSIC.2003.1319090},
  keywords = {annotated sequence diagram,automated software testing,FDT,formal description,formal
	specification,ITU-T,Markov chain Markup Language,Markov chain usage
	model,Markov processes,MaTeLo,MCML,MCUM,message sequence chart,MSC,nonfunctional
	requirement,program testing,real time system,real-time systems,software
	tools,specification-based testing,statistical testing,statistical
	usage testing,system testing,test case definition,testing and test
	control notation version 3,time-critical system,TTCN-3,XML,XML-based
	representation format},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{durfee_agent_1997,
  author = {E.H. Durfee and D.L. Kiskis and W.P. Birmingham},
  title = {The agent architecture of the University of Michigan Digital Library},
  journal = {Software Engineering. IEE Proceedings- [see also Software, IEE Proceedings]},
  year = {1997},
  volume = {144},
  pages = {61--71},
  number = {1},
  abstract = {The University of Michigan Digital Library (UMDL) architecture encapsulates
	the many functionalities required in a digital library as a population
	of modular, goal-oriented, specialised `agents'. These agents participate
	in markets for exchanging goods and services, and team their abilities
	to compose complex services. Realising the UMDL agent architecture
	requires us to provide sound mechanisms to encapsulate functions
	as agents, protocols to support the evolution of teams and agent
	interactions through markets, and protocols to enable interoperability
	among library agents that are teamed. The software-engineering aspects
	of our effort (the tools, techniques and experiences gained) are
	the focus of this paper},
  doi = {10.1109/TAI.1997.632297},
  issn = {1364-5080},
  keywords = {academic libraries,agent architecture,agent interactions,agent teams,cooperative
	systems,goal-oriented agents,library agent interoperability,library
	automation,object oriented programming,protocols,software-engineering,UMDL,University
	of Michigan Digital Library},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{ebeling_placement_1995,
  author = {C. Ebeling and L. McMurchie and S.A. Hauck and S. Burns},
  title = {Placement and routing tools for the Triptych FPGA},
  journal = {Very Large Scale Integration (VLSI) Systems, IEEE Transactions on},
  year = {1995},
  volume = {3},
  pages = {473â€•482},
  number = {4},
  abstract = {Field-programmable gate arrays (FPGAs) are becoming an increasingly
	important implementation medium for digital logic. One of the most
	important keys to using FPGAs effectively is a complete, automated
	software system for mapping onto the FPGA architecture. Unfortunately,
	many of the tools necessary require different techniques than traditional
	circuit implementation options, and these techniques are often developed
	specifically for only a single FPGA architecture. In this paper we
	describe automatic mapping tools for Triptych, an FPGA architecture
	with improved logic density and performance over commercial FPGAs.
	These tools include a simulated-annealing placement algorithm that
	handles the routability issues of fine-grained FPGAs, and an architecture-adaptive
	routing algorithm that can easily be retargeted to other FPGAs. We
	also describe extensions to these algorithms for mapping asynchronous
	circuits to Montage, the first FPGA architecture to completely support
	asynchronous and synchronous interface applications },
  doi = {10.1109/32.268919},
  issn = {1063-8210},
  keywords = {architecture-adaptive routing algorithm,asynchronous circuits,automated
	software system,automatic mapping tools,circuit layout CAD,digital
	logic,field programmable gate arrays,fine-grained circuits,logic
	CAD,logic density,Montage,network routing,placement tools,routing
	tools,simulated annealing,simulated-annealing placement,Triptych
	FPGA,VLSI},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{edwards_prevention_2007,
  author = {D. Edwards and S. Simmons and N. Wilde},
  title = {Prevention, Detection and Recovery from Cyber-Attacks Using a Multilevel
	Agent Architecture},
  booktitle = {System of Systems Engineering, 2007. SoSE '07. IEEE International
	Conference on},
  year = {2007},
  pages = {1--6},
  abstract = {Intelligent software agents offer great potential for improving the
	operation and response of power grids. These agents are networked
	applications that could be vulnerable to cyber attacks. The goal
	of this research is to prevent known attacks, and to reduce or eliminate
	the consequences of successful attacks. A multilevel security architecture
	is presented that contains small, verifiable agents at each level
	with a well defined duty. Layers are designed to intercept dangerous
	or malformed information before it has a chance to damage the computational
	agents. Cross-monitoring ensures that any malfunctioning agent in
	the multilayer system is detected and the consequences of the attack
	are prevented or corrected. At the lowest level of the architecture
	are the computational agents. These agents are replicated to provide
	redundancy. Each replicate is mutated using a novel technique to
	prevent an attack from succeeding. Our mutation engine alters the
	program without changing the functionality.},
  doi = {10.1109/WI-IATW.2007.76},
  keywords = {computational agents,cyber attack,grid computing,intelligent software
	agent,multilayer system,multilevel agent architecture,multilevel
	security,multilevel security architecture,power distribution control,power
	grids,program mutation,security model,software agent security},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{edwards_integrating_2004,
  author = {George T. Edwards and Douglas C. Schmidt and Aniruddha Gokhale},
  title = {Integrating publisher/subscriber services in component middleware
	for distributed real-time and embedded systems},
  booktitle = {Proceedings of the 42nd annual Southeast regional conference},
  year = {2004},
  pages = {171â€•176},
  address = {Huntsville, Alabama},
  publisher = {ACM},
  abstract = {Although component-based software development has widespread acceptance
	in the enterprise business and desktop application domains, developers
	of distributed real-time and embedded (DRE) systems have encountered
	limitations with the available component middleware platforms, such
	as the CORBA Component Model (CCM) and the Java 2 Enterprise Edition
	(J2EE). These limitations often preclude developers of DRE systems
	from fully exploiting the benefits of component software. In particular,
	component middleware platforms lack standards-based publisher/subscriber
	communication mechanisms that support key quality-of-service (QoS)
	requirements, such as low latency, bounded jitter, and end-to-end
	operation priority propagation. QoS-enabled publisher/subscriber
	services are available in object middleware platforms, such as Real-time
	CORBA, but such services have not been integrated into component
	middleware due to a number of development and configuration challenges.This
	paper provides three contributions to the integration of publisher/subscriber
	services in component middleware. First, we outline key challenges
	associated with integrating publisher/subscriber services into component
	middleware. Second, we describe a methodology for resolving these
	challenges based on software patterns. Third, we describe a pattern-oriented
	component middleware platform that we have developed to integrate
	publisher/subscriber services into component middleware applications.},
  doi = {10.1145/986537.986577},
  isbn = {1-58113-870-9},
  keywords = {component middleware,corba component model,model-based systems,real-time
	event service},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=986537.986577\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{edwards_contract-checking_2004,
  author = {S.H. Edwards and M. Sitaraman and B.W. Weide and E. Hollingsworth},
  title = {Contract-checking wrappers for C++ classes},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2004},
  volume = {30},
  pages = {794â€•810},
  number = {11},
  abstract = {Two kinds of interface contract violations can occur in component-based
	software: A client component can fail to satisfy a requirement of
	a component it is using, or a component implementation can fail to
	fulfill its obligations to the client. The traditional approach to
	detecting and reporting such violations is to embed assertion checks
	into component source code, with compile-time control over whether
	they are enabled. This works well for the original component developers,
	but it fails to meet the needs of component clients who do not have
	access to source code for such components. A wrapper-based approach,
	in which contract checking is not hard-coded into the underlying
	component but is "layered" on top of it, offers several relative
	advantages. It is practical and effective for C++ classes. Checking
	code can be distributed in binary form along with the underlying
	component, it can be installed or removed without requiring recompilation
	of either the underlying component or the client code, it can be
	selectively enabled or disabled by the component client on a per-component
	basis, and it does not require the client to have access to any special
	tools (which might have been used by the component developer) to
	support wrapper installation and control. Experimental evidence indicates
	that wrappers in C++ impose-modest additional overhead compared to
	inlining assertion checks.},
  doi = {10.1109/TSE.2004.80},
  issn = {0098-5589},
  keywords = {65,assertion checker,binary component,binary components,C++ class,C++
	language,class invariant,class invariants,coding techniques,component
	source code,component-based software,contract-checking wrapper,data
	encapsulation,debugging aids,design by contract,formal specification,formal
	verification,Index Terms- Assertion checkers,object-oriented programming,postconditions,preconditions,program
	debugging,specification.,wrapper-based approach},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{elleuch_software_2007,
  author = {N. Elleuch and A. Khalfallah and S. Ben Ahmed},
  title = {Software Architecture in Model Driven Architecture},
  booktitle = {Computational Intelligence and Intelligent Informatics, 2007. ISCIII
	'07. International Symposium on},
  year = {2007},
  pages = {219--223},
  abstract = {The development of software architecture is complex due to the absence
	of a standard way that would lead the generation of software architecture
	artifacts. This paper proposes a method that would allow to derive
	the software architecture of any system based on its analysis model.
	For that purpose, we introduce a new layer to the model driven architecture
	(MDA) that takes into account the software architecture. The analysis
	model is termed the architecture independant model (AIM), which is
	compliant to the UML 2.0 metamodel. In addition, we consider the
	software architecture in the architecture specific model (ASM), which
	complies to the defined architectural metamodel. The mapping of AIM
	into ASM is conducted by using the both metamodels.},
  doi = {10.1109/ISCIII.2007.367392},
  keywords = {architectural metamodel,architecture independent model,architecture
	specific model,metacomputing,model driven architecture,software architecture
	artifacts,UML 2.0 metamodel},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{ellis_focusingsoftware_2002,
  author = {H.J.C. Ellis and J.C. McKim},
  title = {Focusing on software engineering education: a graduate certificate
	in software engineering},
  booktitle = {Frontiers in Education, 2002. FIE 2002. 32nd Annual},
  year = {2002},
  volume = {2},
  pages = {F2Gâ€•16--F2G-21 vol.2},
  abstract = {The creation and evolution of a graduate-level certificate program
	in software engineering is presented in this paper. The software
	engineering certificate offered by Rensselaer Polytechnic Institute
	(RPI) is a four-course sequence of graduate courses in the area of
	software engineering and is intended to provide students with a focused
	understanding of the principles of software engineering and their
	application to software environments. The paper discusses the motivation
	for the original construction of the Software Engineering certificate
	program by the Rensselaer at Hartford (RH) branch of RPI and discusses
	the expected audience for the program. The program itself is detailed
	including an overview of the student audience, an explanation of
	the prerequisites of the program, and descriptions of the graduate
	courses in the certificate. The paper describes recent changes to
	the content of the Software Engineering certificate and discusses
	the factors that have influenced the certificate as it has evolved.},
  doi = {10.1109/FIE.2002.1158175},
  isbn = {0190-5848 },
  keywords = {computer science education,four-course sequence,graduate courses,graduate
	level certificate program,Rensselaer Polytechnic Institute,software
	engineering,software engineering certificate,Software Engineering
	certificate,software engineering education,student audience},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{elwasif_component_2007,
  author = {Wael R. Elwasif and David E. Bernholdt and Lee A. Berry and Donald
	B. Batchelor},
  title = {Component framework for coupled integrated fusion plasma simulation},
  booktitle = {Proceedings of the 2007 symposium on Component and framework technology
	in high-performance and scientific computing},
  year = {2007},
  pages = {93â€•100},
  address = {Montreal, Quebec, Canada},
  publisher = {ACM},
  abstract = {Successful simulation of the complex physics that affect magnetically
	confined fusion plasma remains an important target milestone towards
	the development of viable fusion energy. Major advances in the underlying
	physics formulations, mathematical modeling, and computational tools
	and techniques are needed to enable a complete fusion simulation
	on the emerging class of large scalecapability parallel computers
	that are coming on-line in the next few years. Several pilot projects
	are currently being undertaken to explore different (partial) code
	integration and coupling problems, and possible solutions that may
	guide the larger integration endeavor. In this paper, we present
	the design and implementation details of one such project, a component
	based approach to couple existing codes to model the interaction
	between high power radio frequency (RF) electromagnetic waves, and
	magnetohydrodynamics (MHD) aspects of the burning plasma. The framework
	and component design utilize a light coupling approach based on high
	level view of constituent codes that facilitates rapid incorporation
	of new components into the integrated simulation framework. The work
	illustrates the viability of the light coupling approach to better
	understand physics and stand-alone computer code dependencies and
	interactions, as a precursor to a more tightly coupled integrated
	simulation environment.},
  doi = {10.1145/1297385.1297401},
  isbn = {978-1-59593-867-1},
  keywords = {components,coupled simulation,framework,fusion},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1297385.1297401\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{emmerich_distributed_2002,
  author = {Wolfgang Emmerich},
  title = {Distributed component technologies and their software engineering
	implications},
  booktitle = {Proceedings of the 24th International Conference on Software Engineering},
  year = {2002},
  pages = {537â€•546},
  address = {Orlando, Florida},
  publisher = {ACM},
  abstract = {In this state of the art report, we review advances in distributed
	component technologies, such as the Enterprise Java Beans specification
	and the CORBA Component Model. We assess the state of industrial
	practice in the use of distributed components. We show several architectural
	styles for whose implementation distributed components have been
	used successfully. We review the use of iterative and incremental
	development processes and the notion of model driven architecture.
	We then assess the state of the art in research into novel software
	engineering methods and tools for the modelling, reasoning and deployment
	of distributed components. The open problems identified during this
	review result in the formulation of a research agenda that will contribute
	to the systematic engineering of distributed systems based on component
	technologies.},
  doi = {10.1145/581339.581405},
  isbn = {1-58113-472-X},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=581339.581405\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{eusterbrock_context-aware_2004,
  author = {J. Eusterbrock},
  title = {Context-aware code certification},
  booktitle = {Automated Software Engineering, 2004. Proceedings. 19th International
	Conference on},
  year = {2004},
  pages = {358â€•361},
  abstract = {One challenging issue in automated software engineering is to ensure
	safety of software execution in changing contexts. In such a scenario,
	various users, the "code consumers", download an application from
	a remote server and execute it in their heterogeneous environments.
	In this paper, a generic meta-level framework (C3) that allows easy
	adaptation to different contexts for automated safety certification
	of annotated programs is presented. Context-dependent safety requirements
	are decoupled from the program specification. The Floyd-Hoarc verification
	method is extended, and a verification condition generator for deriving
	generic safety preconditions in terms of generic safety predicates
	is devised and implemented. The generated safety conditions are simplified
	and transformed into a negated normal form. This translates the safety
	verification task into the equivalent task to disprove the existence
	of a counter example in relation to the selected context. One distinguishing
	feature of C3 is that safety contexts are meta-level interface specifications.
	Lifting maps the proof tasks onto the meta-level. Context-dependent
	safety checking is performed by meta-level reasoning and constraint-solving.
	A proof of concept implementation was applied to automatically certify
	absence of context-specific runtime errors and to identify bugs in
	several cases},
  doi = {10.1109/ASE.2004.1342722},
  isbn = {1068-3062},
  keywords = {annotated programs,automated safety certification,automated software
	engineering,bug identification,constraint solving,context-aware code
	certification,context-dependent safety checking,context-dependent
	safety requirements,context-specific runtime errors,Floyd-Hoarc verification,formal
	specification,metalevel interface specifications,metalevel reasoning,program
	debugging,program diagnostics,program specification,program verification,safety
	verification,safety-critical software,verification condition generator},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Faheem2005,
  author = {Hossam M Faheem},
  title = {A design of multiagent-based framework for volume image construction
	and analysis.},
  journal = {Biomed Sci Instrum},
  year = {2005},
  volume = {41},
  pages = {259--264},
  abstract = {This paper describes a design of a multiagent-based system that can
	be used to manage the acquisition and analysis of ultrasonograph
	images. The major concept is to design a management framework consisting
	of multiple intelligent agents to direct the ultrasonograph image
	acquisition and analysis operations carried out using a high-speed
	bit-parallel architecture efficiently as well as to allow for the
	construction of 3D images from 2D ones. Volume image operations need
	reactivity, autonomy, and intelligence of software. Therefore, agents
	can play an important role in enhancing the overall operation of
	medical image analysis. The system suggests a set of image analysis
	operations including smoothing, noise removal, and enhancing techniques.
	These operations will be implemented using parallel processing architectures
	while the management framework will consist of different agent types
	such as: simple reflex agents, agents that keep track of the world,
	goal-based agents, and utility-based agents. These agents interact
	with each other and exchange data among themselves in order to achieve
	a comprehensive speed in performing the volume image construction
	operations. Guided with the fact that the agent consists of program
	and architecture, the system deploys parallel processing architectures
	to implement the image analysis operations. The system is considered
	a step towards a complete multiagent-based framework for medical
	image acquisition and analysis.},
  institution = {Department of Computer Science, Ain Shams University, Faculty of
	Computer and Information Science, Cairo.},
  keywords = {Algorithms; Artificial Intelligence; Computer Communication Networks;
	Computer Graphics; Computing Methodologies; Image Enhancement, methods;
	Image Interpretation, Computer-Assisted, methods; Imaging, Three-Dimensional,
	methods; Information Storage and Retrieval, methods; Numerical Analysis,
	Computer-Assisted; Pattern Recognition, Automated, methods; Signal
	Processing, Computer-Assisted; Software; Ultrasonography, methods},
  owner = {user},
  pmid = {15850115},
  timestamp = {2008.10.19}
}

@ARTICLE{Falasconi1997,
  author = {S. Falasconi and G. Lanzola and M. Stefanelli},
  title = {An ontology-based multi-agent architecture for distributed health-care
	information systems.},
  journal = {Methods Inf Med},
  year = {1997},
  volume = {36},
  pages = {20--29},
  number = {1},
  month = {Jan},
  abstract = {Managing patients in a shared-care context is a knowledge-intensive
	activity. To support cooperative work in medical care, computer technology
	should both augment the capabilities of individual specialists and
	enhance their ability of interacting with each other and with computational
	resources. Thus, a major shift is needed from centralized first generation
	health-care information systems to distributed environments composed
	of several interconnected agents, cooperating in maintaining a full
	track of the patient clinical history and supporting health-care
	providers in all the phases of the patient-management process. This
	paper outlines a general methodology to make architectural choices
	while designing or integrating new software components into a distributed
	health-care information system. A particular stress is laid on the
	specification of shared conceptual models, or ontologies, providing
	agents committing to them with the common semantic foundation required
	for effective interoperation.},
  institution = {Dept. of Informatics and Systems Science, University of Pavia, Italy.
	sabina@ipvaimed6.unipv.it},
  keywords = {Computer Communication Networks; Computer Systems; Information Systems;
	Software},
  owner = {user},
  pmid = {9121371},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{fang_template_2006,
  author = {Min Fang and Jing Ying and Minghui Wu},
  title = {A Template Engineering Based Framework for Automated Software Development},
  booktitle = {Computer Supported Cooperative Work in Design, 2006. CSCWD '06. 10th
	International Conference on},
  year = {2006},
  pages = {1â€•6},
  abstract = {This paper presents a framework for automated software development:
	Xauto, which includes four key elements: layer language, template,
	framework and component. Based on software system patterns, an automatic
	development process is supported and realized by template engineering
	and the mapping of layer languages. The authors expound the mapping
	patterns of Xauto framework in the three aspects of model, view and
	controller, and demonstrate them by the relevant template examples.
	The research on Xauto framework solidifies the mature solutions dealing
	with problems in certain domains and makes it reusable, and facilitates
	the change of software development mode from personal workshop to
	template engineering. This shift, therefore, will promote the automatic
	degree of software development and make it more efficient},
  doi = {10.1109/ICSE.1991.130640},
  keywords = {automated software development,Automated Software Development,automatic
	programming,Code Generation,object-oriented methods,object-oriented
	programming,Pattern,software reusability,software system patterns,Template,Template
	Engineering,template engineering based framework,Xauto framework},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{fayman_system_1996,
  author = {J.A. Fayman and E. Rivlin and H.I. Christensen},
  title = {A system for active vision driven robotics},
  booktitle = {Robotics and Automation, 1996. Proceedings., 1996 IEEE International
	Conference on},
  year = {1996},
  volume = {3},
  pages = {1986--1992 vol.3},
  abstract = {In this paper, we present an agent architecture/active vision research
	tool called the Active Vision Shell (AV-shell). The AV-shell can
	be viewed as a programming framework for expressing perception and
	action routines in the context of situated robotics. The AV-shelf
	is a powerful interactive C-shell style interface providing many
	capabilities important in an agent architecture such as the ability
	to combine perceptive capabilities of active vision with capabilities
	provided by other robotic devices, the ability to interact with a
	wide variety of active vision devices, a set of image routines and
	the ability to compose the routines into continuously running perception
	action processes. Finally, we present an application example of AV-shell},
  doi = {10.1109/ROBOT.1996.506163},
  keywords = {active vision,active vision driven robotics,Active Vision Shell,agent
	architecture,AV-shell,cooperative systems,image routines,interactive
	C-shell,interactive systems,perceptive capabilities,robot vision},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{fekete_agent-based_2008,
  author = {K. Fekete and S. Nikolovski and D. Puzak and G. Slipac and H. Keko},
  title = {Agent-based modelling application possibilities for Croatian electricity
	market simulation},
  booktitle = {European Electricity Market, 2008. EEM 2008. 5th International Conference
	on},
  year = {2008},
  pages = {1--6},
  abstract = {Croatia is today in the first phase of restructuring and opening its
	electricity market. At this moment, electricity market is open to
	eligible customers only. Eligible customers are able to choose their
	supplier and currently these are all non-households. Starting from
	July 1st, 2008 all customers, including households, will become eligible
	and the market will hence be fully open. Therefore, a suitable method
	for Croatian market simulation is needed. This paper presents possibilities
	of using an agent-based model to simulate the Croatian electricity
	market. Agent-based models represent each market participant as an
	independent software agent. Each agent has its own strategy and decision-making
	process. Agents are able to learn and adapt to their environment.
	EMCAS (electricity market complex adaptive system) is a computer
	simulation program for agent-based simulation. This paper presents
	an EMCAS case representing mutual influences of market participants
	in the emerging Croatian market. Simulations and future developments
	are presented.},
  doi = {10.1109/EEM.2008.4579056},
  keywords = {Agent-based modelling,agent-based modelling application,computer simulation
	program,Croatia,Croatian electricity market,decision-making process,Electricity
	market,electricity market complex adaptive system,electricity market
	simulation,eligible customers,EMCAS,EMCAS software,independent software
	agent,market participant,Market participant,power engineering computing,power
	markets},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{feldmann_influences_1997,
  author = {K. Feldmann and R. Feuerstein and K. Gotz},
  title = {Influences of storage conditions on component cracking},
  booktitle = {Polymeric Electronics Packaging, 1997. Proceedings., The First IEEE
	International Symposium on},
  year = {1997},
  pages = {23â€•32},
  abstract = {The increasing miniaturisation in electronics production with simultaneous
	increase in functionality leads to finer structures and larger chip-size
	high pin count components. The enlargement of the component size
	increases the danger of component damage due to moisture absorption.
	To avoid potential component failure through tears at the compound
	or through subsequent corrosion, these cracking-endangered components
	are delivered in specific containers, drypacks, and are stored in
	nitrogen set. Furthermore, the user normally retests the components
	before releasing them in a series of destructive, cost- and time-consuming
	tests. Despite these extensive measures, components often fail during
	reflow soldering but are mostly recognised later, during use. The
	existing damage model describes the connection between storage conditions
	and its effects with component quality during processing inadequately.
	In this paper, the influence of different storage conditions on cracking
	behaviour of high pin count components is examined. The aim is to
	register all relevant influential parameters in an expanded damage
	model and to quantify effects on later processing of the components.
	Effective strategies can be developed based on the new damage model
	for component storage and transportation. It was validated that over
	a component-specific relative longitudinal change (approx. 3\%),
	cracking is registered. Whether this limit is exceeded depends on
	the maximum soldering temperature, the moisture absorbed by the component,
	its general tensile stress behaviour and the heating rate },
  doi = {10.1109/PEP.1997.656469},
  keywords = {assembling,chip-size,component cracking,component damage,component
	failure,component quality,component size,component storage,component
	testing,component transportation,component-specific relative longitudinal
	change,compound tearing,corrosion,cracking behaviour,cracking-endangered
	components,cracks,damage model,drypack delivery,electronics production,encapsulation,failure
	analysis,functionality,heating rate,high pin count components,integrated
	circuit packaging,integrated circuit reliability,integrated circuit
	testing,materials handling,miniaturisation,moisture,moisture absorption,N2,nitrogen
	set storage,reflow soldering,soldering temperature,storage conditions,tensile
	stress behaviour,thermal stresses},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{fellner_classification_2000,
  author = {K.J. Fellner and K. Turowski},
  title = {Classification framework for business components},
  booktitle = {System Sciences, 2000. Proceedings of the 33rd Annual Hawaii International
	Conference on},
  year = {2000},
  pages = {10 pp.},
  abstract = {Components and component orientation is often depicted as the next
	step after object orientation. A huge number of proposals and implementations
	of component models, frameworks, and standards are available nowadays,
	leading to many different definitions of the term software component.
	Additionally often the terms component, object, object framework,
	are not clearly distinguished. Moreover, if a component is defined,
	the definitions often vary. To achieve a clear understanding what
	the core features of a software component are, we provide a classification
	framework to classify each of the proposed models, frameworks, or
	standards. The goal of this classification is to obtain a consolidated
	and clear definition of what a component constitutes. As our focus
	lies in the application of components in the business application
	domain, we also clearly indicate the differences and additional characteristics
	of business components to generic software components. In addition,
	we extend the classification framework. This extended classification
	framework serves as a basis for the characterization of existing
	component oriented approaches in the business domain, like Enterprise
	JavaBeans, SanFrancisco, or SAP Business Objects. Component orientation
	is actually enlisted to solve the core problems of software development,
	like reuse, better integration of legacy systems, or software complexity.
	Thus, often leading to improper use of the term component oriented.
	The extended classification framework allows identification of any
	lack of properties of a given approach that claims to be component
	oriented.},
  keywords = {business application domain,business components,business data processing,business
	domain,classification framework,component model,component orientation,component
	oriented approaches,distributed object management,Enterprise JavaBeans,generic
	software components,Java,legacy systems,object framework,object orientation,object-oriented
	programming,SanFrancisco,SAP Business Objects,software complexity,software
	component,software development},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{qian_feng_ats_2007,
  author = {Qian Feng and Meng Chen and Luo Jin},
  title = {ATS Software Architecture Based on Function Interface},
  booktitle = {Electronic Measurement and Instruments, 2007. ICEMI '07. 8th International
	Conference on},
  year = {2007},
  pages = {2--283-2-287},
  abstract = {The purpose of studying automatic test system (ATS) software architecture
	is to improve the ATS instrument interchangeability, promote automation
	in software development and portability of test program sets (TPSs).
	Based on the research and analysis of the characteristics and existing
	shortcomings of the current ATS software architecture, this paper
	puts forward a modular and standard ATS software architecture based
	on the function interface which can satisfy the demands of the general-purpose
	ATS software. Then it undertakes a reasonable partitioning of the
	software layers according to the ABBET standard. This software architecture
	based on the function interface is much better in the aspects of
	the instrumental independence, the openness of software system and
	the portability of TPS.},
  doi = {10.1109/ICEMI.2007.4350673},
  keywords = {ABBET standard,ATS,automatic test system software architecture,function
	interface,general-purpose ATS software,instrument interchangeability,software
	layers partitioning,software portability,test program sets portability,the
	function interface,TPS},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{tie_feng_applying_2006,
  author = {Tie Feng and J.I. Maletic},
  title = {Applying Dynamic Change Impact Analysis in Component-based Architecture
	Design},
  booktitle = {Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed
	Computing, 2006. SNPD 2006. Seventh ACIS International Conference
	on},
  year = {2006},
  pages = {43--48},
  abstract = {Change impact analysis plays an important role in maintenance and
	evolution of component-based software architecture. Viewing component
	replacement as a change to composition-based software architecture,
	this paper proposes a component interaction trace based approach
	to support dynamic change impact analysis at software architecture
	level. Given an architectural change, our approach determines the
	architecture elements causing the change and impacted by the change.
	Firstly, component-based software architecture and component interaction
	trace are defined. An algorithm for generating component interaction
	trace from static structure model of software architecture and UML
	sequence diagram is provided. Secondly, the taxonomy of changes on
	composition-based software architecture is presented, according to
	which a set of impact rules are suggested to determine the transfer
	of the changes in component and among components. Thirdly, by performing
	slicing on component interaction traces according to impact rules,
	the impact analysis results are obtained. Finally, the architecture
	design of SOCIAT, a tool supporting our approach, is developed and
	explained.},
  doi = {10.1109/SNPD-SAWN.2006.21},
  keywords = {Change impact analysis},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{fenkam_constructing_2002,
  author = {P. Fenkam and H. Gall and M. Jazayeri},
  title = {Constructing CORBA-supported oracles for testing: a case study in
	automated software testing},
  booktitle = {Automated Software Engineering, 2002. Proceedings. ASE 2002. 17th
	IEEE International Conference on},
  year = {2002},
  pages = {129â€•138},
  abstract = {As the complexity of applications and therefore of their testing process
	grows, the importance of automating the testing activity increases.
	The testing process includes test case generation, test sequencing,
	oracle construction, test execution and result interpretation. Automatic
	generation of test cases from formal specifications has received
	considerable attention. Relatively little work has been reported,
	however, on constructing oracles for supporting efficient and automatic
	execution of such test cases. We present a technique for constructing
	a CORBA-supported VDM oracle for black-box testing starting from
	a VDM-SL specification. This specification is used to automatically
	verify the results of operations implemented in a high-level programming
	language. We present a case study of the technique applied to a Java
	application for generic access control. The technique is applicable
	to any CORBA-compliant programming language.},
  isbn = {1527-1366 },
  keywords = {automated software testing,black-box testing starting,case study,CORBA-supported
	oracles,distributed object management,formal specification,formal
	specifications,generic access control,high-level programming language,Java,object-oriented
	programming,program testing,result interpretation,software reliability,specification
	languages,test case generation,test execution,test sequencing,VDM,VDM-SL
	specification,Vienna development method,Vienna Development Method},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{feredj_approach_2004,
  author = {M. Feredj and F. Boulanger and M. Mbobi},
  title = {An approach for domain-polymorph component design},
  booktitle = {Information Reuse and Integration, 2004. IRI 2004. Proceedings of
	the 2004 IEEE International Conference on},
  year = {2004},
  pages = {145â€•150},
  abstract = {Heterogeneous modelling and design tools allow the design of software
	systems using several computation models. The designed system is
	built by assembling components that obey a computation model. The
	internal behavior of a component is specified either in some programming
	language or by assembling sub-components that obey a possibly different
	computation model. When the same behavior is used in several computation
	models, it must be implemented in as many components as there are
	models, or, if the design platform supports it, it may be implemented
	as a generic component. Model-specific components require the recoding
	of the same core behavior several times, and generic components may
	not take model-specific features into account. In this paper, we
	introduce the notion of domain-polymorph component. Such a component
	is able to adapt a core behavior to the semantics of several computation
	models. The core behavior is implemented only once and is automatically
	adapted to the semantics of different computation models. Domain-polymorph
	components can be chosen by a system designer and integrated in a
	computation model: they will benefit from an appropriate execution
	environment and their semantics will be adapted to the host model.
	The designer will have the choice for several parameters of the adaptation.
	Contrary to generic components, such components adapt their behavior
	to the host model instead of letting the host model interpret their
	generic behavior. We also present an implementation of the concept
	of domain-polymorph component in the Ptolemy II framework.},
  keywords = {appropriate execution environment,domain-polymorph component design,formal
	specification,model-specific component,object-oriented programming,programming
	language,Ptolemy II framework,software system},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{ferguson_touring_1992,
  author = {I.A. Ferguson},
  title = {Touring Machines: autonomous agents with attitudes},
  journal = {Computer},
  year = {1992},
  volume = {25},
  pages = {51--55},
  number = {5},
  abstract = {A multilayered integrated architecture for controlling autonomous
	mobile agents, or Touring Machines, is introduced. This architecture
	combines capabilities for producing a range of reactive and deliberative
	behaviors in dynamic, unpredictable domains. The approach is influenced
	by recent work on reactive and behavior-based agent architectures.
	Touring Machines blend sophisticated and simplified control features.
	Experience shows that this layered architecture can be configured
	to behave with robustness and flexibility simultaneously in dynamic
	settings. The implementation of the control architecture on a testbed
	is reported},
  doi = {10.1109/DEXA.2001.953142},
  issn = {0018-9162},
  keywords = {autonomous mobile agents,behavior-based agent architectures,control
	architecture,deliberative behaviors,dynamic settings,flexibility,layered
	architecture,mobile robots,multilayered integrated architecture,robustness,simplified
	control features,testbed,Touring Machines,unpredictable domains},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{firby_openingdoor_1997,
  author = {R.J. Firby},
  title = {Opening the door to robotic agents},
  journal = {IEEE Expert},
  year = {1997},
  volume = {12},
  pages = {7--9},
  number = {2},
  abstract = {What do agent researchers dream of? The answer is simple-building
	a general-purpose, robotic agent that works and communicates with
	human beings in natural settings. Although this goal remains elusive,
	researchers have made real progress in two important areas: defining
	clear avenues for advance as they reframe the issues involved in
	building agents; and the advent of mobile robots with manipulators
	and computer-vision systems has made possible a meaningful evaluation
	and comparison of competing agent architectures. Together, these
	factors are opening the door to rapid advances in the construction
	of intelligent machines. The paper discusses a new framework in robotic
	agent research involving planning systems and reactive systems},
  doi = {10.1109/69.591456},
  issn = {0885-9000},
  keywords = {agent architectures,agent research,computer-vision systems,general-purpose
	robot,intelligent control,intelligent machines,manipulators,mobile
	robots,planning systems,reactive systems,robotic agents,robots},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{flener_correct-schema-guided_1997,
  author = {P. Flener and Kung-Kiu Lau and M. Ornaghi},
  title = {Correct-schema-guided synthesis of steadfast programs},
  booktitle = {Automated Software Engineering, 1997. Proceedings., 12th IEEE International
	Conference},
  year = {1997},
  pages = {153â€•160},
  abstract = {It can be argued that for (semi-)automated software development, program
	schemas are indispensable, since they capture not only structured
	program design principles but also domain knowledge, both of which
	are of crucial importance for hierarchical program synthesis. Most
	researchers represent schemas purely syntactically (as higher-order
	expressions). This means that the knowledge captured by a schema
	is not formalised. We take a semantic approach and show that a schema
	can be formalised as an open (first-order) logical theory that contains
	an open logic program. By using a special kind of correctness for
	open programs, called steadfastness, we can define and reason about
	the correctness of schemas. We also show how to use correct schemas
	to synthesise steadfast programs},
  doi = {10.1109/ASE.1998.732605},
  keywords = {domain knowledge,hierarchical program synthesis,higher-order expressions,informal
	knowledge capture,logic programming,open first-order logical theory,open
	logic program,program schema formalisation,program synthesis,program
	verification,programming theory,schema correctness,semi-automated
	software development,steadfast programs,structured program design
	principles,syntactic representation},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{l._foltyn_reflective-cognitive_2006,
  author = {L. Foltyn and J. Tozicka and M. Rollo and M. Pechoucek and P. Jisl},
  title = {Reflective-Cognitive Architecture: From Abstract Concept to Self-Adapting
	Agent},
  booktitle = {Distributed Intelligent Systems: Collective Intelligence and Its
	Applications, 2006. DIS 2006. IEEE Workshop on},
  year = {2006},
  pages = {326--334},
  abstract = {This paper presents the general framework of the reflective-cognitive
	agent architecture. In our architecture we use modular approach to
	the reflection as we found it to be a promising way how to throw
	multi-agent systems together with the computing with limited resources,
	the programme code reusability and the automated code generation.
	The architecture is lightweight and it enables the agent to alter
	its own code in runtime using reflection according to the changes
	in the environment. At the end of the paper we present results of
	the architecture implementation showing the plausibility of the created
	prototype},
  doi = {10.1109/ICCIAS.2006.294196},
  keywords = {agent reflection,automated code generation,cognitive systems,multiagent
	system,programme code reusability,reflective-cognitive agent architecture,self-adapting
	agent,self-adjusting systems},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{franklin_happy_1996,
  author = {D.F. Franklin and R.E. Kahn and M.J. Swain and R.J. Firby},
  title = {Happy patrons make better tippers: creating a robot waiter using
	Perseus and the Animate Agent architecture},
  booktitle = {Automatic Face and Gesture Recognition, 1996., Proceedings of the
	Second International Conference on},
  year = {1996},
  pages = {253--258},
  abstract = {As autonomous robots become increasingly adept at performing simple
	tasks like moving from place to place and picking up and delivering
	objects, it is becoming apparent that an important area of robotic
	research is that of developing natural interfaces for controlling
	them. In the context of building a robot â€œwaiterâ€?, the authors
	demonstrate the use of the Perseus architecture for gesture recognition,
	teamed with the Animate Agent architecture for tightly coupled perception
	and action. Of particular significance is the ease of implementing
	this task utilizing the architectures and routines they have already
	created for other tasks },
  doi = {10.1109/AFGR.1996.557273},
  keywords = {action,Animate Agent architecture,autonomous robots,catering industry,gesture
	recognition,intelligent control,mobile robots,natural interfaces,path
	planning,perception,Perseus architecture,robot vision,robot waiter,routines},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{freeman_elevating_1999,
  author = {R. Freeman and G. Shoopman},
  title = {Elevating interface device design in the test program set development
	process},
  booktitle = {AUTOTESTCON '99. IEEE Systems Readiness Technology Conference, 1999.
	IEEE},
  year = {1999},
  pages = {253â€•258},
  abstract = {This paper describes a Test Program Set (TPS) Interface Device development
	process which will improve signal performance, lower cost, and enhance
	maintainability. Hardware is designed as modular components and can
	be tested prior to incorporation into an interface device. Automated
	software tools ensure accurate Unit Under Test (UUT) data entry and
	optimize the use of modular hardware. Once established, this development
	process produces quick turnaround pre-tested interface devices with
	high electrical and mechanical integrity},
  doi = {10.1109/AUTEST.1999.800387},
  keywords = {accurate UUT data entry,automated software tools,automatic test equipment,automatic
	test software,crosstalk,enhanced maintainability,interface device
	design,lower cost,modular components,modular hardware,peripheral
	interfaces,quick turnaround pre-tested interface devices,signal performance,software
	tools,streamlined interface design,TPS development process},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{xiao_fu_modeling_2006,
  author = {Xiao Fu and Xinyu Da and Zhenhua Yu},
  title = {Modeling Dynamic Software Architecture Based on Ï€-Net},
  booktitle = {Information and Communication Technologies, 2006. ICTTA '06. 2nd},
  year = {2006},
  volume = {2},
  pages = {2861--2865},
  abstract = {Software architecture is a key aspect of the design of large-scale
	and complicated software system. Architecture description languages
	(ADLs) provide a formal specification of the architecture in terms
	of components and connectors and how they are composed together.
	As existing ADLs are difficult to describe the dynamic software architecture,
	a Ï€-net based dynamic software architecture model (DSAM) is presented,
	in which two complementary formalisms, namely Object-Oriented Petri
	nets (OPN) and Ï€-calculus, are adopted as formal theory bases. DSAM
	stresses on description of dynamic software architecture. Moreover,
	DSAM uses formal methods of Petri nets and Ï€-calculus to analyze,
	simulate and verify software architecture. Finally, to illustrate
	the favorable representation capability of DSAM, an example of dynamic
	software architecture is provided and the potential applications
	are pointed out.},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{yujian_fu_mapping_2008,
  author = {Yujian Fu and Zhijiang Dong and Junhua Ding and Xudong He},
  title = {Mapping Software Architecture Specification to Rewriting Logic (Short
	Paper)},
  booktitle = {Quality Software, 2008. QSIC '08. The Eighth International Conference
	on},
  year = {2008},
  pages = {376--381},
  abstract = {In this paper we present a systematic translation algorithm that maps
	a software architecture model to rewriting logics. We consider a
	nowadays typical component-based software architecture model - SAM.
	SAM is a formal software architecture model that integrates two formalisms
	- Petri nets and temporal logic. Our goal is to effectively describe
	the component based software architecture model SAM using a rewriting
	based semantics. This algorithm is implemented in Maude, a high performance
	declarative programming language that supports membership and rewriting
	logics. The contribution of this paper is we defined the translation
	algorithm to rewriting logic to show an interleaving semantic matching
	between the behavior model Petri net and rewriting logic.},
  doi = {10.1109/QSIC.2008.16},
  isbn = {1550-6002},
  keywords = {component-based software architecture,declarative programming language,interleaving
	semantics,Maude,Petri net,rewriting based semantics,rewriting logic,rewriting
	systems,SAM,software architecture specification,Software architecture
	specification,systematic translation algorithm},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{yujian_fu_method_2006,
  author = {Yujian Fu and Zhijiang Dong and Xudong He},
  title = {A Method for Realizing Software Architecture Design},
  booktitle = {Quality Software, 2006. QSIC 2006. Sixth International Conference
	on},
  year = {2006},
  pages = {57--64},
  abstract = {A software architecture design provides a high-level abstraction of
	system topology, functionality, and/or behavior; which provides the
	basis for early system understanding and analysis as well as the
	foundation for subsequent detailed design and implementation. However,
	research on software architecture in the past decade primarily focused
	on architecture description languages and their analysis techniques
	and less progress was made on automatically realizing software architecture
	designs. In this paper, we present a method for automatically generating
	an implementation from a software architectural description. The
	implementation not only captures the functionality of the given architecture
	description, but also contains additional monitoring code for ensuring
	desirable behavior properties through run-time verification. Our
	method takes a software description written in SAM, a software architecture
	model integrating dual formal methods Petri nets and temporal logic,
	and generates Java code. More specifically, the structure of a SAM
	architecture description produces Arch-Java code the behavior models
	of components/connectors represented in Petri nets lead to plain
	Java code, and the property specifications defined in temporal logic
	generates Aspect J code; the above code segments are then integrated
	into Java code},
  doi = {10.1109/QSIC.2006.2},
  isbn = {1550-6002},
  keywords = {Arch-Java code,Aspect J code,Java code,monitoring,property specification,run-time
	verification,software architecture design,software architecture model,system
	analysis,system behavior,system functionality,system topology,system
	understanding},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{yujian_fu_approach_2005,
  author = {Yujian Fu and Zhijiang Dong and Xudong He},
  title = {An approach to validation of software architecture model},
  booktitle = {Software Engineering Conference, 2005. APSEC '05. 12th Asia-Pacific},
  year = {2005},
  pages = {8 pp.},
  abstract = {Software architectures shift developers' focus from lines-of-code
	to coarser-grained architectural elements and their interconnection
	structure. However, the benefits of architecture description languages
	(ADLs) cannot be fully captured without an automated realization
	of software architecture designs because manually shifting from a
	model to its implementation is error-prone. We propose an integrated
	approach for automatically translating software architecture design
	models to an implementation and validating the translation as well
	as the implementation by exploring runtime verification technique
	and aspect-oriented programming. Specifically, system properties
	are not only verified against design models, but also verified during
	the execution of the generated implementation of software architecture
	design. A prototype tool, SAM Parser, is developed to demonstrate
	the approach on SAM (Software Architecture Model). In SAM Parser,
	all the realization and verification code can be automatically generated
	without human intervention. In this paper, we first brief describe
	the approach report on a case study conducted at an e-commerce scenario,
	an online shopping system to assess the benefits of automated realization
	of software architecture design and validation in a Web service domain.},
  doi = {10.1109/APSEC.2005.33},
  isbn = {1530-1362 },
  keywords = {architecture description language,e-commerce,online shopping system,runtime
	verification technique,SAM Parser,software architecture model validation},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{gall_architectural_1997,
  author = {H. Gall and R. Jazayeri and R. Klosch and G. Trausmuth},
  title = {The architectural style of component programming},
  booktitle = {Computer Software and Applications Conference, 1997. COMPSAC '97.
	Proceedings., The Twenty-First Annual International},
  year = {1997},
  pages = {18â€•25},
  abstract = {Component programming is a multiparadigm approach to software construction
	based on highly generic components. Because component programming
	is concerned with source-code components, it is assumed by many to
	be a low-level approach to software development that affects only
	the development of source code libraries. On the contrary, this paper
	shows that the concepts of component programming go beyond library
	and source code issues and define a new conceptual attempt to software
	development with generic components. We show that component programming
	is an architectural style that supports the building of classes of
	software architectures in a specific domain. Component programming
	can be applied in the early stages of software development when architectural
	issues are to be determined. All the benefits of using an architectural
	style, therefore, can also be gained by using component programming:
	it guides the engineer in the problem decomposition towards the design
	and implementation of a system. The paper presents the architectural
	style of component programming and the insights we gained about component
	programming as we tried to define it as an architectural style},
  doi = {10.1109/CMPSAC.1997.624696},
  keywords = {architectural style,component programming,generic components,multiparadigm
	approach,problem decomposition,programming,software construction,software
	development,software engineering,software libraries,source code libraries,source-code
	components,subroutines,system design,system implementation},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{gallagher_software_2008,
  author = {K. Gallagher and A. Hatch and M. Munro},
  title = {Software Architecture Visualization: An Evaluation Framework and
	Its Application},
  journal = {Software Engineering, IEEE Transactions on},
  year = {2008},
  volume = {34},
  pages = {260--270},
  number = {2},
  abstract = {In order to characterize and improve software architecture visualization
	practice, the paper derives and constructs a qualitative framework,
	with seven key areas and 31 features, for the assessment of software
	architecture visualization tools. The framework is derived by the
	application of the Goal Question Metric paradigm to information obtained
	from a literature survey and addresses a number of stakeholder issues.
	The evaluation is performed from multiple stakeholder perspectives
	and in various architectural contexts. Stakeholders can apply the
	framework to determine if a particular software architecture visualization
	tool is appropriate to a given task. The framework is applied in
	the evaluation of a collection of six software architecture visualization
	tools. The framework may also be used as a design template for a
	comprehensive software architecture visualization tool.},
  doi = {10.1109/TSE.2007.70757},
  issn = {0098-5589},
  keywords = {goal question metric paradigm application,multiple stakeholder perspectives,software
	architecture visualization tools,Software Architectures,Visualization
	techniques and methodologies},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{gannod_automated_1998,
  author = {G.C. Gannod and Yonghao Chen and B.H.C. Cheng},
  title = {An automated approach for supporting software reuse via reverse engineering},
  booktitle = {Automated Software Engineering, 1998. Proceedings. 13th IEEE International
	Conference on},
  year = {1998},
  pages = {94â€•103},
  abstract = {Formal approaches to software reuse rely heavily upon a specification
	matching criterion, where a search query using formal specifications
	is used to search a library of components indexed by specifications.
	In previous investigations, we addressed the use of formal methods
	and component libraries to support software reuse and construction
	of software based on component specifications. A difficulty for all
	formal approaches to software reuse is the creation of the formal
	indices. We have developed an approach to reverse engineering that
	is based on the use of formal methods to derive formal specifications
	of existing programs. In this paper, we present an approach for combining
	software reverse engineering and software reuse to support populating
	specification libraries for the purposes of software reuse. In addition,
	we discuss the results of our initial investigations into the use
	of tools to support an entire process of populating and using a specification
	library to construct a software application},
  doi = {10.1109/ASE.1998.732586},
  keywords = {automated software reuse support,component library searching,component
	specifications,computer aided software engineering,formal indices,formal
	methods,formal specification,formal specifications,indexing,reverse
	engineering,search query,software application construction,software
	libraries,software reusability,software tools,specification libraries,specification
	matching criterion},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{gao_testing_2005,
  author = {J. Gao and R. Espinoza and Jingsha He},
  title = {Testing coverage analysis for software component validation},
  booktitle = {Computer Software and Applications Conference, 2005. COMPSAC 2005.
	29th Annual International},
  year = {2005},
  volume = {1},
  pages = {463â€•470 Vol. 2},
  abstract = {Constructing component-based software using reusable components is
	becoming a widely used approach. Since the quality of a component-based
	system is highly dependent on the quality of its components, component
	quality validation becomes very critical to both component vendors
	and users. Effectively validating component quality needs adequate
	test models and testing coverage criteria. This paper proposes an
	adequate test model and test coverage criteria for component validation.
	The paper discusses a dynamic approach to analyze component test
	coverage based on the proposed test model and test coverage criteria.
	The major contribution of this paper is its dynamic test coverage
	analysis solution to monitor API-based component validation and reuse.
	The paper reports the recent development efforts of a component test
	coverage analysis tool, and presents an application example.},
  doi = {10.1109/ICCBSS.2008.16},
  isbn = {0730-3157},
  keywords = {API-based component validation monitoring,application program interfaces,component
	quality validation,component test coverage,component testing,component-based
	software,object-oriented programming,program testing,program verification,reusable
	components,software component validation,software quality,software
	reusability,system monitoring,test coverage analysis,testing coverage
	analysis},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{gao_systematic_2006,
  author = {Jerry Gao and Deepa Gopinathan and Quan Mai and Jingsha He},
  title = {A Systematic Regression Testing Method and Tool For Software Components},
  booktitle = {Computer Software and Applications Conference, 2006. COMPSAC '06.
	30th Annual International},
  year = {2006},
  volume = {1},
  pages = {455â€•466},
  abstract = {In component-based software engineering, software systems are mainly
	constructed based on reusable components, such as third-party components
	and in-house built components. Hence, system quality depends on the
	quality of the involved components. Any change of a component, it
	must be re-tested at the unit level, and re-integrated to form component-based
	application systems. Although a number of recently published papers
	address regression testing and maintenance of component-based systems,
	very few papers discuss how to identify component changes and impacts
	at the unit level, and find out the reusable test cases in a component's
	test suite to support its evolution. This paper focuses on component
	API-based changes and impacts, and proposes a systematic re-test
	method for software components based on a component API-based test
	model. The proposed method has been implemented in a component test
	tool, known as COMPTest. It can be used to automatically identify
	component-based API changes and impacts, as well as reusable test
	cases in a component test suite. The paper also reports this tool
	and its application results},
  doi = {10.1109/89.650304},
  isbn = {0730-3157},
  keywords = {application program interface,application program interfaces,component
	API test model,component test suite,component-based application system,component-based
	software engineering,component-based system,COMPTest,in-house built
	component,object-oriented programming,program testing,reusable component,reusable
	test case,software component testing,software reusability,software
	system,system component quality,systematic regression testing,systematic
	retest},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{gao_component_2005,
  author = {J. Gao and M.-C. Shih},
  title = {A component testability model for verification and measurement},
  booktitle = {Computer Software and Applications Conference, 2005. COMPSAC 2005.
	29th Annual International},
  year = {2005},
  volume = {2},
  pages = {211â€•218 Vol. 1},
  abstract = {Since components are the major building blocks for component-based
	systems, developing high quality components is becoming very critical
	for in component-based software engineering. To generate high quality
	components, we must pay attention to component testability to ensure
	that reusable components not only can be tested by component vendors,
	but also can be easily validated by component users. Therefore, component
	testability analysis, verification and measurement become very important
	research topic in testing components and component-based systems.
	This paper discusses the component testability in a quantifiable
	approach based on a component testability analysis model. Engineers
	can use this model to verify and measure component testability during
	a component development process. Based on this testability model,
	the paper discusses component testability verification, and proposes
	a pentagram model for testability measurement.},
  doi = {10.1109/COMPSAC.2005.17},
  isbn = {0730-3157},
  keywords = {and,component quality,component reusability,component testability,component
	testability analysis model,component testability measurement,component
	testability verification,component testing,component-based software
	testing,component-based system development,object-oriented programming,program
	testing,program verification,software quality,software reusability,testability
	analysis,testability measurement},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{gao_monitoring_2000,
  author = {J. Gao and E.Y. Zhu and S. Shim and Lee Chang},
  title = {Monitoring software components and component-based software},
  booktitle = {Computer Software and Applications Conference, 2000. COMPSAC 2000.
	The 24th Annual International},
  year = {2000},
  pages = {403â€•412},
  abstract = {Component engineering is gaining substantial interest in the software
	engineering community. A lot of research effort has been devoted
	to analysis and design methods for component-based software. However,
	only few papers have addressed the testing and maintenance problems
	of component-based software. This paper discusses component traceability
	and maintenance issues and solutions in supporting software components.
	It proposes a Java framework and a systematic approach to support
	the tracking and monitoring of software components in component-based
	programs. Application examples and the supporting system are described.
	Moreover, the paper introduces the concept of traceable components,
	including requirements, design guidelines and architecture style.
	The presented results are useful to add systematic component tracking
	features into the current Java and EJB (Enterprise JavaBeans) technology
	to support software components, including third-party components
	in software maintenance},
  doi = {10.1109/ASMC.2000.902559},
  keywords = {component engineering,component traceability,component tracking,component-based
	software,design guidelines,Enterprise JavaBeans,Java,Java framework,program
	diagnostics,requirements,software architecture style,software component
	monitoring,software engineering,software maintenance,software testing,software
	tracking,subroutines,third-party components,traceable components},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{gear_reengineering_2005,
  author = {Andrew Le Gear and Jim Buckley},
  title = {Reengineering towards components using "Reconn-exion"},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {2005},
  volume = {30},
  pages = {370â€•373},
  number = {5},
  abstract = {Continuing to develop software from scratch will not be feasible indefinitely.
	Reusing existing software would seem to be a viable solution to this
	problem. The paradigm of component-based development (CBD) explicitly
	accounts for reuse in its process. Unfortunately the majority of
	existing software systems are not implemented using CBD, thus reusing
	portions of this software using CBD becomes difficult. Reengineering
	and maintenance research contains a plethora of software analysis
	and restructuring techniques that could be used to help us exploit
	legacy applications for reuse. This thesis focuses on two such techniques
	and combines variations of them for the purpose of component recovery:
	A feature location technique called Software Reconnaissance and a
	design recovery technique called Software Reflexion Modelling. Their
	combination is called "Component Reconn-exion." We describe the technique,
	highlight results and evaluation to date and finally discuss further
	work necessary to complete our contribution as a PhD. thesis.},
  doi = {10.1145/1095430.1081765},
  keywords = {component recovery,features,reuse},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1095430.1081765\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{geisterfer_software_2006,
  author = {C.J.M. Geisterfer and S. Ghosh},
  title = {Software component specification: a study in perspective of component
	selection and reuse},
  booktitle = {Commercial-off-the-Shelf (COTS)-Based Software Systems, 2006. Fifth
	International Conference on},
  year = {2006},
  pages = {9 pp.},
  abstract = {In component-based software engineering research, much effort has
	gone into developing specification techniques for software components.
	There exist many software component specification techniques, from
	interface description language (IDL), to design-by-contract based,
	to formal methods. However, much of the focus of the research literature
	is aimed at component specification for the development of components,
	not their use. The current best practices for component specification
	ignore information that is vital in determining if an available,
	ready to use component contains precisely the functional and extra-functional
	properties required and if that component can be used in the target
	environment. These specification techniques do not sufficiently support
	selection and reuse of software components. This paper evaluates
	some of the current component specification techniques with respect
	to the needs of component selection and reuse. From this evaluation,
	some recommendations made as to advancing the development of component
	specifications to include the purposes of component selection and
	reuse.},
  doi = {10.1049/ip-sen:20000914},
  keywords = {component based development,component reuse,component selection,component
	specification,component-based software engineering,design-by-contract
	based method,formal method,formal specification,interface description
	language,object-oriented programming,software component reusability,software
	component selection,software component specification,software reusability,software
	selection},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{george_requirements_2005,
  author = {B. George and H.L. Singh and S.A. Bohner and D. Gracanin},
  title = {Requirements Capture for Cougaar Model-Driven Architecture System},
  booktitle = {Software Engineering Workshop, 2005. 29th Annual IEEE/NASA},
  year = {2005},
  pages = {109--117},
  abstract = {The Cognitive Agent Architecture (Cougaar) is a distributed agent
	architecture framework originally developed to build large scale
	logistics applications for the Department of Defense. In order to
	facilitate rapid development of complex military and commercial Cougaar
	applications, we are devising "recipes" on how to automate the Cougaar
	application development process. The envisioned Cougaar Model Driven
	Architecture (CMDA) system will capture requirements from the user
	and automatically generate related software artifacts such as requirement,
	design, code and test cases. The automated generation of software
	artifacts, even if partial in nature, should considerably increase
	the productivity of Cougaar developers. Recognizing the sophistication
	of the Cougaar framework and the limitations of current transformation
	technologies, we have systematically developed an approach that combines
	component assembly in the large with transformations in the small.
	The approachÂ’s ability to generate accurate software artifacts depends
	largely on how user requirements are captured and transformed. The
	CMDA system employs innovative approaches and techniques to enable
	users represent their requirements properly. This paper briefly describes
	the CMDA system, its user interface and the various inputs captured
	from the user.},
  doi = {10.1109/ISCC.2006.84},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{ghanea-hercock_agent-based_2003,
  author = {R. Ghanea-Hercock},
  title = {An agent-based user-authentication system},
  journal = {Intelligent Systems, IEEE},
  year = {2003},
  volume = {18},
  pages = {67--73},
  number = {3},
  abstract = {Managing the necessary public and private keys in a large organization
	is a serious challenge. Software agents can be an adaptive and responsive
	mechanism for managing users trying to connect to network resources.
	BTexact Technologies Intelligent Systems Laboratory has developed
	the Phobos agent architecture. Phobos uses a distributed team of
	cooperative autonomous agents to collectively authenticate user access
	requests. The advantages are that the agents can query multiple information
	sources to select the level of trust to delegate to a user and that
	n agents must concur to authenticate the user, hence increasing overall
	security. Phobos provides numerous security services to automate
	user authentication and trust-management processes.},
  doi = {10.1109/TVT.2003.819450},
  issn = {1541-1672},
  keywords = {adaptive responsive mechanism,agent-based user authentication system,cryptography,distributed
	cooperative autonomous agent team,message authentication,multiple
	information source querying,network resources,Phobos agent architecture,private
	keys,public keys,security,trust management,user access request authentication},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{giannakopoulou_automata-based_2001,
  author = {D. Giannakopoulou and K. Havelund},
  title = {Automata-based verification of temporal properties on running programs},
  booktitle = {Automated Software Engineering, 2001. (ASE 2001). Proceedings. 16th
	Annual International Conference on},
  year = {2001},
  pages = {412â€•416},
  abstract = {This paper presents an approach to checking a running program against
	Linear Temporal Logic (LTL) specifications. LTL is a widely used
	logic for expressing properties of programs viewed as sets of executions.
	Our approach consists of translating LTL formulae to finite-state
	automata, which are used as observers of the program behavior. The
	translation algorithm we propose modifies standard LTL to Buchi automata
	conversion techniques to generate automata that check finite program
	traces. The algorithm has been implemented in a tool, which has been
	integrated with the generic JPaX framework for runtime analysis of
	Java programs.},
  isbn = {1527-1366 },
  keywords = {automata-based verification,Buchi automata conversion,finite automata,finite
	state automata,formal specification,generic JPaX framework,Java programs,linear
	temporal logic specifications,observers,program behavior,program
	verification,running programs,runtime analysis,temporal logic,temporal
	properties},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{giarla_implementing_2000,
  author = {A.J. Giarla and W.L. Simerly},
  title = {Implementing AI-ESTATE in a component based architecture, Phase-II
	},
  booktitle = {AUTOTESTCON Proceedings, 2000 IEEE},
  year = {2000},
  pages = {438â€•450},
  abstract = {This paper discusses Phase-II of an Air Force funded SBIR R\&D project
	concerned with implementing the IEEE 1232 Standard known as AI-ESTATE
	in a component based Automatic Test System (ATS). The intent of our
	Phase-II work is to provide the Air Force with a â€œworking toolâ€?
	that demonstrates the utility of our architectural approach as well
	as the utility of AI-ESTATE. The intent of our component based approach
	to ATS design is to provide a de-coupling of the diagnostic reasoner
	from the rest of the ATS as well as provide for a true open system
	at a reduced procurement and maintenance costs. AI-ESTATE is used
	to provide a standardized interface between the reasoner and the
	other ATS elements as well as provide standardized information and
	data model formats. The architecture consists of a COTS Test System
	(VXI package with LabVIEW), four â€œdomainâ€? type components: Diagnostic
	Engine Component (DEC), Application Executive Component (AEC), Test
	System Component (TSC) and Model Editing Component (MEC) that plug
	into our â€œInteroperable, Connectivity Enabling Frameworkâ€? (ICEF)
	based on COM/DCOM. The four components and framework are discussed
	in general terms. The DEC implements AI-ESTATE and is further discussed
	in derail as are the other components. Also discussed are operational
	uses, diagnostic development, vendor competition and benefits},
  doi = {10.1109/AUTEST.2000.885625},
  keywords = {AEC,AI-ESTATE,Air Force,Application Executive Component,architecture,ATS
	design,automatic test equipment,automatic test software,Automatic
	Test System,component based architecture,computer architecture,COTS
	Test System,D project,DEC,decoupling,Diagnostic Engine Component,diagnostic
	reasoner,IEEE 1232 Standard,LabVIEW,MEC,military computing,Model
	Editing Component,phase-II,SBIR R\&,standardized interface,Test System
	Component,TSC,VXI package},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{gill_importance_2006,
  author = {Nasib S. Gill},
  title = {Importance of software component characterization for better software
	reusability},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {2006},
  volume = {31},
  pages = {1â€•3},
  number = {1},
  abstract = {Component-based software development (CBSD) is the process of assembling
	existing software components in an application such that they interact
	to satisfy a predefined functionality. This approach can potentially
	be used to reduce software development costs, assemble systems rapidly,
	and reduce the maintenance overhead. One of the key challenges faced
	by software developers is to make component-based development (CBD)
	an efficient and effective approach. Since components are to be reused
	across various products and product-families, components must be
	characterized and tested properly. The present paper is a survey
	paper and firstly, it discusses CBD and related issues that help
	improving software reuse. Testing of third party components is a
	very difficult task in the absence a properly characterized software
	component. Besides improving software reusability, component characterization
	also provides better understanding of architecture, better retrieval,
	better usage and better cataloguing. This paper mainly discusses
	the essence of proper component characterization that ultimately
	helps the developers in software reuse, which is highly desirable
	in component-based software development. Further, paper also discusses
	other benefits of component characterization that are most essential
	in component-based development.},
  doi = {10.1145/1108768.1108771},
  keywords = {component characterization,component-based development,software components,software
	reusability},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1108768.1108771\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{gill_reusability_2003,
  author = {Nasib S. Gill},
  title = {Reusability issues in component-based development},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {2003},
  volume = {28},
  pages = {4â€•4},
  number = {4},
  abstract = {Component-based development (CBD) advocates the acquisition, adaptation,
	and integration of reusable software components to rapidly develop
	and deploy complex software systems with minimum engineering effort
	and resource cost. Software reusability is an attribute that refers
	to the expected reuse potential of a software component. Software
	reuse not only improves productivity but also has a positive impact
	on the quality and maintainability of software products. The paper
	first discusses CBD and its associated challenges, and later outlines
	the issues concerning component reusability and its benefits in terms
	of cost and time-savings. Guidelines are presented to further assist
	software engineers in the development of reusable software products
	and to extract reusable components from existing software. Quality
	and productivity improvement activities within organisations adopting
	CBD can also benefit from the adoption of these guidelines.},
  doi = {10.1145/882240.882255},
  keywords = {component-based development (cbd),software components,software reuse},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=882240.882255\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{gill_dependency_2008,
  author = {Nasib S. Gill and Balkishan},
  title = {Dependency and interaction oriented complexity metrics of component-based
	systems},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {2008},
  volume = {33},
  pages = {1â€•5},
  number = {2},
  abstract = {Component-Based Development (CBD) practice is gaining popularity among
	software developers in the software industry. Researcher community
	is striving hard to identify the attributes characterizing component-based
	development and further proposing metrics that may help in controlling
	the complexity of the component-based systems. The present paper
	introduces a set of component-based metrics, namely, Component Dependency
	Metric (CDM) and Component Interaction Density Metric (CIDM), which
	measure the dependency and coupling aspects of the software components
	respectively. Graph theoretic notions have been used to illustrate
	the dependency and interaction among software components for all
	the four cases chosen for present study. Dependency and interaction-oriented
	complexity metrics for component-based systems have been computed.
	The results of the present study are quite encouraging and may further
	help the researchers in controlling the complexity of component-based
	systems so as to minimize the integration and maintenance costs.},
  doi = {10.1145/1350802.1350810},
  keywords = {component dependency metric (cdm),component interaction dependency
	metric (cidm),dependency-oriented complexity metric (docm)},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1350802.1350810\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{gill_few_2004,
  author = {Nasib S. Gill and P. S. Grover},
  title = {Few important considerations for deriving interface complexity metric
	for component-based systems},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {2004},
  volume = {29},
  pages = {4â€•4},
  number = {2},
  abstract = {Component-based software engineering (CBSE) represents an exciting
	and promising paradigm for software development. Software components
	are one of the key issues in CBSE. The software development community
	is continuously seeking new methods for improving software quality
	and enhancing development productivity. There is an increasing need
	for component-based metrics to help manage and foster quality in
	component-based software development. The traditional software product
	and process metrics are neither suitable nor sufficient in measuring
	the complexity of software components, which ultimately is necessary
	for quality and productivity improvement within organisations adopting
	CBSE. In this paper, we propose an interface complexity metric (ICM)
	aimed at measuring the complexity of a software component based on
	the interface characterisation model of a software component that
	mainly include such as interface signature, interface constraints,
	interface packaging and configurations. Based on the value of this
	metric, the complexity of the software component could be managed
	within reasonable complexity limits. In this way, the software components
	could be kept simple which in turn help in enhancing the quality
	and productivity.},
  doi = {10.1145/979743.979758},
  keywords = {interface characterisation,interface complexity,interface complexity
	metric,software components},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=979743.979758\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{gill_component-based_2003,
  author = {N. S. Gill and P. S. Grover},
  title = {Component-based measurement: few useful guidelines},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {2003},
  volume = {28},
  pages = {4â€•4},
  number = {6},
  abstract = {Software industries are striving for new techniques and approaches
	that could improve software developer productivity, reduce time-to-market,
	deliver excellent performance and produce systems that are flexible,
	scalable, secure, and robust. Only software components can meet these
	demands and following this; component-based software engineering
	(CBSE) has emerged, which has generated tremendous interest in software
	development community. The paradigm shift to software components
	appears inevitable, necessitating drastic changes to current software
	development and business practices. The scope of this paper is to
	suggest few necessary guidelines for deriving component-based metrics.
	The paper discusses issues related to component-based development
	(CBD) and suggests a general definition of software component based
	on several existing definitions. Further, the paper outlines the
	necessity of component measurement and also discusses the limitations
	of traditional software metrics for component-based development (CBD)
	systems. Finally, this paper also suggest few necessary guidelines
	for CBD measurement and proposes some relevant metrics applicable
	to CBD systems, which after proper quantification and validation
	may help guiding risk and quality management of component-based systems.},
  doi = {10.1145/966221.966237},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=966221.966237\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{goguen_software_1996,
  author = {Joseph Goguen and Doan Nguyen and JosÃ© Meseguer and Luqi and Luqi
	and Du Zhang and Valdis Berzins},
  title = {Software component search},
  journal = {Journal of Systems Integration},
  year = {1996},
  volume = {6},
  pages = {93â€•134},
  number = {1},
  month = mar,
  abstract = {An important problem in software development is to make better use
	of software libraries by improving the search and retrieval process,
	that is, by making it easier to find the few components you may want
	among the many you do not want. This paper suggests some ideas to
	improve this process: (1) Associate analgebraic specification with
	each software component; these specifications should include complete
	syntactic information, but need have onlypartial semantic information.
	(2) User queries consist of syntactic declarations plus results forsample
	executions. (3) User queries may be posed in standard programming
	notation, which is then automatically translated into algebraic notation.
	(4) Search is organized asranked multi-level filtering, where each
	level yields aranked set of partial matches. (5) Early stages of
	filtering narrow the search space by using computationally simple
	procedures, such as checking that the number of types is adequate.
	(6) Middle levels may findpartial signature matches. (7) Pre-computedcatalogues
	(i.e., indexes) can speed up early and middle level filtering. (8)
	Semantic information is used in a final filter withterm rewriting,
	but complete verification is not attempted. (9) The series of filters
	is implementedincrementally, so as to backtrack to lower ranked components
	in case of failure. This approach avoids the need for complex theorem
	proving, and does not require any knowledge of algebraic specification
	from the user. Moreover, it does not require either specifications
	or queries to be complete or even fully correct, because it yields
	partial matches ranked by how well they fit the query. The paper
	concludes with a description of some preliminary experiments and
	some suggestions for further experiments.},
  doi = {10.1007/BF02262753},
  owner = {user},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1007/BF02262753}
}

@ARTICLE{goknar_momco:_2001,
  author = {I.C. Goknar and H. Kutuk and Sung-Mo Kang},
  title = {MOMCO: method of moment components for passive model order reduction
	of RLCG interconnects},
  journal = {Circuits and Systems I: Fundamental Theory and Applications, IEEE
	Transactions on},
  year = {2001},
  volume = {48},
  pages = {459â€•474},
  number = {4},
  abstract = {We introduce a new concept called moment components and a new method
	based on it to obtain passive reduced-order models of interconnect
	networks. In this method, the impedance matrix moments of the interconnect
	network are partitioned into their inductive, capacitive and mixed
	inductive/capacitive moment components. The method of moment components
	is described in a formal manner using analysis and synthesis equalities.
	Two significant contributions of the method of moment components
	are: 1) new decomposition of moments into parts which reflect passivity
	in all moments of passive networks; and 2) the analysis equalities
	impart a regular pattern in terms of the moment components, thus
	simplifying the moment generation for our method. None of these features
	is observable in conventional complete moment terms. Two new methods
	for obtaining passive reduced-order models based on the method of
	moment components are introduced. The passive reduced-order models
	are obtained by matching their impedance moment components to those
	of the original interconnect network through the synthesis equalities.
	Due to nonnegative definiteness of the moment components, the match
	in the moment components preserves the passivity of the original
	interconnect in the reduced-order model. The method of moment components
	does not have the instability problem of general moment matching
	techniques. The reduced-order model is specifically targeted for
	fast timing simulators so that interconnect effects can be simulated
	efficiently},
  doi = {10.1109/81.917983},
  issn = {1057-7122},
  keywords = {capacitive moment components,circuit simulation,distributed parameter
	networks,fast timing simulators,impedance matrix,impedance matrix
	moments partitioning,impedance moment components,inductive moment
	components,integrated circuit interconnections,interconnect networks,linear
	network analysis,method of moment components,method of moments,MOMCO,moment
	generation,passive model order reduction,passive networks,passive
	reduced-order models,reduced order systems,RLCG interconnects,synthesis
	equalities,timing},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{gomaa_automated_2007,
  author = {Hassan Gomaa and Michael E. Shin},
  title = {Automated Software Product Line Engineering and Product Derivation},
  booktitle = {System Sciences, 2007. HICSS 2007. 40th Annual Hawaii International
	Conference on},
  year = {2007},
  pages = {285a},
  abstract = {This paper describes a prototype automated software product line engineering
	environment, in which a multiple view model of the product line architecture
	and components are developed and stored in a product line repository.
	Automated software product derivation consists of tailoring the product
	line architecture given the product features and selecting the components
	to be included in the product. The automated environment is built
	on top of Rational Rose RT. Automated support is provided for developing
	multiple product line views, using the feature model as the unifying
	view, an underlying product line meta-model that provides a schema
	for a product line repository, support for consistency checking among
	the multiple views, and support for feature-based product line derivation},
  doi = {10.1109/ASE.2006.4},
  isbn = {1530-1605},
  keywords = {automated software product line engineering,computer aided software
	engineering,consistency checking,feature-based product line derivation,multiple
	view model,product development,product features,product line architecture,product
	line meta-model,product line repository,product line schema,Rational
	Rose RT,software architecture,software prototyping,software reusability},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{gonzlez_mrcc:multi-resolution_2007,
  author = {Enrique GonzÃ¡lez and Adith Perez and Juan Cruz and CÃ©sar Bustacara},
  title = {MRCC: A Multi-Resolution Cooperative Control Agent Architecture},
  booktitle = {Intelligent Agent Technology, 2007. IAT '07. IEEE/WIC/ACM International
	Conference on},
  year = {2007},
  pages = {391--394},
  abstract = {The cooperation problem in a complex robotic environment requires
	the use of explicit mechanisms and control architectures specifically
	designed to manage the cooperation. Powerful ways of structuring
	cooperation complexity are required, including models, methodologies,
	software architectures, information systems and frameworks. In this
	paper the intentional cooperation control model MRCC, Multi-Resolution
	Cooperation Control is presented; in particular, the internal agent
	architecture is studied in detail. The model takes advantage of the
	Multi Agent Systems (MAS) benefits in order to provide an integral
	and flexible architecture. The main idea of the MRCC model is based
	on a hierarchical decomposition of the MAS cooperative control, where
	each layer manages the decisions at different granularity and abstraction
	levels. Higher levels take into account general aspects of the system
	control and generate influences over lower ones. Lower levels manage
	negotiation mechanism to achieve cooperative actions and control
	of individual agents. The internal agent architecture is built using
	a concurrent approach, where several behaviors interact to integrate
	in a coherent way the operation and contribution of the different
	MRCC levels. The preliminary simulation results demonstrate that
	the model can be applied in different multi robot tasks.},
  doi = {10.1109/CDC.2004.1429540},
  keywords = {Cooperative systems,Multiagent systems,Multirobot systems},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Gore2006,
  author = {Amit Gore and Shantanu Chakrabartty and Sudeshna Pal and Evangelyn
	Alocilja},
  title = {A multi-channel femtoampere-sensitivity conductometric array for
	biosensing applications.},
  journal = {Conf Proc IEEE Eng Med Biol Soc},
  year = {2006},
  volume = {Suppl},
  pages = {6489--6492},
  abstract = {Rapid detection of pathogens using field deployable biosensors requires
	integrated sensing and data processing. Detection of low concentration
	of biological agents is possible using accurate and real-time signal
	characterization devices. This paper presents a multi-channel conductometric
	array that can detect and measure current up to femtoampere range.
	The architecture uses a novel semi-synchronous SigmaDelta modulation
	that allows measurement of ultra-small currents by using a hysteretic
	comparison technique. The architecture achieves higher energy efficiency
	over a conventional SigmaDelta by reducing the total switching cycles
	of the comparator. A 3 mm x 3 mm chip implementing a 42 channel potentiostat
	array has been prototyped in a 0.5 microm CMOS technology. Measured
	results show 10 bits of resolution, with a sensitivity of upto 50
	fA of current. The power consumption of the potentiostat is 11 microW
	per channel at a sampling rate of 250 kHz. The multi-channel potentiostat
	has been integrated with a conductometric biosensor for field deployable
	applications. Results with a Bacillus Cereus based biosensor demonstrate
	the effectiveness of the potentiostat in characterizing different
	concentration levels of pathogens in realtime.},
  doi = {10.1109/IEMBS.2006.260865},
  institution = {Department of Electrical and Computer Engineering, Michigan State
	University, East Lansing, MI 48824, USA. goeamit@egr.msu.edu},
  keywords = {Biosensing Techniques, instrumentation; Microelectrodes; Signal Processing,
	Computer-Assisted; Software},
  owner = {user},
  pmid = {17959433},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1109/IEMBS.2006.260865}
}

@INPROCEEDINGS{goulo_component-based_2005,
  author = {Miguel GoulÃ£o},
  title = {Component-based software engineering: a quantitative approach},
  booktitle = {Companion to the 20th annual ACM SIGPLAN conference on Object-oriented
	programming, systems, languages, and applications},
  year = {2005},
  pages = {238â€•239},
  address = {San Diego, CA, USA},
  publisher = {ACM},
  abstract = {Note: OCR errors may be found in this Reference List extracted from
	the full text article. ACM has opted to expose the complete List
	rather than only correct and linked references.},
  doi = {10.1145/1094855.1094953},
  isbn = {1-59593-193-7},
  keywords = {component-based software engineering,empirical software engineering,metamodel,object
	constraint language},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1094855.1094953\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{gracanin_towardsmodel-driven_2004,
  author = {D. Gracanin and S.A. Bohner and M. Hinchey},
  title = {Towards a model-driven architecture for autonomic systems},
  booktitle = {Engineering of Computer-Based Systems, 2004. Proceedings. 11th IEEE
	International Conference and Workshop on the},
  year = {2004},
  pages = {500--505},
  abstract = {Agent based systems and architectures provide a firm foundation for
	design and development of an autonomic system. The key challenge
	is the selection and efficient use of effective agent architecture.
	A model-driven approach accommodates the underlying architecture
	to automate, as much as possible, the development process. The Cognitive
	Agent Architecture (COUGAAR) is a distributed agent architecture
	that provides the primary components and an implementation platform
	for this research. COUGAAR has been developed primarily for very
	large-scale, distributed applications that are characterized by hierarchical
	task decompositions and as such is well suited for autonomic systems.
	We propose a framework for the agent-based, model-driven architecture
	for autonomic applications development. The framework consists of
	two main parts, General COUGAAR application model (GCAM) and general
	domain application model (GDAM). Some COUGAAR related performance
	issues are also discussed.},
  doi = {10.1109/WCICA.2006.1712816},
  keywords = {agent based systems,autonomic systems,Cognitive Agent Architecture,distributed
	applications,general COUGAAR application model,general domain application
	model,model-driven architecture},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{grassi_towards_2004,
  author = {Vincenzo Grassi and Raffaela Mirandola},
  title = {Towards automatic compositional performance analysis of component-based
	systems},
  booktitle = {Proceedings of the 4th international workshop on Software and performance},
  year = {2004},
  pages = {59â€•63},
  address = {Redwood Shores, California},
  publisher = {ACM},
  abstract = {To make predictive analysis an effective tool for component-based
	software development (CBSD), it should be, as much as possible: compositional,
	to allow the re-use of known information about the properties of
	existing components, and automatic, to keep the pace with the timeliness
	and cost-effectiveness promises of CBSD. Towards this end, focusing
	on the predictive analysis of performance properties, we define a
	simple language, based on an abstract component model, to describe
	a component assembly, outlining which information should be included
	in it to support compositional performance analysis. Moreover, we
	outline a mapping of the constructs of the proposed language to elements
	of the RT-UML Profile, to give them a precisely defined "performance
	semantics", and to get a starting point for the exploitation of proposed
	UML-based methodologies and algorithms for performance analysis.},
  doi = {10.1145/974044.974052},
  isbn = {1-58113-673-0},
  keywords = {component specification,performance,predictive analysis,software component},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=974044.974052\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{graubmann_semantic_2006,
  author = {Peter Graubmann and Mikhail Roshchin},
  title = {Semantic Annotation of Software Components},
  booktitle = {Software Engineering and Advanced Applications, 2006. SEAA '06. 32nd
	EUROMICRO Conference on},
  year = {2006},
  pages = {46â€•53},
  abstract = {The aim of this contribution is to present concepts and to propose
	techniques and a methodical support for automated software composition
	using "rich" semantic descriptions of components and services which
	we call annotations. Our approach is based upon a component description
	reference model for which both, semantic description patterns and
	inference mechanisms are defined. They offer variability in expressiveness,
	reasoning power and the required analysis depth for the identification
	of component properties and qualities. The approach comprises two
	major concepts - logic-on-demand and the triple semantic model. They
	both turn out to be essential for the entire modelling process and,
	in particular for the automated reasoning techniques based on semantic
	annotations},
  doi = {10.1109/NSSMIC.2006.353687},
  isbn = {1089-6503},
  keywords = {automated reasoning,automated software composition,component description
	reference model,inference mechanism,inference mechanisms,logic-on-demand,object-oriented
	programming,programming language semantics,semantic annotation,semantic
	description patterns,software components,triple semantic model},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{greiner_comparison_1994,
  author = {John Greiner},
  title = {A comparison of parallel algorithms for connected components},
  booktitle = {Proceedings of the sixth annual ACM symposium on Parallel algorithms
	and architectures},
  year = {1994},
  pages = {16â€•25},
  address = {Cape May, New Jersey, United States},
  publisher = {ACM},
  abstract = {This paper presents a comparison of the pragmatic aspects of some
	parallel algorithms for finding connected components, together with
	optimizations on these algorithms. The algorithms being compared
	are two similar algorithms by Shiloach-Vishkin [22] and Awerbuch-Shiloach
	[2], a randomized contraction algorithm based on algorithms by Reif
	[21] and Phillips [20], and a hybrid algorithm [11]. Improvements
	are given for the first two to improve performance significantly,
	although without improving their asymptotic complexity. The hybrid
	combines features of the others and is generally the fastest of those
	tested. Timings were made using NESL [4] code as executed on a Connection
	Machine 2 and Cray Y-MP/C90.},
  doi = {10.1145/181014.181021},
  isbn = {0-89791-671-9},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=181014.181021\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{grosu_automated_2001,
  author = {R. Grosu and Y.A. Liu and S. Smolka and S.D. Stoller and Jingyu Yan},
  title = {Automated software engineering using concurrent class machines},
  booktitle = {Automated Software Engineering, 2001. (ASE 2001). Proceedings. 16th
	Annual International Conference on},
  year = {2001},
  pages = {297â€•304},
  abstract = {Concurrent Class Machines are a novel state-machine model that directly
	captures a variety of object-oriented concepts, including classes
	and inheritance, objects and object creation, methods, method invocation
	and exceptions, multithreading and abstract collection types. The
	model can be understood as a precise definition of UML activity diagrams
	which, at the same time, offers an executable, object-oriented alternative
	to event-based statecharts. It can also be understood as a visual,
	combined control and data flow model for multithreaded object-oriented
	programs. We first introduce a visual notation and tool for Concurrent
	Class Machines and discuss their benefits in enhancing system design.
	We then equip this notation with a precise semantics that allows
	us to define refinement and modular refinement rules. Finally, we
	summarize our work on generation of optimized code, implementation
	and experiments, and compare with related work.},
  doi = {10.1109/ICSM.1990.131336},
  isbn = {1527-1366 },
  keywords = {abstract collection types,automated software engineering,classes,concurrent
	class machines,data flow model,event-based statecharts,exception
	handling,exceptions,inheritance,method invocation,multithreaded object-oriented
	programs,multithreading,multi-threading,object creation,object-oriented
	concepts,object-oriented programming,optimized code,semantics,software
	engineering,specification languages,state-machine model,system design,UML
	activity diagrams,visual notation},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{grover_few_2007,
  author = {P. S. Grover and Rajesh Kumar and Arun Sharma},
  title = {Few useful considerations for maintaining software components and
	component-based systems},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {2007},
  volume = {32},
  pages = {1â€•5},
  number = {5},
  abstract = {Component-Based Systems (CBS) maintenance may require several activities
	different than normal applications, such as upgrading the functionality
	of black-box components (for which code may not be available), replacement
	of older version components with the new ones for better and improved
	functionality, tracing the problem of compatibility between the new
	components with system, and so on. The focus of this paper is on
	investigating several issues and concerns about maintainability of
	component-based systems. It also explores the acceptance of maintainability
	characteristics and sub-characteristics as defined in ISO9126 quality
	model for CBS. The paper proposes two new sub-characteristics, namely
	trackability and portability, to be included under the maintenance
	activity.},
  doi = {10.1145/1290993.1290995},
  keywords = {component-based systems,components,iso9126,maintainability},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1290993.1290995\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{grumer_automated_2007,
  author = {M. Grumer and M. Wendt and C. Steger and R. Weiss and U. Neffe and
	A. Muhlberger},
  title = {Automated Software Power Optimization for Smart Card Systems with
	Focus on Peak Reduction},
  booktitle = {Computer Systems and Applications, 2007. AICCSA '07. IEEE/ACS International
	Conference on},
  year = {2007},
  pages = {506â€•512},
  abstract = {The complexity of embedded systems is continuously growing due to
	the increasing requirements on performance. In portable systems such
	as smart cards, not only performance is an important attribute, but
	also the power and energy consumed by a given application. Sources
	of energy used in smart card systems like batteries and electromagnetic
	fields are not such ideal elements, as their effectiveness depends
	heavily on the energy consumed over time. Optimization strategies
	proposed so far are implemented statically. Often a manual measurement
	of the current profile followed by manual optimizations is carried
	out. This procedure is very time consuming. We present a method for
	reducing automatically the power of an application based on a compiler
	optimization.},
  doi = {10.1109/AICCSA.2007.370929},
  keywords = {automated software power optimization,batteries,compiler optimization,computational
	complexity,electromagnetic fields,embedded systems,embedded systems
	complexity,optimising compilers,optimization strategies,peak reduction,smart
	card systems,smart cards},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{guo_researchmulti-agent_2001,
  author = {Chaozhen Guo and Wu Dong and Jia Wu},
  title = {Research on multi-agent for general group decision support system
	},
  booktitle = {Computer Supported Cooperative Work in Design, The Sixth International
	Conference on, 2001},
  year = {2001},
  pages = {308â€•312},
  abstract = {The research work presented in this paper is based on the concrete
	background of a group decision support system (GDSS) and on modelling
	and abstracting the process of the GDSS. The paper presents the design
	of a general GDSS based on multi-agent approaches. In this design,
	we have abstracted a suite of procedures which can allow communication
	between the general GDSS system's coordinator (server) and its decision-maker
	(customer). It overcomes the limitations of traditional systems and
	can be used for group decision-making in many fields. This paper
	focuses on introducing the design that many components were given,
	in the form of the Component Object Model (COM), and the idea of
	an â€œagentâ€? was embodied in the design of these components. It
	offers some universal components for GDSS system developers, including
	a GDSS communication component, a GDSS site monitor agent component,
	a cryptogram agent component, a preference agent component, an alternation
	agent component, a cooperation agent component, etc. A prototype
	has been implemented},
  doi = {10.1109/CSCWD.2001.942277},
  keywords = {alternation agent component,COM,component design,Component Object
	Model,cooperation agent component,cryptogram agent component,decision-maker,distributed
	object management,GDSS communication component,GDSS site monitor
	agent component,general group decision support system,group decision
	support systems,multi-agent system,multi-agent systems,preference
	agent component,prototype implementation,server-customer communication,subroutines,system
	coordinator},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{jiang_guo_scheduling_2004,
  author = {Jiang Guo and Yuehong Liao},
  title = {The scheduling algorithms in software architecture modeling},
  booktitle = {Engineering of Computer-Based Systems, 2004. Proceedings. 11th IEEE
	International Conference and Workshop on the},
  year = {2004},
  pages = {36--43},
  abstract = {A critical issue for complex component-based systems design is the
	modeling and analysis of architecture. Architectural specifications
	of software systems show them as a collection of interrelated components,
	and constitute what has been called the software architecture level
	of software design. It is at this level where the description and
	verification of structural properties of the system are naturally
	addressed. The scheduling algorithms play important roles in the
	software architecture modeling. These algorithms can be used to model
	the performance aspects of the software architecture. We have developed
	a tool - the Web-based software architecture prototyping system (SAPS)
	- in a distributed environment to meet the requirements of integrating
	software components into heterogeneous networks. This paper discusses
	the scheduling algorithms used in the software architecture modeling.},
  doi = {10.1109/ECBS.2004.1316680},
  keywords = {component-based systems design,scheduling algorithm,software architecture
	modeling,Web-based software architecture prototyping system},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{jiang_guo_collaboration-oriented_2006,
  author = {Jiang Guo and Yuehong Liao and B. Parviz},
  title = {A collaboration-oriented software architecture modeling system \$JArchiDesigner},
  booktitle = {Engineering of Computer Based Systems, 2006. ECBS 2006. 13th Annual
	IEEE International Symposium and Workshop on},
  year = {2006},
  pages = {2 pp.},
  abstract = {A critical issue for complex component-based systems design is the
	modeling and analysis of architecture. Architectural specifications
	of software systems show them as a collection of interrelated components,
	and constitute what has been called the software architecture level
	of software design. However, since most of software architecture
	modeling tools, such as rational software architecture (RSA), run
	as stand-alone programs, there are still some inconveniency and incapability
	in fields of team designing and data management. The JArchiDesigner
	System was designed as efficient, secure, and manageable networking
	JArchiDesigner system to support collaborative design process. With
	the JArchiDesigner System, multiple users are able to concurrently
	access and manipulate the same software architecture information
	stored in a server machine. This paper discusses the software architecture
	design and implementation of JArchiDesigner System.},
  doi = {10.1109/ECBS.2006.5},
  keywords = {architectural specification,collaboration-oriented software architecture
	modeling system,collaborative design process,component-based systems
	design,data management,networking JArchiDesigner system,rational
	software architecture,server machine,software architecture modeling
	tool,stand-alone program,team designing},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{guo_research_2005,
  author = {Yinzhang Guo and Guoyou Zhang and Lipin Xie and Yubin Xu},
  title = {The research and design of business component reuse in enterprise
	information system integration},
  booktitle = {Information Technology and Applications, 2005. ICITA 2005. Third
	International Conference on},
  year = {2005},
  volume = {2},
  pages = {41â€•44 vol.2},
  abstract = {In this paper, the problem of business component reuse is researched
	in the integration of rapid reconfigurable enterprise information
	system. The model of function component is provided, which is based
	on analyzing and abstracting function commonness from different components
	in business component domain, the compiling parameters of function
	component denote a business component type, and the running parameters
	denote the difference of business components. At the same time, the
	syntax description of the function components is presented. The reuse
	of business component is improved in enterprise information system
	integration by designing function component.},
  doi = {10.1109/ICITA.2005.276},
  keywords = {business component,business component reuse,business function component,distributed
	object management,enterprise information system integration,function
	component,management information systems,object-oriented programming,reuse,software
	architecture,software reusability},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{zhang_guoliang_design_2007,
  author = {Zhang Guoliang and Ke Xizheng},
  title = {Design and Implementation of DF-2 Multi-Agent Autonomous Mobile Robot},
  booktitle = {Electronic Measurement and Instruments, 2007. ICEMI '07. 8th International
	Conference on},
  year = {2007},
  pages = {1--752-1-756},
  abstract = {Developing and training of autonomous mobile robot based on multi-agent
	theory is a hotspot in AI and robot research. The general outline,
	system design, hardware choosing, agent architecture design and key
	technique of DF-2 Autonomous Mobile robot developing based on multi-agent
	theory are proposed in this essay. Finally, the current research
	is introduced.},
  doi = {10.1109/DEPCOS-RELCOMEX.2007.25},
  keywords = {agent architecture design,AI,Autonomous Mobile Robot,Design and Implementation,DF-2
	multi-agent autonomous robot,hardware choosing,mobile robot,mobile
	robots,multi-agent theory,multi-robot systems},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{gustavsson_road_2005,
  author = {P.M. Gustavsson and T. Planstedt},
  title = {The road towards multi-hypothesis intention simulation agents architecture
	- fractal information fusion modeling},
  booktitle = {Simulation Conference, 2005 Proceedings of the Winter},
  year = {2005},
  pages = {10 pp.},
  abstract = {This paper presents the road towards multi-hypothesis intention simulation
	agents architecture and is focused on the fractal information fusion
	model (FIF) that are formed to support a systems-thinking in an agent
	architecture that aligns with the global information grid, NATO net
	enabled capabilities and Swedish armed force enterprise architecture
	initiatives. The Joint Directors of Laboratories information fusion
	model and the Observe, Orient, Decide, Act loop by John Boyd is combined
	and used as the foundation together with the knowledge model, level
	of conceptual interoperability shaping the FIF-model. The FIF-model's
	effect in shaping of the multi-hypothesis intention simulation agents
	architecture is presented.},
  doi = {10.1109/WSC.2005.1574548},
  keywords = {agent architecture,fractal information fusion modeling,global information
	grid,grid computing,multihypothesis intention simulation agents,NATO
	Net enabled capabilities,systems-thinking},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{gbel_encapsulation_2004,
  author = {Steffen GÃ¶bel},
  title = {Encapsulation of structural adaptation by composite components},
  booktitle = {Proceedings of the 1st ACM SIGSOFT workshop on Self-managed systems},
  year = {2004},
  pages = {64â€•68},
  address = {Newport Beach, California},
  publisher = {ACM},
  abstract = {Component-based software engineering offers a way to partition complex
	systems into well-defined parts. Adaptation mechanisms are crucial
	to enable run-time reconfiguration and to increase the reuse of these
	parts in other applications and environments. In this paper we utilize
	the concept of composite components to map component parameters to
	different predefined internal configurations of subcomponents. The
	structural adaptation is thereby encapsulated and hidden from other
	parts of the system. Configuration variations allow to specify parameterizable
	configuration patterns. Some extensions to UML diagrams are introduced
	to model reconfiguration steps. Optional adaptation and aspect operators
	as additional constituents of composite components increase the flexibility
	of the presented approach.},
  doi = {10.1145/1075405.1075418},
  isbn = {1-58113-989-6},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1075405.1075418\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{gbel_textlessitextgreatercomquadtextless/itextgreater_2004,
  author = {Steffen GÃ¶bel and Christoph Pohl and Simone RÃ¶ttger and Steffen
	Zschaler},
  title = {The {\textbackslash}textlessi{\textbackslash}textgreaterCOMQUAD{\textbackslash}textless/i{\textbackslash}textgreater
	component model: enabling dynamic selection of implementations by
	weaving non-functional aspects},
  booktitle = {Proceedings of the 3rd international conference on Aspect-oriented
	software development},
  year = {2004},
  pages = {74â€•82},
  address = {Lancaster, UK},
  publisher = {ACM},
  abstract = {The reliability of non-functional contracts is crucial for many software
	applications. This added to the increasing attention this issue lately
	received in software engineering. Another development in software
	engineering is toward component-based systems. The interaction of
	both, non-functional aspects and components, is a relatively new
	research area, which the COMQUAD project is focusing on.Our component
	model, presented in this paper, enables the specification and runtime
	support of non-functional aspects in component-based systems. At
	the same time, a clear separation of non-functional properties and
	functionally motivated issues is provided. We achieve this by extending
	the concepts of the existing component-based systems Enterprise JavaBeans
	(EJB) and CORBA Components (CCM). Non-functional aspects are described
	orthogonally to the application structure using descriptors, and
	are woven into the running application by the component container
	acting as a contract manager. The container implicitly instantiates
	component specifications and connects them according to the current
	requests. The selection of actual implementations depends on the
	particular client's non-functional requirements. This technique also
	enables adaptation based on the specific quantitative capabilities
	of the running system.In this paper we give a detailed description
	of the COMQUAD component model and the appropriate container support.
	We also provide a simple case study of a multimedia application for
	better understanding.},
  doi = {10.1145/976270.976281},
  isbn = {1-58113-842-3},
  keywords = {adaptivity,aosd,components,non-functional properties,qos},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=976270.976281\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{haghpanah_approximation_2007,
  author = {N. Haghpanah and S. Moaven and J. Habibi and M. Kargar and S.H. Yeganeh},
  title = {Approximation Algorithms for Software Component Selection Problem},
  booktitle = {Software Engineering Conference, 2007. APSEC 2007. 14th Asia-Pacific},
  year = {2007},
  pages = {159â€•166},
  abstract = {Today's software systems are more frequently composed from preexisting
	commercial or non-commercial components and connectors. These components
	provide complex and independent functionality and are engaged in
	complex interactions. Component-Based Software Engineering (CBSE)
	is concerned with composing, selecting and designing such components.
	As the popularity of this approach and hence number of commercially
	available software components grows, selecting a set of components
	to satisfy a set of requirements while minimizing cost is becoming
	more difficult. This problem necessitates the design of efficient
	algorithms to automate component selection for software developing
	organizations. We address this challenge through analysis of Component
	Selection, the NP-complete process of selecting a minimal cost set
	of components to satisfy a set of objectives. Due to the high order
	of computational complexity of this problem, we examine approximating
	solutions that make the component selection process practicable.
	We adapt a greedy approach and a genetic algorithm to approximate
	this problem. We examined the performance of studied algorithms on
	a set of selected ActiveX components. Comparing the results of these
	two algorithms with the choices made by a group of human experts
	shows that we obtain better results using these approximation algorithms.},
  doi = {10.1109/SEFM.2007.19},
  isbn = {1530-1362},
  keywords = {ActiveX component,approximation algorithm,component-based software
	engineering,computational complexity,genetic algorithm,genetic algorithms,greedy
	algorithms,greedy approach,object-oriented programming,software component
	selection,software developing organization,software selection},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{hajri_infrastructure_2005,
  author = {R.B. Hajri and L.L. Jilani and H.H.B. Ghezala},
  title = {An infrastructure to help development with reuse},
  booktitle = {Computer Systems and Applications, 2005. The 3rd ACS/IEEE International
	Conference on},
  year = {2005},
  pages = {139},
  abstract = {Summary form only given. This paper discusses that in order to help
	development with reusable software components, various reuse libraries
	should be available for users, components retrieval, use and comprehension
	should be facilitated and relations between components should be
	considered. To achieve this goal, we propose a reuse infrastructure-IRL
	which preferences various reuse libraries in order to make them available
	for users all over the world and to facilitate their access, 2) supplies
	components reuse guidelines, samples and/or demos to facilitate their
	comprehension and use and 3) extracts components relationships to
	facilitate their composition and integration. IRL infrastructure
	is based on a component meta-model and a search process. The meta-model
	represents a meta-library associated with IRL and encapsulates different
	views of a reusable component. The search process is strategic and
	supplies different search techniques which can be applied to search
	for components independently of their source libraries search techniques.
	This infrastructure provides an ontology which represents components
	meta-knowledge. This ontology provides, in one hand a unified vocabulary
	for various reuse libraries by which ambiguities on the components
	semantics are removed. On the other hand, this ontology facilitates
	components search and filling in the meta-library associated with
	IRL infrastructure. In this paper, the IRL infrastructure is presented,
	and an experimentation evaluating it is discussed.},
  doi = {10.1109/AICCSA.2005.1387128},
  keywords = {component meta-model,components retrieval,components semantics,IRL,meta
	data,meta-knowledge,meta-library,object-oriented programming,ontologies
	(artificial intelligence),ontology,reusable software components,reuse
	infrastructure,reuse libraries,search process,software libraries,software
	reusability},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{hameurlain_flexible_2007,
  author = {N. Hameurlain},
  title = {Flexible Behavioural Compatibility and Substitutability for Component
	Protocols: A Formal Specification},
  booktitle = {Software Engineering and Formal Methods, 2007. SEFM 2007. Fifth IEEE
	International Conference on},
  year = {2007},
  pages = {391â€•400},
  abstract = {Component compatibility and substitutability are widely recognized
	as the main issues in component- based software engineering (CBSE).
	Most of existing approaches suffer from the problem of component
	adaptation. Indeed, components compatibility and substitutability
	are performed component-to- component without taking into account
	the context. This paper proposes a new framework where more flexible
	component protocols compatibility and substitutability relations
	that depend on the context (environment) can be defined. The proposed
	approach is based on the notion of component protocol's usability,
	that is a component such that there exists an environment ensuring
	the completion and / or the proper termination of the composition
	of the involved component protocol and that environment. Two optimistic
	protocols compatibility relations together with two optimistic protocols
	behavioral subtyping relations related to the principle of substitutability
	are proposed. Moreover, behavioral refinement of component protocols
	is studied, and a link between protocols refinement and their usability
	is established. The soundness of the approach is shown.},
  doi = {10.1109/ASPEC.2007.38},
  keywords = {component protocol compatibility,component protocol substitutability,component-based
	software engineering,formal specification,object-oriented programming},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{hamlet_theory_2001,
  author = {Dick Hamlet and Dave Mason and Denise Woit},
  title = {Theory of software reliability based on components},
  booktitle = {Proceedings of the 23rd International Conference on Software Engineering},
  year = {2001},
  pages = {361â€•370},
  address = {Toronto, Ontario, Canada},
  publisher = {IEEE Computer Society},
  abstract = {We present a foundational theory of software system reliability based
	on components. The theory describes how component developers can
	design and test their components to produce measurements that are
	later used by system designers to calculate composite system reliability
	â€” without implementation and test of the system being designed.
	The theory describes how to make component measurements that are
	independent of operational profiles, and how to incorporate the overall
	system-level operational profile into the system reliability calculations.
	In principle, the theory resolves the central problem of assessing
	a component, which is: a component developer cannot know how the
	component will be used and so cannot certify it for an arbitrary
	use; but if the component buyer must certify each component before
	using it, component-based development loses much of its appeal. This
	dilemma is resolved if the component developer does the certification
	and provides the results in such a way that the component buyer can
	factor in the usage information later, without repeating the certification.
	Our theory addresses the basic technical problems inherent in certifying
	components to be released for later use in an arbitrary system.},
  isbn = {0-7695-1050-7},
  keywords = {cbse,cots,foundational theory,reliability composition,software components},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=381473.381511\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{hamza-lup_component_2008,
  author = {G.L. Hamza-Lup and A. Agarwal and R. Shankar and C. Iskander},
  title = {Component selection strategies based on system requirements' dependencies
	on component attributes},
  booktitle = {Systems Conference, 2008 2nd Annual IEEE},
  year = {2008},
  pages = {1â€•5},
  abstract = {Rapid increases in systems complexity have raised the need to exploit
	the "design \& reuse" principle to its full potential. The proposed
	research is targeted towards component reuse, specifically towards
	component selection. We assume a component specification method has
	been chosen and a component library has been designed and built.
	The problem we address in this paper is choosing a subset of components
	from a library of components, such that the resulting integrated
	system satisfies certain requirements. Our proposed approach contains
	two main stages. First, we address those requirements that can help
	us reduce our search space and secondly, we perform an intelligent
	search in our reduced search space. In the second stage we apply
	a Greedy approach for selecting components from our reduced search
	space. The challenge here is assessing how well a certain component
	satisfies the performance requirements of the target system, as these
	performance requirements usually refer to the system as a whole and
	not to individual components. To address this challenge we focused
	on mapping system performance requirements onto component characteristics.
	We will illustrate our proposed approach for component selection
	with a simplified example of selecting the components for a 4x4 mesh-based
	NOC (Network-on-Chip) architecture.},
  doi = {10.1109/ICGSE.2008.29},
  keywords = {component library,component reuse,Component selection,component selection
	strategies,component specification method,formal specification,greedy
	algorithms,greedy approach,network-on-chip,network-on-chip architecture,optimal
	architectures,system requirements,systems analysis,systems complexity},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{hassan_tracing_2008,
  author = {M.O. Hassan and H. Basson},
  title = {Tracing Software Architecture Change Using Graph Formalisms in Distributed
	Systems},
  booktitle = {Information and Communication Technologies: From Theory to Applications,
	2008. ICTTA 2008. 3rd International Conference on},
  year = {2008},
  pages = {1--6},
  abstract = {In the context of software architecture evolution, understanding the
	implications of change impact propagation is necessary for various
	activities including at first the change management. A software evolution
	implies an implementation of series of change operations to be applied
	on the software architecture. These operations could be of several
	types such as substitution of components, adding new components,
	suppression of subcomponents, change of interface etc. Our contribution
	aims at providing a model permitting to foresee the impact propagation
	of any change intended to be applied on a software architecture component.
	It is also motivated the growing need of companies and organizations
	for software systems that can be adapted, in a flexible and timely
	way, to changes occurring in the application domain. This paper presents
	a graph-based model addressed to help software evolution management
	to identify the change effect propagation throughout different components
	of software architecture.},
  doi = {10.1109/ICTTA.2008.4530365},
  keywords = {change management,graph formalisms,software architecture change tracing,software
	architecture evolution},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{hassan_tracking_2008-1,
  author = {W.H. Hassan and N. Fisal},
  title = {A tracking mechanism using agents for personal mobility in IP networks},
  booktitle = {Industrial Informatics, 2008. INDIN 2008. 6th IEEE International
	Conference on},
  year = {2008},
  pages = {995--999},
  abstract = {This paper presents our ongoing work in developing an agent architecture
	to support personal mobility in IP networks. In particular, the paper
	discusses a tracking mechanism that facilitates location management
	using agents where both host and user mobility is accorded by the
	network. Towards this end, a multi-layered agent architecture is
	proposed and this acts as an overlay providing intelligence in an
	IP-based network. The agent messaging and collaboration provide the
	necessary signaling and control mechanism for tracking user movement
	across multiple domains. This represents a departure from traditional
	methods of providing mobility services where intelligence is normally
	restricted to end systems. Ultimately, the role of the collaborative
	agent architecture is to provide an IP-based network with the ability
	to exhibit dasiaintelligentpsila behaviour i.e. awareness, dynamism
	and adaptation, in terms of user movement and preferences.},
  doi = {10.1109/ICCS.1994.474135},
  isbn = {1935-4576},
  keywords = {agent architecture,IP networks,location management,mobility management
	(mobile radio),personal mobility,tracking,tracking mechanism},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{hawryzkiewycz_agent_2003,
  author = {I.T. Hawryzkiewycz},
  title = {Agent architectures to support collaborative processes},
  booktitle = {Web Information Systems Engineering, 2003. WISE 2003. Proceedings
	of the Fourth International Conference on},
  year = {2003},
  pages = {157--165},
  abstract = {This paper proposes an approach to developing agents that can be reused
	in a number of collaborative applications. It particularly concerns
	those applications that evolve rapidly over time and need continual
	reconfiguration. This particularly applies to knowledge intensive
	processes in virtual environments, which requires quick reconfiguration
	to make then sustainable. It bases the agents architecture on a collaborative
	metamodel, which provides the ontology for building reusable software
	agents within collaborative environments. The paper describes the
	collaborative metamodel, an example and a prototype implementation
	and ways to integrate the model with agent systems.},
  keywords = {agent architectures,agent development,collaborative applications,collaborative
	environments,collaborative metamodel,collaborative processes,knowledge
	intensive processes,quick reconfiguration,software agent reuse,virtual
	environments,virtual organizations},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{he_applying_2008,
  author = {Ruan He and Marc Lacoste},
  title = {Applying component-based design to self-protection of ubiquitous
	systems},
  booktitle = {Proceedings of the 3rd ACM workshop on Software engineering for pervasive
	services},
  year = {2008},
  pages = {9â€•14},
  address = {Sorrento, Italy},
  publisher = {ACM},
  abstract = {Ubiquitous environments both require strong and yet flexible protection,
	due to their highly dynamic character, and to the diversity of their
	security requirements. Autonomic security provides an elegant solution
	to the problem by applying the idea of flexibility to the security
	space itself, and automating reconfiguration of the protection mechanisms.
	The result is a self-protected system, running (almost) without any
	user intervention. In this paper, we show how the component-based
	software paradigm can help to realize such systems. We give an overview
	of the main issues involved in the design and the implementation.
	To strike the best balance between autonomic behavior and protection,
	many different mechanisms are needed. We propose an integrated solution
	for self-protected pervasive systems in terms of a 3-level architecture
	containing two control loops at the network and node levels. We present
	some preliminary results concerning an end-to-end framework based
	on this architecture, and its corresponding implementation.},
  doi = {10.1145/1387229.1387233},
  isbn = {978-1-60558-214-6},
  keywords = {autonomic computing,component-based software engineering,self-protection,ubiquitous
	system},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1387229.1387233\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{helsinger_cougaar:robust_2005,
  author = {A. Helsinger and T. Wright},
  title = {Cougaar: A Robust Configurable Multi Agent Platform},
  booktitle = {Aerospace Conference, 2005 IEEE},
  year = {2005},
  pages = {1--10},
  abstract = {Aerospace operations face unreliable, slow networks, dynamic conditions,
	and expensive launches. Therefore, researchers are investigating
	autonomous agent architectures for space system control. However,
	agent systems to date lack proven reliability, scalability, and cost-effectiveness.
	Under the UltraLog program, the United States Defense Advanced Research
	Projects Agency (DARPA) has sponsored the development of the Cougaar
	agent architecture (open-source at http://cougaar.org), a robust
	reusable agent framework. UltraLog used Cougaar to build a large-scale
	distributed prototype planning application, which was assessed under
	machine kills, network cuts and degradations, and increased workload.
	The results indicate that Cougaar agent applications would enable
	reliable cost-effective aerospace applications},
  doi = {10.1109/IIH-MSP.2007.258},
  keywords = {aerospace control,aerospace operations,autonomous agent architectures,configurable
	multiagent platform,Cougaar,DARPA,distributed prototype planning
	application,machine kills,network cuts,network degradations,reusable
	agent framework,space system control,UltraLog program,United States
	Defense Advanced Research Projects Agency},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{hemer_formal_2005,
  author = {David Hemer},
  title = {A formal approach to component adaptation and composition},
  booktitle = {Proceedings of the Twenty-eighth Australasian conference on Computer
	Science - Volume 38},
  year = {2005},
  pages = {259â€•266},
  address = {Newcastle, Australia},
  publisher = {Australian Computer Society, Inc.},
  abstract = {Component based software engineering (CBSE), can in principle lead
	to savings in the time and cost of software development, by encouraging
	software reuse. However the reality is that CBSE has not been widely
	adopted. From a technical perspective, the reason is largely due
	to the difficulty of locating suitable components in the library
	and adapting these components to meet the specific needs of the user.Formal
	approaches to retrieval - using formal notations for interface specification,
	and semantic based matching techniques - have been proposed as a
	solution to the retrieval problem. These approaches are aimed at
	overcoming the lack of precision and ambiguity associated with text-based
	component interfaces, requirements and retrieval techniques. However
	these approaches fail to adequately address the problem of component
	adaptation and composition.In this paper we describe how component
	adaptation and composition strategies can be defined using parameterised
	library templates. We define a variety of templates, including wrapper
	templates that adapt a single program component, and architecture
	templates that combine program components. We include definitions
	for sequential architectures, independent architectures and alternative
	architectures. These library templates are formally specified, so
	we are able to employ existing formal-based retrieval strategies
	to match problem specifications against library templates. We discuss
	how adaptation and composition can be semi-automated by the library
	templates defined in this paper in combination with existing retrieval
	strategies.},
  isbn = {1-920-68220-1},
  keywords = {component adaptation,component-based software engineering,retrieval,specification
	matching},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1082161.1082190\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{john_herbert_mobile_2006,
  author = {John Herbert and John O'Donoghue and Gao Ling and Kai Fei and Chien-Liang
	Fok},
  title = {Mobile Agent Architecture Integration for a Wireless Sensor Medical
	Application},
  booktitle = {Web Intelligence and Intelligent Agent Technology Workshops, 2006.
	WI-IAT 2006 Workshops. 2006 IEEE/WIC/ACM International Conference
	on},
  year = {2006},
  pages = {235--238},
  abstract = {Wireless sensor nodes are used to monitor patient vital signs in a
	medical application. To ensure proper patient care is provided, real-time
	patient data must be managed correctly in the context of relevant
	patient information and medical knowledge. The data management system
	(DMS) is an agent-based architecture that aims to provide flexible,
	effective data management within a wireless patient sensor network
	(WPSN). The DMS is built primarily on the sophisticated JADE agent
	platform. JADE runs on resource-rich platforms such as servers, PCs,
	PDAs and high-end mobile phones. The lightweight Agilla agent platform
	can run on resource constrained sensor nodes. An integrated mobile
	agent based architecture combining Jade and Agilla is presented.
	This makes best use of the more sophisticated agent platform for
	high-level functionality and the lighter agent middleware for low-level
	sensor data collection. The resulting system is a unified agent architecture
	that runs on heterogeneous platforms on a wireless network},
  doi = {10.1109/WI-IATW.2006.90},
  keywords = {Agilla agent,data management system,JADE agent,mobile agent architecture
	integration,mobile agents,patient care,patient monitoring,patient
	vital signs monitoring,telemedicine,wireless patient sensor network,wireless
	sensor medical application,wireless sensor networks},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{herring_manifolds:_1998,
  author = {C. Herring and S. Kapian},
  title = {Manifolds: cellular component organizations},
  booktitle = {Technology of Object-Oriented Languages, 1998. TOOLS 28. Proceedings},
  year = {1998},
  pages = {287â€•298},
  abstract = {The article reports on initial investigation into a software architecture
	for component systems. Described is a geometrically based, cellular
	framework for the organization of component computation and communication.
	These structures are called Cellular Component Manifolds. The starting
	point for this work is the observation that the concept of address
	space is a pervasive abstraction across all computing and communicating
	systems. Accordingly, a generalized model of an idealized address
	space is proposed. A particular geometry with a suitable algebraic
	structure is chosen to provide addressing within this model. This
	results in an n-dimensional, hierarchical, cellular aggregate address
	space that is also a vector space with â€œniceâ€? properties. The
	address spare, when populated with suitable components, becomes a
	Cellular Component Manifold. A component system architecture is given.
	This software architecture consists of three major layers: component
	framework framework, component frameworks and components. In addition
	to providing a basis for component system organization, this structure
	natively supports a range of scientific and engineering applications.
	As an example, the architecture is applied to the problem of handoff
	management in wireless overlay networks},
  doi = {10.1109/ICCA.2007.4376768},
  keywords = {address space,algebraic structure,cellular aggregate address space,Cellular
	Component Manifolds,cellular component organizations,component computation,component
	framework framework,component system architecture,component system
	organization,component systems,engineering applications,generalized
	model,geometrically based cellular framework,handoff management,object-oriented
	programming,software architecture,storage allocation,storage management,vector
	space,wireless overlay networks},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{higuchi_robust_2004,
  author = {Isao Higuchi and Shinto Eguchi},
  title = {Robust Principal Component Analysis with Adaptive Selection for Tuning
	Parameters},
  journal = {J. Mach. Learn. Res.},
  year = {2004},
  volume = {5},
  pages = {453â€•471},
  abstract = {The present paper discusses robustness against outliers in a principal
	component analysis (PCA). We propose a class of procedures for PCA
	based on the minimum psi principle, which unifies various approaches,
	including the classical procedure and recently proposed procedures.
	The reweighted matrix algorithm for off-line data and the gradient
	algorithm for on-line data are both investigated with respect to
	robustness. The reweighted matrix algorithm is shown to satisfy a
	desirable property with local convergence, and the on-line gradient
	algorithm is shown to satisfy an asymptotical stability of convergence.
	Some procedures in the class involve tuning parameters, which control
	sensitivity to outliers. We propose a shape-adaptive selection rule
	for tuning parameters using K-fold cross validation.},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1005332.1005348\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{hoareau_middleware_2008,
  author = {Didier Hoareau and Yves MahÃ©o},
  title = {Middleware support for the deployment of ubiquitous software components},
  journal = {Personal Ubiquitous Comput.},
  year = {2008},
  volume = {12},
  pages = {167â€•178},
  number = {2},
  abstract = {A number of emerging distributed platforms include fixed and robust
	workstations but, like dynamic and pervasive networks, are often
	built from mobile and resource-constrained devices. These networks
	are characterized by the volatility of their hosts and connections,
	which may lead to network fragmentation. Although increasingly common,
	they remain a challenging target for distributed applications. In
	this paper, we focus on component-based distributed applications
	by addressing the distribution and the deployment of software components
	on dynamic pervasive networks. We present a distribution scheme and
	some associated middleware mechanisms that allow a component to provide
	its services in an ubiquitous way. First, an architecture description
	language extension is proposed in order to specify a deployment,
	driven by constraints on the resources needed by components. Then,
	a propagative and autonomic deployment process is explained, which
	is based on a consensus algorithm adapted for dynamic networks. Lastly,
	implementation details and experiment results are given.},
  keywords = {deployment,distributed components,dynamic networks,middleware.,pervasive
	networks,ubiquitous applications},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1348803.1348809\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{houshmand_extendmeaning_2004,
  author = {K. Houshmand and S. Goli and R. Esmaili and C.H. Pham},
  title = {Extend the meaning of "R" to "R4" in ART (automated software regression
	technology) to improve quality and reduce R\&D and production costs},
  booktitle = {Engineering Management Conference, 2004. Proceedings. 2004 IEEE International},
  year = {2004},
  volume = {1},
  pages = {70â€•74 Vol.1},
  abstract = {Regression testing has been conventionally employed to check the effectiveness
	of a solution, track existing issues and any new issues created by
	the result of fixing the old issues. Positioned at the tail end of
	the software cycle, regression testing technology can hardly influence
	or contribute to earlier phases such as architect, design, implementation
	or device testing. Extending the "R" in ART to R4 (regression, research,
	retain \& grow expertise and early exposure) has been proving. R4
	is not only providing ART with more powerful tools to detect issues
	as early as in the architect phase, but also arming R\&D software
	with more proactive practices to avoid costly catastrophic problems
	from propagating to customer sites. This paper attempts to share
	some best practices and contributions from Cisco-ARF (a Cisco automated
	regression/research facility) whose charter is to ensure the quality
	of product lines running on tens of million lines of code. These
	award-winning practices have proven to save multi-million dollars
	in repair costs, thousands of engineering hours, and continue to
	set the higher standards for testing technology under proactive leadership
	and management to gain higher quality and customer satisfaction.},
  doi = {10.1109/IEMC.2004.1407078},
  keywords = {automated software regression technology,Cisco-ARF,cost reduction,customer
	satisfaction,DP industry,production costs reduction,R\&D software,regression
	testing,research and development,software development management,software
	quality,software reliability,testing},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{howe_components-first_2004,
  author = {Emily Howe and Matthew Thornton and Bruce W. Weide},
  title = {Components-first approaches to CS1/CS2: principles and practice},
  journal = {SIGCSE Bull.},
  year = {2004},
  volume = {36},
  pages = {291â€•295},
  number = {1},
  abstract = {Among the many ways to focus CS1/CS2 content, two have been published
	that emphasize concepts of component-based software engineering.
	Courses based on these two instances of a "components-first" approach
	are remarkably similar in several crucial respectsâ€•which is surprising
	because they were developed independently and with very different
	objectives. Indeed, the two versions are based on virtually the same
	principles for content organization, and they share many common features
	that are unusual for CS1/CS2. Yet, they are notably different in
	other ways. Detailed analysis of similarities and differences suggests
	that it might be possible to transfer some of their claimed and documented
	advantages to other approaches within the programming-first paradigm
	for CS1/CS2, by rearranging the content of such courses in accord
	with the underlying principles of the components-first approach.},
  doi = {10.1145/1028174.971404},
  keywords = {component-based software,components-first,cs1,cs2,objects-first,programming-first,software
	components},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1028174.971404\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{hu_metadata-driven_2004,
  author = {Jihua Hu},
  title = {Metadata-driven framework of management of information resources
	in railway industry},
  booktitle = {Geoscience and Remote Sensing Symposium, 2004. IGARSS '04. Proceedings.
	2004 IEEE International},
  year = {2004},
  volume = {5},
  pages = {2929â€•2932 vol.5},
  abstract = {This paper develops a metadata-driven framework of information resource
	management based on metadata and XML in railway industry. This framework
	consists of three levels of structure including client, server and
	metadata database. Client can adopt traditional desktop, Web page
	or mobile Web page. Server is composed of many components, including
	facade component dealing with request/response, basic metadata management:
	component and other advanced components driving basic components
	and data through metadata. Facade component calls and destroys components
	dynamically through querying metadata database. Basic components
	include metadata-maintaining components, metadata-querying components
	and components that can extract data according to metadata. Advanced
	components include data market components, data interoperation components
	and semantic interoperation components etc, whose definitions and
	realization flow are explicated in this paper},
  doi = {10.1109/IGARSS.2004.1370308},
  keywords = {client-server system,client-server systems,data extraction,data interoperation
	components,data market components,desktop,facade component,geographic
	information systems,information management,information resource management,meta
	data,metadata database querying,metadata management,metadata-driven
	framework,metadata-maintaining components,metadata-querying components,mobile
	computing,mobile Web page,query processing,railway industry,railways,request-response,semantic
	interoperation components,Web sites,XML},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{miao_huaikou_formalizing_2006,
  author = {Miao Huaikou and Sun Junmei and Cao Xiaoxia},
  title = {Formalizing and analyzing service oriented software architecture
	style},
  booktitle = {Enterprise Distributed Object Computing Conference, 2006. EDOC '06.
	10th IEEE International},
  year = {2006},
  pages = {387--390},
  abstract = {The concept of software architecture (SA) provides a new way for the
	transition from the construct and requirement to implementation.
	Software architecture style is a classification of SA. Different
	style has different system characteristic. Through the research of
	software architecture style, we can direct the software development
	well using SA. Formalizing software architecture style made the communication
	more precise and convenient at the level of SA. Formalizing software
	architecture style will be beneficial to formal verification and
	comparison of different style. This paper proposes the service oriented
	software architecture style, formalizes the new service oriented
	SA style using formal specification notation Z, gives the definition
	of match and composition of service component, analyses the replacement
	of SA style and proves four theorems of replacement},
  doi = {10.1109/EDOC.2006.29},
  isbn = {1541-7719},
  keywords = {service oriented software architecture style,software architecture
	classification},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{li_huan_backbone_2007,
  author = {Li Huan and Qin Zheng and Yu Fan and Qin Jun and Yang Bo},
  title = {Backbone Discovery in Social Networks},
  booktitle = {Web Intelligence, IEEE/WIC/ACM International Conference on},
  year = {2007},
  pages = {100--103},
  abstract = {AI planning is the main stream method for automatic semantic Web service
	composition (SWSC) research. However, planning based SWSC method
	can only return service composition upon user requirement description
	and lacks flexibility to deal with environment change. Deliberate
	agent architecture, such as BDI agent, is hopeful to make SWSC more
	intelligent. In this paper, we propose an automatic SWSC enabling
	method for AgentSpeak agent. Firstly, conversion algorithm from OWL-S
	Web service description to agent's plan set (OWLS2APS) is presented.
	Target service is converted to agent's goal and related services
	are converted into agent's plan set. Then, SWSC is automatically
	performed through agent's intention formation. Agent invokes Web
	service according to service sequence converted back from its intention.
	Agent can behave rationally with rules or ask for human intervention
	when SWSC or service invocation is not feasible. At last, a case
	study on enterprise credit rating service composition is presented
	to illustrate the method.},
  doi = {10.1109/GPC.WORKSHOPS.2008.20},
  keywords = {agent architecture,agent intention execution,agent intention formation,AgentSpeak
	agent,artificial intelligence planning,automatic semantic Web service
	composition,enterprise credit rating service composition,OWL-S Web
	service description,OWLS2APS,service sequence,user requirement description},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Huang2007,
  author = {Chien-feng Huang and Jasleen Kaur and Ana Maguitman and Luis M Rocha},
  title = {Agent-based model of genotype editing.},
  journal = {Evol Comput},
  year = {2007},
  volume = {15},
  pages = {253--289},
  number = {3},
  abstract = {Evolutionary algorithms rarely deal with ontogenetic, non-inherited
	alteration of genetic information because they are based on a direct
	genotype-phenotype mapping. In contrast, several processes have been
	discovered in nature which alter genetic information encoded in DNA
	before it is translated into amino-acid chains. Ontogenetically altered
	genetic information is not inherited but extensively used in regulation
	and development of phenotypes, giving organisms the ability to, in
	a sense, re-program their genotypes according to environmental cues.
	An example of post-transcriptional alteration of gene-encoding sequences
	is the process of RNA Editing. Here we introduce a novel Agent-based
	model of genotype editing and a computational study of its evolutionary
	performance in static and dynamic environments. This model builds
	on our previous Genetic Algorithm with Editing, but presents a fundamentally
	novel architecture in which coding and non-coding genetic components
	are allowed to co-evolve. Our goals are: (1) to study the role of
	RNA Editing regulation in the evolutionary process, (2) to understand
	how genotype editing leads to a different, and novel evolutionary
	search algorithm, and (3) the conditions under which genotype editing
	improves the optimization performance of traditional evolutionary
	algorithms. We show that genotype editing allows evolving agents
	to perform better in several classes of fitness functions, both in
	static and dynamic environments. We also present evidence that the
	indirect genotype/phenotype mapping resulting from genotype editing
	leads to a better exploration/exploitation compromise of the search
	process. Therefore, we show that our biologically-inspired model
	of genotype editing can be used to both facilitate understanding
	of the evolutionary role of RNA regulation based on genotype editing
	in biology, and advance the current state of research in Evolutionary
	Computation.},
  doi = {10.1162/evco.2007.15.3.253},
  institution = {Los Alamos National Laboratory, Los Alamos, NM 87545, USA.},
  keywords = {Algorithms; Computational Biology, methods; Computer Simulation; Evolution,
	Molecular; Genome; Genotype; Models, Genetic; Models, Statistical;
	Models, Theoretical; Oscillometry; Phenotype; RNA Editing; Software},
  owner = {user},
  pmid = {17705779},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1162/evco.2007.15.3.253}
}

@INPROCEEDINGS{huang_using_2007,
  author = {C. Huang and C. Young and H. Liu and S.F. Tzou and D. Tsui and A.
	Tsai and E. Chang},
  title = {Using design based binning to improve defect excursion control for
	45nm production},
  booktitle = {Semiconductor Manufacturing, 2007. ISSM 2007. International Symposium
	on},
  year = {2007},
  pages = {1â€•3},
  abstract = {For advanced device (45 nm and below), we proposed a novel method
	to monitor systematic and random excursion. By integrating design
	information and defect inspection results into automated software
	(DBB), we can identify design/process marginality sites with defect
	inspection tool. In this study, we applied supervised binning function
	(DBC) and defect criticality index (DCI) to identify systematic and
	random excursion problems on 45 nm SRAM wafers. With established
	SPC charts, we will be able to detect future excursion problem in
	manufacturing line early.},
  doi = {10.1109/ISSM.2007.4446790},
  isbn = {1523-553X},
  keywords = {automated software,defect criticality index,defect excursion control,defect
	inspection,design based binning,manufacturing line,random excursion,semiconductor
	device manufacture,size 45 nm,SPC chart,SRAM chips,SRAM wafer,supervised
	binning function,systematic excursion},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{qi_huang_software_2006,
  author = {Qi Huang and Kaiyu Qin and Wenyong Wang},
  title = {A Software Architecture Based on Multi-Agent and Grid Computing for
	Electric Power System Applications},
  booktitle = {Parallel Computing in Electrical Engineering, 2006. PAR ELEC 2006.
	International Symposium on},
  year = {2006},
  pages = {405--410},
  abstract = {This paper first reviews the application of computing technology in
	electric power system, with emphasis on the evolution of the software
	architecture. And then proposes a software architecture to implement
	power system operation and computing. As power systems have become
	more complicated, software tools and simulations have played a more
	important role. The operation and control of the power system is
	actually becoming a huge data-intensive, information-intensive, communication-intensive
	and computing-intensive application. Power systems rely more heavily
	on computerized communications and control. Therefore, system security
	has become increasingly dependent on protecting the integrity of
	the associated information systems. Grid computing is generally regarded
	as a software technology to fully use the spare computing resources.
	However, the philosophy of grid computing can be used in engineering
	case to play an important role in power system distributed monitoring,
	control and distributed parallel computing. This paper proposes a
	software architecture, which depends on grid computing for hardware
	support and agent technology for software support, to seamlessly
	integrate the dispersed computing resources to implement high-performance
	operation and computing in electric power system. Some comparative
	test cases are studied, and the test results show that the combination
	of agent and grid computing can enhance the performance of the distributed
	computing system},
  keywords = {distributed parallel computing system,Electric power system,electric
	power system application,grid computing,Grid computing,multiagent
	system,Multi-agent technology,power engineering computing,power system
	control,power system distributed monitoring,power system operation,power
	system security},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{hui_supporting_2001,
  author = {K. Hui and J. Appavoo and R. Wisniewski and M. Auslander and D. Edelsohn
	and B. Gamsa and O. Krieger and B. Rosenburg and M. Stumm},
  title = {Supporting hot-swappable components for system software},
  booktitle = {Hot Topics in Operating Systems, 2001. Proceedings of the Eighth
	Workshop on},
  year = {2001},
  pages = {170},
  abstract = {Summary form only given. A hot-swappable component is one that can
	be replaced with a new or different implementation while the system
	is running and actively using the component. For example, a component
	of a TCP/IP protocol stack, when hot-swappable, can be replaced (perhaps
	to handle new denial-of-service attacks or improve performance),
	without disturbing existing network connections. The capability to
	swap components offers a number of potential advantages such as:
	online upgrades for high availability systems, improved performance
	due to dynamic adaptability and simplified software structures by
	allowing distinct policy and implementation options to be implemented
	in separate components (rather than as a single monolithic component)
	and dynamically swapped as needed. In order to hot-swap a component,
	it is necessary to (i) instantiate a replacement component; (ii)
	establish a quiescent state in which the component is temporarily
	idle; (iii) transfer state from the old component to the new component;
	(iv) swap the new component for the old; and (v) deallocate the old
	component.},
  keywords = {client-server systems,denial-of-service attacks,dynamic adaptability,high
	availability systems,hot-swappable components,implementation options,monolithic
	component,network connections,object-oriented programming,old component
	deallocation,online upgrades,operating systems (computers),quiescent
	state,replacement component,simplified software structures,system
	software,TCP/IP protocol stack},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{hunt_transitioning_2006,
  author = {Christopher Hunt and Martin Wickham and Milos Dusek},
  title = {Transitioning to Lead-free: the Effect on Low Cycle Fatigue of Contaminating
	Solder Alloys},
  booktitle = {Electronics Systemintegration Technology Conference, 2006. 1st},
  year = {2006},
  volume = {1},
  pages = {448â€•456},
  abstract = {As RoHS comes into force, the impact on industry is to force it to
	struggle with mixed inventory, conventional tin-lead and lead-free.
	There are many stories circulating in the industry where the label
	does not correspond with the components. Consequently there is real
	concern of mixing different alloy systems that will have unpredictable
	results in terms of low cycle fatigue performance. Some workers have
	shown that lead at the 1 to 10\% level may not be evenly distributed
	throughout the solder joint (Oliver, et. al., 2002). Segregation
	of contaminates at the joint level may well result in structural
	weakness. What is the effect of this on thermal fatigue. The work
	reported here will describe experiments where the lead level in solder
	joints was controlled by altering the plating on component terminations
	and using controlled solder compositions. Microstructural examination
	verifies the segregation of lead. The built assemblies were then
	thermally cycled between -55 and 125degC for 2000 cycles to assess
	this effect on reliability. The work has indicated that there should
	be few solder joint reliability problems when mixing SnPb and LF
	components and solder alloys (with Pb contamination in the range
	1 to 10\%). Very few thermal cycle fatigue failures were experienced
	other than within two component groups. Ball grid array components
	did fail generally, in the rings of balls adjacent to the edge of
	the silicon die within the package. However, the failures in these
	devices were largely restricted to SnPb alloy dominated systems,
	i.e. SnPb terminated components soldered with SnPb or SAC alloy solder
	pastes. Uncontaminated SAC systems or those systems contaminated
	with low levels of Pb showed fewer failures and thus must be considered
	more reliable. Indeed, the system showing greatest thermal cycle
	fatigue in BGA components was the SnPb terminated EGAs with SnPb
	solder. All other systems were shown to perform better. The other
	component type to show significant failures, - were the QFP components
	where failures were confined to Sn-plated components with SAC solder.
	The process window for SAC alloy soldering is narrower than for equivalent
	SnPb processing and small additions of Pb may help widen the process
	window, improving the reliability for these soldering batches. It
	is therefore probable that these differences in QFP component reliability
	are batch related. Hot peel tests were also run to simulate problems
	that may occur is secondary wave operations where the fillet strength
	collapses and components can detach with little force at temperatures
	above 180degC This paper will discuss these results and the likely
	impact on the industry and the necessary precautions},
  doi = {10.1109/ESTC.2006.280041},
  keywords = {ball grid array,ball grid arrays,BGA components,contamination,controlled
	solder compositions,crystal microstructure,lead free transition,lead
	level,low cycle fatigue effect,microstructural examination,QFP components,SnPb,solder
	alloys contamination,thermal fatigue,thermal stress cracking,tin
	alloys},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{kao-shing_hwang_homogeneous_2003,
  author = {Kao-Shing Hwang and H.C.-H. Hsu and A. Liu},
  title = {A homogeneous agent architecture for robot navigation},
  booktitle = {Neural Networks and Signal Processing, 2003. Proceedings of the 2003
	International Conference on},
  year = {2003},
  volume = {1},
  pages = {310--315 Vol.1},
  abstract = {A homogeneous agent architecture consists of agents having the same
	or similar structures in one system. The design introduces the advantage
	of reducing the development time. In this architecture, the engineer
	would only design one or few templates for agents, and every agent
	is constructed by one of these templates. Two categories of agents
	in the proposed system are the primitive agent and the behavioral
	agent. The behavioral agents play the role of leaders, so that they
	can produce output signals through combining the signals of those
	primitive agents. Hence, we could increase new capability of the
	whole system flexibly through adding new primitive agents with different
	capability and adding new behavioral agents to process their opinions.
	In this paper, we use this homogeneous agent architecture to control
	a autonomous mobile robot, and it performs well in simulation.},
  doi = {10.1109/IAT.2005.43},
  keywords = {ART-based AHC,autonomous mobile robot control,behavioral agent,development
	time reduction,homogeneous agent architecture,mobile robots,multiagent
	system,path planning,primitive agent,reinforcement learning,robot
	navigation},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{ibrahim_distributed_2006,
  author = {M.A.M. Ibrahim},
  title = {Distributed Network Management with Secured Mobile Agent Support},
  booktitle = {Hybrid Information Technology, 2006. ICHIT '06. International Conference
	on},
  year = {2006},
  volume = {1},
  pages = {244--251},
  abstract = {Network computing is changing rapidly these days. The mobile agent
	technology invented to overcome the complexity resulting due to the
	increasing size of network components rises new network management
	schemes. Many prototype applications providing mobile agent capability
	have been proposed for being used in network management. E-commerce
	and information retrieval are some of them. The motive behind the
	agent mobility is that, it addresses some limitations faced by traditional
	centralized client-server architecture, which are mainly, minimizing
	bandwidth consumption, supporting network load balancing, enhancing
	scalability as well as flexibility, increase fault tolerance and
	solve problems caused by unreliable network connections. However,
	despite its benefits, mobile agent systems still pose security threats.
	In this paper we propose a mobile agent architecture that supports
	flexible and reliable interaction of autonomous components in a distributive
	network environment. We present a management scheme in a hierarchical
	level that provides to a user with a reliable and flexible global
	access to Internet/network information services. We further describe
	a protection mechanism to both agents and their hosting sites of
	execution called agent servers},
  doi = {10.1109/ICHIT.2006.253494},
  keywords = {bandwidth consumption,distributed network management,mobile agent
	architecture,mobile agents,network information services,network load
	balancing,resource allocation,secured mobile agent support},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{ince_approach_1988,
  author = {D.C. Ince and S. Hekmatpour},
  title = {An approach to automated software design based on product metrics
	},
  journal = {Software Engineering Journal},
  year = {1988},
  volume = {3},
  pages = {53â€•56},
  number = {2},
  abstract = {Recent research has attempted to quantify the quality of a software
	design in terms of measurable properties. A weakness of this research
	is that it ignores the fact that software design involves the exploration
	of alternative software architectures. An approach to software design
	selection is described which involves examining a design solution
	space. It relies on the formulation of the problem of selecting software
	designs in operational research terms. The paper concludes with a
	description of a software tool based on the approach},
  doi = {10.1109/32.991319},
  issn = {0268-6961},
  keywords = {automated software design,automatic programming,design solution space,operations
	research,product metrics,software architectures,software design selection,software
	engineering,software quality,software tool,software tools},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{interrante_active_1994,
  author = {L.D. Interrante and D.M. Rochowiak},
  title = {Active rescheduling for automated guided vehicle systems},
  journal = {Intelligent Systems Engineering},
  year = {1994},
  volume = {3},
  pages = {87--100},
  number = {2},
  abstract = {The paper examines the use of knowledge-based techniques to generate
	a framework for the active rescheduling of an automated guided vehicle
	system in a manufacturing environment. The authors' approach to active
	rescheduling uses `cues' drawn from events on the shop-floor to trigger
	rescheduling. Simulation experiments are used to capture knowledge
	about the shop-floor and various scheduling strategies. An extensible
	agent architecture is developed to facilitate active rescheduling},
  doi = {10.1109/MIS.2005.13},
  issn = {0963-9640},
  keywords = {active rescheduling,automated guided vehicle systems,automatic guided
	vehicles,extensible agent architecture,knowledge-based techniques,manufacturing
	environment,shop-floor},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{irmert_runtime_2008,
  author = {Florian Irmert and Thomas Fischer and Klaus Meyer-Wegener},
  title = {Runtime adaptation in a service-oriented component model},
  booktitle = {Proceedings of the 2008 international workshop on Software engineering
	for adaptive and self-managing systems},
  year = {2008},
  pages = {97â€•104},
  address = {Leipzig, Germany},
  publisher = {ACM},
  abstract = {Developing software applications which manage, optimize or adapt themselves
	at runtime requires an architecture which provides adaptation of
	software components at runtime. An architecture model that has gained
	a lot of attention in recent years is SOA (service-oriented architecture).
	In a SOA environment services as well as applications build up complex
	dependencies. Therefore it is crucial for self-managing SOA applications
	to adapt services at runtime without interference of the application
	execution and the service availability. In this paper, we discuss
	the problems arising from the requirement of runtime adaptation and
	present our solution by replacing service implementations at execution
	time in a service-oriented component model. For a seamless integration
	we strive for a transparent and atomic replacement of a service implementation
	in respect to the other services/applications.},
  doi = {10.1145/1370018.1370036},
  isbn = {978-1-60558-037-1},
  keywords = {adaptation,component replacement,migration,modularity,service-oriented
	architecture},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1370018.1370036\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{isozaki_new_2005,
  author = {T. Isozaki and K. Horiuchi and H. Kashimura},
  title = {A New E-mail Agent Architecture Based on Semi-supervised Bayesian
	Networks},
  booktitle = {Computational Intelligence for Modelling, Control and Automation,
	2005 and International Conference on Intelligent Agents, Web Technologies
	and Internet Commerce, International Conference on},
  year = {2005},
  volume = {1},
  pages = {739--744},
  abstract = {A new e-mail agent architecture with a Bayesian network (BN) has been
	investigated in order to detect important e-mail (IM) of office users.
	The BN has nodes related with usersÂ’ resultant behaviors during
	the e-mail operation, which enables to adapt the agent for usersÂ’
	intentions by implicit feedbacks, called semi-supervised learning.
	We have investigated 5 examinees for 2 months. It is certain that
	our BN is so effective for the detection of IM, because we obtain
	an accuracy of as high as 0.924 by a fully supervised learning. Moreover,
	the similar accuracy can be obtained in the semi-supervised learning,
	where the nodes of resultant behaviors can be properly working in
	a cycle of the implicit feedback, as an alternative for usersÂ’ questionnaires.},
  doi = {10.1109/CIMCA.2005.1631352},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{jahnke_engineering_2001,
  author = {Jens H. Jahnke},
  title = {Engineering component-based net-centric systems for embedded applications},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {2001},
  volume = {26},
  pages = {218â€•228},
  number = {5},
  abstract = {The omnipresence of the Internet and the World Wide Web (Web) via
	phone lines, cable-TV, power lines, and wireless RF devices has created
	an inexpensive media for telemonitoring and remotely controlling
	distributed electronic appliances. The great variety of potential
	benefits of aggregating and connecting embedded systems over the
	Internet is matched by the currently unsolved problem of how to design,
	test, maintain, and evolve such heterogeneous, collaborative systems.
	Recently, component-oriented software development has shown great
	potential for cutting production costs and improving the maintainability
	of systems. We discuss component-oriented engineering of embedded
	control software in the light of emerging requirements of distributed,
	net-centric systems. Our approach is baed on applying the graphical
	specification language SDL for composing complex networks of embedded
	software components. From the SDL specification, we generate internet-aware
	connector components to local embedded controller networks. The described
	research is carried out in a collaborative effort between industry
	and academia.},
  doi = {10.1145/503271.503239},
  keywords = {component-oriented development,embedded softare,network-centric computing,sdl},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=503271.503239\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{jain_assessment_2003,
  author = {Hemant Jain and Padmal Vitharana and Fatemah "Mariam" Zahedi},
  title = {An assessment model for requirements identification in component-based
	software development},
  journal = {SIGMIS Database},
  year = {2003},
  volume = {34},
  pages = {48â€•63},
  number = {4},
  abstract = {Software development literature is replete with studies that demonstrate
	how ineffective requirements analysis (RA) has led to failed applications.
	Some of the difficulties encountered in RA however are due to inherent
	limitations in traditional approach to software development. On the
	other hand, component-based software development (CBSD) presents
	a unique approach to developing software. Components advertise the
	services they offer and could be organized in a knowledge-base (i.e.,
	repository). CBSD paradigm provides an effective communication vehicle
	for users and analysts by enabling them to uncover requirements as
	they navigate through the component knowledge-base.In this paper,
	we draw from the information processing theory (IPT) on problem solving
	to develop an assessment model for evaluating the impact of CBSD
	on requirements identification, arguing that the access to components
	in a knowledge-base facilitates the requirements identification.
	The key elements of the IPT on problem solving are information processing
	system of the problem solver, task environment and internal representation
	of the problem space. We propose that access to a component knowledge-base
	enhances information processing system of the problem solver and
	simplifies the task environment which together improve user's internal
	representation of the problem space. This theoretical framework makes
	it possible to empirically test the impact of CBSD on requirements
	identification process.},
  doi = {10.1145/957758.957765},
  keywords = {component-based software development,information processing theory,requirements
	analysis},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=957758.957765\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{james_using_2000,
  author = {C.J. James and D. Lowe},
  title = {Using independent component analysis \& dynamical embedding to isolate
	seizure activity in the EEG},
  booktitle = {Engineering in Medicine and Biology Society, 2000. Proceedings of
	the 22nd Annual International Conference of the IEEE},
  year = {2000},
  volume = {2},
  pages = {1329â€•1332 vol.2},
  abstract = {We present a methodology for isolating the underlying seizure activity
	in multichannel scalp EEG. A number of seizure segments from various
	patients are extracted from the fetal EEG recorded in an epilepsy
	monitoring unit. We use the method of independent component analysis
	(ICA) in order to decompose the recorded scalp EEG into its underlying
	temporal and spatial components. Seizure-related activity in the
	independent components is identified by first performing a dynamical
	embedding on each component. Through a value linked to the dynamic
	complexity of the EEG segments it is possible to visually analyze
	the spatiotemporal components. In this proof of principle study,
	in 4 seizure EEGs analyzed we can identify what appear to be the
	relevant seizure components-there is more than one in each case.
	We identify seizure related activity by choosing those temporal components
	that depict a decrease in complexity around seizure onset, coupled
	with focal activity in the region grossly identified as being the
	origin from the raw scalp EEG. In addition to seizure related components,
	artifactual components are also adequately isolated by ICA. Decomposing
	the EEG in this way means that the scalp EEG can either be â€œremappedâ€?
	using only the identified seizure components, or further in-depth
	analysis on the seizure can be undertaken on the spatiotemporal components
	directly. Although subjective, these preliminary results indicate
	that ICA coupled with complexity analysis may be beneficial in processing
	the epileptiform EEG prior to further in-depth analysis},
  doi = {10.1109/DCC.2008.27},
  keywords = {artifactual components,blind separation,dynamic complexity,dynamical
	embedding,electroencephalography,epilepsy monitoring,epileptiform
	EEG,fetal EEG,higher order statistics,higher-order statistics,independent
	component analysis,medical signal processing,mixed input data,multichannel
	scalp EEG,paediatrics,seizure activity isolation,seizure segments,signal
	classification,singular value decomposition,spatial components,SVD,temporal
	components},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{a._jansen_software_2005,
  author = {A. Jansen and J. Bosch},
  title = {Software Architecture as a Set of Architectural Design Decisions},
  booktitle = {Software Architecture, 2005. WICSA 2005. 5th Working IEEE/IFIP Conference
	on},
  year = {2005},
  pages = {109--120},
  abstract = {Software architectures have high costs for change, are complex, and
	erode during evolution. We believe these problems are partially due
	to knowledge vaporization. Currently, almost all the knowledge and
	information about the design decisions the architecture is based
	on are implicitly embedded in the architecture, but lack a first-class
	representation. Consequently, knowledge about these design decisions
	disappears into the architecture, which leads to the aforementioned
	problems. In this paper, a new perspective on software architecture
	is presented, which views software architecture as a composition
	of a set of explicit design decisions. This perspective makes architectural
	design decisions an explicit part of a software architecture. Consequently,
	knowledge vaporization is reduced, thereby alleviating some of the
	fundamental problems of software architecture.},
  doi = {10.1109/WICSA.2005.61},
  owner = {user},
  timestamp = {2008.10.04}
}

@ARTICLE{jarzabek_synergy_1999-1,
  author = {Stan Jarzabek and Peter Knauber},
  title = {Synergy between component-based and generative approaches},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {1999},
  volume = {24},
  pages = {429â€•445},
  number = {6},
  abstract = {Building software systems out of pre-fabricated components is a very
	attractive vision. Distributed Component Platforms (DCP) and their
	visual development environments bring this vision closer to reality
	than ever. At the same time, some experiences with component libraries
	warn us about potential problems that arise in case of software system
	families or systems that evolve over many years of changes. Indeed,
	implementation level components, when affected by many independent
	changes, tend to grow in both size and number, impeding reuse. In
	this paper, we analyze in detail this effect and propose a program
	construction environment, based on generative techniques, to help
	in customization and evolution of component-based systems. This solution
	allows us to reap benefits of DCPs during runtime and, at the same
	time, keep components under control during system construction and
	evolution. In the paper, we describe such a construction environment
	for component-based systems that we built with a commercial generator
	and illustrate its features with examples from our domain engineering
	project. The main lesson learnt from our project is that generative
	techniques can extend the strengths of the component-based approach
	in two important ways: Firstly, generative techniques automate routine
	component customization and composition tasks and allow developers
	work more productively, at a higher abstraction level. Secondly,
	as custom components with required properties are generated on demand,
	we do not need to store and manage multiple versions of components,
	components do not overly grow in size, helping developers keep the
	complexity of an evolving system under control.},
  doi = {10.1145/318774.319260},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=318774.319260\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{jarzabek_engineering_2000,
  author = {S. Jarzabek and R. Seviora},
  title = {Engineering components for ease of customisation and evolution},
  journal = {Software, IEE Proceedings -},
  year = {2000},
  volume = {147},
  pages = {237â€•248},
  number = {6},
  abstract = {Building software systems from prefabricated components is a very
	attractive vision. Distributed component platforms (DCP) and their
	visual development environments bring this vision closer. However,
	some experiences with component libraries warn us about potential
	problems that arise when software-system families or systems evolve
	over many years of changes. Indeed, implementation-level components,
	when affected by many independent changes, tend to grow in both size
	and number, impeding reuse. This unwanted effect is analysed in detail.
	It is argued that components affected by frequent unexpected changes
	require higher levels of flexibility than the `plug-and-play' paradigm
	is able to provide. A program construction environment is proposed,
	based on generative programming techniques, to help in customisation
	and evolution of components that require much flexibility. This solution
	allows the benefits of DCPs to be reaped during runtime and, at the
	same time, keeps components under control during system construction
	and evolution. Salient features of a construction environment for
	component based systems are discussed. Its implementation with commercial
	reuse technology FusionTM is described. The main lesson learnt from
	the project is that generative-programming techniques can extend
	the strengths of the component based approach in two important ways:
	1) generative-programming techniques automate routine component customisation
	and composition tasks and allow developers work more productively,
	at a higher abstraction level; 2) as custom components with required
	properties are generated on demand, it is not necessary to store
	and manage multiple versions of components},
  doi = {10.1109/ICCBSS.2006.26},
  issn = {1462-5970},
  keywords = {abstraction level,commercial reuse technology,component based approach,component
	based systems,component engineering,component libraries,configuration
	management,construction environment,custom components,DCPs,distributed
	component platforms,distributed object management,Fusion,generative
	programming techniques,generative-programming techniques,implementation-level
	components,independent changes,management of change,prefabricated
	components,program construction environment,routine component customisation,software
	reusability,software systems,software-system families,system construction,visual
	development environments},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{jiao_eliminating_2003,
  author = {Wenpin Jiao and Hong Mei},
  title = {Eliminating mismatching connections between components by adopting
	an agent-based approach},
  booktitle = {Tools with Artificial Intelligence, 2003. Proceedings. 15th IEEE
	International Conference on},
  year = {2003},
  pages = {358â€•365},
  abstract = {During component composition, mismatches may occur on different aspects,
	such as interaction behaviors between components and features imposed
	by architectural styles. In this paper, we studied architectural
	mismatches related to connecting components using a specified architectural
	style, which implies that the connections supported by components
	may be incompatible with the connection supposed by the architectural
	style. First, we formalized components involved in different architectural
	styles in the pi-calculus. Next, we studied the formal foundation
	of the interconnectivity between components to exploit under what
	situation two heterogeneous components are possible to interconnect
	together properly. Then, we described an adaptor-based solution for
	composing components supporting different architectural styles by
	introducing the notation of negative component. In the end of this
	paper, we presented an agent-based implementation for the solution,
	in which agents are used to wrap components and can automatically
	transform messages specific to one architectural style into messages
	specific to another style by using architectural style-specific knowledge
	that agents possess.},
  doi = {10.1109/COMPSAC.2006.117},
  isbn = {1082-3409 },
  keywords = {adaptor-based solution,agent-based approach,architectural mismatches,architectural
	style,component composition,component-based software development,components
	interconnectivity,connecting components,heterogeneous components,interaction
	behavior,message specific,message transformation,mismatching connection,negative
	component,object-oriented programming,pi calculus,pi-calculus,software
	agents,software architecture,style-specific knowledge},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{zhang_jingjun_researchaspect_2007,
  author = {Zhang Jingjun and Li Hui and Li Furong},
  title = {Research on Aspect Connectors for Software Architecture Adaptation},
  booktitle = {Software Engineering Workshop, 2007. SEW 2007. 31st IEEE},
  year = {2007},
  pages = {63--66},
  abstract = {Software connectors are an important part of software architecture,
	which are responsible for the interactions between components, have
	an important impact on software architecture adaptation. The current
	research on connectors which contribute to enhancing software architecture
	adaptation is not yet mature. In this article we present an approach
	that introduces Aspect-Oriented technique into connectors based on
	the combination of AOSD and CBSD, and extracts the non-functional
	attributes of system as the first-class entities just like components.
	We also propose the definition of what we have called aspect connectors
	and their models by means of UML. This approach utilizes the pointcut
	and advice mechanism of Aspect-Oriented Programming to achieve the
	interactions between components, which is an effective solution to
	enhance software architecture adaptation. Finally, a case of printer
	system is used to expound the specific process, which shows the interactions
	between aspect connectors and components, and verified the improvement
	of software architecture adaptation which explained from three aspects.},
  doi = {10.1109/SEW.2007.31},
  isbn = {1550-6215},
  keywords = {aspect connectors,aspect-oriented technique,nonfunctional attributes
	extraction,printer system,software connectors,UML},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{jrgensen_language_2004,
  author = {Bo NÃ¸rregaard JÃ´rgensen},
  title = {Language support for incremental integration of independently developed
	components in Java},
  booktitle = {Proceedings of the 2004 ACM symposium on Applied computing},
  year = {2004},
  pages = {1316â€•1322},
  address = {Nicosia, Cyprus},
  publisher = {ACM},
  abstract = {The aim of component-based software development is to assembly applications
	from existing components, writing as little extra code as possible.
	For programmers, assembly of applications from existing components
	should increase reuse, thus allowing them to concentrate on value-added
	tasks and to produce high-quality software within a shorter time.
	For users, component-based software development promises tailor made
	functionality from the adaptaion of ready-made components. However,
	this ideal scenario has not yet become reality: today, many applications
	are still developed from scratch, and there are still relatively
	few ready-made components that can be easily reused in new applications.
	Why is it so? We believe that part of the answer is that current
	object-oriented programming languages are missing support for non-invasive
	dynamic adaptation that works at the level of multiple objects simultaneously.
	Such support would allow unanticipated, incremental modifications
	of a system's components at runtime. In this paper we propose Lasagne/J
	an extension of the Java programming language that helps programmers
	to overcome many of the integration problems that they face when
	assembling new applications from components developed by independent
	component vendors.},
  doi = {10.1145/967900.968166},
  isbn = {1-58113-812-1},
  keywords = {component integration,component-based development,dynamic adaptation,independent
	extensibility,non-invasive evolution},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=967900.968166\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{lianfang_kong_multi-layered_2007,
  author = {Lianfang Kong and Lei Xiao},
  title = {A Multi-Layered Control Architecture of Intelligent Agent},
  booktitle = {Control and Automation, 2007. ICCA 2007. IEEE International Conference
	on},
  year = {2007},
  pages = {1454--1458},
  abstract = {Usually control architecture of an intelligent Agent is structured
	as a single level, all related control issues are determined in this
	level. Since the Agent may work in a complex organizational context
	and it may have multiple complicated interdependent tasks to process.
	Obviously, it is difficult to construct an integrated control framework
	in which all the control issues are determined in an efficient way.
	In this paper, a three-layered control architecture for an intelligent
	Agent is presented. The control Agent architecture consists of three
	layers: global control layer, detailed control layer and execution
	layer, the first two control layers are decision making layers. Each
	control layer in this control architecture has different function
	in control process: global control layer deals with the formation
	of high level goals and objectives for the Agent. Detailed control
	layer deals with feasibility and implementation of operations. The
	execution layer executes those actions determined by the upper control
	layers. Furthermore we use TAEMS (task analysis environment modeling
	and simulation) to represent structures of those tasks that agent
	has to execute. The primary advantage of this three-layered control
	Agent architecture is that all the control issues can be addressed
	concurrently and done in an efficient way.},
  doi = {10.1109/ICCA.2007.4376602},
  keywords = {control agent architecture,control architecture,decision making,decision
	making layer,detailed control layer,execution layer,global control
	layer,integrated control framework,intelligent agent,intelligent
	Agent,intelligent control,multi-layered,multilayered control architecture,TAEMS,task
	analysis environment modeling,task analysis environment simulation},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Konsynski1990,
  author = {B. R. Konsynski and F. W. McFarlan},
  title = {Information partnerships--shared data, shared scale.},
  journal = {Harv Bus Rev},
  year = {1990},
  volume = {68},
  pages = {114--120},
  number = {5},
  abstract = {How can one company gain access to another's resources or customers
	without merging ownership, management, or plotting a takeover? The
	answer is found in new information partnerships, enabling diverse
	companies to develop strategic coalitions through the sharing of
	data. The key to cooperation is a quantum improvement in the hardware
	and software supporting relational databases: new computer speeds,
	cheaper mass-storage devices, the proliferation of fiber-optic networks,
	and networking architectures. Information partnerships mean that
	companies can distribute the technological and financial exposure
	that comes with huge investments. For the customer's part, partnerships
	inevitably lead to greater simplification on the desktop and more
	common standards around which vendors have to compete. The most common
	types of partnership are: joint marketing partnerships, such as American
	Airline's award of frequent flyer miles to customers who use Citibank's
	credit card; intraindustry partnerships, such as the insurance value-added
	network service (which links insurance and casualty companies to
	independent agents); customer-supplier partnerships, such as Baxter
	Healthcare's electronic channel to hospitals for medical and other
	equipment; and IT vendor-driven partnerships, exemplified by ESAB
	(a European welding supplies and equipment company), whose expansion
	strategy was premised on a technology platform offered by an IT vendor.
	Partnerships that succeed have shared vision at the top, reciprocal
	skills in information technology, concrete plans for an early success,
	persistence in the development of usable information for all partners,
	coordination on business policy, and a new and imaginative business
	architecture.},
  institution = {Information Systems Special Interest Group, Harvard Business School.},
  keywords = {Economic Competition; Industry, organization /&/ administration; Information
	Systems; Interinstitutional Relations; Organization and Administration;
	Planning Techniques; United States},
  owner = {user},
  pmid = {10107083},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{koolmanojwong_agent_2004,
  author = {S. Koolmanojwong and R. Jiamthapthaksin and J. Daengdej},
  title = {An agent architecture for competitive application environment},
  booktitle = {Aerospace Conference, 2004. Proceedings. 2004 IEEE},
  year = {2004},
  volume = {5},
  pages = {3079--3089 Vol.5},
  abstract = {Multi-agent technology has recently become one of the promising technologies
	used in various complex applications. The use of the concept of multi-agent
	system can increase productivity of organization both in cooperative
	and competitive environments. In complex environments, there could
	be a large number of agents working together. When working with different
	types of agents, the agents may not know whether they are dealing
	with teammates or opponents. For the agents, error may easily occur
	unexpectedly in such a complex situation. This is simply because
	an unsecured or equivocated environment may be intentionally generated
	by a type of agent referred to as "a lying agent". This is especially
	true in the competitive environment where the lying agent needs to
	achieve its goal and resources are very limited. With such possibility,
	there is a clear potential benefit for ones who intend to apply the
	technology to understand how the lying agents could behave. In this
	paper, by using a special kind of auction existing in the real-world
	as our problem domain, we exploit an agent architecture and also
	modeling technique used by the lying agent. The experimental result
	has shown that the lying agents can efficiently achieve their goals
	while leaving others to only complete their tasks, but not with what
	the others had intended to do originally.},
  isbn = {1095-323X},
  keywords = {agent architecture,application environment,auction,competitive environment,cooperative
	environment,equivocated environment,lying agent,multiagent system,multiagent
	technology,unsecured environment},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Koutkias2005,
  author = {Vassilis G Koutkias and Ioanna Chouvarda and Nicos Maglaveras},
  title = {A multiagent system enhancing home-care health services for chronic
	disease management.},
  journal = {IEEE Trans Inf Technol Biomed},
  year = {2005},
  volume = {9},
  pages = {528--537},
  number = {4},
  month = {Dec},
  abstract = {In this paper, a multiagent system (MAS) is presented, aiming to enhance
	monitoring, surveillance, and educational services of a generic medical
	contact center (MCC) for chronic disease management. In such a home-care
	scenario, a persistent need arises for efficiently monitoring the
	patient contacts and the MCC's functionality, in order to effectively
	manage and interpret the large volume of medical data collected during
	the patient sessions with the system, and to assess the use of MCC
	resources. Software agents were adopted to provide the means to accomplish
	such real-time information-processing tasks, due to their autonomous,
	reactive and/or proactive nature, and their effectiveness in dynamic
	environments by incorporating coordination strategies. Specifically,
	the objective of the MAS is to monitor the MCC environment, detect
	important cases, and inform the healthcare and administrative personnel
	via alert messages, notifications, recommendations, and reports,
	prompting them for actions. The main aim of this paper is to present
	the overall design and implementation of a proposed MAS, emphasizing
	its functional model and architecture, as well as on the agent interactions
	and the knowledge-sharing mechanism incorporated, in the context
	of a generic MCC.},
  institution = {Lab of Medical Informatics, Medical School, Aristotle University
	of Thessaloniki, Greece. bikout@med.auth.gr},
  keywords = {Artificial Intelligence; Databases, Factual; Decision Support Systems,
	Clinical, organization /&/ administration; Diagnosis, Computer-Assisted,
	methods; Home Care Services, organization /&/ administration; Information
	Dissemination, methods; Information Storage and Retrieval, methods;
	Medical Records Systems, Computerized, organization /&/ administration;
	Telemedicine, methods; Therapy, Computer-Assisted, methods; User-Computer
	Interface},
  owner = {user},
  pmid = {16379370},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{kumar_adaptive_2000,
  author = {S. Kumar and P.R. Cohen and H.J. Levesque},
  title = {The adaptive agent architecture: achieving fault-tolerance using
	persistent broker teams},
  booktitle = {MultiAgent Systems, 2000. Proceedings. Fourth International Conference
	on},
  year = {2000},
  pages = {159--166},
  abstract = {Brokered multi-agent systems can be incapacitated and rendered non-functional
	when the brokers become inaccessible due to failures that can occur
	in any distributed software system. We propose that the theory of
	teamwork can be used to specify robust brokered architectures that
	can recover from broker failures, and we present the adaptive agent
	architecture (AAA) to show the feasibility of this approach. The
	previous teamwork theory based on joint intentions assumes that team
	members remain in a team as long as the team exists. We extend this
	theory to allow dynamic broker teams whose members can change with
	time. We also introduce a theory of restorative maintenance goals
	that enables the brokers in an AAA broker team to start new brokers
	and recruit them to the broker team. As a result, an AAA-based multi-agent
	system can maintain a specified number of functional brokers in the
	system despite broker failures, thus effectively becoming a self-healing
	system},
  doi = {10.1109/ICSE.2001.919184},
  keywords = {adaptive agent architecture,brokered multi-agent systems,distributed
	algorithms,distributed software system,formal logic,persistent broker
	teams,restorative maintenance goals,robust brokered architectures,self-healing
	system,transport protocols},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{lantz_task_2003,
  author = {F. Lantz and D. Stromberg},
  title = {Task management in sensor-provided operator platforms},
  booktitle = {Integration of Knowledge Intensive Multi-Agent Systems, 2003. International
	Conference on},
  year = {2003},
  pages = {596--601},
  abstract = {An agent architecture for modelling of operator controlled platform
	interaction is presented. The approach accounts for distributed decision
	making in autonomous but cooperating operator guided mobile platforms.
	Each platform has a kernel which interacts with operators, sensors
	and other platforms. In platform-operator interaction, the agent
	model facilitates task management and automation control. For platform-platform
	interaction, a solution is proposed concerning the conflict between
	resource control and resource sharing in a network. In platform-sensor
	interaction, a buy \& sell model is applied. The agent architecture
	concept is based on well-established theories for operator situation
	awareness, cognitive systems engineering and data fusion.},
  keywords = {agent architecture,automation control,buy \& sell model,cognitive
	systems,cognitive systems engineering,data fusion,distributed decision
	making,human computer interaction,intelligent control,operator controlled
	platform interaction modeling,operator guided mobile platform,platform-operator
	interaction,platform-platform interaction,platform-sensor interaction,resource
	allocation,resource sharing,task management},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Lanzola1999,
  author = {G. Lanzola and L. Gatti and S. Falasconi and M. Stefanelli},
  title = {A framework for building cooperative software agents in medical applications.},
  journal = {Artif Intell Med},
  year = {1999},
  volume = {16},
  pages = {223--249},
  number = {3},
  month = {Jul},
  abstract = {Exploiting the information technology may have a great impact on improving
	cooperation and interoperability among the different professionals
	taking part to the process of delivering health care services. New
	paradigms are therefore being devised considering software systems
	as autonomous agents able to help professionals in accomplishing
	their duties. To this aim those systems should encapsulate the skills
	for solving a given set of tasks and possess the social ability to
	cooperate in order to fetch the required information and knowledge.
	This paper illustrates a methodology facilitating the development
	of interoperable intelligent software agents for medical applications
	and proposes a generic computational model for implementing them.
	That model may be specialized in order to support all the different
	information and knowledge related requirements of a Hospital Information
	System. The architecture is being tested for implementing a prototype
	system able to coordinate the joint efforts of the professionals
	involved in managing patients affected by Acute Myeloid Leukemia.},
  institution = {Department of Informatics and Systems Science, University of Pavia,
	Italy. giordano@aim.unipv.it},
  keywords = {Computer Simulation; Hospital Information Systems; Humans; Leukemia,
	Myeloid; Neural Networks (Computer); Patient Care Management; Software},
  owner = {user},
  pii = {S0933365799000081},
  pmid = {10397303},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{seokcheon_lee_estimating_2005,
  author = {Seokcheon Lee and S. Kumara},
  title = {Estimating global stress environment by observing local behavior
	in distributed multiagent systems},
  booktitle = {Automation Science and Engineering, 2005. IEEE International Conference
	on},
  year = {2005},
  pages = {215--219},
  abstract = {A multiagent system can be considered survivable if it adapts itself
	to varying stresses without considerable performance degradation.
	Such an adaptivity comprises of identifying the behavior of the agents
	in a society, relating them to stress situations, and then invoking
	control rules. This problem is a hard one, especially in distributed
	multiagent systems wherein the agent behaviors tend to be nonlinear
	and dynamic. In this paper, we study a supply chain planning system
	implemented in COUGAAR (cognitive agent architecture) and develop
	a methodology for identifying the behavior of agents through their
	behavioral parameters, and relating those parameters to stress situations.
	One important aspect of our approach is that we identify the stress
	situations of agents in the society by observing local behavior of
	one representative agent. This approach is motivated by the fact
	that a local time series can have the information of the dynamics
	of the entire system in deterministic dynamical systems. We validate
	our approach empirically through identifying the stress situations
	using k-nearest neighbor algorithm based on the behavioral parameters.},
  doi = {10.1109/IAT.2005.18},
  keywords = {cognitive agent architecture,cognitive systems,deterministic dynamical
	system,distributed multiagent system,global stress environment estimation,k-nearest
	neighbor algorithm,local time series,observing local behavior,supply
	chain management,supply chain planning system,survivability},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Leonardi2007,
  author = {Giorgio Leonardi and Silvia Panzarasa and Silvana Quaglini and Mario
	Stefanelli and Wil M P van der Aalst},
  title = {Interacting agents through a web-based health serviceflow management
	system.},
  journal = {J Biomed Inform},
  year = {2007},
  volume = {40},
  pages = {486--499},
  number = {5},
  month = {Oct},
  abstract = {The management of chronic and out-patients is a complex process which
	requires the cooperation of different agents belonging to several
	organizational units. Patients have to move to different locations
	to access the necessary services and to communicate their health
	status data. From their point of view there should be only one organization
	(Virtual Health-Care Organization) which provides both virtual and
	face-to-face encounters. In this paper we propose the Serviceflow
	Management System as a solution to handle these information and the
	communication requirements. The system consists of: (a) the model
	of the care process represented as a Serviceflow and developed using
	the Workflow Management System YAWL; (b) an organizational ontology
	representing the VHCO; and (c) agreements and commitments between
	the parties defined in a contract (represented as an XML document).
	On the basis of a general architecture we present an implementation
	in the area of Diabetes management.},
  doi = {10.1016/j.jbi.2006.12.002},
  institution = {>},
  keywords = {Delivery of Health Care, methods/organization /&/ administration;
	Expert Systems; Internet; Italy; Management Information Systems;
	Models, Organizational; Telemedicine, methods/organization /&/ administration;
	User-Computer Interface},
  owner = {user},
  pii = {S1532-0464(06)00136-5},
  pmid = {17258510},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1016/j.jbi.2006.12.002}
}

@INPROCEEDINGS{shi_li_new_2000,
  author = {Shi Li and Zhen Ye and Zengqi-Sun},
  title = {A new agent architecture for RoboCup tournament: cognitive architecture},
  booktitle = {Intelligent Control and Automation, 2000. Proceedings of the 3rd
	World Congress on},
  year = {2000},
  volume = {1},
  pages = {199--202 vol.1},
  abstract = {Many researchers are involved in designing and manufacturing new-types
	of robots to win the RoboCup tournament. As for the simulator league
	of RoboCup tournament, the SoccerServer system is a typical multi-agent
	system. In general, there are three kinds of agent architectures:
	deliberation, reactive agent and hybrid architecture. In imitating
	the cognitive architecture of the human's brain, a new type of agent
	architecture-cognitive architecture-is proposed and a robot soccer
	team underlying this framework is established. Our goal is to enable
	the intelligent robots possess the same structures as our brain and
	to develop the functions for each module to improve the intelligence
	of the robots},
  doi = {10.1109/ISADS.2001.917427},
  keywords = {agent architecture,cognitive architecture,cognitive systems,intelligent
	control,intelligent robots,mobile robots,multiple-agent system,RoboCup
	tournament,robot soccer team,SoccerServer system},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{yongzhong_li_new_2008,
  author = {Yongzhong Li and Jing Xu and Bo Zhao and Ge Yang},
  title = {A new mobile agent architecture for wireless sensor networks},
  booktitle = {Industrial Electronics and Applications, 2008. ICIEA 2008. 3rd IEEE
	Conference on},
  year = {2008},
  pages = {1562--1565},
  abstract = {In this paper, an universal mobile agent architecture for WSNs is
	proposed. In the traditional client/server based network computing,
	local data is transferred to destination directly. In mobile-agent
	based computing, mobile agent is dedicated to eliminate information
	redundancy and communication overhead in order to prolong network
	lifetime.The advantages of the mobile agent paradigm make it more
	suitable for wireless sensor networks(WSNs) than client/server paradigm.
	The ways of eliminate information redundancy and communication overhead
	has been discussed in detail technical requirements and key techniques
	for wireless sensor networks are also analyzed.},
  doi = {10.1109/IIH-MSP.2008.289},
  keywords = {communication overhead reduction,information redundancy reduction,mobile
	agent architecture,mobile agents,wireless sensor networks},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{fang-chang_lin_cooperation_1995,
  author = {Fang-Chang Lin and J.Y.-J. Hsu},
  title = {Cooperation and deadlock-handling for an object-sorting task in a
	multi-agent robotic system},
  booktitle = {Robotics and Automation, 1995. Proceedings., 1995 IEEE International
	Conference on},
  year = {1995},
  volume = {3},
  pages = {2580--2585 vol.3},
  abstract = {This paper presents a deadlock-free cooperation protocol for an object-sorting
	task in a multi-agent system. The object-sorting task in a distributed
	robotic system is introduced and a cooperation protocol for the task
	along with the agent architecture is proposed. The agents are based
	on a homogeneous agent architecture that consists of search, motion,
	and communication modules coordinated through a global state. The
	deadlock problem for the object-sorting task is addressed and several
	deadlock-handling strategies are provided to guarantee the cooperation
	protocol is deadlock-free},
  isbn = {1050-4729},
  keywords = {communication modules,cooperation protocol,cooperative systems,deadlock-free
	cooperation,deadlock-handling,distributed control,distributed robotic
	system,homogeneous mobile robots,intelligent control,mobile robots,motion
	module,multi-agent robotic system,navigation,object recognition,object-sorting
	task,obstacle avoidance,path planning,protocols,search module},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{jianguo_liu_agent_2006,
  author = {Jianguo Liu and Yansheng Lu},
  title = {Agent Architecture Suitable for Simulation of Virtual Human Intelligence},
  booktitle = {Intelligent Control and Automation, 2006. WCICA 2006. The Sixth World
	Congress on},
  year = {2006},
  volume = {1},
  pages = {2521--2525},
  abstract = {This paper presents a new agent architecture called BSOAA (based smart
	object agent architecture). BSOAA extends BDI agent from three aspects:
	First, enhancing agent's ability to KR and inference because of applying
	a new KR method based smart object to represent agent's mental states;
	Second, as including sensibility desire and philosophy in mental
	states, the autonomy of agent is increasing; at last, since the influence
	of instinct is considered, agent gets the ability of real time reactive
	in some emergent situation. As BSOAA extends agent's mental states,
	including sensibility, philosophy and instinct, it is suitable for
	simulation of virtual human intelligence. Finally, an example named
	AIPlayer based BSOAA in such simulation is provided},
  doi = {10.1109/INDIN.2008.4618247},
  keywords = {Agent,based smart object agent architecture,BDI,Smart Object,Virtual
	human,virtual human intelligence simulation,virtual reality},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{yi_liu_intelligent_2004,
  author = {Yi Liu and Yam San Chee},
  title = {Intelligent pedagogical agents with multiparty interaction support},
  booktitle = {Intelligent Agent Technology, 2004. (IAT 2004). Proceedings. IEEE/WIC/ACM
	International Conference on},
  year = {2004},
  pages = {134--140},
  abstract = {Most current virtual world systems focus on the interaction between
	a single agent and the user. This simplification does not reflect
	the richness of a real social environment. The quantitative increment
	from the simple two-party interaction to a multi-party interaction
	does not merely increase the difficulty linearly. In fact, it leads
	to a much more complex situation involving multimodal communication,
	utterance understanding, and interaction style. Here, we introduce
	a four-layer agent architecture with multiparty interaction support.
	A Newtonian law learning environment based on this agent architecture
	is presented and how multiple agents cooperate to improve user learning
	is illustrated. The agent's interaction ability within a multiparty
	environment can be realized in three sections: planning and task
	execution, communication and understanding, as well as learning and
	coaching. Our entire system can be regarded as a step toward addressing
	and solving issues related to effective teaching in a multi-user
	environment within a sophisticated domain.},
  doi = {10.1109/IAT.2004.1342993},
  keywords = {agent interaction ability,computer aided instruction,four-layer agent
	architecture,intelligent pedagogical agents,interaction style,multimodal
	communication,multiparty environment,multiparty interaction support,multiple
	agents,multiuser environment,Newtonian law learning environment,physics
	education,planning execution,quantitative increment,task execution,two-party
	interaction,user learning,utterance understanding,virtual world systems},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Lovis2008,
  author = {Christian Lovis and Dirk Colaert and Veli N Stroetmann},
  title = {DebugIT for patient safety - improving the treatment with antibiotics
	through multimedia data mining of heterogeneous clinical data.},
  journal = {Stud Health Technol Inform},
  year = {2008},
  volume = {136},
  pages = {641--646},
  abstract = {The concepts and architecture underlying a large-scale integrating
	project funded within the 7th EU Framework Programme (FP7) are discussed.
	The main objective of the project is to build a tool that will have
	a significant impact for the monitoring and the control of infectious
	diseases and antimicrobial resistances in Europe; This will be realized
	by building a technical and semantic infrastructure able to share
	heterogeneous clinical data sets from different hospitals in different
	countries, with different languages and legislations; to analyze
	large amounts of this clinical data with advanced multimedia data
	mining and finally apply the obtained knowledge for clinical decisions
	and outcome monitoring. There are numerous challenges in this project
	at all levels, technical, semantical, legal and ethical that will
	have to be addressed.},
  institution = {University Hospitals of Geneva, University of Geneva, Switzerland.
	christian.lovis@hcuge.ch},
  keywords = {Anti-Bacterial Agents, therapeutic use; Artificial Intelligence; Bacterial
	Infections, drug therapy; Computer Communication Networks; Decision
	Support Systems, Clinical; Drug Information Services; Drug Resistance,
	Bacterial; Drug Toxicity; Europe; Humans; Information Storage and
	Retrieval; Medical Records Systems, Computerized; Multilingualism;
	Multimedia; Software; User-Computer Interface},
  owner = {user},
  pmid = {18487803},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{xudong_ma_utilizing_2005,
  author = {Xudong Ma and Xianzhong Dai and Dongyao Wang and Xin Jin},
  title = {Utilizing external perception and intelligence in a networked robotic
	system},
  booktitle = {Intelligent Robots and Systems, 2005. (IROS 2005). 2005 IEEE/RSJ
	International Conference on},
  year = {2005},
  pages = {2154--2159},
  abstract = {Contrary to limited local functions and machine intelligence of a
	single mobile robot, a networked mobile robot can utilize abundant
	resources over networks, especially the Internet that extend the
	network and robot applications to a new field. Potential online assistance
	from network might be computers, digital sensors, database, even
	operators. It is currently an important issue of the Internet-based
	robotic system and can often lead to significant improvement in machine
	intelligence and system performance. In this paper a new approach
	utilizing network resources is proposed for indoor applications of
	our Internet-based robotic system. Developed with this strategy,
	the robot, instead of only being considered as a passive remote tool,
	can actively seek for help from assistant network resources for perception
	or intelligence enhancement. The layered framework of the Internet-based
	robotic system is outlined as application background, and the definition,
	implementation and utilizing strategy of network resources based
	on OAA (open agent architecture) discussed, with two practical examples
	as speech recognition and global vision perception given to demonstrate
	the potential applications. Finally, the integration to the framework
	of the Intranet/Internet-based robotic system with the example utilizations
	is presented.},
  doi = {10.1109/COASE.2005.1506771},
  keywords = {Agent,external perception,intelligent robots,Internet-based robotic
	system,machine intelligence,mobile robot,Mobile robot,mobile robots,Networked
	Perception,networked robotic system,Networked robotics,open agent
	architecture,resource allocation,robot intelligence,robot vision,speech
	recognition,vision perception},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Marinagi2000,
  author = {C. C. Marinagi and C. D. Spyropoulos and C. Papatheodorou and S.
	Kokkotos},
  title = {Continual planning and scheduling for managing patient tests in hospital
	laboratories.},
  journal = {Artif Intell Med},
  year = {2000},
  volume = {20},
  pages = {139--154},
  number = {2},
  month = {Oct},
  abstract = {Hospital laboratories perform examination tests upon patients, in
	order to assist medical diagnosis or therapy progress. Planning and
	scheduling patient requests for examination tests is a complicated
	problem because it concerns both minimization of patient stay in
	hospital and maximization of laboratory resources utilization. In
	the present paper, we propose an integrated patient-wise planning
	and scheduling system which supports the dynamic and continual nature
	of the problem. The proposed combination of multiagent and blackboard
	architecture allows the dynamic creation of agents that share a set
	of knowledge sources and a knowledge base to service patient test
	requests.},
  institution = {Software and Knowledge Engineering Laboratory, Institute of Informatics
	and Telecommunications, National Centre for Scientific Research 'Demokritos',
	15310, Aghia Paraskevi, Greece.},
  keywords = {Artificial Intelligence; Hospital Administration; Hospital Planning;
	Humans; Laboratories, Hospital, organization /&/ administration;
	Laboratory Techniques and Procedures; Personnel Staffing and Scheduling},
  owner = {user},
  pii = {S0933-3657(00)00061-0},
  pmid = {10936750},
  timestamp = {2008.10.19}
}

@ARTICLE{Masseroli2004,
  author = {M. Masseroli and S. Bonacina and F. Pinciroli},
  title = {Java-based browsing, visualization and processing of heterogeneous
	medical data from remote repositories.},
  journal = {Conf Proc IEEE Eng Med Biol Soc},
  year = {2004},
  volume = {5},
  pages = {3326--3329},
  abstract = {The actual development of distributed information technologies and
	Java programming enables employing them also in the medical arena
	to support the retrieval, integration and evaluation of heterogeneous
	data and multimodal images in a web browser environment. With this
	aim, we used them to implement a client-server architecture based
	on software agents. The client side is a Java applet running in a
	web browser and providing a friendly medical user interface to browse
	and visualize different patient and medical test data, integrating
	them properly. The server side manages secure connections and queries
	to heterogeneous remote databases and file systems containing patient
	personal and clinical data. Based on the Java Advanced Imaging API,
	processing and analysis tools were developed to support the evaluation
	of remotely retrieved bioimages through the quantification of their
	features in different regions of interest. The Java platform-independence
	allows the centralized management of the implemented prototype and
	its deployment to each site where an intranet or internet connection
	is available. Giving healthcare providers effective support for comprehensively
	browsing, visualizing and evaluating medical images and records located
	in different remote repositories, the developed prototype can represent
	an important aid in providing more efficient diagnoses and medical
	treatments.},
  doi = {10.1109/IEMBS.2004.1403935},
  institution = {Dipartimento di Bioingegneria, Politecnico di Milano, Milano, Italy.},
  owner = {user},
  pmid = {17270994},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1109/IEMBS.2004.1403935}
}

@ARTICLE{Masseroli2001,
  author = {M. Masseroli and F. Pinciroli},
  title = {Web architecture for the remote browsing and analysis of distributed
	medical images and data.},
  journal = {Stud Health Technol Inform},
  year = {2001},
  volume = {84},
  pages = {43--47},
  number = {Pt 1},
  abstract = {To provide easy retrieval, integration and evaluation of multimodal
	medical images and data in a web browser environment, distributed
	application technologies and Java programming were used to develop
	a client-server architecture based on software agents. The server
	side manages secure connections and queries to heterogeneous remote
	databases and file systems containing patient personal and clinical
	data. The client side is a Java applet running in a web browser and
	providing a friendly medical user interface to perform queries on
	patient and medical test data and integrate and visualize properly
	the various query results. A set of tools based on Java Advanced
	Imaging API enables to process and analyze the retrieved bioimages,
	and quantify their features in different regions of interest. The
	platform-independence Java technology makes the developed prototype
	easy to be managed in a centralized form and provided in each site
	where an intranet or internet connection can be located. Giving the
	healthcare providers effective tools for browsing, querying, visualizing
	and evaluating comprehensively medical images and records in all
	locations where they can need them - e.g. emergency, operating theaters,
	ward, or even outpatient clinics- the implemented prototype represents
	an important aid in providing more efficient diagnoses and medical
	treatments.},
  institution = {Dipartimento di Bioingegneria, Politecnico di Milano, 20133 Milano,
	Italy. masseroli@biomed.polimi.it},
  keywords = {Ambulatory Care Facilities; Cardiology; Computer Communication Networks,
	organization /&/ administration; Computer Systems; Humans; Image
	Processing, Computer-Assisted; Information Storage and Retrieval;
	Medical Records Systems, Computerized, organization /&/ administration;
	Programming Languages; Software; Telemedicine},
  owner = {user},
  pmid = {11604703},
  timestamp = {2008.10.19}
}

@ARTICLE{Matsushima1993,
  author = {T. Matsushima},
  title = {Constructing a distributed object-oriented system with logical constraints
	for fluorescence-activated cell sorting.},
  journal = {Proc Int Conf Intell Syst Mol Biol},
  year = {1993},
  volume = {1},
  pages = {266--274},
  abstract = {This paper describes a fully distributed biological-object system
	that supports FACS (Fluorescence Activated Cell Sorter) instrumentation.
	The architecture of the system can be applied to any laboratory automation
	system that involves distributed instrument control and data management.
	All component processes of FACS (such as instrument control, protocol
	design, data analysis, and data visualization), which may run on
	different machines, are modeled as cooperatively-working "agents."
	Communication among agents is performed through shared-objects by
	triggered methods. This shared-object metaphor encapsulates the details
	of network programming. The system facilitates the annotation of
	classes with first-order formulae that express logical constraints
	on objects; these constraints are automatically maintained upon updates.
	Also, the shared-object communication and polymorphic triggered methods
	are exploited to produce a homogeneous interface for instrument control.},
  institution = {Herzenberg Laboratory, Genetics Department, Stanford University,
	CA 94305, USA.},
  keywords = {Automation; Databases, Factual; Flow Cytometry, instrumentation; Logic;
	Programming Languages; Software; User-Computer Interface},
  owner = {user},
  pmid = {7584345},
  timestamp = {2008.10.19}
}

@ARTICLE{maturana_methodologies_2005,
  author = {F.P. Maturana and R.J. Staron and K.H. Hall},
  title = {Methodologies and tools for intelligent agents in distributed control},
  journal = {Intelligent Systems, IEEE},
  year = {2005},
  volume = {20},
  pages = {42--49},
  number = {1},
  abstract = {Agent technology also provides an appropriate framework to integrate
	knowledge with production actions. Knowledge integration depends
	on balanced information in and across organizations. So, replacing
	the central models with distributed intelligent agents make it possible
	to cope with demands more efficiently. In this, each agent represents
	a specific activity to support overall operations. With agents, maintaining
	the overall system is easier because instead of changes occurring
	to a colossal program, changes take place on a small scale without
	disrupting overall operations. We've developed a set of tools and
	methodologies to help design, build, test, and verify such systems.
	Using these tools and methodologies, we've devised an agent architecture
	that we've applied to a US Navy shipboard chilled-water system.},
  doi = {10.1109/WI.2007.36},
  issn = {1541-1672},
  keywords = {agent architecture,agent technology,agents,command and control systems,control,distributed,distributed
	control,distributed intelligent agents,emergent,intelligent agent,intelligent
	control,knowledge integration,learning,mediator,negotiation,ships,US
	Navy shipboard chilled-water system},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{mcgann_deliberative_2008,
  author = {C. McGann and F. Py and K. Rajan and H. Thomas and R. Henthorn and
	R. McEwen},
  title = {A deliberative architecture for AUV control},
  booktitle = {Robotics and Automation, 2008. ICRA 2008. IEEE International Conference
	on},
  year = {2008},
  pages = {1049--1054},
  abstract = {Autonomous Underwater Vehicles (AUVs) are an increasingly important
	tool for oceanographic research demonstrating their capabilities
	to sample the water column in depths far beyond what humans are capable
	of visiting, and doing so routinely and cost-effectively. However,
	control of these platforms to date has relied on fixed sequences
	for execution of pre-planned actions limiting their effectiveness
	for measuring dynamic and episodic ocean phenomenon. In this paper
	we present an agent architecture developed to overcome this limitation
	through on-board planning using Constraint- based Reasoning. Preliminary
	versions of the architecture have been integrated and tested in simulation
	and at sea.},
  doi = {10.1109/QSIC.2008.15},
  isbn = {1050-4729},
  keywords = {agent architecture,autonomous underwater vehicles,AUV control,constraint-based
	reasoning,deliberative architecture,dynamic ocean phenomenon,episodic
	ocean phenomenon,oceanographic research,oceanography,on-board planning,underwater
	vehicles},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{merrill_towardneurocognitive_2007,
  author = {K. Merrill and T. Bean and M. O'Rourke and D. Edwards},
  title = {Toward a Neurocognitive Agent Architecture for AUVs},
  booktitle = {Oceans 2007},
  year = {2007},
  pages = {1--7},
  abstract = {We present a neurocognitive architecture for autonomous underwater
	vehicles that takes its inspiration from current knowledge of the
	neurobiological basis of human cognition and is based upon modeling
	the large-scale functionality of the human brain. We build on the
	tradition of taking inspiration from the human cognitive architecture,
	making explicit attempts at modeling aspects of human cognition and
	in addition to modeling human cognition, we attempt to model the
	neurobiological basis of some aspects of human cognition. We describe
	the functional organization of the human brain, which can be roughly
	ordered in a three-tiered architecture including the sensorimotor
	functions, cognitive functions, and executive functions. In the architecture
	proposed, each of these functional systems is implemented as a subsystem
	of the whole agent architecture consisting of a set of psychologically
	and conceptually inspired modules.},
  doi = {10.1109/ICCA.2007.4376602},
  keywords = {autonomous underwater vehicle,AUV,cognitive function,cognitive systems,conceptually
	inspired module,decentralised control,executive function,human brain,human
	cognition,multivariable control systems,neurobiological basis,neurocognitive
	agent architecture,neurocontrollers,oceanographic techniques,psychologically
	inspired module,sensorimotor function,three-tiered architecture,underwater
	vehicles},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Milano2004,
  author = {Michela Milano and Andrea Roli},
  title = {MAGMA: a multiagent architecture for metaheuristics.},
  journal = {IEEE Trans Syst Man Cybern B Cybern},
  year = {2004},
  volume = {34},
  pages = {925--941},
  number = {2},
  month = {Apr},
  abstract = {In this work, we introduce a multiagent architecture called the MultiAGent
	Metaheuristic Architecture (MAGMA) conceived as a conceptual and
	practical framework for metaheuristic algorithms. Metaheuristics
	can be seen as the result of the interaction among different kinds
	of agents: The basic architecture contains three levels, each hosting
	one or more agents. Level-0 agents build solutions, level-1 agents
	improve solutions, and level-2 agents provide the high level strategy.
	In this framework, classical metaheuristic algorithms can be smoothly
	accommodated and extended. The basic three level architecture can
	be enhanced with the introduction of a fourth level of agents (level-3
	agents) coordinating lower level agents. With this additional level,
	MAGMA can also describe, in a uniform way, cooperative search and,
	in general, any combination of metaheuristics. We describe the entire
	architecture, the structure of agents in each level in terms of tuples,
	and the structure of their coordination as a labeled transition system.
	We propose this perspective with the aim to achieve a better and
	clearer understanding of metaheuristics, obtain hybrid algorithms,
	suggest guidelines for a software engineering-oriented implementation
	and for didactic purposes. Some specializations of the general architecture
	will be provided in order to show that existing metaheuristics [e.g.,
	greedy randomized adaptive procedure (GRASP), ant colony optimization
	(ACO), iterated local search (ILS), memetic algorithms (MAs)] can
	be easily described in our framework. We describe cooperative search
	and large neighborhood search (LNS) in the proposed framework exploiting
	level-3 agents. We show also that a simple hybrid algorithm, called
	guided restart ILS, can be easily conceived as a combination of existing
	components in our framework.},
  institution = {DEIS-University of Bologna, 40136 Bologna, Italy. mmilano@deis.unibo.it},
  owner = {user},
  pmid = {15376840},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{millet_web_2007,
  author = {P. Millet and J.-C. Heudin},
  title = {Web Mining in the EVA Intelligent Agent Architecture},
  booktitle = {Web Intelligence and Intelligent Agent Technology Workshops, 2007
	IEEE/WIC/ACM International Conferences on},
  year = {2007},
  pages = {368--371},
  abstract = {This paper describes the architecture of the fourth version of the
	evolutionary virtual agent (EVA). This new light-weight java-based
	implementation is based on a dynamical rule-based subsumption architecture,
	an XML knowledge base and a scheme kernel for scripting behavior
	rules. Using this architecture, the agent is able to answer questions
	in natural language while learning a user's profile. It also extracts
	relevant information from the Web through search engines queries
	and use them in the flow of conversation. This paper describes the
	architecture and analyses its Web mining process.},
  doi = {10.1109/CERMA.2006.16},
  keywords = {dynamical rule-based subsumption architecture,EVA intelligent agent
	architecture,evolutionary virtual agent,Intelligent AgentConversational
	CharacterWeb Mining,light-weight Java-based implementation,natural
	language,relevance feedback,relevant information extraction,scheme
	kernel,scripting behavior rules,search engines queries,user profile
	learning,Web mining,XML knowledge base},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{mnsman_system_2000,
  author = {S. Mnsman and P. Flesher},
  title = {System or security managers adaptive response tool},
  booktitle = {DARPA Information Survivability Conference and Exposition, 2000.
	DISCEX '00. Proceedings},
  year = {2000},
  volume = {2},
  pages = {56--68 vol.2},
  abstract = {This paper describes the design and function of a system being developed
	under a DARPA SBIR Phase I and II project for Adaptive Network and
	Security Management. The system is called SoSMART (System or Security
	Managers Adaptive Response Tool). The objective of our system is
	to provide an automatic, adaptive response capability that provides
	24/7 around the clock monitoring and response to system and security
	functions across a network of completing systems. To achieve this
	objective we combine an agent architecture and Case-based reasoning
	(CBR) with available system management or security tools. The agent
	architecture is used for tool integration, functional abstraction
	and as a medium for distributed reasoning. The CBR is used to define
	incident/response pairings that can recognize situations that require
	response and associate response actions with those situations. An
	important aspect of our system is its use of CBR's adaptation process
	to allow it to dynamically adapt it's control and monitoring activity
	as the system operates. This paper contains an overview of our approach,
	a description of important system details, a worked example of the
	systems operation and finishes with a summary},
  keywords = {adaptive response tool,agent architecture,case-based reasoning,distributed
	reasoning,SoSMART,system management,System or Security Managers Adaptive
	Response Tool},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{morignot_pair_1997,
  author = {P. Morignot and O. Aycard and F. Charpillet},
  title = {A pair of heterogeneous agents in a unique vehicle for object motion},
  booktitle = {Tools with Artificial Intelligence, 1997. Proceedings., Ninth IEEE
	International Conference on},
  year = {1997},
  pages = {508--513},
  abstract = {We present a multi agent architecture for controlling a mobile robot
	in an unpredictable environment. This architecture has been developed
	with the objective of coordinating the various competences of the
	robot (e.g., perception, navigation, planning). The architecture
	is made up of two agents: the first one specialized for cognitive
	tasks, the second one dedicated to control of the robot's physical
	devices. This two agent architecture guarantees a good robustness
	of the system, as the navigation modules can run independently of
	the cognitive ones },
  doi = {10.1109/WI-IATW.2006.90},
  keywords = {cognitive tasks,cooperative systems,heterogeneous agents,mobile robot
	control,mobile robots,motion control,multi agent architecture,navigation
	modules,object motion,path planning,physical devices,robust control,robustness,two
	agent architecture,unique vehicle,unpredictable environment},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Murphy1998,
  author = {S. N. Murphy and T. Ng and D. F. Sittig and G. O. Barnett},
  title = {Using web technology and Java mobile software agents to manage outside
	referrals.},
  journal = {Proc AMIA Symp},
  year = {1998},
  pages = {101--105},
  abstract = {A prototype, web-based referral application was created with the objective
	of providing outside primary care providers (PCP's) the means to
	refer patients to the Massachusetts General Hospital and the Brigham
	and Women's Hospital. The application was designed to achieve the
	two primary objectives of providing the consultant with enough data
	to make decisions even at the initial visit, and providing the PCP
	with a prompt response from the consultant. The system uses a web
	browser/server to initiate the referral and Java mobile software
	agents to support the workflow of the referral. This combination
	provides a light client implementation that can run on a wide variety
	of hardware and software platforms found in the office of the PCP.
	The implementation can guarantee a high degree of security for the
	computer of the PCP. Agents can be adapted to support the wide variety
	of data types that may be used in referral transactions, including
	reports with complex presentation needs and scanned (faxed) images
	Agents can be delivered to the PCP as running applications that can
	perform ongoing queries and alerts at the office of the PCP. Finally,
	the agent architecture is designed to scale in a natural and seamless
	manner for unforeseen future needs.},
  institution = {Laboratory of Computer Science, Massachusetts General Hospital, Boston,
	USA.},
  keywords = {Computer Communication Networks; Computer Security; Hospitals, General;
	Massachusetts; Primary Health Care; Referral and Consultation, organization
	/&/ administration; Software},
  owner = {user},
  pmid = {9929190},
  timestamp = {2008.10.19}
}

@ARTICLE{Nageba2007,
  author = {Ebrahim Nageba and Jocelyne Fayn and Paul Rubel},
  title = {A generic task-driven multi-agent telemedicine system.},
  journal = {Conf Proc IEEE Eng Med Biol Soc},
  year = {2007},
  volume = {2007},
  pages = {3733--3736},
  abstract = {Pervasive Telemedicine is an emerging research discipline, which focuses
	on the development and the application of ubiquitous computing technology
	for healthcare purposes. However, the current telemedicine systems
	lack to be self adaptable to handle different types of data such
	as vital biosignals, images, video and textual data. In addition,
	they do not use the full capabilities of the computing devices on
	which they run. Unfortunately, the existing telemedicine systems
	do not pay enough attention to the quality level of their offered
	services nor offer adequate resources management for meeting bandwidth
	and end-to-end communication delays. In this paper we propose an
	information and communication architecture of a generic telemedicine
	system based on a knowledge base and intelligent agents interacting
	each with the other in a synergetic way to perform several medical
	tasks for a high level of quality of service (QoS). The medical assistance
	to skiers and high mountains resorts residents will be used in particular
	as an example of applicability scenario and models personalization.},
  doi = {10.1109/IEMBS.2007.4353143},
  institution = {Methodologies of Information Processing, Cardiology research group,
	INSA de Lyon, INSERM ERM107 MTIC, Université de Lyon, BRON, F-69677,
	France. Ebrahim.Nageba@insa-lyon.fr},
  keywords = {Computer Communication Networks; Decision Support Systems, Clinical;
	Diagnosis, Computer-Assisted, methods; Expert Systems; France; Information
	Dissemination, methods; Information Storage and Retrieval, methods;
	Medical Records Systems, Computerized; Telemedicine, methods; User-Computer
	Interface},
  owner = {user},
  pmid = {18002809},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1109/IEMBS.2007.4353143}
}

@ARTICLE{Nagin2002,
  author = {V. A. Nagin and I. V. Potapov and S. V. Selishchev},
  title = {Design of acquisition devices management subsystem for IEEE 1073
	compliant software agents.},
  journal = {Stud Health Technol Inform},
  year = {2002},
  volume = {90},
  pages = {774--779},
  abstract = {The paper addresses the issue of device management system design for
	software agents compliant with IEEE 1073 device communication standard.
	Based on middleware architecture the device control layer represents
	a universal versatile object-oriented application-programming interface.
	The approach presented in the paper allows to implement plug-and-play
	integration and interoperability of medical acquisition devices within
	the medical device system be means of common middleware services.
	Adherence to Medical Data Information Base nomenclature, component
	part of IEEE 1073 communication standard, adds necessary consistency
	to presented component-based infrastructure.},
  institution = {Moscow State Institute of Electronic Technology, Moscow, 103498,
	K-498, Russia.},
  keywords = {Electrocardiography; Russia; Software; Systems Integration},
  owner = {user},
  pmid = {15460797},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{seppo_nyrkko_ontology-based_2007,
  author = {Seppo Nyrkko and Lauri Carlson and Matti Keijola and Helena Ahonen-Myka
	and Jyrki Niemi and Jussi Piitulainen and Sirke Viitanen and Martti
	Meri and Lauri Seitsonen and Petri Mannonen and Jani Juvonen},
  title = {Ontology-based Knowledge in Interactive Maintenance Guide},
  booktitle = {System Sciences, 2007. HICSS 2007. 40th Annual Hawaii International
	Conference on},
  year = {2007},
  pages = {47},
  abstract = {This paper describes 4M, a language technology research project where
	a dialogue system is applied on a mobile platform in a maintenance
	job scenario. The human-machine interface uses speech synthesis and
	recognition, assisted with a hypertext display. We describe a modular
	agent architecture, composed of independent program components which
	are implemented by or communicate using ontology programming techniques.
	Domain content and lingware are developed and shared using standard
	Web ontology formats and ontology-aware offline tools. A contribution
	of the project is the attention paid to standardization to help provide
	the system with new content and to migrate it to new domains, languages
	and purposes},
  doi = {10.1109/SYSOSE.2007.4304228},
  isbn = {1530-1605},
  keywords = {4M language technology research project,dialogue system,domain content
	development,human-machine interface,hypermedia,hypertext display,interactive
	maintenance job guide,interactive systems,lingware development,mobile
	platform,modular agent architecture,ontology programming,ontology-aware
	offline tool,ontology-based knowledge,program components,speech recognition,Web
	ontology format},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{ott_connecting_1999,
  author = {R. Ott and H. Reiter},
  title = {Connecting EIB components to distributed Java applications},
  booktitle = {Emerging Technologies and Factory Automation, 1999. Proceedings.
	ETFA '99. 1999 7th IEEE International Conference on},
  year = {1999},
  volume = {1},
  pages = {23--26 vol.1},
  abstract = {This paper describes an architecture called `EIBAgents for Java' that
	enables Java applications to access EIB (European Installation Bus)
	components. The architecture is abstract to allow open and flexible
	extensions of the EIB agent architecture. It is divided into three
	major Java base classes called EIBAgent, EIBReceiver and EIBEiblet.
	The EIBAgent is the actual interface that provides methods for accessing
	EIB group objects. EIBAgents can be implemented to access an EIB
	system via communication mechanisms such as serial communication
	(RS-232) or TCP/IP socket communication. EIBReceivers are described
	as the request forwarder to EIBAgent implementations and can be used
	to forward requests across different EIBAgent servers. EIBEiblets
	are applet style pieces of Java code that implement logical behavior
	or graphical user interfaces for the EIB system. The paper also shows
	a sample of a distributed Java application that uses the EIB agent
	architecture to access an EIB system from various platforms},
  doi = {10.1109/TSMCB.2004.826830},
  keywords = {applet style Java code pieces,distributed Java applications,EIB component
	connection,EIBAgent,EIBAgent servers,EIBAgents for Java,EIBEiblet,EIBReceiver,European
	Installation Bus,Java base classes,logical behavior,request forwarder,serial
	communication,system buses,TCP/IP socket communication,transport
	protocols},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{park_generic_1994,
  author = {J.T. Park and Y.H. Cho},
  title = {A generic manager/agent architecture for TMN applications},
  booktitle = {Singapore ICCS '94. Conference Proceedings.},
  year = {1994},
  volume = {2},
  pages = {794--798 vol.2},
  abstract = {A generic manager/agent architecture for the design of the TMN applications
	is presented. The architecture is based on the software platform
	technology for the efficient integration of various evolving management
	applications, information models, and communication protocols. The
	requirements of the platform architecture for the effective realization
	of the TMN network management model have been analyzed. Based on
	these requirements, a generic manager/agent architecture, TMA (telecommunication
	management architecture), has been designed, and the components of
	TMA are described in detail},
  doi = {10.1109/ICNNSP.2003.1279272},
  keywords = {communication protocols,generic manager/agent architecture,information
	models,management applications integration,platform architecture,protocols,software
	platform technology,Telecommunication Management Architecture,telecommunication
	network management,TMN applications,TMN network management model},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{payne_experience_1997,
  author = {T.R. Payne and P. Edwards and C.L. Green},
  title = {Experience with rule induction and k-nearest neighbor methods for
	interface agents that learn},
  journal = {Knowledge and Data Engineering, IEEE Transactions on},
  year = {1997},
  volume = {9},
  pages = {329--335},
  number = {2},
  abstract = {Interface agents are being developed to assist users with a variety
	of tasks. To perform effectively, such agents need knowledge of user
	preferences. An agent architecture has been developed which observes
	a user performing tasks, and identifies features which can be used
	as training data by a learning algorithm. Using the learned profile,
	an agent can give advice to the user on dealing with new situations.
	The architecture has been applied to two different information filtering
	domains: classifying incoming mail messages (Magi) and identifying
	interesting USENet news articles (UNA). This paper describes the
	architecture and examines the results of experimentation with different
	learning algorithms and different feature extraction strategies within
	these domains},
  doi = {10.1109/ETFA.1999.815334},
  issn = {1041-4347},
  keywords = {agent architecture,electronic mail,feature extraction,feature extraction
	strategies,information filtering,information filtering domains,information
	retrieval systems,instance-based learning,intelligent e-mail filter,interesting
	USENet news articles,interface agents,k-nearest neighbor methods,learned
	profile,learning algorithm,learning algorithms,mail messages,rule
	induction,training data,user modelling,user preferences},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{pignon_multiple_1994,
  author = {J.P. Pignon and C. Bizingre},
  title = {Multiple agents architecture for intelligent command and control
	system of AUVs : application to the MARIUS vehicle},
  booktitle = {OCEANS '94. 'Oceans Engineering for Today's Technology and Tomorrow's
	Preservation.' Proceedings},
  year = {1994},
  volume = {3},
  pages = {III/126--III/131 vol.3},
  abstract = {This paper addresses the problem of intelligent command and control
	systems for AUVs. It presents a conceptual multiple agents based
	architecture proposed for the MARIUS prototype vehicle (devoted to
	coastal survey missions). The paper presents the characteristics
	of the vehicle system and a synthesis of the functional requirements
	to be fulfilled; it describes the proposed multiagents architecture
	for the onboard mission management system, with details on architecture,
	functionalities and communications capabilities of the agents It
	describes the mapping of the functional decomposition of the RIMS
	on this multiple agents architecture},
  doi = {10.1109/OCEANS.1994.364184},
  keywords = {AUV,coastal survey missions,functional decomposition,intelligent command
	system,intelligent control,intelligent control system,marine systems,MARIUS
	vehicle,mobile robots,multiagents architecture,multiple agents architecture,onboard
	mission management system},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{prouskas_real-time_2004,
  author = {K.-V. Prouskas and J.V. Pitt},
  title = {A real-time architecture for time-aware agents},
  journal = {Systems, Man, and Cybernetics, Part B, IEEE Transactions on},
  year = {2004},
  volume = {34},
  pages = {1553--1568},
  number = {3},
  abstract = {This paper describes the specification and implementation of a new
	three-layer time-aware agent architecture. This architecture is designed
	for applications and environments where societies of humans and agents
	play equally active roles, but interact and operate in completely
	different time frames. The architecture consists of three layers:
	the April real-time run-time (ART) layer, the time aware layer (TAL),
	and the application agents layer (AAL). The ART layer forms the underlying
	real-time agent platform. An original online, real-time, dynamic
	priority-based scheduling algorithm is described for scheduling the
	computation time of agent processes, and it is shown that the algorithm's
	O(n) complexity and scalable performance are sufficient for application
	in real-time domains. The TAL layer forms an abstraction layer through
	which human and agent interactions are temporally unified, that is,
	handled in a common way irrespective of their temporal representation
	and scale. A novel O(n2) interaction scheduling algorithm is described
	for predicting and guaranteeing interactions' initiation and completion
	times. The time-aware predicting component of a workflow management
	system is also presented as an instance of the AAL layer. The described
	time-aware architecture addresses two key challenges in enabling
	agents to be effectively configured and applied in environments where
	humans and agents play equally active roles. It provides flexibility
	and adaptability in its real-time mechanisms while placing them under
	direct agent control, and it temporally unifies human and agent interactions.},
  doi = {10.1109/ICMLC.2005.1526967},
  issn = {1083-4419},
  keywords = {abstraction layer,algorithm complexity,application agents layer,April
	real-time run-time layer,dynamic priority-based scheduling,human
	computer interaction,human-agent interaction,programming languages,real-time
	agent architecture,time aware layer,time-aware agents,workflow management
	software,workflow management system},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Prouskas2004,
  author = {Konstantinos-Vassileios Prouskas and Jeremy V Pitt},
  title = {A real-time architecture for time-aware agents.},
  journal = {IEEE Trans Syst Man Cybern B Cybern},
  year = {2004},
  volume = {34},
  pages = {1553--1568},
  number = {3},
  month = {Jun},
  abstract = {This paper describes the specification and implementation of a new
	three-layer time-aware agent architecture. This architecture is designed
	for applications and environments where societies of humans and agents
	play equally active roles, but interact and operate in completely
	different time frames. The architecture consists of three layers:
	the April real-time run-time (ART) layer, the time aware layer (TAL),
	and the application agents layer (AAL). The ART layer forms the underlying
	real-time agent platform. An original online, real-time, dynamic
	priority-based scheduling algorithm is described for scheduling the
	computation time of agent processes, and it is shown that the algorithm's
	O(n) complexity and scalable performance are sufficient for application
	in real-time domains. The TAL layer forms an abstraction layer through
	which human and agent interactions are temporally unified, that is,
	handled in a common way irrespective of their temporal representation
	and scale. A novel O(n2) interaction scheduling algorithm is described
	for predicting and guaranteeing interactions' initiation and completion
	times. The time-aware predicting component of a workflow management
	system is also presented as an instance of the AAL layer. The described
	time-aware architecture addresses two key challenges in enabling
	agents to be effectively configured and applied in environments where
	humans and agents play equally active roles. It provides flexibility
	and adaptability in its real-time mechanisms while placing them under
	direct agent control, and it temporally unifies human and agent interactions.},
  institution = {Imperial College, London SW7 2BT, UK.},
  keywords = {Algorithms; Decision Making, Computer-Assisted; Decision Support Techniques;
	Expert Systems; Humans; Online Systems; Personnel Staffing and Scheduling;
	Software; Software Design; Time Factors; User-Computer Interface},
  owner = {user},
  pmid = {15484924},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{rahimi_mpiab:novel_2003,
  author = {S. Rahimi and A. Narayanan and M. Sabharwal},
  title = {MPIAB: a novel agent architecture for parallel processing},
  booktitle = {Intelligent Agent Technology, 2003. IAT 2003. IEEE/WIC International
	Conference on},
  year = {2003},
  pages = {554--557},
  abstract = {This paper presents MPIAB, an agent based architecture for parallel
	processing. The architecture is developed to model the functions
	of standard MPI using Java agents. It remedies the deficiencies that
	exist in MPI, including the incapability to operate in heterogeneous
	environments. The architecture also addresses other issues such as
	effective utilization of system resources by dynamically selecting
	the least busy computing nodes through the computation of a threshold
	barrier value. Our proposed agent architecture would integrate the
	power of Java technology and agents to support efficient communication
	and synchronization of the nodes over the network for parallel processing.},
  doi = {10.1109/ICSMC.2001.973016},
  keywords = {agent architecture,computing nodes,heterogeneous environment,Java
	agents,MPIAB,node communication,node synchronization,parallel processing,standard
	MPI,system resource utilization,threshold barrier value},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Rammal2008,
  author = {Ali Rammal and Sylvie Trouilhet and Nicolas Singer and Jean-Marie
	Pécatte},
  title = {An adaptive system for home monitoring using a multiagent classification
	of patterns.},
  journal = {Int J Telemed Appl},
  year = {2008},
  pages = {136054},
  abstract = {This research takes place in the S(MA)(2)D project which proposes
	software architecture to monitor elderly people in their own homes.
	We want to build patterns dynamically from data about activity, movements,
	and physiological information of the monitored people. To achieve
	that, we propose a multiagent method of classification: every agent
	has a simple know-how of classification. Data generated at this local
	level are communicated and adjusted between agents to obtain a set
	of patterns. The patterns are used at a personal level, for example
	to raise an alert, but also to evaluate global risks (epidemic, heat
	wave). These data are dynamic; the system has to maintain the built
	patterns and has to create new patterns. So, the system is adaptive
	and can be spread on a large scale.},
  doi = {10.1155/2008/136054},
  institution = {IRIT-Centre Universitaire de information et d' Recherche (CUFR) Jean-François
	Champollion, site de Castres, Avenue Georges Pompidou, 81100 Castres,
	France.},
  owner = {user},
  pmid = {18437224},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1155/2008/136054}
}

@ARTICLE{Rialle2003,
  author = {Vincent Rialle and Jean Baptiste Lamy and Norbert Noury and Lionel
	Bajolle},
  title = {Telemonitoring of patients at home: a software agent approach.},
  journal = {Comput Methods Programs Biomed},
  year = {2003},
  volume = {72},
  pages = {257--268},
  number = {3},
  month = {Nov},
  abstract = {To address the issue of the increasing social, economical and medical
	needs of maintaining at home people in loss of autonomy while preserving
	privacy and quality of life, the authors present a software agent
	based telemonitoring and alarm raising system. The article describes
	the overall architecture, the various components of the model, and
	the methodology that has been used. It specifically addresses the
	issue of reflecting in the object oriented model of the system various
	dimensions including: the physical world of in-home bio-signal sensors,
	the numerical world of software agents and Internet-related technologies,
	and the medical and social worlds of patients, physicians and caregivers.
	In the model, the main stream of information goes from the biophysical
	world of patients at home to the socio-medical world of carers through
	a chain of devices including in-home sensors, local area network,
	home computer, remote server, and carers' computers. Each device
	hosts software agents with different levels of knowledge and complexity.
	Internet and Java technologies provide the building blocks of the
	designed telemonitoring software. Laboratory experiments have been
	realized using a fully equipped 'smart' demonstration home for telecare.
	The study takes place into a more general research project on 'smart'
	homes for telecare conducted at the Hospital Centre of Grenoble,
	France.},
  institution = {Laboratory TIMC-IMAG UMR CNRS 5525, Grenoble, France. vincent.rialle@imag.fr},
  keywords = {Computer Security; Monitoring, Physiologic, methods; Quality of Life;
	Software; Telemedicine},
  owner = {user},
  pii = {S016926070200161X},
  pmid = {14554139},
  timestamp = {2008.10.19}
}

@ARTICLE{Rico1996,
  author = {M. Rico and M. Bruix and C. González and R. I. Monsalve and R. Rodríguez},
  title = {1H NMR assignment and global fold of napin BnIb, a representative
	2S albumin seed protein.},
  journal = {Biochemistry},
  year = {1996},
  volume = {35},
  pages = {15672--15682},
  number = {49},
  month = {Dec},
  abstract = {Napin BnIb is a representative member of the 2S albumin seed proteins,
	which consists of two polypeptide chains of 3.8 and 8.4 kDa linked
	by two disulfide bridges. In this work, a complete assignment of
	the 1H spectra of napin BnIb has been carried out by two-dimensional
	NMR sequence-specific methods and its secondary structure determined
	on the basis of spectral data. A calculation of the tertiary structure
	has been performed using approximately 500 distance constraints derived
	from unambiguously assigned NOE cross-correlations and distance geometry
	methods. The resulting global fold consists of five helices and a
	C-terminal loop arranged in a right-handed spiral. The folded protein
	is stabilized by two interchain disulfide bridges and two additional
	ones between cysteine residues in the large chain. The structure
	of napin BnIb represents a third example of a new and distinctive
	folding pattern first described for the hydrophobic protein from
	soybean and nonspecific lipid transfer proteins from wheat and maize.
	The presence of an internal cavity is not at all evident, which rules
	out in principle the napin BnIb as a carrier of lipids. The determined
	structure is compatible with activities attributed to these proteins
	such as phospholipid vesicle interaction, allergenicity, and calmodulin
	antagonism. Given the sequence homology of BnIb with other napins
	and napin-type 2S albumin seed proteins from different species, it
	is likely that all these proteins share a common architecture. The
	determined structure will be crucial to establish structure-function
	relationships and to explore the mechanisms of folding, processing,
	and deposition of these proteins. It will also provide a firm basis
	for a rational use of genetic engineering in order to develop improved
	transgenic plants.},
  doi = {10.1021/bi961748q},
  institution = {Instituto de Estructura de la Materia, CSIC, Madrid, Spain.},
  keywords = {Amino Acid Sequence; Antifungal Agents, pharmacology; Brassica, chemistry;
	Carrier Proteins, metabolism; Disulfides, chemistry/metabolism; Fatty
	Acid-Binding Proteins; Magnetic Resonance Spectroscopy; Models, Molecular;
	Molecular Sequence Data; Molecular Weight; Myelin P2 Protein, metabolism;
	Neoplasm Proteins; Plant Proteins, chemistry; Protein Conformation;
	Protein Folding; Protein Structure, Secondary; Protein Structure,
	Tertiary; Seeds, chemistry; Sequence Alignment; Software; Structure-Activity
	Relationship},
  owner = {user},
  pii = {bi961748q},
  pmid = {8961930},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1021/bi961748q}
}

@ARTICLE{Ritman2002,
  author = {Erik L Ritman},
  title = {Molecular imaging in small animals--roles for micro-CT.},
  journal = {J Cell Biochem Suppl},
  year = {2002},
  volume = {39},
  pages = {116--124},
  abstract = {X-ray micro-CT is currently used primarily to generate 3D images of
	micro-architecture (and the function that can be deduced from it)
	and the regional distribution of administered radiopaque indicators,
	within intact rodent organs or biopsies from large animals and humans.
	Current use of X-ray micro-CT can be extended in three ways to increase
	the quantitative imaging of molecular transport and accumulation
	within such specimens. (1) By use of heavy elements, other than the
	usual iodine, attached to molecules of interest or to surrogates
	for those molecules. The accumulation of the indicator in the physiological
	compartments, and the transport to and from such compartments, can
	be quantitated from the imaged spatial distribution of these contrast
	agents. (2) The high spatial resolution of conventional X-ray attenuation-based
	CT images can be used to improve the quantitative nature of radionuclide-based
	tomographic images (SPECT & PET) by providing correction for attenuation
	of the emitted gamma rays and the accurate delineation of physiological
	spaces known to selectively accumulate those indicators. Similarly,
	other imaging modalities which also localize functions in 2D images
	(such as histological sections subsequently obtained from the same
	specimen), can provide a synergistic combination with CT-based 3D
	microstructure. (3) By increasing the sensitivity and specificity
	of X-ray CT image contrast by use of methods such as: K-edge subtraction
	imaging, X-ray fluorescence imaging, imaging of the various types
	of scattered X-ray and the consequences of the change in the speed
	of X-rays through different tissues, such as refraction and phase
	shift. These other methods of X-ray imaging can increase contrast
	by more than an order of magnitude over that due to conventionally-used
	attenuation of X-ray. To fully exploit their potentials, much development
	of radiopaque indicators, scanner hardware and image reconstruction
	and analysis software will be needed.},
  doi = {10.1002/jcb.10415},
  institution = {Department of Physiology and Biophysics, Mayo Clinic, Rochester,
	MN 55905, USA. elran@mayo.edu},
  keywords = {Animals; Animals, Laboratory; Disease Models, Animal; Image Enhancement;
	Sensitivity and Specificity; Tomography, X-Ray Computed, instrumentation/methods},
  owner = {user},
  pmid = {12552611},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1002/jcb.10415}
}

@INPROCEEDINGS{romdhani_hierarchical_2006,
  author = {I. Romdhani and J. Munoz and H. Bettahar and A. Bouabdallah},
  title = {Hierarchical Home Agent Architecture for Mobile IP Communications},
  booktitle = {Computers and Communications, 2006. ISCC '06. Proceedings. 11th IEEE
	Symposium on},
  year = {2006},
  pages = {136--141},
  abstract = {While the Mobile IP protocol does not exclude the use of multiple
	Home Agents (HAs), it does not impose any particular model either.
	Recent solutions propose that a mobile node uses multiple HAs located
	in different physical links in his home network. However, no architectural
	scheme is proposed either for unicast or multicast communications.
	In this paper, we propose a Hierarchical Home Agent architecture
	in which each Home Agent (HA) is assigned a specific topological
	level in its domain. The mobile node is notified about such hierarchy
	and chooses the closest HA on a perconnection basis. Our primary
	goal is to optimise both unicast and multicast routing for mobile
	nodes. We prove that our solution avoids redundant multicast traffic
	in the home domain and it is easy to implement using the current
	Mobile IP specification without extra cost.},
  doi = {10.1109/5.910852},
  isbn = {1530-1346 },
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{schattenberg_planning_2001,
  author = {B. Schattenberg and A.M. Uhrmacher},
  title = {Planning agents in JAMES},
  journal = {Proceedings of the IEEE},
  year = {2001},
  volume = {89},
  pages = {158--173},
  number = {2},
  abstract = {Testing is an obligatory step in developing multiagent systems. For
	testing multiagent systems in virtual, dynamic environments, simulation
	systems are required that support a modular, declarative construction
	of experimental frames, that facilitate the embedding of a variety
	of agent architectures and that allow an efficient parallel, distributed
	execution. We introduce the system JAMES (a Java based agent modeling
	environment for simulation). In JAMES, agents and their dynamic environment
	are modeled as reflective, time-triggered state automata. Its possibilities
	to compose experimental frames based on predefined components, to
	express temporal interdependencies, to capture the phenomenon of
	proactiveness and reflectivity of agents are illuminated by experiments
	with planning agents. The underlying planning system is a general-purpose
	system, about which no empirical results exist besides traditional
	static benchmark tests. We analyze the interplay between heuristics
	for selecting goals, viewing range, commitment strategies, explorativeness,
	and trust in the persistence of the world and uncover properties
	of the the agent, the planning engine, and the chosen test scenario:
	TILEWORLD},
  doi = {10.1109/TSMCA.2003.817394},
  issn = {0018-9219},
  keywords = {agent architectures,bibliographies,commitment strategies,experimental
	frames,general-purpose system,heuristics,JAMES,Java based agent modeling
	environment for simulation,modular declarative construction,multiagent
	system testing,multiagent systems development,parallel distributed
	execution,planning agents,planning engine,predefined components,proactiveness,temporal
	interdependencies,test scenario,TILEWORLD,time-triggered state automata,virtual
	dynamic environments},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{schermerhorn_social_2007,
  author = {P. Schermerhorn and M. Scheutz},
  title = {Social, Physical, and Computational Tradeoffs in Collaborative Multi-agent
	Territory Exploration Tasks},
  booktitle = {Artificial Life, 2007. ALIFE '07. IEEE Symposium on},
  year = {2007},
  pages = {295--302},
  abstract = {The performance of embodied multi-agent systems depends, in addition
	to the agent architectures of the employed agents, on their physical
	characteristics (e.g., sensory range, speed, etc.) and group properties
	(e.g., number of agents, types of agents, etc.). Consequently, it
	is difficult to evaluate the performance of a multi-agent system
	based on the performance of an agent architecture alone, even in
	homogeneous teams. In this paper, we propose a method for analyzing
	the performance of multi-agent systems based on the notion of "performance-cost-tradeoff,"
	which attempts to determine the relations among different cost-dimensions
	by performing a performance sampling of these dimensions and comparing
	them relative to their associated costs. Specifically, we investigate
	the performance-cost tradeoffs of four candidate architectures for
	a multi-agent territory exploration task in which a group of agents
	is required to visit a set of checkpoints randomly placed in an environment
	in the shortest time possible. Performance tradeoffs between three
	dimensions (sensory range, group size, and prediction) are then used
	to illustrate the cost-benefit analyses performed to determine the
	best agent configurations for different practical settings.},
  doi = {10.1109/ALIFE.2007.367809},
  keywords = {agent architecture,collaborative multiagent territory exploration,performance
	sampling,performance-cost-tradeoff},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{scheutz_architectural_2004,
  author = {M. Scheutz and V. Andronache},
  title = {Architectural mechanisms for dynamic changes of behavior selection
	strategies in behavior-based systems},
  journal = {Systems, Man, and Cybernetics, Part B, IEEE Transactions on},
  year = {2004},
  volume = {34},
  pages = {2377--2395},
  number = {6},
  abstract = {Behavior selection is typically a "built-in" feature of behavior-based
	architectures and hence, not amenable to change. There are, however,
	circumstances where changing behavior selection strategies is useful
	and can lead to better performance. In this paper, we demonstrate
	that such dynamic changes of behavior selection mechanisms are beneficial
	in several circumstances. We first categorize existing behavior selection
	mechanisms along three dimensions and then discuss seven possible
	circumstances where dynamically switching among them can be beneficial.
	Using the agent architecture framework activation, priority, observer,
	and component (APOC), we show how instances of all (nonempty) categories
	can be captured and how additional architectural mechanisms can be
	added to allow for dynamic switching among them. In particular, we
	propose a generic architecture for dynamic behavior selection, which
	can integrate existing behavior selection mechanisms in a unified
	way. Based on this generic architecture, we then verify that dynamic
	behavior selection is beneficial in the seven cases by defining architectures
	for simulated and robotic agents and performing experiments with
	them. The quantitative and qualitative analyzes of the results obtained
	from extensive simulation studies and experimental runs with robots
	verify the utility of the proposed mechanisms.},
  doi = {10.1109/TSMCB.2004.837309},
  issn = {1083-4419},
  keywords = {Action selection,agent architecture framework activation,agent architectures,architectural
	mechanism,behavior selection,behavior-based systems,dynamic behavior
	modification,dynamic behavior selection strategy,mobile robots,robotic
	agents},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{seragiotto_performance_2005,
  author = {C. Seragiotto and T. Fahringer},
  title = {Performance analysis for distributed and parallel Java programs with
	Aksum},
  booktitle = {Cluster Computing and the Grid, 2005. CCGrid 2005. IEEE International
	Symposium on},
  year = {2005},
  volume = {2},
  pages = {1024--1031 Vol. 2},
  abstract = {This paper deals with the challenging problem of performance analysis
	for Java programs. We describe procedures and requirements for instrumenting,
	monitoring, and analyzing distributed Java codes, and introduces
	Aksum, a highly customizable and flexible system for performance
	analysis that helps programmers to semi-automatically locate and
	understand performance problems in parallel and distributed Java
	programs. We also describe a sophisticated agent architecture as
	part of Aksum for static and dynamic instrumentation of Java programs.
	Experiments are presented for a widely distributed application running
	on a heterogeneous set of machines with different operating systems
	to illustrate the usefulness of our approach.},
  doi = {10.1109/ASWEC.2006.10},
  keywords = {agent architecture,Aksum system,distributed Java code monitoring,distributed
	Java programs,Java program instrumentation,operating systems,parallel
	Java programs,performance analysis},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{shadbolt_planning_1991,
  author = {N. Shadbolt},
  title = {Planning and reflection in autonomous agents},
  booktitle = {Intelligent Agents, IEE Colloquium on},
  year = {1991},
  pages = {4/1--4/6},
  abstract = {In 1985 the author began a two year ESRC funded project to look at
	the interpretation and generation of cooperative dialogue between
	agents. The idea was to derive discourse from an underlying planning
	system-the belief was that the fundamental driving force behind dialogue
	is the problem solving ability of the agent. The research showed,
	via the construction of a number of computational models, that flexible
	dialogue could be obtained under this organisation. However, a number
	of important lessons emerged about the nature and character of effective
	agent architectures. These are being followed up, in particular,
	the role of reflection and planning in agent architectures is being
	investigated},
  doi = {10.1109/2.144395},
  keywords = {autonomous agents,computational models,planning system},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{weiguang_shao_agent_1998,
  author = {Weiguang Shao and Wei-Tek Tsai and S. Rayadurgam and R. Lai},
  title = {An agent architecture for supporting individualized services in Internet
	applications},
  booktitle = {Tools with Artificial Intelligence, 1998. Proceedings. Tenth IEEE
	International Conference on},
  year = {1998},
  pages = {140--147},
  abstract = {This paper presents the agent architecture of an Internet application
	development tool called Distributed Interactive Web-site Builder
	(DIWB). Together with the component object model and a layering framework,
	the agent architecture can be used to build Internet applications
	that support individualized services. The DIWB can construct pages
	dynamically at runtime and can be easily customized for individual
	users. The architecture consists of two cooperating agents that compose
	pages at runtime using components and data stored in various databases
	(agencies). The page agent composes a page by retrieving page definition
	and requesting the component agent to construct individual components.
	The component agent retrieves user preferences, and page component
	definitions from the databases and returns the results to the page
	agent},
  doi = {10.1109/TAI.1997.632297},
  keywords = {agent architecture,application development tool,component agent,component
	object model,cooperating agents,cooperative systems,customization,databases,Distributed
	Interactive Web-site Builder,DIWB,individualized services,layering
	framework,page definition,runtime,user preferences},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Silverman1997,
  author = {B. G. Silverman and K. Moidu and B. E. Clemente and L. Reis and D.
	Ravichandar and C. Safran},
  title = {HOLON: a Web-based framework for fostering guideline applications.},
  journal = {Proc AMIA Annu Fall Symp},
  year = {1997},
  pages = {374--378},
  abstract = {HOLON is a research and development effort in extending middleware
	in the healthcare field to support application development, in general,
	and guideline applications, in particular. This framework makes use
	of open standards for architecture, software, guideline KBs, clinical
	repository models, information encodings, and intelligent system
	modules and agents. By pursuing the use of such standards in our
	middleware components, we hope eventually to maximize reusability
	of the HOLON framework by others who also adhere to these open standards.
	This research reflects lessons learned about the extensions needed
	in these standards if healthcare middleware frameworks are to transparently
	support application developers and their users over the web.},
  institution = {George Washington University, USA. barry@seas.gwu.edu},
  keywords = {Artificial Intelligence; Computer Communication Networks; Computer
	Systems; Diabetes Mellitus, diagnosis/therapy; Guidelines as Topic;
	Humans; Medical Informatics Applications; Programming Languages;
	Software},
  owner = {user},
  pmid = {9357651},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{smolko_design_2001,
  author = {D. Smolko},
  title = {Design and evaluation of the mobile agent architecture for distributed
	consistency management},
  booktitle = {Software Engineering, 2001. ICSE 2001. Proceedings of the 23rd International
	Conference on},
  year = {2001},
  pages = {799--800},
  abstract = {The proposed mobile agent architecture for carrying out incremental
	consistency checks between sets of distributed software engineering
	documents is described and evaluated. Functionality of architectural
	components and collaboration between them throughout the consistency
	check are described. Architecture simulation, based on concurrent
	"execution" of state chart models of components, is used for evaluation
	of scalability in a number of system configurations. This work represents
	the first part of a thesis, which aims to establish applicability
	of mobile agent technology to the domain of distributed consistency
	management.},
  doi = {10.1109/WCICA.2000.859945},
  isbn = {0270-5257 },
  keywords = {distributed consistency management,distributed software engineering
	documents,document handling,incremental consistency checks,mobile
	agent architecture,scalability,software architecture simulation,state
	chart models},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Stacey2007,
  author = {Michael Stacey and Carolyn McGregor and Mark Tracy},
  title = {An architecture for multi-dimensional temporal abstraction and its
	application to support neonatal intensive care.},
  journal = {Conf Proc IEEE Eng Med Biol Soc},
  year = {2007},
  volume = {2007},
  pages = {3752--3756},
  abstract = {Temporal abstraction (TA) provides the means to instil domain knowledge
	into data analysis processes and allows transformation of low level
	numeric data to high level qualitative narratives. TA mechanisms
	have been primarily applied to uni-dimensional data sources equating
	to single patients in the clinical context. This paper presents a
	framework for multi-dimensional TA (MDTA) enabling analysis of data
	emanating from numerous patients to detect multiple conditions within
	the environment of neonatal intensive care. Patient agents which
	perform temporal reasoning upon patient data streams are based on
	the Event Calculus and an active ontology provides a central knowledge
	core where rules are stored and agent responses accumulated, thus
	permitting a level of multi-dimensionality within data abstraction
	processes. Facilitation of TA across a ward of patients offers the
	potential for early detection of debilitating conditions such as
	Sepsis, Pneumothorax and Periventricular Leukomalacia (PVL), which
	have been shown to exhibit advance indicators in physiological data.
	Preliminary prototyping for patient agents has begun with promising
	results and a schema for the active rule repository outlined.},
  doi = {10.1109/IEMBS.2007.4353148},
  institution = {Health Informatics Research Group, School of Computing and Mathematics,
	University of Western Sydney, Locked Bag 1797, Penrith South DC,
	1797, NSW, Australia. mstacey@scm.uws.edu.au},
  keywords = {Australia; Database Management Systems; Decision Support Systems,
	Clinical; Diagnosis, Computer-Assisted, methods; Expert Systems;
	Intensive Care, Neonatal, methods; Medical Records Systems, Computerized;
	Monitoring, Physiologic, methods},
  owner = {user},
  pmid = {18002814},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1109/IEMBS.2007.4353148}
}

@INPROCEEDINGS{steele_xml-based_2005,
  author = {R. Steele and T. Dillon and P. Pandya and Y. Ventsov},
  title = {XML-based mobile agents},
  booktitle = {Information Technology: Coding and Computing, 2005. ITCC 2005. International
	Conference on},
  year = {2005},
  volume = {2},
  pages = {42--48 Vol. 2},
  abstract = {Current mobile agent systems are based on agent architectures that
	are partially or fully implementation programming language-specific.
	Mobile agent implementation in a specific programming language has
	usage limitations as the inter agent communication and agent migration
	to other agent hosts needs support for the same language. Widely
	used Java-based agents have an advantage of easy agent migration
	through bytecode transfer but this also imposes the requirement of
	a Java virtual machine (JVM) on each agent host where agents can
	migrate. To achieve a truly system independent and agent system architecture
	independent solution, we propose a XML-based mobile agent architecture.
	This paper presents a mobile agent system design based on the use
	of XML-based agent code, the UDDI registry for agent registration
	and lookup/discovery and XML Web service calls for mobile agent intercommunication
	and migration. This also facilitates industry to have easier and
	less risky adaptation from existing agent system implementation.},
  doi = {10.1109/ITCC.2005.302},
  keywords = {agent architecture,agent migration,agent registration,Java virtual
	machine,Java-based agent,mobile agent intercommunication,mobile agent
	system,mobile agents,UDDI,UDDI registry,web services,XML Web service,XML-based
	agent code},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Sundaram2001,
  author = {M. Sundaram and S. Y. Yao and J. C. Ingram and Z. A. Berry and F.
	Abidi and C. E. Cass and S. A. Baldwin and J. D. Young},
  title = {Topology of a human equilibrative, nitrobenzylthioinosine (NBMPR)-sensitive
	nucleoside transporter (hENT1) implicated in the cellular uptake
	of adenosine and anti-cancer drugs.},
  journal = {J Biol Chem},
  year = {2001},
  volume = {276},
  pages = {45270--45275},
  number = {48},
  month = {Nov},
  abstract = {The human equilibrative nucleoside transporter hENT1, the first identified
	member of the ENT family of integral membrane proteins, is the primary
	mechanism for the cellular uptake of physiologic nucleosides, including
	adenosine, and many anti-cancer nucleoside drugs. We have produced
	recombinant hENT1 in Xenopus oocytes and used native and engineered
	N-glycosylation sites in combination with immunological approaches
	to experimentally define the membrane architecture of this prototypic
	nucleoside transporter. hENT1 (456 amino acid residues) is shown
	to contain 11 transmembrane helical segments with an amino terminus
	that is intracellular and a carboxyl terminus that is extracellular.
	Transmembrane helices are linked by short hydrophilic regions, except
	for a large glycosylated extracellular loop between transmembrane
	helices 1 and 2 and a large central cytoplasmic loop between transmembrane
	helices 6 and 7. Sequence analyses suggest that this membrane topology
	is common to all mammalian, insect, nematode, protozoan, yeast, and
	plant members of the ENT protein family.},
  doi = {10.1074/jbc.M107169200},
  institution = {Membrane Transport Research Group, Departments of Physiology and
	Oncology, University of Alberta and Cross Cancer Institute, Edmonton,
	Alberta T6G 2H7, Canada.},
  keywords = {Adenosine, pharmacokinetics; Algorithms; Amino Acids, chemistry; Animals;
	Antineoplastic Agents, pharmacokinetics; Biological Transport; Cell
	Membrane, metabolism; Cytoplasm, metabolism; Dose-Response Relationship,
	Drug; Enzyme-Linked Immunosorbent Assay; Equilibrative Nucleoside
	Transporter 1; Glycosylation; Humans; Immunoblotting; Immunohistochemistry;
	Membrane Transport Proteins, metabolism; Protein Structure, Tertiary;
	Recombinant Proteins, metabolism; Software; Thioinosine, analogs
	/&/ derivatives/chemistry; Xenopus, metabolism},
  owner = {user},
  pii = {M107169200},
  pmid = {11584005},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1074/jbc.M107169200}
}

@INPROCEEDINGS{suwu_agent_2001,
  author = {W. Suwu and A. Das},
  title = {An agent system architecture for e-commerce},
  booktitle = {Database and Expert Systems Applications, 2001. Proceedings. 12th
	International Workshop on},
  year = {2001},
  pages = {715--719},
  abstract = {Mobile agents are autonomous objects that are able to migrate from
	node to node in a computer network. A number of general purpose mobile
	agent architectures have been developed in the recent past. Though
	the core abstractions and functionalities are common to them, they
	employ a number of ad hoc solutions that make them either too restrictive
	or too general for the purpose of an open mobile-agent based e-commerce
	system. This led us to develop an mobile agent architecture of our
	own that will effectively address the issues of communication, tracking
	and naming in a manner that is most suitable for e-commerce applications.
	Our implementation is based on Java 2. In this paper, we describe
	the current state Of our agent system infrastructure, focusing on
	the implementation issues on agent mobility, communication and tracking},
  doi = {10.1109/ROBOT.1995.525646},
  keywords = {agent naming,agent system architecture,agent system infrastructure,agent
	tracking,computer network,Java 2,open mobile-agent based e-commerce
	system},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Szyperski1992,
  author = {T. Szyperski and P. Güntert and S. R. Stone and K. Wüthrich},
  title = {Nuclear magnetic resonance solution structure of hirudin(1-51) and
	comparison with corresponding three-dimensional structures determined
	using the complete 65-residue hirudin polypeptide chain.},
  journal = {J Mol Biol},
  year = {1992},
  volume = {228},
  pages = {1193--1205},
  number = {4},
  month = {Dec},
  abstract = {The three-dimensional structure of the N-terminal 51-residue domain
	of recombinant hirudin in aqueous solution was determined by 1H nuclear
	magnetic resonance (NMR) spectroscopy, and the resulting high-quality
	solution structure was compared with corresponding structures obtained
	from studies with the intact, 65-residue polypeptide chain of hirudin.
	On the basis of 580 distance constraints derived from nuclear Overhauser
	effects and 109 dihedral angle constraints, a group of 20 conformers
	representing the solution structure of hirudin(1-51) was computed
	with the program DIANA and energy-minimized with a modified version
	of the program AMBER. Residues 3 to 30 and 37 to 48 form a well-defined
	molecular core with two antiparallel beta-sheets composed of residues
	14 to 16 and 20 to 22, and 27 to 31 and 36 to 40, and three reverse
	turns at residues 8 to 11 (type II), 17 to 20 (type II') and 23 to
	26 (type II). The average root-mean-square deviation of the individual
	NMR conformers relative to their mean co-ordinates is 0.38 A for
	the backbone atoms and 0.77 A for all heavy atoms of these residues.
	Increased structural disorder was found for the N-terminal dipeptide
	segment, the loop at residues 31 to 36, and the C-terminal tripeptide
	segment. The solution structure of hirudin(1-51) has the same molecular
	architecture as the corresponding polypeptide segment in natural
	hirudin and recombinant desulfatohirudin. It is also closely similar
	to the crystal structure of the N-terminal 51-residue segment of
	hirudin in a hirudin-thrombin complex, with root-mean-square deviations
	of the crystal structure relative to the mean solution structure
	of 0.61 A for the backbone atoms and 0.91 A for all heavy atoms of
	residues 3 to 30 and 37 to 48. Further coincidence is found for the
	loop formed by residues 31 to 36, which shows increased structural
	disorder in all available solution structures of hirudin, and of
	which residues 32 to 35 are not observable in the electron density
	map of the thrombin complex. Significant local structural differences
	between hirudin(1-51) in solution and hirudin in the crystalline
	thrombin complex were identified mainly for the N-terminal tripeptide
	segment and residues 17 to 21. These are further analyzed in an accompanying
	paper.},
  institution = {Institut für Molekularbiologie und Biophysik, Eidgenössische Technische
	Hochschule-Hönggerberg, Zürich, Switzerland.},
  keywords = {Fibrinolytic Agents, chemistry; Heat; Hirudins, analogs /&/ derivatives/chemistry/metabolism;
	Hydrogen Bonding; Hydrogen-Ion Concentration; Macromolecular Substances;
	Magnetic Resonance Spectroscopy; Mathematical Computing; Models,
	Molecular; Peptide Fragments, chemistry/metabolism; Protein Conformation;
	Protons; Recombinant Proteins, chemistry/metabolism; Software; Solutions,
	chemistry; Surface Properties; Thrombin, chemistry},
  owner = {user},
  pii = {0022-2836(92)90325-E},
  pmid = {1335515},
  timestamp = {2008.10.19}
}

@ARTICLE{Tabaraki2006,
  author = {R. Tabaraki and T. Khayamian and A. A. Ensafi},
  title = {Wavelet neural network modeling in QSPR for prediction of solubility
	of 25 anthraquinone dyes at different temperatures and pressures
	in supercritical carbon dioxide.},
  journal = {J Mol Graph Model},
  year = {2006},
  volume = {25},
  pages = {46--54},
  number = {1},
  month = {Sep},
  abstract = {A wavelet neural network (WNN) model in quantitative structure property
	relationship (QSPR) was developed for predicting solubility of 25
	anthraquinone dyes in supercritical carbon dioxide over a wide range
	of pressures (70-770 bar) and temperatures (291-423 K). A large number
	of descriptors were calculated with Dragon software and a subset
	of calculated descriptors was selected from 18 classes of Dragon
	descriptors with a stepwise multiple linear regression (MLR) as a
	feature selection technique. Six calculated and two experimental
	descriptors, pressure and temperature, were selected as the most
	feasible descriptors. The selected descriptors were used as input
	nodes in a wavelet neural network (WNN) model. The wavelet neural
	network architecture and its parameters were optimized simultaneously.
	The data was randomly divided to the training, prediction and validation
	sets. The predictive ability of the model was evaluated using validation
	data set. The root mean squares error (RMSE) and mean absolute errors
	were 0.339 and 0.221, respectively, for the validation data set.
	The performance of the WNN model was also compared with artificial
	neural network (ANN) model and the results showed the superiority
	of the WNN over ANN model.},
  doi = {10.1016/j.jmgm.2005.10.012},
  institution = {Department of Chemistry, Isfahan University of Technology, Isfahan
	84154, Iran.},
  keywords = {Anthraquinones, chemistry; Carbon Dioxide, chemistry; Coloring Agents,
	chemistry; Models, Chemical; Models, Molecular; Neural Networks (Computer);
	Pressure; Quantitative Structure-Activity Relationship; Solubility;
	Temperature},
  owner = {user},
  pii = {S1093-3263(05)00149-X},
  pmid = {16337156},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1016/j.jmgm.2005.10.012}
}

@ARTICLE{Taira1996,
  author = {R. K. Taira and C. M. Breant and H. M. Chan and L. Huang and D. J.
	Valentino},
  title = {Architectural design and tools to support the transparent access
	to hospital information systems, radiology information systems, and
	picture archiving and communication systems.},
  journal = {J Digit Imaging},
  year = {1996},
  volume = {9},
  pages = {1--10},
  number = {1},
  month = {Feb},
  abstract = {The fragmentation of the electronic patient record among hospital
	information systems (HIS), radiology information systems (RIS), and
	picture archiving and communication systems (PACS) makes the viewing
	of the complete medical patient record inconvenient. The purpose
	of this report is to describe the system architecture, development
	tools, and implementation issues related to providing transparent
	access to HIS, RIS, and PACS information. A client-mediator-server
	architecture was implemented to facilitate the gathering and visualization
	of electronic medical records from these independent heterogeneous
	information systems. The architecture features intelligent data access
	agents, run-time determination of data access strategies, and an
	active patient cache. The development and management of the agents
	were facilitated by data integration CASE (computer-assisted software
	engineering) tools. HIS, RIS, and PACS data access and translation
	agents were successfully developed. All pathology, radiology, medical,
	laboratory, admissions, and radiology reports for a patient are available
	for review from a single integrated workstation interface. A data
	caching system provides fast access to active patient data. New network
	architectures are evolving that support the integration of heterogeneous
	software subsystems. Commercial tools are available to assist in
	the integration procedure.},
  institution = {Department of Radiological Sciences, University of California, Los
	Angeles 90024-1721, USA.},
  keywords = {Computer Systems; Database Management Systems; Hospital Information
	Systems; Medical Records Systems, Computerized; Radiology Information
	Systems; Software; Systems Integration},
  owner = {user},
  pmid = {8814763},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{takada_proposed_2006,
  author = {M. Takada and S. Tano and M. Iwata and T. Hashiyama},
  title = {A proposed home agent architecture to infer user feeling from user
	action pattern},
  booktitle = {Systems, Man and Cybernetics, 2006. SMC '06. IEEE International Conference
	on},
  year = {2006},
  volume = {6},
  pages = {4818--4824},
  abstract = {Many household electrical appliances that use information technology
	have recently been developed. We can operate these appliances from
	anywhere, and research has shown that such appliances support users
	in their daily lives. The basis for supporting the user is detecting
	repetitive operation by the user. We think it is important for a
	home agent to detect user operation and consider the users feeling
	when they are operating the appliance. The home agent takes into
	account the degree of importance and pleasure of the operation pattern.
	In this paper, we propose a home agent that predicts the user's operation,
	detects forgotten operation and supposes a degree of importance and
	pleasure for an operation pattern and evaluated its performance in
	a simulated environment.},
  doi = {10.1109/DIS.2006.62},
  keywords = {domestic appliances,Emotion Extraction,Home Agent,home agent architecture,home
	computing,household electrical appliance,Intention Extraction,mobile
	agents,Ubiquitous Computing},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{teranishi_geographical_2008,
  author = {Y. Teranishi and H. Tanaka and Y. Ishi and M. Yoshida},
  title = {A Geographical Observation System based on P2P Agents},
  booktitle = {Pervasive Computing and Communications, 2008. PerCom 2008. Sixth
	Annual IEEE International Conference on},
  year = {2008},
  pages = {615--620},
  abstract = {In this paper, a novel design and implementation of a geographical
	observation system based on P2P agents is described. We propose sensor
	agent architecture to realize efficient and real-time notifications
	of sensor information. In this architecture, sensors act as agents
	and send updated notification messages to the observer agents which
	satisfy specified conditions by making use of multiple overlays.
	These features are implemented on P2P agent software 'PIAX'. In this
	implementation, Web browsers and cellular phones can act as observer
	agents.},
  doi = {10.1109/HICSS.2007.411},
  keywords = {cellular phones,Geographical Observation,geographical observation
	system,observer agents,P2P Agent,P2P agent software,peer-to-peer
	computing,sensor agent architecture,Web browsers},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{thurston_distributed_2002,
  author = {T. Thurston and Huosheng Hu},
  title = {Distributed agent architecture for port automation},
  booktitle = {Computer Software and Applications Conference, 2002. COMPSAC 2002.
	Proceedings. 26th Annual International},
  year = {2002},
  pages = {81--87},
  abstract = {In the near future, container ports will no longer be able to expand
	into the surrounding land and will thus be unable to meet the storage
	requirements due to the boom in world trade. A solution to this problem
	is to increase the container throughput of the port by reducing the
	amount of time necessary to load and unload a ship. This paper presents
	a distributed agent architecture to achieve this task. Under such
	architecture, an intelligent planning algorithm is continuously optimised
	by the dynamic and co-operative rescheduling of yard resources such
	as quay cranes and container vehicles.},
  doi = {10.1109/KIMAS.2003.1245107},
  isbn = {0730-3157 },
  keywords = {container ports,container throughput,container vehicles,cranes,distributed
	agent architecture,dynamic co-operative rescheduling,flexible manufacturing
	systems,intelligent planning algorithm,marine systems,port automation,quay
	cranes,storage requirements,yard resources},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Turck2007,
  author = {F. De Turck and J. Decruyenaere and P. Thysebaert and S. Van Hoecke
	and B. Volckaert and C. Danneels and K. Colpaert and G. De Moor},
  title = {Design of a flexible platform for execution of medical decision support
	agents in the intensive care unit.},
  journal = {Comput Biol Med},
  year = {2007},
  volume = {37},
  pages = {97--112},
  number = {1},
  month = {Jan},
  abstract = {This paper addresses the design of a generic and scalable platform
	for the execution of medical decision support agents in the intensive
	care unit (ICU). As will be motivated, medical decision support agents
	can impose high computational load and in practical setups a large
	amount of such agents are typically running in parallel. Future ICU
	systems will rely on extensive medical decision support. However,
	in current systems only one workstation is typically dedicated for
	the execution of medical decision support agents. Therefore, we propose
	an architecture based on middleware technology to allow for easy
	distribution of the agents along multiple workstations. The architecture
	allows for easy integration with a general ICU data flow management
	architecture.},
  doi = {10.1016/j.compbiomed.2005.10.004},
  institution = {Department of Information Technology, Ghent University, Sint-Pietersnieuwstraat
	41, B-9000 Ghent, Belgium. filip.deturck@intec.ugent.be},
  keywords = {Computer Security; Computer Systems; Decision Support Systems, Clinical;
	Decision Support Techniques; Humans; Intensive Care Units; Local
	Area Networks; User-Computer Interface},
  owner = {user},
  pii = {S0010-4825(05)00121-6},
  pmid = {16364282},
  timestamp = {2008.10.19},
  url = {http://dx.doi.org/10.1016/j.compbiomed.2005.10.004}
}

@ARTICLE{Tuttle1996,
  author = {M. S. Tuttle and D. D. Sherertz and N. E. Olson and S. J. Nelson
	and M. S. Erlbaum and K. D. Keck and A. N. Davis and O. N. Suarez-Munist
	and S. S. Lipow and W. G. Cole and L. M. Fagan and R. D. Acuff and
	C. E. Crangle and M. A. Musen and S. W. Tu and G. C. Wiederhold and
	R. W. Carlson},
  title = {Toward reusable software components at the point of care.},
  journal = {Proc AMIA Annu Fall Symp},
  year = {1996},
  pages = {150--154},
  abstract = {An architecture built from five software components -a Router, Parser,
	Matcher, Mapper, and Server -fulfills key requirements common to
	several point-of-care information and knowledge processing tasks.
	The requirements include problem-list creation, exploiting the contents
	of the Electronic Medical Record for the patient at hand, knowledge
	access, and support for semantic visualization and software agents.
	The components use the National Library of Medicine Unified Medical
	Language System to create and exploit lexical closure-a state in
	which terms, text and reference models are represented explicitly
	and consistently. Preliminary versions of the components are in use
	in an oncology knowledge server.},
  institution = {Lexical Technology, Inc., Alameda, California, USA.},
  keywords = {Computer Systems; Medical Oncology; Medical Records Systems, Computerized;
	Point-of-Care Systems; Software; Unified Medical Language System},
  owner = {user},
  pmid = {8947646},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{ventouras_independent_2004,
  author = {E. Ventouras and M. Moatsos and C. Papageorgiou and A. Rabavilas
	and N. Uzunoglu},
  title = {Independent component analysis applied to the P600 component of event-related
	potentials},
  booktitle = {Engineering in Medicine and Biology Society, 2004. IEMBS '04. 26th
	Annual International Conference of the IEEE},
  year = {2004},
  volume = {1},
  pages = {80â€•83},
  abstract = {The analysis of the P600 component of event-related potentials (ERPs)
	has attracted attention due to its relation to covert cognitive mechanisms,
	in connection to memory processes. The component may often be low-amplitude,
	compared to other components such as the P300. Independent component
	analysis (ICA) techniques have been successfully applied in ERP processing,
	in the framework of blind source separation (BSS) for unmixing recorded
	potentials into a sum of temporally independent and spatially fixed
	components. In the present work ICA was used for reconstructing averaged
	ERPs in the time window of the P600 component, selecting a subset
	of independent components' projections to the original electrode
	recording positions. The selection is based on two empirical criteria,
	selecting the projection that reconstructs a P600 nearest temporally
	to the original P600, or selecting the projection combination - up
	to a preselected maximum number of combined projections providing
	maximum reconstructed P600 amplitude. The techniques are tested on
	ERPs recorded from healthy subjects and psychiatric patients, notably
	improving the differentiation of the two groups, based on either
	the amplitude or the latency of the reconstructed P600 component,
	in comparison to results achieved using the original ERPs.},
  doi = {10.1049/ip-sen:20000913},
  keywords = {bioelectric potentials,biomedical electrodes,blind source separation,Blind
	Source Separation (BSS),cognitive mechanisms,electrode,electroencephalography,event-related
	potentials,Event-related Potentials (ERPs),independent component
	analysis,Independent Component Analysis (ICA),medical signal processing,memory
	processes,P300 component,P600 component,psychiatric patients,signal
	reconstruction},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{verbraeck_component-based_2004,
  author = {Alexander Verbraeck},
  title = {Component-based distributed simulations: the way forward?},
  booktitle = {Proceedings of the eighteenth workshop on Parallel and distributed
	simulation},
  year = {2004},
  pages = {141â€•148},
  address = {Kufstein, Austria},
  publisher = {ACM},
  abstract = {Parallel simulation and distributed simulation sometimes appear to
	be two different worlds. Where parallel simulation aims at increasing
	the speed of a single model by distributing it over more processors,
	distributed simulation looks at ways to link entire models or federates
	that run on different computers. In the case of distributed simulation,
	the models themselves are hard to distribute, and often they each
	run on one processor as a monolithic model. This paper advocates
	to build the more traditional simulation models in such a way, that
	they can be easily distributed. As simulationists, we can learn from
	component-based theory from the software engineering field to prepare
	our models for distribution, and parts of our models for reuse. The
	results of several projects show that componentizing simulation models
	can have many advantages. The results also show that it is not easy
	to create models in a componentized way, and that current methods
	for simulation model building should be adapted.},
  doi = {10.1145/1013329.1013353},
  isbn = {0-7695-2111-8},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1013329.1013353\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{veres_class_2004,
  author = {S.M. Veres and J. Luo},
  title = {A class of BDI agent architectures for autonomous control},
  booktitle = {Decision and Control, 2004. CDC. 43rd IEEE Conference on},
  year = {2004},
  volume = {5},
  pages = {4746--4751 Vol.5},
  abstract = {A class of beliefs-desires-intentions (BDI) agent architecture is
	presented for control systems with a high degree of autonomy. The
	architecture contains agents for modelling, controller optimization,
	implementation and to monitor performance. The global convergence
	of performance of the agent system is proven under three mild assumptions.
	Relevant features of the agent structure are competing modellers
	and controllers. The benefit is an enhanced ability to learn new
	plant dynamics of varying complexity and controller adaptation. The
	new family of control agent architectures is called cautiously optimistic,
	a name to reflect the most important property of the new architecture:
	modelling results are applied with caution for control but current
	models are accepted until measurements do not contradict them with
	a margin. A cautiously optimistic control agent (COCA) is proven
	to have converging performance to a nearly optimal performance for
	stationary dynamics of a real plant under fairly general assumptions.},
  doi = {10.1109/IAT.2007.98},
  isbn = {0191-2216},
  keywords = {adaptive control,autonomous control,beliefs-desires-intentions agent
	architecture,cautiously optimistic control agent,competing controllers,competing
	modellers,controller adaptation,controller optimization,convergence,global
	convergence,intelligent control,monitor performance,plant dynamics,stationary
	dynamics},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{vidal_insideagent_2001,
  author = {J.M. Vidal and P.A. Buhler and M.N. Huhns},
  title = {Inside an agent},
  journal = {Internet Computing, IEEE},
  year = {2001},
  volume = {5},
  pages = {82--86},
  number = {1},
  abstract = {When we discuss agent-based system construction with software developers
	or ask students to implement common agent architectures using object-oriented
	techniques, we find that it is not trivial for them to create an
	elegant system design from the standard presentation of these architectures
	in textbooks or research papers. To better communicate our interpretation
	of popular agent architectures, we draw UML (Unified Modeling Language)
	diagrams to guide an implementer's design. However, before we describe
	these diagrams, we need to review some basic features of agents.
	The paper considers an architecture showing a simple agent interacting
	with an environment. The agent senses its environment, uses what
	it senses to choose an action, and then performs the action through
	its effectors. Sensory input can include received messages, and action
	can be the sending of messages. To construct an agent, we need a
	more detailed understanding of how it functions. In particular, if
	we are to build one using conventional object-oriented analysis and
	design techniques, we should know in what ways an agent is more than
	just a simple object},
  doi = {10.1109/4236.895147},
  issn = {1089-7801},
  keywords = {Internet agents,object-oriented techniques,UML},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{vieira_role_2002,
  author = {Marlon Vieira and Debra Richardson},
  title = {The role of dependencies in component-based systems evolution},
  booktitle = {Proceedings of the International Workshop on Principles of Software
	Evolution},
  year = {2002},
  pages = {62â€•65},
  address = {Orlando, Florida},
  publisher = {ACM},
  abstract = {In this position paper we argue that component dependencies should
	be treated as a first class problem in component-based systems evolution.
	We discuss some issues related to dependencies and present an overview
	of a technique to describe and analyze dependencies in component-based
	systems.},
  doi = {10.1145/512035.512051},
  isbn = {1-58113-545-9},
  keywords = {component-based systems,dependencies,evolution},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=512035.512051\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{vitharana_risks_2003,
  author = {Padmal Vitharana},
  title = {Risks and challenges of component-based software development},
  journal = {Commun. ACM},
  year = {2003},
  volume = {46},
  pages = {67â€•72},
  number = {8},
  abstract = {Component developers, application assemblers, and customers must all
	know CBSD advantages and disadvantages before developing components
	and component-based applications.},
  doi = {10.1145/859670.859671},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=859670.859671\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{vitharana_design_2003,
  author = {Padmal Vitharana and Fatemah "Mariam" Zahedi and Hemant Jain},
  title = {Design, retrieval, and assembly in component-based software development},
  journal = {Commun. ACM},
  year = {2003},
  volume = {46},
  pages = {97â€•102},
  number = {11},
  abstract = {Until recently, anecdotal evidence could only suggest CBSD superiority
	in requirements identification. Here is a set of testable hypotheses
	to help distinguish hype from fact.},
  doi = {10.1145/948383.948387},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=948383.948387\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{van_vliet_software_2008,
  author = {H. van Vliet},
  title = {Software Architecture Knowledge Management},
  booktitle = {Software Engineering, 2008. ASWEC 2008. 19th Australian Conference
	on},
  year = {2008},
  pages = {24--31},
  abstract = {Software architecture is a recognized and indispensable part of system
	development. Software architecture is often defined in terms of components
	and connectors, or the "high-level conception of a system". In recent
	years, there has been an awareness that not only the design itself
	is important to capture, but also the knowledge that has led to this
	design. This so-called architectural knowledge concerns the set of
	design decisions and their rationale. Capturing architectural knowledge
	is difficult. Part of it is tacit and difficult to verbalize. Like
	developers, software architects are not inclined to document their
	solutions. Establishing ways to effectively manage and organize architectural
	knowledge is one of the key challenges of the field of software architecture.
	This architectural knowledge plays a role during development, when
	architects, developers, and other stakeholders must communicate about
	the system to be developed, possibly in a global setting. It also
	plays a role during the evolution of a system, when changes are constrained
	by decisions made earlier.},
  doi = {10.1109/ASWEC.2008.4483186},
  isbn = {1530-0803},
  keywords = {architectural knowledge,knowledge management,system development,system
	evolution},
  owner = {user},
  timestamp = {2008.10.04}
}

@ARTICLE{voas_predictingbadly_1997,
  author = {E. Voas and F. Charron and G. McGraw and K. Miller and M. Friedman},
  title = {Predicting how badly â€œgoodâ€? software can behave},
  journal = {Software, IEEE},
  year = {1997},
  volume = {14},
  pages = {73â€•83},
  number = {4},
  abstract = {Using fault injection and failure-tolerance measurement with ultrarare
	inputs, the authors create on automated software environment that
	can supplement traditional testing methods. Applied to four case
	studies, their methods promise to make software more robust},
  doi = {10.1109/52.595959},
  issn = {0740-7459},
  keywords = {automated software environment,failure-tolerance measurement,fault
	injection,program testing,software fault tolerance,software tools,ultrarare
	inputs},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{voeten_system_1998,
  author = {J.P.M. Voeten and P.H.A. van der Putten and M.C.W. Geilen and M.P.J.
	Stevens},
  title = {System level modelling for hardware/software systems},
  booktitle = {Euromicro Conference, 1998. Proceedings. 24th},
  year = {1998},
  volume = {1},
  pages = {154â€•161 vol.1},
  abstract = {Industry is facing a crisis in the design of complex hardware/software
	systems. Due to the increasing complexity, the gap between the generation
	of a product idea and the realisation of a working system is expanding
	rapidly. To manage complexity and to shorten design cycles, industry
	is forced to look at system level languages towards specification
	and design. We report on the system level modelling language called
	POOSL. The language is very expressive and is able to model dynamic
	hard real time behaviour as well as static (architecture and topology)
	structure in an object oriented fashion. The language integrates
	a process part, based on the process algebra CCS, with a data part,
	based on the concepts of traditional object oriented programming
	languages. Unlike many modelling languages today, POOSL is equipped
	with a complete mathematical semantics. Currently a number of automated
	software tools (model editing simulator and compiler tools) are available.
	It is shown how the language and tools allow the estimation of a
	performance parameter of a datalink protocol},
  doi = {10.1109/MILCOM.1998.722536},
  keywords = {automated software tools,calculus of communicating systems,compiler
	tools,complex hardware/software systems,datalink protocol,design
	cycles,dynamic hard real time behaviour,hardware/software systems,mathematical
	semantics,model editing simulator,object oriented fashion,object-oriented
	languages,object-oriented programming,performance parameter,POOSL,process
	algebra CCS,real-time systems,simulation languages,system level modelling,system
	level modelling language,traditional object oriented programming
	languages,working system},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{vyzovitis_framework_2001,
  author = {D. Vyzovitis and K.L. Clark},
  title = {A framework for developing reactive information agents with heterogeneous
	communication capabilities},
  booktitle = {Autonomous Decentralized Systems, 2001. Proceedings. 5th International
	Symposium on},
  year = {2001},
  pages = {263--270},
  abstract = {We present a generic framework for developing complex distributed
	applications in open information environments, using reactive agents
	as basic building blocks. We address agent architecture and collaboration
	issues by introducing a minimal agent architecture, the Reactive
	Information Agent, and adopting a flexible asynchronous communication
	infrastructure, the Inter-Agent Communication Model. Finally, we
	present an implementation of the framework which allows agent designers
	to rapidly develop multi-agent systems based on simple heterogeneous
	computational elements, which are mobile, adaptive, reflective, and
	dynamically reconfigurable during the system lifetime},
  doi = {10.1109/ICMAS.2000.858448},
  keywords = {agent architecture,agent collaboration,complex distributed applications,dynamically
	reconfigurable systems,flexible asynchronous communication,heterogeneous
	communication,Inter-Agent Communication Model,mobile agents,open
	information environments,Reactive Information Agent,reflective systems},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Vazquez-Salceda2003,
  author = {J. Vázquez-Salceda and J. A. Padget and U. Cortés and A. López-Navidad
	and F. Caballero},
  title = {Formalizing an electronic institution for the distribution of human
	tissues.},
  journal = {Artif Intell Med},
  year = {2003},
  volume = {27},
  pages = {233--258},
  number = {3},
  month = {Mar},
  abstract = {The use of multi-agent systems (MAS) in health-care domains is increasing.
	Such agent-mediated medical systems can manage complex tasks and
	have the potential to adapt gracefully to unexpected events. However,
	in these kinds of systems the issues of privacy, security and trust
	are particularly sensitive in relation to matters such as agents'
	access to patient records, what is acceptable behaviour for an agent
	in a particular role and the development of trust both between (heterogeneous)
	agents and between users and agents. To address these issues we propose
	a formal normative framework, deriving from and developing the notion
	of an electronic institution. Such institutions provide a framework
	to define and police norms that guide, control and regulate the behaviour
	of the heterogeneous agents that participate in the institution.
	These norms define the acceptable actions that each agent may perform
	depending on the role or roles it is playing, and clearly specifies
	the data it may access and/or modify in playing those roles. In this
	paper, we present the formalization of Carrel, a virtual organization
	for the procurement of organs and tissues for transplantation purposes,
	as an electronic institution using the ISLANDER institution specification
	language as formalizing languages. We demonstrate aspects of the
	formalization of such an institution, example fragments in the language
	used for the textual specification, and how such formalization can
	be used as a blueprint in the implementation of the final agent architecture,
	through techniques such as skeleton generation.},
  institution = {Departament de Llenguatges i Sistemes Informàtics, Universitat Politècnica
	de Catalunya, c/Jordi Girona 1-3, E08034 Barcelona, Spain. jvazquez@lsi.upc.es},
  keywords = {Artificial Intelligence; Confidentiality; Decision Making, Computer-Assisted;
	Humans; Negotiating; Software; Tissue Banks; Tissue and Organ Procurement},
  owner = {user},
  pii = {S0933365703000058},
  pmid = {12667738},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{wagner_comparative_2006,
  author = {C. Wagner and T. Margaria and H.-G. Pagendarm},
  title = {Comparative Analysis of Tools for Automated Software Re-engineering
	Purposes},
  booktitle = {Leveraging Applications of Formal Methods, Verification and Validation,
	2006. ISoLA 2006. Second International Symposium on},
  year = {2006},
  pages = {433â€•440},
  abstract = {Model driven analysis and design are increasingly recognized as powerful
	methodologies for software development and evolution. Model driven
	approaches in particular can be also combined with formal methods,
	offering advantages for verification, analysis, and testing. In order
	to use model driven methods for re-designing existing software, however,
	one needs additional facilities to transform the existing source
	code into adequate models. We examine in this paper a large number
	of existing tools for the analysis and processing of C/C++ source
	code, and evaluate them with respect to their usefulness in the model
	driven development process of software re-design projects. Our goal
	is to identify tools suitable to convert C or C++ code into a code-level
	model. Preferably, we want to use existing compilers, adequate system
	tools or other research and commercial programs to automate the process
	of deriving code-level models of large software applications.},
  doi = {10.1109/IMTC.2008.4547280},
  keywords = {automated software re-engineering,C++ listings,C-C++ source code,compilers,model
	driven development process,program compilers,software development,software
	engineering,systems re-engineering},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Walczak2003,
  author = {Steven Walczak},
  title = {A multiagent architecture for developing medical information retrieval
	agents.},
  journal = {J Med Syst},
  year = {2003},
  volume = {27},
  pages = {479--498},
  number = {5},
  month = {Oct},
  abstract = {Information that is available on the world wide web (WWW) is already
	more vast than can be comprehensibly studied by individuals and this
	quantity is increasing at a staggering pace. The quality of service
	delivered by physicians is dependent on the availability of current
	information. The agent paradigm offers a means for enabling physicians
	to filter information and retrieve only information that is relevant
	to current patient treatments. As with many specialized domains,
	agent-based information retrieval in medical domains must satisfy
	several domain-dependent constraints. A multiple agent architecture
	is developed and described in detail to efficiently provide agent-based
	information retrieval from the WWW and other explicit information
	resources. A simulation of the proposed multiple agent architecture
	shows a 97\% decrease in information overload and an 85\% increase
	in information relevancy over existing meta-search tools (with even
	larger gains over standard search engines).},
  institution = {Health Administration and Information Systems Programs, The Business
	School, University of Colorado at Denver, Campus Box 165, PO Box
	173364, Denver, Colorado 80217-3364, USA. swalczak@carbon.cudenver.edu},
  keywords = {Artificial Intelligence; Computer Systems, standards; Databases, Bibliographic,
	standards; Evidence-Based Medicine; Humans; Information Storage and
	Retrieval, standards; Internet, standards; Medical Informatics, standards;
	Neural Networks (Computer); Terminology as Topic; User-Computer Interface},
  owner = {user},
  pmid = {14584625},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{walkowiak_incident_2007,
  author = {T. Walkowiak and T. Wilk},
  title = {Incident Detection and Analysis in Communication and Information
	Systems by Fuzzy Logic},
  booktitle = {Dependability of Computer Systems, 2007. DepCoS-RELCOMEX '07. 2nd
	International Conference on},
  year = {2007},
  pages = {205--212},
  abstract = {The paper presents the model of automatic detection system which uses
	fuzzy logic methodology for intrusion detection and analysis in sophisticated
	distributed system. The proposed system functions are to detect the
	occurrence of intrusions, their severity and impact on the system
	work. The authors indicate that the current system reveals to be
	experimental in nature. We propose the hierarchical agent architecture
	to increase the detection response speed and scalability of the system.},
  doi = {10.1109/IROS.2007.4399516},
  keywords = {fuzzy logic,fuzzy set theory,hierarchical agent architecture,incident
	analysis,incident detection,intrusion detection,sophisticated distributed
	system},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{jun_wang_modeling_2007,
  author = {Jun Wang and Weiru Chen and Jun Liu},
  title = {A Modeling of Software Architecture Reliability},
  booktitle = {Network and Parallel Computing Workshops, 2007. NPC Workshops. IFIP
	International Conference on},
  year = {2007},
  pages = {983--986},
  abstract = {This paper introduces software architecture reliability estimation
	and some typical software reliability model based architecture. We
	modify model of software reliability estimation so that to improve
	precision of estimating software architecture reliability. At the
	same time, this paper proposed the simplified method of calculating
	software architecture reliability based on the state transition matrix.
	The improved model is validated by an application system in the paper,
	and the result show that the precision is effectively increased.},
  doi = {10.1109/NPC.2007.17},
  keywords = {matrix algebra,software architecture reliability estimation,software
	architecture reliability modeling,state transition matrix},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{jiwen_wang_self-healing_2005,
  author = {Jiwen Wang and Chenghao Guo and Fengyu Liu},
  title = {Self-healing based software architecture modeling and analysis through
	a case study},
  booktitle = {Networking, Sensing and Control, 2005. Proceedings. 2005 IEEE},
  year = {2005},
  pages = {873--877},
  abstract = {An increasingly important requirement for software systems is the
	capability to adapt at runtime in varying resources, system errors,
	and changing requirements. Although there exist research papers for
	modeling self-healing based software systems, little has been focused
	on the formalized analysis of this kind of systems. This paper mainly
	argues for software architecture based on self-healing software systems.
	After given architectural style requirements, style characteristics,
	the self-healing mechanisms of the software architecture framework
	is designed according to the analysis of the requirements and the
	characteristics given before. Based on this model, a case study (command
	and control system) is analyzed by applying a formal systematic software
	architecture specification and analysis methodology (SAM). Conclusion
	shows this new model can satisfy the system's time constraint requirements
	and improve the system's availability.},
  doi = {10.1109/WICSA.2005.65},
  keywords = {architectural style requirements,command and control system case study,formal
	systematic software architecture specification and analysis methodology,self-healing
	based software architecture analysis,self-healing based software
	architecture modeling,style characteristics,system errors,time constraint
	requirements,varying resources},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{wang_approach_2006,
  author = {Lei Wang and Padmanabhan Krishnan},
  title = {An Approach to Provisioning E-Commerce Applications with Commercial
	Components},
  booktitle = {e-Business Engineering, 2006. ICEBE '06. IEEE International Conference
	on},
  year = {2006},
  pages = {323â€•330},
  abstract = {Component-based development is a trend towards building e-commerce
	applications. However, commercial components are rarely used during
	the development. The reason is that existing approaches to selecting
	and composing components suffer from the problem that the components
	retrieved usually do not exactly fit with other components in the
	system being developed. While formal methods can be used to describe
	and check semantic characteristics to better match components, there
	are practical limitations which restrict their adoption. We have
	proposed a framework to support a semantic description and selection
	of components. We used Simple Component Interface Language (SCIL)
	to describe user requirements and pre-built components from the current
	component sources. Specifications in SCIL can be translated to a
	variety of models including those that have a formal basis. In this
	paper, we perform a case study of searching commercial components
	for a generic e-commerce application. We specify the commercial components
	in SCIL and use two specific tools: jMocha and Alloy Analyser to
	identify the correct components that suit a particular task},
  doi = {10.1109/ICSMC.2001.973002},
  keywords = {Alloy Analyser,commercial component,component source,component-based
	development,e-commerce application provisioning,electronic commerce,formal
	method,formal specification,jMocha,object-oriented programming,semantic
	component description,semantic component selection,Simple Component
	Interface Language,software tools,user requirement},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{mengqiu_wang_internal_2005,
  author = {Mengqiu Wang and M. Purvis and M. Nowostawski},
  title = {An internal agent architecture incorporating standard reasoning components
	and standards-based agent communication},
  booktitle = {Intelligent Agent Technology, IEEE/WIC/ACM International Conference
	on},
  year = {2005},
  pages = {58--64},
  abstract = {This paper discusses a general architecture for intelligent software
	agents. It can be used to construct agents that engage in high-level
	reasoning by employing standard reasoning engines as plug-in components,
	while communicating with other agents by means of the standard FIPA-based
	communication protocols. The approach discussed uses internal micro-agents
	and declarative goals to form a hierarchical internal agent architecture.
	It has been implemented and tested with three high-level formal reasoning
	system components that are used in conjunction with an existing agent
	platform, OPAL, which supports the FIPA (Foundation for Intelligent
	Physical Agents) communication standards.},
  doi = {10.1109/TAI.1998.744830},
  keywords = {agent specification,declarative agent programming language,FIPA-based
	communication protocols,hierarchical internal agent architecture,high
	level languages,high-level formal reasoning system components,intelligent
	software agent architecture,internal micro-agents,OPAL multiagent
	platform,standards-based agent communication},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{wang_distributed_2007,
  author = {Nanbor Wang and Rooparani Pundaleeka and Johan Carlsson},
  title = {Distributed component support for integrating large-scale parallel
	HPC applications},
  booktitle = {Proceedings of the 2007 symposium on Component and framework technology
	in high-performance and scientific computing},
  year = {2007},
  pages = {141â€•150},
  address = {Montreal, Quebec, Canada},
  publisher = {ACM},
  abstract = {Component-based software engineering (CBSE) is now a widely accepted
	paradigm for developing large-scale commercial software. The Common
	Component Architecture (CCA) and its associated Babel toolsuite is
	designed to enable CBSE for High Performance Computing (HPC) scientific
	applications. Many scientific applications have adopted the CBSE
	paradigm and demonstrated its effectiveness using CCA.},
  doi = {10.1145/1297385.1297408},
  isbn = {978-1-59593-867-1},
  keywords = {cbse,cca,framework,hpc,parallel and distributed computing},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1297385.1297408\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@ARTICLE{Wannemacher2002,
  author = {Amy J Wannemacher and Gregory P Schepers and Kevin A Townsend},
  title = {Antihypertensive medication compliance in a Veterans Affairs Healthcare
	System.},
  journal = {Ann Pharmacother},
  year = {2002},
  volume = {36},
  pages = {986--991},
  number = {6},
  month = {Jun},
  abstract = {OBJECTIVE: To compare compliance rates associated with categories
	of antihypertensive medications in a Veteran's Affairs (VA) Healthcare
	System by use of readily available data and standard software. METHODS:
	Prescriptions from the Veteran's Health Information System Technology
	Architecture (VISTA) database for angiotension-converting enzyme
	(ACE) inhibitors, angiotensin II receptor blockers (ARBs), beta-blockers,
	calcium-channel blockers (CCBs), diuretics, and a miscellaneous group
	of antihypertensives filled or refilled during a 12-month period
	were included in the analysis. Claims data for each prescription
	were exported from the VISTA database to Microsoft Excel, and compliance
	rates were calculated by use of a methodology reported elsewhere.
	Mean compliance rates for each antihypertensive category were compared.
	RESULTS: A total of 26 201 prescription records accounting for 51
	927 separate prescription fills or refills were included. The majority
	of prescriptions (77\%) were associated with calculated compliance
	rates >80\%. The CCB category was associated with a significantly
	higher compliance rate (p < 0.001) than the beta-blockers (95\% CI
	1.3\% to 3.7\%), diuretics (95\% CI 1.4\% to 3.8\%), and miscellaneous
	agents (95\% CI 1.7\% to 7.5\%). The ACE inhibitor category was associated
	with a significantly higher rate (p < 0.001) than the beta-blockers
	(95\% CI 0.7\% to 3.0\%), diuretics (95\% CI 0.7\% to 3.0\%), and
	miscellaneous agents (95\% CI 1.1\% to 6.8\%). The ARB category had
	a higher compliance rate (p < 0.001) than the miscellaneous category
	(95\% CI 1.2\% to 11.9\%). There were no significant differences
	in compliance rates among ACE inhibitors, CCBs, or ARBs. CONCLUSIONS:
	VA outpatients are relatively compliant when taking their antihypertensive
	medications as measured by prescription refill rates. Compliance
	rates for CCBs and ACE inhibitors are higher than those for beta-blockers,
	diuretics, and agents such as clonidine, methyldopa, hydralazine,
	and reserpine. Compliance for ARBs compared favorably with those
	of CCBs and ACE inhibitors. The methods used in this evaluation can
	be easily implemented at other institutions as part of ongoing medication
	compliance improvement efforts.},
  institution = {VA Ann Arbor Healthcare System, MI 48105-2300, USA.},
  keywords = {Aged; Aged, 80 and over; Antihypertensive Agents, administration /&/
	dosage/classification/therapeutic use; Databases, Factual; Delivery
	of Health Care, statistics /&/ numerical data; Female; Humans; Hypertension,
	drug therapy; Male; Michigan; Middle Aged; Ohio; Outpatients, statistics
	/&/ numerical data; Patient Compliance; Prescriptions, Drug, statistics
	/&/ numerical data; Veterans, statistics /&/ numerical data},
  owner = {user},
  pmid = {12022897},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{washizaki_model-view_2005,
  author = {H. Washizaki and Y. Fukazawa},
  title = {A model-view separation architecture for GUI application components},
  booktitle = {Information Technology: Coding and Computing, 2005. ITCC 2005. International
	Conference on},
  year = {2005},
  volume = {2},
  pages = {359â€•364 Vol. 2},
  abstract = {Graphical user interface (GUI) application component is a compound
	component, which consists of fine-grained components (such as GUI
	widgets) and specific logic. GUI application components fabricated
	by conventional techniques expose low extensibility because of the
	incomplete separation between the application logic part and GUI
	part inside the components. In this paper, we propose a new component
	architecture, "BeaM", which realizes complete separation between
	both parts inside the GUI application components. We have implemented
	a development environment corresponding to the proposed architecture
	in Java language. As a result of experimental evaluations, it is
	found that BeaM is useful to develop GUI application components as
	JavaBeans components with high extensibility.},
  doi = {10.1109/ITCC.2005.17},
  keywords = {application logic part,application program interfaces,BeaM component
	architecture,development environment,distributed object management,fine-grained
	components,graphical user interface,graphical user interfaces,GUI
	application components,GUI widgets,Java,Java language,JavaBeans components,model-view
	separation architecture,object-oriented programming,program visualisation,programming
	environments,software architecture},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{wei_component_2008,
  author = {Wang Wei and Li Tong},
  title = {Component behavior relativity analysis},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {2008},
  volume = {33},
  pages = {1â€•7},
  number = {2},
  abstract = {Component-Based Development, CBD for short, is becoming a main stream
	software development paradigm which reuses the off-the-shelf components
	and assembles them together to form a component-based application.
	CBD paradigm obviously has aroused a great attention among the engineering
	and academic domain and many new concepts, tools, and development
	methodologies have been coming out. However, CBD paradigms are usually
	questioned by its feasibility and performance. The inherent reason
	of this dilemma is lacking a methodology to guide researchers or
	engineers to clarify the behavior relativity among components. In
	this paper we proposed a new formalism, named component network,
	which is an extension of Petri net and its powerful capability to
	modeling distributed system is the guarantee of successful modeling
	component-based system. Further more, a series operation and theorems
	are presented, which are concerned to be the footstone of component
	behavior relativity analysis and software evolution research.},
  doi = {10.1145/1350802.1350814},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1350802.1350814\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{weiss_reviewing_2007,
  author = {K.A. Weiss},
  title = {Reviewing Aerospace Proposals with respect to Software Architecture},
  booktitle = {Aerospace Conference, 2007 IEEE},
  year = {2007},
  pages = {1--20},
  abstract = {One of the most difficult aspects of evaluating competing software
	architectures is the lack of a basis of comparison. More often than
	not, software architecture document formats, modeling techniques,
	and descriptions are so different from one supplier to the next that
	a direct comparison is virtually impossible. Furthermore, these architecture
	descriptions may have varying degrees of completeness, making it
	even more difficult to select the best alternative. However, reviewers
	are given insight into the stage of development and maturity of the
	software architecture based on the specificity of the documentation
	as well as the level of abstraction of the design artifacts that
	comprise each supplier's architecture. By organizing the information
	contained within the architecture documentation into a standard form,
	reviewers are enabled to compare seemingly disparate architectures.
	This paper outlines a method for organizing and then evaluating the
	information contained within competing software architecture descriptions
	that helps alleviate some of these difficulties. The information
	organization technique and subsequent analysis are demonstrated on
	the 4D/RCS reference architecture for intelligent control systems.},
  doi = {10.1109/WICSA.2008.34},
  isbn = {1095-323X},
  keywords = {4D/RCS reference architecture,aerospace proposals,architecture documentation,design
	artifacts,intelligent control systems,software architecture document
	formats},
  owner = {user},
  timestamp = {2008.10.04}
}

@ARTICLE{whittle_models_1995,
  author = {Ben Whittle},
  title = {Models and languages for component description and reuse},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {1995},
  volume = {20},
  pages = {76â€•89},
  number = {2},
  abstract = {This paper brings together the current research on reusable component
	models and component description languages for reuse. The paper contains
	a description and comparison of the 3C and REBOOT component models.
	The importance and further development of the 3C model is discussed.
	The component description language field is surveyed, and an introduction
	is given to the languages LIL, ACT TWO, \&Pi;, Meld, CDL, CIDER,
	LILEANNA, and RESOLVE. All of these languages are aimed at describing
	reusable components in the design stages of development. Criteria
	for examining component description languages are introduced and
	used as the basis of a comparison of the languages. The paper concludes
	with suggestions for the convergence of these developments, and suggestions
	for further work in this field.},
  doi = {10.1145/224155.224165},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=224155.224165\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{wickramasinghe_adaptive_2004,
  author = {L.K. Wickramasinghe and L.D. Alahakoon},
  title = {Adaptive agent architecture inspired by human behavior},
  booktitle = {Intelligent Agent Technology, 2004. (IAT 2004). Proceedings. IEEE/WIC/ACM
	International Conference on},
  year = {2004},
  pages = {450--453},
  abstract = {Intelligent agent technology can be considered as a step towards the
	next stage of artificial intelligence. This new technology attempts
	to bridge the gap between man and machine. When bridging the man-machine
	gap, one of the main issues to address is how to make agents capable
	of autonomous actions in a dynamic environment. Inspired by human
	behavior, psychology and brain science, This work presents a layered
	agent architecture which combines two fundamental forms of adaptation:
	learning and evolution. Each layer depicts the functionality of a
	human being making the agent better prepared to face environment
	dynamisms. The novel feature of the proposed architecture is, it
	enables the agent to evolve such that, the best action required for
	a given state of the environment is identified through learning rather
	than using a pre defined set of actions or plans.},
  doi = {10.1109/IAT.2004.1342935},
  keywords = {adaptive agent architecture,agent evolution,agent learning,autonomous
	agents,brain science,human behavior,intelligent agent technology,man-machine
	gap,psychology},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{wickramasinghe_novel_2004,
  author = {L.K. Wickramasinghe and L.D. Alahakoon},
  title = {A novel adaptive decision making agent architecture inspired by human
	behavior and brain study models},
  booktitle = {Hybrid Intelligent Systems, 2004. HIS '04. Fourth International Conference
	on},
  year = {2004},
  pages = {142--147},
  abstract = {Intelligent agent technology, which expects to combine the marked
	trends in history of computing such as ubiquity, interconnection,
	intelligence, delegation and human orientation can be considered
	as a step towards the next stage of artificial intelligence. This
	new technology attempts to reduce the gap between man and machine.
	The remarkable ability of a human being to make decisions is art
	ongoing learning and evolutionary process. Therefore, when reducing
	the man-machine gap, one of the main issues to address is how to
	make agents decision makers in a human oriented way. The paper presents
	novel conceptual agent framework to provide human like decisions
	inspired by human behavior and brain study models. The proposed learning
	and evolutionary agent architecture make the agent capable of handling
	the dynamism in the environment too. The experiments illustrated
	with the banking application demonstrate how the proposed framework
	enables a software agent to make decisions in a human oriented manner.},
  doi = {10.1109/ICHIS.2004.10},
  keywords = {banking application,brain models,brain study model,decision making,decision
	making agent architecture,evolutionary agent architecture,evolutionary
	process,human oriented manner,intelligent agent technology,man-machine
	gap,software agent},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{williams_software_1988,
  author = {L.G. Williams},
  title = {Software process modeling: a behavioral approach},
  booktitle = {Software Engineering, 1988., Proceedings of the 10th International
	Conference on},
  year = {1988},
  pages = {174â€•186},
  abstract = {An approach is presented to software process modeling which is based
	on behavior descriptions of software development activities. The
	use of behavioral descriptions makes it possible to describe the
	software process at any desired level of abstraction and, therefore,
	assists in accommodating aspects of the process which are poorly
	understood. This approach also provides the ability to reason about
	the software process and is sufficiently rigorous to provide a basis
	for structuring automated software environments. An overview of the
	model is presented, followed by a formal definition. Examples are
	given to illustrate the application of the model to existing software
	processes and software methods. Finally, the implications of the
	model for automated software environments are discussed},
  doi = {10.1109/49.7866},
  keywords = {abstraction,behavior descriptions,formal definition,programming environments,software
	development activities,software engineering,software environments,software
	process modeling},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{winter_synchronous_1999,
  author = {V.L. Winter},
  title = {A synchronous paradigm for modeling stable reactive systems},
  booktitle = {High-Assurance Systems Engineering, 1999. Proceedings. 4th IEEE International
	Symposium on},
  year = {1999},
  pages = {257â€•265},
  abstract = {This paper describes a modeling technique for single-agent reactive
	systems, that is influenced by the modeling paradigm of Parnas as
	well as by the synchronous paradigms of LUSTRE and ESTEREL. In this
	paradigm, single-agent reactive systems are modeled in a universe
	having a discrete clock. This discretization of time greatly reduces
	the temporal complexity of the model. We believe that the advantage
	of this reduction in temporal complexity is that the resulting model
	is in many ways better suited to automated software construction
	and analysis techniques (e.g., deductive synthesis, transformation,
	and verification) than models that are based on continuous representations
	of time},
  doi = {10.1109/WPC.1999.777742},
  keywords = {automated software construction,computational complexity,deductive
	synthesis,ESTEREL,formal specification,LUSTRE,modeling paradigm,Parnas,single-agent
	reactive systems,stable reactive systems modelling,synchronous paradigm,systems
	analysis,temporal complexity},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{wobcke_representation_1996,
  author = {W. Wobcke},
  title = {The representation of plans in rational agent architectures},
  booktitle = {Intelligent Information Systems, 1996., Australian and New Zealand
	Conference on},
  year = {1996},
  pages = {22--25},
  abstract = {In a previous paper, the author presented a framework for rational
	agent architectures that use explicit representations of beliefs
	and intentions and a theory of belief and intention revision. He
	also motivated a specific logic of belief and intention with reference
	to standard puzzles from the literature. He considers the relationship
	between the BDI-architecture and classical planning systems such
	as NOAH and NONLIN. In particular, he shows that any nonlinear hierarchical
	plan without repeated actions can be represented using his formalism
	in such a way that any allowable execution sequence of the plan can
	be realized using a system implementing AGM theory revision. This
	result is surprising, because the AGM theory's use of total pre-orders
	on beliefs suggests that nonlinear plans cannot be represented: they
	can because the execution of such plans is sequential},
  doi = {10.1109/ICII.2001.983582},
  keywords = {AGM theory revision,allowable execution sequence,BDI-architecture,belief
	logic,belief maintenance,belief revision,classical planning systems,explicit
	representations,formal logic,intention logic,intention revision,NOAH,NONLIN,nonlinear
	hierarchical plan,plan representation,rational agent architectures,sequential
	plan execution,total pre-orders},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{wobcke_bdi_2005,
  author = {W. Wobcke and V. Ho and A. Nguyen and A. Krzywicki},
  title = {A BDI agent architecture for dialogue modelling and coordination
	in a smart personal assistant},
  booktitle = {Intelligent Agent Technology, IEEE/WIC/ACM International Conference
	on},
  year = {2005},
  pages = {323--329},
  abstract = {In this paper, we discuss the architectural aspects of a smart personal
	assistant (SPA) system that enables users to access a range of applications
	from a range of devices using multi-modal natural language dialogue.
	Each back-end application is a personal assistant specializing in
	one specific task such as e-mail or calendar management, and typically
	each has its own user model, enabling it to adapt to the user's changing
	preferences. The PDA interface to the SPA must present the system
	as a single unified set of back-end applications, enabling the user
	to conduct a dialogue in which it is easy to switch between these
	applications. Furthermore, the system's interaction with the user
	must be tailored to their current device. The SPA is implemented
	using an agent platform and includes a special BDI coordinator agent
	with plans both for coordinating the actions of the individual assistants
	and for encoding the system's dialogue model. The plan-based dialogue
	model is at a high level of abstraction, enabling the domain-independent
	plans in the dialogue model to be reused in different SPA systems.},
  doi = {10.1109/ICSMC.2005.1571203},
  keywords = {back-end application,BDI coordinator agent architecture,human computer
	interaction,interactive systems,multimodal natural language dialogue,notebook
	computers,PDA interface,plan-based dialogue model,smart personal
	assistant,system dialogue model},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{weihang_wu_safety_2004,
  author = {Weihang Wu and T. Kelly},
  title = {Safety tactics for software architecture design},
  booktitle = {Computer Software and Applications Conference, 2004. COMPSAC 2004.
	Proceedings of the 28th Annual International},
  year = {2004},
  pages = {368--375 vol.1},
  abstract = {The influence of architecture in assurance of system safety is being
	increasingly recognised in mission-critical software applications.
	Nevertheless, most architectural strategies have not been developed
	to the extent necessary to ensure safety of these systems. Moreover,
	many software safety standards fail to discuss the rationale behind
	the adoption of alternative architectural mechanisms. Safety has
	not been explicitly considered by existing software architecture
	design methodologies. As a result, there is little practical guidance
	on how to address safety concerns in 'shaping' a 'safe' software
	architecture. This work presents a method for software architecture
	design within the context of safety. This method is centred upon
	extending the existing notion of architectural tactics to include
	safety as a consideration. The approach extends existing software
	architecture design methodologies and demonstrates the true value
	of deployment of specific protection mechanisms. The feasibility
	of this method is demonstrated by an example.},
  doi = {10.1109/CMPSAC.2004.1342860},
  isbn = {0730-3157},
  keywords = {mission-critical software,safety tactics,software architecture design,software
	safety standards,system safety},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{xia_cimo_2000,
  author = {Yan Xia and A.T.S. Ho and YuCheng Zhang},
  title = {CIMO - Component Integration MOdel},
  booktitle = {Software Engineering Conference, 2000. APSEC 2000. Proceedings. Seventh
	Asia-Pacific},
  year = {2000},
  pages = {344â€•348},
  abstract = {The Component Object Model (COM) represents a binary interface standard
	that allows developers to build specialized software components that
	interface in a common way with other software components. After being
	compiled, these components are integrated into an application and
	can interoperate with each other in a reliable, controlled manner.
	Can the components be integrated into an application and interoperate
	without recompiled? This article describes the Component Integration
	Model (CIMO), a software platform that allows the components written
	by different software programmers to be integrated into an application
	and to inter-operate without re-compiling. Firstly, the paper concentrates
	on a general overview of CIMO and describes the constitution and
	functions of the CIMO architecture. Secondly, the paper presents
	the definition of the CIMO component concept and addresses how CIMO
	facilitates users to establish scalable component-based applications
	and how CIMO supports the synchronous and asynchronous communication
	between components. Thirdly, the paper explains how CIMO sets up
	the deployment of components and processes after users have developed
	components based on CIMO specifications},
  doi = {10.1109/APSEC.2000.896718},
  keywords = {asynchronous communication,binary interface standard,CIMO,Component
	Integration Model,component interoperation,Component Object Model,distributed
	object management,formal specification,object-oriented methods,scalable
	component-based applications,specialized software components,specifications,subroutines,synchronous
	communication},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{xie_improving_2006,
  author = {Tao Xie},
  title = {Improving Effectiveness of Automated Software Testing in the Absence
	of Specifications},
  booktitle = {Software Maintenance, 2006. ICSM '06. 22nd IEEE International Conference
	on},
  year = {2006},
  pages = {355â€•359},
  abstract = {Program specifications can be valuable in improving the effectiveness
	of automated software testing in generating test inputs and checking
	test executions for correctness. Unfortunately, specifications are
	often absent from programs in practice. We present a framework for
	improving effectiveness of automated testing in the absence of specifications.
	The framework supports a set of related techniques, including redundant-test
	detection, non-redundant-test generation, test selection, test abstraction,
	and program-spectra comparison. The framework has been implemented
	and empirical results have shown that the developed techniques within
	the framework improve the effectiveness of automated testing by detecting
	high percentage of redundant tests among test inputs generated by
	existing tools, generating non-redundant test inputs to achieve high
	structural coverage, reducing inspection efforts for detecting problems
	in the program, and exposing behavioral differences during regression
	testing},
  doi = {10.1109/ICSM.2006.31},
  isbn = {1063-6773},
  keywords = {automated software testing,nonredundant-test generation,program testing,program-spectra
	comparison,redundant-test detection,test abstraction,test selection},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{xie_researchsoftware_2007,
  author = {Xiong Xie and Weishi Zhang},
  title = {Research on Software Component Adaptation Based on Semantic Specification},
  booktitle = {Network and Parallel Computing Workshops, 2007. NPC Workshops. IFIP
	International Conference on},
  year = {2007},
  pages = {969â€•974},
  abstract = {Software component adaptation is a crucial problem in component-based
	software engineering. In this paper, a component model is described
	firstly in mathematical specification which is helpful to specify
	formally component adaptation. Three component adaptation architectures
	are described in formal semantic, including sequential architecture,
	alternative architecture and parallel architecture. The conditions
	of adaptation architecture are analyzed and the system will select
	automatically a proper architecture to adapt the components according
	to the architecture application conditions. The specification of
	the complex component can be obtained automatically based on the
	specification of the adapted components. To compose a component based
	on its semantics specification, the proposed architecture supports
	semantic representation of components and does not depend on the
	computing environment. The proposed approach in the paper offers
	a guarantee to the formal analysis of component adaptation and the
	validation of the proper component adaptation.},
  doi = {10.1109/TPAMI.2004.1273960},
  keywords = {component-based software engineering,formal semantic,mathematical
	specification,parallel architecture,semantic specification,semantic
	Web,sequential architecture,software architecture,software component
	adaptation},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{xu_basic_2004,
  author = {Guoqing Xu and Zongyuan Yang and Haitao Huang},
  title = {A basic model for components implementation of software architecture},
  journal = {SIGSOFT Softw. Eng. Notes},
  year = {2004},
  volume = {29},
  pages = {1â€•11},
  number = {5},
  abstract = {Components defined in software architecture have two features: as
	basic elements of the architecture, they must conform to the architectural
	constraints and in the meantime, similar to the common components,
	they should be designed flexibly enough to be able to be developed
	independently for the late third party integration. However, these
	two important issues have always been handled separately from different
	point of views, which leads to the extra work, confusions in the
	program structures as well as the difficulty in maintenance. This
	paper presents a basic model of the architecture-based components
	implementation to band these two issues together. It firstly describes
	a novel design pattern, triple-C pattern which stands for {\textbackslash}textlessu{\textbackslash}textgreaterC{\textbackslash}textless/u{\textbackslash}textgreateromponents-{\textbackslash}textlessu{\textbackslash}textgreaterC{\textbackslash}textless/u{\textbackslash}textgreaterommunicate-through-{\textbackslash}textlessu{\textbackslash}textgreaterC{\textbackslash}textless/u{\textbackslash}textgreateronnector.
	This pattern not only emphasizes that implementation must completely
	conform to the architectural definition, but also attempts to change
	the fundamental way of components communication with suggesting provided
	service should be transferred through the connector instead of directly
	between the client and server components. Second, it describes a
	novel ADL JCMPL, toolset JCMP and techniques to keep architectural
	conformance in the implementation as well as support the architectural
	integration from separate components. Finally, this model is evaluated
	in a case study.},
  doi = {10.1145/1022494.1022522},
  keywords = {component,jcmp,jcmpl,software architecture,triple-c pattern},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=1022494.1022522\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{jia_xu_software_2006,
  author = {Jia Xu},
  title = {A Software Architecture For Complex Real-Time Embedded Systems},
  booktitle = {Mechatronic and Embedded Systems and Applications, Proceedings of
	the 2nd IEEE/ASME International Conference on},
  year = {2006},
  pages = {1--8},
  abstract = {A software architecture that makes it much easier to verify the system
	timing properties is presented. The software architecture is based
	on a pre-run-time scheduling approach and is suitable for many complex
	real-time embedded systems. A comparison of the software architecture
	based on pre-run-time scheduling with software architectures based
	on priority scheduling is also presented},
  doi = {10.1109/MESA.2006.296981},
  keywords = {complex real-time embedded systems,pre-run-time scheduling,processor
	scheduling,system verification},
  owner = {user},
  timestamp = {2008.10.04}
}

@ARTICLE{Xu2001,
  author = {Y. Xu and L. D'Alessio and M. C. Jaulent and D. Sauquet and S. Spahni
	and P. Degoulet},
  title = {Integrating medical applications in an open architecture through
	generic and reusable components.},
  journal = {Stud Health Technol Inform},
  year = {2001},
  volume = {84},
  pages = {63--67},
  number = {Pt 1},
  abstract = {Allowing exchange of information and cooperation among network-wide
	distributed and heterogeneous applications is a major need of current
	health care information systems. It forces the development of open
	and modular integration architectures. Major issues in the development
	include defining a flexible and robust federation model, developing
	interaction and communication facilities as well as the mechanism
	insuring semantic interoperability. We developed generic and reusable
	software components to ease the construction of any integration platform.
	The Pilot and the Mediator Service components facilitate the execution
	of services and the meaningful transformation of information. They
	have been tested in the context of the SynEx European project to
	construct a multi-agents based integration architecture. The possibility
	of such architectures to take into account the issue of semantic
	interoperability is further discussed.},
  institution = {Department of Medical Informatics, Faculty of Medicine of Broussais-Hôtel
	Dieu, 75270 Paris, France. Xu@hegp.bhdc.jussieu.fr},
  keywords = {Computer Communication Networks, organization /&/ administration/standards;
	Computer Systems; Hypermedia; Medical Informatics Applications; Programming
	Languages; Software; Systems Integration},
  owner = {user},
  pmid = {11604707},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{yanes_morecots:specialized_2006,
  author = {N. Yanes and S.B. Sassi and L.L. Jilani},
  title = {MoReCOTS: a specialized search engine for COTS components on the
	Web},
  booktitle = {Commercial-off-the-Shelf (COTS)-Based Software Systems, 2006. Fifth
	International Conference on},
  year = {2006},
  pages = {7 pp.},
  abstract = {A typical process for COTS-based development consists in five main
	steps including identifying COTS products candidates, selecting the
	most appropriate one and integrating it with the other components.
	Results of the identification step are fundamental for the buy versus
	develop decision. Many potential COTS components are available on
	the Web. Finding an adequate component involves searching among heterogeneous
	descriptions of the components within a broad search space. Thus,
	the use of search systems is required to enhance the COTS components
	location and retrieval effectiveness and efficiency. However, our
	study of current search systems including directories, generic search
	engines, metasearch engines, and even components search engines revealed
	their inadequacy for the COTS components identification. That's why
	we propose "MoReCOTS", a specialized search engine for COTS components
	marketed on the Web. MoReCOTS is based on meta-searching online specialized
	databases maintained by publishers of COTS components catalogues.
	It provides (i) a directory containing a comprehensive list of COTS
	components categories, and (ii) a specialized search interface with
	specific search fields related to COTS components characteristics.},
  doi = {10.1109/ASWEC.2000.844568},
  keywords = {commercial-off-the-shelf component,component search engine,COTS component
	category,COTS component identification,COTS components catalogue,COTS
	components characteristic,COTS product candidate,COTS-based development,Internet,metasearch
	engine,MoReCOTS,object-oriented programming,online specialized database,search
	engines,software packages,World Wide Web},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{yang_sanet:service-agent_2003,
  author = {Qiang Yang and Yong Wang and Zhong Zhang},
  title = {SANet: a service-agent network for call-center scheduling},
  journal = {Systems, Man and Cybernetics, Part A, IEEE Transactions on},
  year = {2003},
  volume = {33},
  pages = {396â€•406},
  number = {3},
  abstract = {We consider a network of service-providing agents, where different
	agents have different capabilities, availability, and cost to solve
	problems. These characteristics are particularly important in practice
	for semi-automated call centers which provide quality customer service
	in real time. We have developed SANet, a service agent network for
	call center automation, to serve as an experimental testbed for our
	research. SANet can select appropriate agents to provide better solutions
	for customer problems according to the changing capabilities and
	availability of service agents in the network. It can also add or
	delete appropriate agents to balance problem-solving quality, efficiency,
	and cost according to the number and types of incoming customer problems.
	On this network, each service agent can be a human service agent,
	an automated software service agent, or a combination of the two.
	This paper describes the architecture, a problem scheduling algorithm
	and an agent assignment algorithm on the SANet. We highlight an application
	in which we apply SANet to a call-center scheduling problem for a
	cable-TV company. Finally, we show the efficiency and adaptability
	of our system via experimental results and discuss related works.},
  doi = {10.1109/SNPD.2007.418},
  issn = {1083-4427},
  keywords = {agent selection,automated software service agent,cable television,cable-TV
	company,call centres,call-center scheduling,human service agent,multi-agent
	systems,problem scheduling algorithm,problem solving,real-time customer
	service,real-time systems,SANet,semi-automated call centers,service-agent
	network,service-providing agents},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{yan_yang_analysis_2006,
  author = {Yan Yang and Gang Zhu and Xiaojin Zhang and Zhangdui Zhong},
  title = {Analysis of Distributed Intelligent Agent Model for QoS Dynamic Scheme
	in GSM/GPRS Network},
  booktitle = {Computational Intelligence and Security, 2006 International Conference
	on},
  year = {2006},
  volume = {1},
  pages = {554--557},
  abstract = {In this paper we study dynamic quality of service scheme in GSM/GPRS
	wireless network. A load balancing architecture constructed by distributed
	intelligent agent has been presented to support real time or burst
	data services. Fuzzy neural network was employed to predict GPRS
	traffic by learning examples. Meanwhile, we have presented a traffic
	estimation algorithm and a simple decision mechanism to deal with
	special applications such as burst data transmission. The simulation
	shows that distributed intelligent agent architecture could significantly
	reduce packet delay, route cost and relieve GPRS bottleneck},
  doi = {10.1109/ICEMI.2007.4350560},
  keywords = {burst data service,burst data transmission,decision mechanism,distributed
	intelligent agent architecture,distributed intelligent agent model,dynamic
	quality of service,fuzzy neural nets,fuzzy neural network,General
	Packet Radio Service,global system for mobile communication,GPRS
	traffic prediction,GSM/GPRS wireless network,load balancing architecture,packet
	delay reduction,packet radio networks,QoS dynamic scheme,radio access
	networks,resource allocation,route cost reduction,traffic estimation
	algorithm},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{qing_yao_business_2008,
  author = {Qing Yao and Jing Zhang and Haiyang Wang},
  title = {Business Process-Oriented Software Architecture for Supporting Business
	Process Change},
  booktitle = {Electronic Commerce and Security, 2008 International Symposium on},
  year = {2008},
  pages = {690--694},
  abstract = {As a business process is essential in running an enterprise, this
	paper aims toward improving software architecture by extracting business-relevant
	semantics from business logic and transforming them as important
	elements that need to be molded and represented in software architecture.
	This allows for the development of a business process-oriented software
	architecture (BPOSA) that can support smoother business process change.
	The paper introduces the structure of BPOSA and its work mode. Then
	it describes the steps involved in designing a process-oriented software
	system, and provides analysis and design methods of extending process-relevant
	attributes for application in business operations.},
  doi = {10.1109/ISECS.2008.46},
  keywords = {business process change,Business Process Management,business process-oriented
	software architecture,business-relevant semantics,Configuration,Process
	Variability,process-oriented software system,SOA,Software Architecture},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{ye_reflective_2008,
  author = {Peng Ye and Shi Ying and Xiang-Yang Jia and Jun-Feng Yao and Ju-Bo
	Luo and Wen-Jie Yuan},
  title = {A Reflective Information Model for Reusing Software Architecture},
  booktitle = {Computing, Communication, Control, and Management, 2008. CCCM '08.
	ISECS International Colloquium on},
  year = {2008},
  volume = {1},
  pages = {270--275},
  abstract = {Reusing software architecture, which is a kind of coarse-grained software
	resources at design time, is always a very difficult problem in the
	realm of software engineer. We consider that there are two fundamental
	reasons for this problem: one is the lack of information which supports
	the process of reusing software architecture; another is the lack
	of effective reusing approach. So this paper proposes RIM4RSAâ€”reflective
	information model for reusing software architecture supporting the
	reuse of architectural level design, which offers the information
	support for reusing software architecture and its constituents at
	design time by modeling the meta-information about the base-level
	to construct a meta-level. Moreover, this paper illustrates the approach
	to construct meta-level and base-level of the RIM4RSA, and presents
	the implementation solution of RIM4RSA. At the same time, we put
	forward an approach for reusing software architecture based on the
	use of RIM4RSA.},
  doi = {10.1109/CCCM.2008.98},
  keywords = {meta-information,reflection},
  owner = {user},
  timestamp = {2008.10.04}
}

@ARTICLE{yegnanarayana_iterative_1998,
  author = {B. Yegnanarayana and C. d'Alessandro and V. Darsinos},
  title = {An iterative algorithm for decomposition of speech signals into periodic
	and aperiodic components},
  journal = {Speech and Audio Processing, IEEE Transactions on},
  year = {1998},
  volume = {6},
  pages = {1â€•11},
  number = {1},
  abstract = {The speech signal may be considered as the output of a time-varying
	vocal tract system excited with quasiperiodic and/or random sequences
	of pulses. The quasiperiodic part may be considered as the deterministic
	or periodic component and the random part as the stochastic or aperiodic
	component of the excitation. We discuss issues involved in identifying
	and separating the periodic and aperiodic components of the source.
	The decomposition is performed on an approximation to the excitation
	signal, instead of decomposing the speech signal directly. The linear
	prediction residual signal is used as an approximation to the excitation
	signal of the vocal tract system. Speech is first analyzed to determine
	the voiced and unvoiced parts of the signal. Decomposition of the
	voiced part into periodic and aperiodic components is then accomplished
	by first identifying the frequency regions of harmonic and noise
	components in the spectral domain. The signal corresponding to the
	noise regions is used as a first approximation to the aperiodic component.
	An iterative algorithm is proposed which reconstructs the aperiodic
	component in the harmonic regions. The periodic component is obtained
	by subtracting the reconstructed aperiodic component signal from
	the residual signal. The individual components of the residual are
	then used to excite the derived all-pole model of the vocal tract
	system to obtain the corresponding components of the speech signal.
	Experiments were conducted using synthetic speech. They demonstrated
	the ability of the algorithm for decomposition of a synthetic speech
	signal made of a mixture of periodic and aperiodic components. Application
	to natural speech is also discussed},
  doi = {10.1109/COMPSAC.2006.17},
  issn = {1063-6676},
  keywords = {all-pole model,aperiodic components,approximation theory,deterministic
	component,excitation signal approximation,experiments,frequency regions,harmonic
	analysis,harmonic components,iterative algorithm,iterative methods,linear
	prediction residual signal,natural speech,noise,noise components,periodic
	components,poles and zeros,prediction theory,quasiperiodic sequences,random
	processes,random sequences,spectral domain,spectral-domain analysis,speech
	analysis,speech processing,speech signals decomposition,speech synthesis,stochastic
	component,synthetic speech,time-varying systems,time-varying vocal
	tract system,unvoiced part,voiced part},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{yeh_effect_1998,
  author = {L.T. Yeh},
  title = {Effect due to a series of heat sinks on component temperatures},
  booktitle = {Thermal and Thermomechanical Phenomena in Electronic Systems, 1998.
	ITHERM '98. The Sixth Intersociety Conference on},
  year = {1998},
  pages = {118â€•123},
  abstract = {It is common practice to employ a small heat sink for hot components
	in order to maintain the appropriate component temperature. Extensive
	studies of heat sink performance have been made either experimentally
	or numerically. However, most of those investigations are limited
	to the case with a single component alone on a printed circuit board
	(PCB). In practical applications, the condition with a single component
	on the board never exists. The temperature of a given component is
	always affected by surrounding components. In some cases, several
	hot components may be placed next to each other, and each individual
	component may also require a heat sink to achieve a desired temperature.
	The effects of nearby components, with or without heat sinks, on
	each other over a PCB has never been examined. The purpose of this
	study is use of 3D CFD code to investigate the thermal performance
	of a series of heat sinks on a group of components which are placed
	together on a printed circuit board},
  isbn = {1089-9870},
  keywords = {3D CFD code,adjacent component effects,circuit analysis computing,component
	temperatures,computational fluid dynamics,cooling,heat sink performance,heat
	sinks,hot components,individual component heat sinks,PCB,printed
	circuit board,printed circuit layout,single component analysis,thermal
	analysis,thermal management (packaging),thermal performance},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{yen_-line_2001,
  author = {I.-L. Yen and L. Khan and B. Prabhakaran and F.B. Bastani and J.
	Linn},
  title = {An on-line repository for embedded software},
  booktitle = {Tools with Artificial Intelligence, Proceedings of the 13th International
	Conference on},
  year = {2001},
  pages = {314â€•321},
  abstract = {The use of off-the-shelf components (COTS) can significantly reduce
	the time and cost of developing large-scale software systems. However,
	there are some difficult problems with the component-based approach.
	First, the developers have to be able to effectively retrieve components.
	This requires the developers to have an extensive knowledge of available
	components and how to retrieve them. After identifying the components,
	the developers also face a steep learning curve to master the use
	of these components. We are developing an On-line Repository for
	Embedded Software (ORES) to facilitate component management and retrieval.
	In this paper, we address the issues of designing software repository
	systems to assist users in obtaining appropriate components and learning
	to understand and use the components efficiently. We use an ontology
	to construct an abstract view of the organization of the components
	in ORES. The ontology structure facilitates repository browsing and
	effective search. We also develop a set of tools to assist with component
	comprehension, including a tutorial manager and a component explorer},
  doi = {10.1109/ICTAI.2001.974479},
  keywords = {component explorer,component management,component retrieval,COTS,information
	retrieval,off-the-shelf components,ORES,repository browsing,software
	repository systems,software reusability,software tools},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{yen_supporting_2004,
  author = {J. Yen and Xiaocong Fan and Shuang Sun and M. McNeese and D. Hall},
  title = {Supporting anti-terrorist analyst teams using agents with shared
	RPD process},
  booktitle = {Computational Intelligence for Homeland Security and Personal Safety,
	2004. CIHSPS 2004. Proceedings of the 2004 IEEE International Conference
	on},
  year = {2004},
  pages = {53--60},
  abstract = {Antiterrorist analysts often need to work in teams with the requirement
	to analyze voluminous amounts of dynamic information in order to
	assess potential terrorist threats. Analysts have a high cognitive
	demand complicated by factors that typically the information has
	restrict access and requires special expertise for interpretation.
	The goal of this research is to enhance team performance by modeling
	and implementing a cognitive agent architecture capable of proactively
	seeking, linking and sharing information using knowledge and experience
	distributed among team members. The agent architecture is empowered
	by a collaborative RPD model - a novel team-based naturalistic decision
	making process derived from Klein's recognition-primed decision framework.},
  doi = {10.1109/CIHSPS.2004.1360207},
  keywords = {agent teamwork,antiterrorist analyst teams,cognitive agent architecture,cognitive
	demand,collaborative RPD model,dynamic information analysis,information
	analysis,information dissemination,information linking,information
	sharing,Klein recognition-primed decision framework,member experience,member
	knowledge,potential terrorist threat assessment,proactively information
	seeking,security,shared RPD process,team-based naturalistic decision
	making process,terrorism},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{kil-sang_yoo_intelligent_2008,
  author = {Kil-Sang Yoo and Won-Hyung Lee},
  title = {An Intelligent Non Player Character Based on BDI Agent},
  booktitle = {Networked Computing and Advanced Information Management, 2008. NCM
	'08. Fourth International Conference on},
  year = {2008},
  volume = {2},
  pages = {214--219},
  abstract = {Digital game-based learning has proven to be a useful and cost effective
	alternative to the traditional classroom-based experience. However,
	current digital learning methods for young learners fail to engage
	audiences accustomed to interactive media. Moreover, most edutainment
	games do not offer players a situated learning experience, and those
	few that do, do not leverage the immensely popular online game market.
	This paper introduces belief desire intention (BDI) agent architecture
	for an online game non-player character that encourages and stimulates
	situational learning in an online role-playing game.},
  doi = {10.1109/MIS.2003.1200732},
  keywords = {BDI,belief desire intention agent architecture,computer aided instruction,computer
	games,digital game-based learning,digital learning methods,edutainment
	games,e-learning,intelligent nonplayer character,interactive media,interactive
	systems,learning systems,NPC,online game market,online game nonplayer
	character,online role playing game,situated learning experience,situational
	learning},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{yoon_effective_2001,
  author = {Hoijin Yoon and Byoungju Choi},
  title = {An effective testing technique for component composition in EJBs},
  booktitle = {Software Engineering Conference, 2001. APSEC 2001. Eighth Asia-Pacific},
  year = {2001},
  pages = {229â€•236},
  abstract = {This paper proposes a new testing technique for component composition
	of EJBs. We define components made by a current developer as white
	box components and components made by another developer as black
	box components. Software from CBSD consists of black box components
	and white box components, and composition errors result from the
	interaction between black box components and white box components,
	or the interaction between two white box components. Our technique
	tests these composition errors. We select test cases by injecting
	a fault into a specific parts of the white box component. The specific
	parts we define in this paper lead to the high effectiveness of our
	technique. We evaluate this effectiveness through an experiment and
	a theorem. In addition, we provide an example in Enterprise JavaBeans.},
  doi = {10.1109/ITHERM.1998.689528},
  isbn = {1530-1362 },
  keywords = {black box components,component composition testing,distributed object
	management,Enterprise JavaBeans,Java,program testing,software,software
	reusability,white box components},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{youngblood_seamlessly_2005,
  author = {M. Youngblood and D.J. Cook and L.B. Holder},
  title = {Seamlessly engineering a smart environment},
  booktitle = {Systems, Man and Cybernetics, 2005 IEEE International Conference
	on},
  year = {2005},
  volume = {1},
  pages = {548--553 Vol. 1},
  abstract = {Developing technologies and systems for automated control of home
	and workplace environments is a challenging problem. We present a
	complete agent architecture for learning to automate a smart environment
	and discuss integration of AT and middleware technologies necessary
	to achieve the goals of this project. Results are demonstrated using
	the MavPad and MavLab intelligent environments.},
  doi = {10.1109/CCGRID.2005.1558673},
  keywords = {automated control,complete agent architecture,home automation,home
	environment,intelligent environment,intelligent materials,learning
	system,learning systems,MavLab,MavPad,smart environment engineering,workplace
	environment},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{huiqun_yu_formal_2004,
  author = {Huiqun Yu and Xudong He and Yi Deng and Lian Mo},
  title = {A formal approach to designing secure software architectures},
  booktitle = {High Assurance Systems Engineering, 2004. Proceedings. Eighth IEEE
	International Symposium on},
  year = {2004},
  pages = {289--290},
  abstract = {Software architecture plays a central role in developing software
	systems that provide basic functionality and satisfy critical properties
	such as reliability and security. However, little has been done to
	formally model software architectures and to systematically enforce
	required properties. We aim to propose a formal approach to designing
	secure software architectures. We use the software architecture model
	(SAM), a general software architecture model combining Petri nets
	and temporal logic, as the underlying formalism. Architecture design
	consists of the functionality part and the security part. Guidelines
	are proposed to design functionality of software architectures at
	both element level and composition level. Software security is enforced
	by stepwise refinement.},
  isbn = {1530-2059 },
  keywords = {formal approach,safety critical software,SAM,secure software architectures,software
	architecture model},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{huiqun_yu_secure_2005,
  author = {Huiqun Yu and Dongmei Liu and Xudong He and Li Yang and Shu Gao},
  title = {Secure software architectures design by aspect orientation},
  booktitle = {Engineering of Complex Computer Systems, 2005. ICECCS 2005. Proceedings.
	10th IEEE International Conference on},
  year = {2005},
  pages = {47--55},
  abstract = {Security design at architecture level is critical to achieve high
	assurance software systems. However, most security design techniques
	for software architectures were in ad hoc fashion and fell short
	in precise notations. This paper proposes a formal aspect-oriented
	approach to designing secure software architectures. The underlying
	formalism is the software architecture model (SAM) that combines
	Petri nets and temporal logic. SAM supports a precise way to model
	the problem domain, its software architecture, and security aspects
	of the software architecture. An integrated architecture is obtained
	by weaving aspect models with the base architecture model. Mechanisms
	in SAM are amenable to analyzing correctness of the architecture
	design.},
  doi = {10.1109/ICECCS.2005.75},
  keywords = {architecture design correctness,aspect orientation,formal aspect-oriented
	design,high assurance software systems,integrated architecture,problem
	domain modeling,SAM,secure software architecture design,security},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{zaha_compatibility_2006,
  author = {J.M. Zaha and A. Albani},
  title = {Compatibility test for coordination aspects of software components},
  booktitle = {Software Engineering Conference, 2006. Australian},
  year = {2006},
  pages = {8 pp.},
  abstract = {Combining third party software components to customer-individual application
	systems requires first, standardized specification techniques for
	describing the technical as well as the business-related aspects
	of the services provided and required by the corresponding software
	components and second, automated compatibility tests in order to
	identify components fulfilling demands specified by component requestors.
	Adequate techniques for the specification of component services are
	consolidated in a multilayered specification framework, where formal
	notations are preferred in order to enable the execution of automated
	compatibility tests. These tests are a prerequisite for the existence
	of component markets where third party software components are traded
	and components that fulfil the specified demands are identified.
	This paper presents an algorithm for the layer of the specification
	framework where coordination aspects of a software component are
	described. On this layer an extension of the object constraint language
	(OCL) by temporal operators is used to specify the succession relationships
	between the services of related software components. Thereby the
	connections to other layers are tagged and existing tests are integrated.},
  doi = {10.1109/ASWEC.2006.22},
  isbn = {1530-0803 },
  keywords = {automated compatibility tests,business-related aspects,component service
	specification,customer-individual application systems,formal specification,multilayered
	specification framework,object constraint language,object-oriented
	languages,object-oriented programming,OCL,program testing,temporal
	operators,third party software components},
  owner = {user},
  timestamp = {2008.10.19}
}

@ARTICLE{Zaharakis1998,
  author = {I. D. Zaharakis and A. D. Kameas and G. C. Nikiforidis},
  title = {A multi-agent architecture for teaching dermatology.},
  journal = {Med Inform (Lond)},
  year = {1998},
  volume = {23},
  pages = {289--307},
  number = {4},
  abstract = {This work proposes the integration of computer-aided instruction systems
	in the curricula of medical education, and describes an intelligent
	tutoring system used for teaching Dermatology. The Dermatology Tutor
	uses a self-organized society of autonomous software agents which
	have different capabilities or roles. The society contains tutor,
	medical and information agents which participate in the tutoring
	process and collaborate through deliberation in order to achieve
	a tutoring task. The agents are built according to a BDI architecture,
	which implements the mental attitudes of beliefs (B), desires (D)
	and intentions (I). Each medical agent is a specialist in a medical
	field, while a tutoring agent, which implements a widely accepted
	dermatology teaching process, coordinates the overall operation of
	the system. Depending on the subject that is to be taught during
	any session, the tutoring agent forms teams of medical agents, which
	in turn use search agents to retrieve information. Although the presented
	multi-agent architecture is dedicated to teaching dermatology (since
	the tutor agent is specialized in Dermatology), it can be extended
	to other domains also with the incorporation of other tutor agents.},
  institution = {Department of Mathematics, University of Patras, Hellas.},
  keywords = {Computer Systems; Computer-Assisted Instruction; Curriculum; Dermatology,
	education; Education, Medical; Teaching},
  owner = {user},
  pmid = {9922950},
  timestamp = {2008.10.19}
}

@ARTICLE{zaremski_specification_1997,
  author = {Amy Moormann Zaremski and Jeannette M. Wing},
  title = {Specification matching of software components},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  year = {1997},
  volume = {6},
  pages = {333â€•369},
  number = {4},
  abstract = {Specification matching is a way to compare two software components,
	based on descriptions of the component's behaviors. In the context
	of software reuse and library retrieval, it can help determine whether
	one component can be substituted for another or how one can be modified
	to fit the requirements of the other. In the context of object-oriented
	programming, it can help determine when one type is a behavioral
	subtype of another. We use formal specifications to describe the
	behavior of software components and, hence, to determine whether
	two components match. We give precise definitions of not just exact
	match, but, more relevantly, various flavors of relaxed match. These
	definitions capture the notions of generalization, specialization,
	and substitutability of software components. Since our formal specifications
	are pre- and postconditions written as predicates in first-order
	logic, we rely on theorem proving to determine match and mismatch.
	We give examples from our implementation of specification matching
	using the Larch Prover.},
  doi = {10.1145/261640.261641},
  owner = {user},
  timestamp = {2008.10.04},
  url = {http://portal.acm.org/citation.cfm?id=261640.261641\&coll=ACM\&dl=ACM\&CFID=5052486\&CFTOKEN=93330998}
}

@INPROCEEDINGS{zayaraz_software_2005,
  author = {G. Zayaraz and P. Thambidurai},
  title = {Software Architecture Selection Framework Based on Quality Attributes},
  booktitle = {INDICON, 2005 Annual IEEE},
  year = {2005},
  pages = {167--170},
  abstract = {Software Architectures are generally designed with particular functional
	and nonfunctional requirements. Organizations often need to choose
	Software Architecture for future development from several competing
	candidate architectures. The various Stakeholders' quality requirements
	need to be considered collectively to describe the quality requirements
	of the envisioned system and therefore build the basis for the comparison
	and selection criteria. Choosing Software Architecture for any system
	still remains a difficult task as many different stake holders are
	involved in the selection process. Stakeholders view on quality requirements
	is different and at times they can also be conflicting in nature.
	Existing software architecture selection methods [1, 2, 3, 4] have
	been analyzed to identify their limitations. To overcome the limitations
	and challenges, a selection framework has been proposed and validated
	based on multiattribute decision making using Hypothetical Equivalents
	[7]. The proposed framework provides the rationale for an architecture
	selection process by comparing the fitness of competing candidate
	architectures for the envisioned system based on the quality requirements
	of different Stakeholders. },
  doi = {10.1109/AUTEST.2004.1436829},
  keywords = {Decision Making,Quality Attributes,Requirements,Software Architecture},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{zewdie_adaptive_2006,
  author = {B. Zewdie and C.R. Carlson},
  title = {Adaptive Component Paradigm for Highly Configurable Business Components},
  booktitle = {Electro/information Technology, 2006 IEEE International Conference
	on},
  year = {2006},
  pages = {185â€•190},
  abstract = {It is unlikely that a business component can always be reused as is,
	without configuration and customization. When change is needed, the
	current state of the art is source code modification (Ferguson et
	al, 2005). However, the ability to deliver wide reuse of components
	depends heavily on the capability to adapt components to the environment
	in which they are being used. To achieve this result, business components
	need to be designed and implemented having change in mind. By change,
	we mean change of the behavior of the component with out changing
	the code base of the component (customization with out programming).
	Thus, there is a need for adaptive component architecture that is
	generic, extensible, and dynamic in dealing with change of behavior
	so that the component can be reused in different environments. In
	order to satisfy these features, component architecture should have
	a mechanism for supporting and configuring itself to achieve customization
	without programming. This paper proposes reference architecture for
	software business component design and development based on adaptive
	component paradigm. Employing adaptive component paradigm for reference
	architecture helps in identifying component internal key abstractions
	and their relationships, which are repeatable and non-arbitrary,
	as constituent parts of business component. The proposed generic
	reference architecture for business components is designed using
	layered architecture pattern. The use of reference component architecture
	facilitates business component specification process by providing
	the initial sets of key abstractions and helps in reducing the cost
	of development as well as maintenance effort},
  doi = {10.1109/ICSMC.2001.973053},
  keywords = {Adaptiive System,adaptive component paradigm,Architecture,code base,Component,component
	architecture,component internal key abstractions,Component Specification,Design
	Principles,generic reference architecture,highly configurable business
	components,layered architecture pattern,object-oriented programming,software
	architecture,software business component,source code modification},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{bo_zhang_agent_2000,
  author = {Bo Zhang and Qingsheng Cai and Xiaoping Chen and Bin Yang and Yang
	Yang},
  title = {An agent team for RoboCup simulator league},
  booktitle = {Intelligent Control and Automation, 2000. Proceedings of the 3rd
	World Congress on},
  year = {2000},
  volume = {1},
  pages = {189--193 vol.1},
  abstract = {RoboCup is an attempt to promote AI and robotics research by providing
	a common task, soccer playing, for evaluation of various theories,
	algorithms and agent architectures. RoboCup consists of both the
	real robot league and the simulator league, where the soccer server
	is a standard software platform. A wide range of key issues on AI
	research emerges when designing simulator teams, such as the agent
	architecture, multi-agent teamwork, machine learning, etc. These
	issues are what we concerned most when developing our simulator team.
	Our team participated the first RoboCup tournament in China, and
	won the second place in the competition},
  doi = {10.1109/DISCEX.2000.821509},
  keywords = {agent architectures,AI,machine learning,mobile robots,multiple-agent
	system,RoboCup,robotic soccer,simulator league},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{bo_zhang_agent_2000-1,
  author = {Bo Zhang and Xiaoping Chen and Guiquan Liu and Qingsheng Cai},
  title = {Agent architecture: a survey on RoboCup-99 simulator teams},
  booktitle = {Intelligent Control and Automation, 2000. Proceedings of the 3rd
	World Congress on},
  year = {2000},
  volume = {1},
  pages = {194--198 vol.1},
  abstract = {RoboCup is an attempt to foster AI and robotics research by providing
	a common task for evaluation of various theories, algorithms, and
	agent architectures. In the RoboCup simulation league, software agents
	play soccer games on an official soccer server over the network.
	When constructing these software agents, issues in area of agent
	architecture arise to satisfy the properties specified by agent theorists.
	This paper presents an overview of the agent architectures used in
	the simulator teams in RoboCup-99. Many types of agent architectures
	are reported and compared. We also provide some open questions for
	discussion and give some possible answers to be verified in the near
	future},
  doi = {10.1109/IS.2006.348450},
  keywords = {agent architectures,AI,mobile robots,multiple agent systems,RoboCup-99,robotic
	soccer,simulator teams},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{qian_zhang_visual_2008,
  author = {Qian Zhang},
  title = {Visual Software Architecture Description Based on Design Space},
  booktitle = {Quality Software, 2008. QSIC '08. The Eighth International Conference
	on},
  year = {2008},
  pages = {366--375},
  abstract = {Boxology is the essence of software architecture description. In comparison
	with text-based languages,well-defined visual notations model software
	architecture at a high level of abstraction. They are easy to understand
	and easy to use due to its simplicity, but less expressive as many
	architectural properties can not be adequately represented. A key
	question to be answered in the design of a visual notation for the
	description of software architectures is what properties should be
	visually represented. This paper applies the theory of design space
	in the development of a visual notation called ExSAVN for software
	architectural modeling. It is based on the design space of software
	architectural elements to determine the properties of software architectures
	that are visually represented. It achieves balance between simplicity
	and expressiveness and supports incremental and iterative architectural
	design through a number of high level language facilities, which
	include the representation of undecided properties, hierarchical
	abstraction and type definition facilities. The paper also illustrated
	the style of ExSAVN by some examples of real systems.},
  doi = {10.1109/QSIC.2008.59},
  isbn = {1550-6002},
  keywords = {architecture description language,Architecture description language,boxology,design
	space,Design space,ExSAVN,Visual notation,visual software architecture},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{zhang_tool_2006,
  author = {Qian Zhang and Jian Wu and Hong Zhu},
  title = {Tool Support to Model-based Quality Analysis of Software Architecture},
  booktitle = {Computer Software and Applications Conference, 2006. COMPSAC '06.
	30th Annual International},
  year = {2006},
  volume = {1},
  pages = {121â€•128},
  abstract = {This paper presents an automated software tool SQUARE (software quality
	and architecture modelling environment). It is designed and implemented
	to support the analysis of software quality from software architectural
	designs. The tool is based on a model-based method and follows a
	structured process to systematically derive quality models from software
	architectural designs by adapting and applying the principles of
	system hazard analysis. Through identification of potential quality
	hazards and their consequences, the quality related properties of
	the components and connectors and the causal relationships between
	them are derived and then translated into a quality model represented
	in a graphical notation. The tool enables automated analysis of the
	quality models in the graphical notation to recognize a number of
	types of software quality features including quality sensitive components,
	quality risks and quality trade-off points, etc. A case study with
	a real e-commerce system is also reported},
  doi = {10.1109/AUTEST.2005.1609114},
  isbn = {0730-3157},
  keywords = {Analysis of software architecture,automated analysis,automated software
	tool SQUARE,Automated software tools,design,e-commerce system,graphical
	notation,model-based quality analysis,object-oriented programming,program
	diagnostics,quality risk,quality sensitive component,quality trade-off
	point,software architecture,Software architecture,software quality,software
	quality and architecture modelling environment,Software quality models,software
	tools,system hazard analysis},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{liu_zhi-guo_approach_2001,
  author = {Liu Zhi-guo and Wang Guang-xing},
  title = {An approach to distributed heterogeneous network management of mobile
	agent architecture based},
  booktitle = {Info-tech and Info-net, 2001. Proceedings. ICII 2001 - Beijing. 2001
	International Conferences on},
  year = {2001},
  volume = {2},
  pages = {228--233 vol.2},
  abstract = {To improve the network and its service reliability, and the availability
	and management in today's high-speed heterogeneous networks, a model
	of the mobile agent architecture based heterogeneous network management
	is proposed. Firstly, the model architecture which provides centralized
	information management for the heterogeneous network is described.
	Secondly, an information exchange interface is defined, which is
	used to exchange information between heterogeneous domains and inside
	a domain. The model, which integrates congestion control and QoS
	control, can provide support for the management of distributed network
	management with end-to-end QoS constraint, covering configuration
	management, fault management and performance management },
  doi = {10.1109/CMPSAC.2002.1044537},
  keywords = {centralized information management,congestion control,distributed
	heterogeneous network management,end-to-end QoS constraint,fault
	management,information exchange interface,mobile agent architecture,network
	interfaces,network reliability,performance management,QoS control,service
	reliability,telecommunication congestion control,telecommunication
	network management,telecommunication network reliability},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{zhong_unified-index_2007,
  author = {Ming Zhong and Yaoxue Zhang and Pengwei Tian and Cunhao Fang and
	Yuezhi Zhou},
  title = {A Unified-Index Based Distributed Specification for Heterogeneous
	Components Management},
  booktitle = {Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed
	Computing, 2007. SNPD 2007. Eighth ACIS International Conference
	on},
  year = {2007},
  volume = {3},
  pages = {599â€•604},
  abstract = {With the development of service-oriented architecture (SOA) and component-based
	software engineering (CBSE), the number of various heterogeneous
	service components increases rapidly. They are stored in traditional
	repositories of different organization and can be independently accessed
	by users. However, it brings difficulty in component discovery and
	reuse. The utility of massive component resource can not be remarkably
	improved. In this paper, a unified-index (UI) based distributed specification
	for heterogeneous components management is introduced. It focuses
	on creating and maintaining a Unified-Index tree on the UI repository
	server, which has a management field grouped by some traditional
	repositories. Each UI repository server maps the components of its
	children traditional repositories into UI format and stores them
	in local UI repository. It also actively monitors the adding of new
	components and removing of invalid ones. Every query of component
	discovery will be diffused to all of the UI repository servers with
	transmitting of the root server. So in this desired architecture,
	users can consistently search and access all the heterogeneous components
	for further assembly and application, satisfying various personalized
	and comprehensive user requirements.},
  doi = {10.1109/SNPD.2007.109},
  keywords = {component,component discovery,component reuse,component-based software
	engineering,consistency,distributed processing,formal specification,heterogeneous
	components management,heterogeneous service components,massive component
	resource,object-oriented programming,repository,service-oriented
	architecture,software architecture,software reusability,UI repository
	server,unified-index based distributed specification,Unified-Index
	tree,user requirements},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{zhongjie_component_2005,
  author = {Wang Zhongjie and Xu Xiaofei and Zhan Dechen},
  title = {A component optimization design method based on component instance
	set decomposition: decreasing component reuse cost},
  booktitle = {Computer and Information Technology, 2005. CIT 2005. The Fifth International
	Conference on},
  year = {2005},
  pages = {836â€•840},
  abstract = {Traditional component design methods pay much attention to component's
	usefulness, while usually ignore the optimization on component's
	usability, such as reuse cost. The aim of this paper is to present
	a method of component reuse cost optimization. For that, a feature-based
	component model is firstly introduced to express the reusable semantics
	of component, with emphasis on reuse mechanism based on variation
	point. By analysis of constituents of component reuse cost, the optimization
	goal, i.e., increasing the proportion of fixed part of a component,
	is put forward. Then a component optimization method based on component
	instance set decomposition (CIS-decomposition) is presented, i.e.,
	according to the reuse frequency of every component instance, separate
	those instances with higher reuse frequency out to get semi-instantiation
	components., hence avoids multiple instantiation and feature item
	implementation in the every reuse process, thereby reduces the reuse
	cost.},
  doi = {10.1109/CIT.2005.4},
  keywords = {component instance set decomposition,component reuse cost,feature-based
	component model,object-oriented programming,optimisation,optimization
	design method,software reusability},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{chuan-sheng_zhou_research_2008-1,
  author = {Chuan-Sheng Zhou},
  title = {Research and Design of Autonomic Software Agent Architecture},
  booktitle = {Intelligent Information Hiding and Multimedia Signal Processing,
	2008. IIHMSP '08 International Conference on},
  year = {2008},
  pages = {958--961},
  abstract = {Today alone with the software agent technology has being applied into
	many information systems, like ERP system, CRM system, SCM system
	and OA system, etc. How to design an autonomic software agent to
	make sure it can apperceive its (outside) environment to decide and
	control its own behavior without outside control is still a big problem.
	Here by research on agent and its collaborative working modes, to
	introduce a software bus based design of autonomic software agent
	architecture and with an example of certain CRM workflow to describe
	its real application.},
  keywords = {Agent,autonomic software agent architecture,CRM,CRM workflow,information
	systems,Software Bus,Workflow,workflow management software},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{jiehan_zhou_ontoarch_2008,
  author = {Jiehan Zhou and E. Niemela and A. Evesti and A. Immonen and P. Savolainen},
  title = {OntoArch Approach for Reliability-Aware Software Architecture Development},
  booktitle = {Computer Software and Applications, 2008. COMPSAC '08. 32nd Annual
	IEEE International},
  year = {2008},
  pages = {1228--1233},
  abstract = {Reliability-aware software architecture development has recently been
	gaining growing attention among software architects. This paper tackles
	the issue by introducing an ontology-based approach, called OntoArch,
	which is characterized by: 1) integration of software reliability
	engineering and software architecture design; 2) targeting reliability-aware
	software architecture development; and 3) OntoArch ontology in the
	context of software architecture design and software reliability
	engineering. The OntoArch approach is validated by applying the OntoArch
	approach to the development of a PIR (Personal Information Repository)
	system architecture.},
  doi = {10.1109/COMPSAC.2008.91},
  isbn = {0730-3157},
  keywords = {OntoArch ontology,personal information repository system architecture,reliability-aware
	software architecture design,reliability-aware software architecture
	development,software architects,software architecture design,software
	reliability engineering},
  owner = {user},
  timestamp = {2008.10.04}
}

@INPROCEEDINGS{yongli_zhu_transformer_2005,
  author = {Yongli Zhu and Lizeng Wu and Xueyu Li and Jinsha Yuan},
  title = {A transformer condition assessment framework based on data mining},
  booktitle = {Power Engineering Society General Meeting, 2005. IEEE},
  year = {2005},
  pages = {1875--1880 Vol. 2},
  abstract = {The framework of an assessment system on transformers' condition is
	proposed in this paper through mainly using data mining techniques.
	Moreover, a warehouse is used to collect transformers' testing data,
	and a multi-agent system is used to design the framework of the software.
	The present framework is open and flexible, so the objective system
	is easy to be developed and maintained. The system can support transformers'
	condition-based maintenance to reduce electric utility's cost. The
	condition of a transformer depends on its design, present and historical
	data relating to its installation environment, load amounts, being
	switched number and so on. Usually the off-line testing results,
	operational data, fault records and weather conditions have been
	stored in different systems, so finding an effective method to utilize
	all this information for condition assessment is difficult. Therefore,
	a data warehouse has been used to integrate all of the above data,
	and some data mining techniques have been used to find the pattern
	and trend of the condition of a transformer. Then whether it is healthy
	can be determined. In order to make the system open and flexible,
	open agent architecture (OAA) is employed to compose the multi-agent
	system. Seven application agents are designed to evaluate transformers'
	conditions synthetically. The Grey correlation method, grey theory
	prediction model GM(1,1), Bayesian network classifier and Bayesian
	network are employed in the agents.},
  doi = {10.1109/PES.2005.1489207},
  keywords = {Bayesian network,Bayesian network classifier,belief networks,condition-based
	maintenance,data warehouse,data warehouses,electric utility cost
	reduction,Grey correlation method,grey systems,grey theory prediction,maintenance
	engineering,multiagent system,open agent architecture,power engineering
	computing,power transformer testing,transformer condition assessment},
  owner = {user},
  timestamp = {2008.10.19}
}

@INPROCEEDINGS{zuniga_geda-3d_2005,
  author = {F. Zuniga and F.F. Ramos and I. Piza},
  title = {GeDA-3D agent architecture},
  booktitle = {Parallel and Distributed Systems, 2005. Proceedings. 11th International
	Conference on},
  year = {2005},
  volume = {2},
  pages = {201--205 Vol. 2},
  abstract = {This paper presents an agent-architecture useful to generate suitable
	behaviors to virtual creatures participating in virtual environments
	created by a declarative description. This architecture is used in
	a platform to design and run dynamic virtual environments that allows
	users to generate virtual scenes involving two phases: 1) description
	of the attributes and intentions of the characters participants throughout
	a declarative description, and 2) graphical simulation of a dynamic
	scene during which the characters interact with each other in order
	to achieve individual and/or collective goals.},
  doi = {10.1109/IROS.2005.1545101},
  isbn = {1521-9097},
  keywords = {GeDA-3D agent architecture,mobile agents,virtual creatures,virtual
	environment,virtual reality},
  owner = {user},
  timestamp = {2008.10.19}
}

@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_keywords:}

