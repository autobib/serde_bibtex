%%% ----------------------------------------------------------------------
%%% BibTeX-file {
%%%    author	 = "{Electronic Visualization Library Service}",
%%%    filename  = "EVL-1999.bib",
%%%    address   = "Konrad-Zuse-Zentrum f{\"u}r
%%%                 Informationstechnik Berlin (ZIB)
%%%                 Scientific Visualization Department
%%%                 Takustr. 7
%%%                 14195 Berlin
%%%                 Germany",
%%%    URL       = "http://visinfo.zib.de/EVlib/",
%%%    email     = "davis@zib.de",
%%%    supported = "yes",
%%%    docstring = "This file contains the complete bibliography of
%%%                 references submitted to the 
%%%                 Electronic Visualization Library for the year 1999.",
%%% }


@Book{EVL-1999-1,
  year =         "1999",
  title =        "Graph Drawing: Algorithms for the Visualization of
                 Graphs",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-1",
  author =       "Ioannis G. Tollis and Giuseppe Di Battista and Peter
                 Eades and Roberto Tamassia",
  abstract =     "This book is designed to describe fundamental
                 algorithmic techniques for constructing drawings of
                 graphs. Suitable as a book or reference manual, its
                 chapters offer an accurate, accessible reflection of
                 the rapidly expanding field of graph drawing.",
  language =     "en",
  copyright =    "Prentice Hall",
  publisher =    "Prentice Hall",
}

@Article{EVL-1999-10,
  pages =        "21--35",
  year =         "1999",
  title =        "Robust retrieval of three-dimensional structures from
                 image stacks",
  author =       "M. Garza-Jinich and P. Meer and V. Medina",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-10",
  language =     "en",
  abstract =     "Robust high-breakdown-point location estimators are
                 employed to analyze image stacks under the piecewise
                 constant image structure model. To reduce the effect of
                 bias along the Z-axis, the class parameters are
                 extracted using three consecutive slices. The
                 segmentation algorithm first determines the most
                 reliable seed regions, which are then used in a
                 region-growing procedure supported by local evidence.
                 The robustness and stability of the proposed technique
                 is shown with both synthetic and real data, the latter
                 consisting of one MRI and one confocal microscopy set.
                 The performance of the algorithm is consistent with the
                 ground truth obtained with manual segmentation by
                 physicians.",
  keywords =     "confocal microscope images, image segmentation,
                 magnetic resonance images robust statistics",
  volume =       "3",
  number =       "1",
  journal =      "Medical Image Analysis",
}

@TechReport{EVL-1999-100,
  year =         "1999",
  title =        "Thermalization of Large Fluid Systems consisting of
                 Lennard-Jones Mixtures",
  author =       "M. Hloucha",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-100",
  abstract =     "This document describes a method for the creation of
                 thermally equilibrated molecular configurations for
                 large scale molecular dynamics simulations and its
                 parallel implementation. The method combines relaxation
                 of a stochastic initial configuration by Monte Carlo
                 moves followed by molecular dynamics. Benchmarks are
                 presented for up to 800000 particles.",
  month =        feb,
  number =       "FZJ-ZAM-IB-9902",
  institution =  "Forschungszentrum J{\"{u}}lich, Central Institue for
                 Applied Mathematics (ZAM)",
}

@Article{EVL-1999-101,
  pages =        "11--29",
  year =         "1999",
  title =        "Compression Methods for Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-101",
  author =       "M. H. Gross and L. Lippert and O. Staadt",
  abstract =     "Compression methods have become of fundamental
                 importance in almost every subfield of scientific
                 visualization. However, unlike image compression,
                 advanced visualization applications impose manifold
                 constraints on the design of appropriate algorithms,
                 where progressiveness, multiresolution or topology
                 preservation are some of the key issues. This paper
                 demonstrates the importance of multiresolution
                 compression methods for visualization using two
                 examples: The first, compression domain volume
                 rendering, enables one to visualize volume data
                 progressively and instantaneously from its compressed
                 data format and is designed for WWW and networked
                 applications. The second one is a multiresolution
                 compression and reconstruction method that allows for
                 progressive coding, transmission and geometric
                 reconstruction of surfaces and volumes. Both of the
                 presented methods are so-called transform coding
                 schemes and use wavelets for data representation.",
  month =        mar,
  volume =       "15",
  keywords =     "visualization, compression, wavelets, volume
                 rendering",
  number =       "1",
  journal =      "Future Generation Computer Systems",
}

@TechReport{EVL-1999-102,
  year =         "1999",
  title =        "Construction of Multiresolution Triangular {B}-Spline
                 Surfaces using Hexagonal Filters",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-102",
  author =       "A. Dreger and M. H. Gross and J. Schlegel",
  abstract =     "We present multiresolution B-spline surfaces of
                 arbitrary order defined over triangular domains. Unlike
                 existing methods, the basic idea of our approach is to
                 construct the triangular basis functions from their
                 tensor product relatives in the spirit of box splines
                 by projecting them onto the barycentric plane. The
                 scheme works for splines of any order where the
                 fundamental building blocks of the surface are
                 hierarchies of triangular B-spline scaling functions
                 and wavelets spanning the complement spaces between
                 levels of different resolution. Although our bases
                 functions have been deduced from the corresponding
                 3D-bases, our decomposition and reconstruction scheme
                 operates directly on the triangular mesh using
                 hexagonal filters. The resulting basis functions are
                 used to approximate triangular surfaces and provide
                 many useful properties, such as multiresolution
                 editing, local level of detail, continuity control,
                 surface compression and much more. The performance of
                 our approach is illustrated by various examples
                 including parametric and nonparametric surface editing
                 and compression.",
  month =        jul,
  keywords =     "Triangular B-spline wavelets, box splines,
                 multiresolution editing, hierarchical surface
                 representation, surface compression, decomposition,
                 reconstruction",
  number =       "327",
  institution =  "Computer Science Department, Institute of Scientific
                 Computing, ETH Z{\"{u}}rich",
}

@TechReport{EVL-1999-103,
  year =         "1999",
  title =        "A Framework for Facial Surgery Simulation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-103",
  author =       "M. Koch and S. H. M. Roth and M. H. Gross and A. P.
                 Zimmermann and H. F. Sailer",
  abstract =     "The accurate prediction of the post-surgical facial
                 shape is of paramount importance for surgical planning
                 in facial surgery. In this paper we present a framework
                 for facial surgery simulation which is based on
                 volumetric finite element modeling. We contrast
                 conven-tional procedures for surgical planning against
                 our system by accompanying a patient during the entire
                 process of planning, medical treatment and simulation.
                 In various preprocessing steps a 3D physically based
                 facial model is reconstructed from CT and laser range
                 scans. All geometric and topological changes are
                 modeled interactively using Alias.ª Applying fully 3D
                 volumetric elasticity allows us to represent important
                 volumetric effects such as incompressibility in a
                 natural and physically accurate way. For computational
                 effi-ciency, we devised a novel set of prismatic shape
                 functions featuring a globally C 1 -continuous surface
                 in combination with a C 0 interior. Not only is it
                 numerically accurate, but this construction enables us
                 to compute smooth and visually appealing facial shapes.
                 An extended evaluation and quantitative analysis of a
                 clinical test series with several female and male
                 patients clearly demonstrates the per-formance of our
                 framework.",
  month =        jun,
  keywords =     "Finite Element Method, Facial Surgery Simulation,
                 Facial Modeling, Data Reconstruction",
  number =       "326",
  institution =  "Institute of Scientific Computing, Computer Science
                 Department, ETH Z{\"{u}}rich",
}

@InProceedings{EVL-1999-104,
  pages =        "291--300",
  year =         "1999",
  title =        "{V}isualization {T}echniques for {T}ime-{O}riented,
                 {S}keletal {P}lans in {M}edical {T}herapy {P}lanning",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-104",
  author =       "Robert Kosara and Silvia Miksch",
  abstract =     "In order to utilize elaborate tools and techniques
                 (like verification) for use with clinical protocols,
                 these must be represented in an appropriate way.
                 Protocols are typically represented by means of formal
                 languages (e.g., Asbru), which are very hard to
                 understand for medical experts and lead to many
                 problems in practical use. Therefore, a powerful user
                 interface is needed. We identify the key problems the
                 user-interface designer is faced with, and present a
                 number of ``classic'' solutions and their shortcomings
                 --- which led to our own solution called AsbruView. Its
                 two different views (Topological View and Temporal
                 View) are presented.",
  month =        jun,
  address =      "Aalborg, Denmark",
  editor =       "Werner Horn and Yuval Shahar and Greger Lindberg and
                 Steen Andreassen and Jeremy Wyatt",
  booktitle =    "Proceedings of the Joint European Conference on
                 Artificial Intelligence in Medicine and Medical
                 Decision Making (AIMDM'99)",
  publisher =    "Springer Verlag",
}

@MastersThesis{EVL-1999-105,
  year =         "1999",
  title =        "{M}etaphors of {M}ovement --- {A} {U}ser {I}nterface
                 for {M}anipulating {T}ime-{O}riented, {S}keletal
                 {P}lans",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-105",
  author =       "Robert Kosara",
  abstract =     "This thesis introduces a user interface that supports
                 the understanding and manipulation of time-oriented,
                 skeletal plans. This user interface is called
                 AsbruView, and is based on the plan representation
                 language Asbru, which is used for medical therapy
                 planning. Clinical protocols are seen in Asbru as
                 time-oriented, skeletal plans. AsbruView utilizes
                 Metaphors of running tracks and traffic control to
                 communicate important concepts and uses glyphs to
                 depict the complex time annotations used in Asbru. Two
                 different views show different aspects of the same set
                 of plans: One shows the topology of plans and which
                 parts have been defined, the other captures the
                 temporal dimension and the structure of plans. We
                 present a number of existing visualization approaches
                 to different problems that we faced and discuss their
                 usefulness for our purpose. We also show why we did not
                 use a knowledge acquisition tool for editing Asbru
                 plans. We have evaluated AsbruView with six domain
                 experts (physicians), who judged it as usable and easy
                 to understand. The findings of that evaluation are
                 presented and discussed.",
  month =        may,
  school =       "Vienna University of Technology, Vienna, Austria",
}

@Article{EVL-1999-106,
  year =         "1999",
  title =        "Locally Toleranced Surface Simplification",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-106",
  author =       "Andr{\'{e}} Gu{\'{e}}ziec",
  abstract =     "We present a technique for simplifying a triangulated
                 surface. Simplifying consists of approximating the
                 surface with another surface of lower triangle count.
                 Our algorithm can preserve the volume of a solid to
                 within machine accuracy; it favors the creation of
                 near-equilateral triangles. We develop novel methods
                 for reporting and representing a bound to the
                 approximation error between a simplified surface and
                 the original, and respecting a variable tolerance
                 across the surface. A different positive error value is
                 reported at each vertex. By linearly blending the error
                 values in between vertices, we define a volume of
                 space, called the error volume, as the union of balls
                 of linearly varying radii. The error volume is built
                 dynamically as the simplification progresses, on top of
                 preexisting error volumes that it contains. We also
                 build a tolerance volume to forbid simplification
                 errors exceeding a local tolerance. The information
                 necessary to compute error values is local to the star
                 of a vertex; accordingly, the complexity of the
                 algorithm is either linear or in O(n log n) in the
                 original number of surface edges, depending on the
                 variant. We extend the mechanisms of error and
                 tolerance volumes to preserve during simplification
                 scalar and vector attributes associated with surface
                 vertices. Assuming a linear variation across triangles,
                 error and tolerance volumes are defined in the same
                 fashion as for positional error. For normals, a
                 corrective term is applied to the error measured at the
                 vertices to compensate for nonlinearities.",
  month =        apr,
  volume =       "5",
  keywords =     "Surface simplification, error volume, tolerance
                 volume",
  number =       "2",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1999-107,
  year =         "1999",
  title =        "Feature Extraction of Separation and Attachment
                 Lines",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-107",
  author =       "David N. Kenwright and Chris Henze and Creon Levit",
  abstract =     "Separation and attachment lines are topologically
                 significant curves that exist on 2D surfaces in 3D
                 vector fields. Two algorithms are presented, one
                 point-based and one element-based, that extract
                 separation and attachment lines using eigenvalue
                 analysis of a locally linear function. Unlike prior
                 techniques based on piecewise numerical integration,
                 these algorithms use robust analytical tests that can
                 be applied independently to any point in a vector
                 field. The feature extraction is fully automatic and
                 suited to the analysis of large-scale numerical
                 simulations. The strengths and weaknesses of the two
                 algorithms are evaluated using analytic vector fields
                 and also results from computational fluid dynamics
                 (CFD) simulations. We show that both algorithms detect
                 open separation lines--a type of separation that is not
                 captured by conventional vector field topology
                 algorithms.",
  month =        apr,
  volume =       "5",
  keywords =     "vector field visualization, vector field topology,
                 flow visualizaiton, feature detection, flow separation,
                 separation line",
  number =       "2",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1999-108,
  year =         "1999",
  title =        "Evaluation of Memoryless Simplification",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-108",
  author =       "Peter Lindstrom and Greg Turk",
  abstract =     "This paper investigates the effectiveness of the
                 Memoryless Simplification approach described by
                 Lindstrom and Turk. Like many polygon simplification
                 methods, this approach reduces the number of triangles
                 in a model by performing a sequence of edge collapses.
                 It differs from most recent methods, however, in that
                 it does not retain a history of the geometry of the
                 original model during simplification. We present
                 numerical comparisons showing that the memoryless
                 method results in smaller mean distance measures than
                 many published techniques that retain geometric
                 history. We compare a number of different vertex
                 placement schemes for an edge collapse in order to
                 identify the aspects of the Memoryless Simplification
                 that are responsible for its high level of fidelity. We
                 also evaluate simplification of models with boundaries,
                 and we show how the memoryless method may be tuned to
                 trade between manifold and boundary fidelity. We found
                 that the memoryless approach yields consistently low
                 mean errors when measured by the Metro mesh comparison
                 tool. In addition to using complex models for the
                 evaluations, we also perform comparisons using a sphere
                 and portions of a sphere. These simple surfaces turn
                 out to match the simplification behaviors for the more
                 complex models that we used.",
  month =        apr,
  volume =       "5",
  keywords =     "Model simplification, surface approximation, level of
                 detail, geometric error, optimization",
  number =       "2",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1999-109,
  year =         "1999",
  title =        "High-Quality Splatting on Rectilinear Grids with
                 Efficient Culling of Occluded Voxels",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-109",
  author =       "Klaus Mueller and Naeem Shareef and Jian Huang and
                 Roger Crawfis",
  abstract =     "Splatting is a popular volume rendering algorithm that
                 pairs good image quality with an efficient volume
                 projection scheme. The current axis-aligned
                 sheet-buffer approach, however, bears certain
                 inaccuracies. The effect of these is less noticeable in
                 still images, but clearly revealed in animated viewing,
                 where disturbing popping of object brightness occurs at
                 certain view angle transitions. In previous work, we
                 presented a new variant of sheet-buffered splatting in
                 which the compositing sheets are oriented parallel to
                 the image plane. This scheme not only eliminates the
                 condition for popping, but also produces images of
                 higher quality. In this paper, we summarize this new
                 paradigm and extend it in a number of ways. We devise a
                 new solution to render rectilinear grids of equivalent
                 cost to the traditional approach that treats the
                 anisotropic volume as being warped into a cubic grid.
                 This enables us to use the usual radially symmetric
                 kernels, which can be projected without inaccuracies.
                 Next, current splatting approaches necessitate the
                 projection of all voxels in the iso-interval(s),
                 although only a subset of these voxels may eventually
                 be visible in the final image. To eliminate these
                 wasteful computations we propose a novel front-to-back
                 approach that employs an occlusion map to determine if
                 a splat contributes to the image before it is
                 projected, thus skipping occluded splats. Additional
                 measures are presented for further speedups. In
                 addition, we present an efficient list-based volume
                 traversal scheme that facilitates the quick
                 modification of transfer functions and iso-values.",
  month =        apr,
  volume =       "5",
  keywords =     "splatting, volume rendering, visualization,
                 rectilinear grids",
  number =       "2",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1999-11,
  pages =        "39--63",
  year =         "1999",
  title =        "Model-based detection of spiculated lesions in
                 mammograms",
  author =       "R. Zwiggelaar and T. C. Parr and I. W. Hutt and C. J.
                 Taylo and S. M. Astley and C. R. M. Boggis",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-11",
  language =     "en",
  abstract =     "Computer-aided mammographic prompting systems require
                 the reliable detection of a variety of signs of cancer.
                 In this paper we concentrate on the detection of
                 spiculated lesions in mammograms. A spiculated lesion
                 is typically characterized by an abnormal pattern of
                 linear structures and a central mass. Statistical
                 models have been developed to describe and detect both
                 these aspects of spiculated lesions. We describe a
                 generic method of representing patterns of linear
                 structures, which relies on the use of factor analysis
                 to separate the systematic and random aspects of a
                 class of patterns. We model the appearance of central
                 masses using local scale-orientation signatures based
                 on recursive median filtering, approximated using
                 principal-component analysis. For lesions of 16 mm and
                 larger the pattern detection technique results in a
                 sensitivity of 80% at 0.014 false positives per image,
                 whilst the mass detection approach results in a
                 sensitivity 80% at 0.23 false positives per image.
                 Simple combination techniques result in an improved
                 sensitivity and specificity close to that required to
                 improve the performance of a radiologist in a prompting
                 environment.",
  keywords =     "central mass detection, mammogram, oriented line
                 patterns, spiculated lesions",
  volume =       "3",
  number =       "1",
  journal =      "Medical Image Analysis",
}

@Article{EVL-1999-110,
  year =         "1999",
  title =        "Large Datasets at a Glance: Combining Textures and
                 Colors in Scientific Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-110",
  author =       "Christopher G. Healey and James T. Enns",
  abstract =     "This paper presents a new method for using texture and
                 color to visualize multivariate data elements arranged
                 on an underlying height field. We combine simple
                 texture patterns with perceptually uniform colors to
                 increase the number of attribute values we can display
                 simultaneously. Our technique builds multicolored
                 perceptual texture elements (or pexels) to represent
                 each data element. Attribute values encoded in an
                 element are used to vary the appearance of its pexel.
                 Texture and color patterns that form when the pexels
                 are displayed can be used to rapidly and accurately
                 explore the dataset. Our pexels are built by varying
                 three separate texture dimensions: height, density, and
                 regularity. Results from computer graphics, computer
                 vision, and human visual psychophysics have identified
                 these dimensions as important for the formation of
                 perceptual texture patterns. The pexels are colored
                 using a selection technique that controls color
                 distance, linear separation, and color category. Proper
                 use of these criteria guarantees colors that are
                 equally distinguishable from one another. We describe a
                 set of controlled experiments that demonstrate the
                 effectiveness of our texture dimensions and color
                 selection criteria. We then discuss new work that
                 studies how texture and color can be used
                 simultaneously in a single display. Our results show
                 that variations of height and density have no effect on
                 color segmentation, but that random color patterns can
                 interfere with texture segmentation. As the difficulty
                 of the visual detection task increases, so too does the
                 amount of color on texture interference increase. We
                 conclude by demonstrating the applicability of our
                 approach to a real-world problem, the tracking of
                 typhoon conditions in Southeast Asia.",
  month =        apr,
  volume =       "5",
  keywords =     "color, color category, experimental design, human
                 vision, linear separation, multivariate dataset,
                 perception, pexel, preattentive processing,
                 psychophysics, scientific visualization, texture,
                 typhoon",
  number =       "2",
  journal =      "IEEE Transactions on Visualization and Computer
                 Graphics",
}

@Article{EVL-1999-111,
  pages =        "221--246",
  year =         "1999",
  title =        "Interval Reductions and Extensions of Orders:
                 Bijections to Chains in Lattices",
  author =       "Stefan Felsner and Jens Gustedt and Michel Morvan",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-111",
  volume =       "15",
  journal =      "Order",
}

@TechReport{EVL-1999-113,
  year =         "1999",
  title =        "Distributed Lighting Networks",
  author =       "P. Kipfer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-113",
  abstract =     "A case study of application-oriented distribution
                 patterns on the Vision system.",
  number =       "3",
  institution =  "Universit{"}at Erlangen-N{"}urnberg, Lehrstuhl
                 f{\"{u}}r Graphische Datenverarbeitung",
}

@TechReport{EVL-1999-114,
  year =         "1999",
  title =        "Interactive Direct Volume Rendering of Dural
                 Arteriovenous Fistulae",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-114",
  author =       "C. Rezk--Salama and P. Hastreiter and K. Eberhardt and
                 B. Tomandl and T. Ertl",
  note =         "submitted to MICCAI'99",
  number =       "4",
  institution =  "Universit{"}at Erlangen-N{"}urnberg, Lehrstuhl
                 f{\"{u}}r Graphische Datenverarbeitung",
}

@TechReport{EVL-1999-115,
  year =         "1999",
  title =        "Virtual Labyrinthoscopy: Visualization of the Inner
                 Ear with Interactive Direct Volume Rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-115",
  author =       "B. Tomandl and P. Hastreiter and K. Eberhardt and H.
                 Greess and U. Nissen",
  number =       "5",
  institution =  "Universit{"}at Erlangen-N{"}urnberg, Lehrstuhl
                 f{\"{u}}r Graphische Datenverarbeitung",
}

@TechReport{EVL-1999-116,
  year =         "1999",
  title =        "Virtual Endoscopic {CT} Angiography ({VECTA}): Value
                 of Perspective Volume Rendering for the Visualization
                 and Therapy Planning of Intracranial Aneurysms",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-116",
  author =       "B. Tomandl and K. Eberhardt and P. Hastreiter and M.
                 Buchfelder and T. Ertl",
  number =       "6",
  institution =  "Universit{"}at Erlangen-N{"}urnberg, Lehrstuhl
                 f{\"{u}}r Graphische Datenverarbeitung",
}

@TechReport{EVL-1999-117,
  year =         "1999",
  title =        "Registration and Visualization of Multimodal Image
                 Data",
  author =       "P. Hastreiter and T. Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-117",
  number =       "10",
  institution =  "Universit{"}at Erlangen-N{"}urnberg, Lehrstuhl
                 f{\"{u}}r Graphische Datenverarbeitung",
}

@InProceedings{EVL-1999-118,
  year =         "1999",
  title =        "Remote 3{D} Visualization using Image-Streaming
                 Techniques",
  author =       "Klaus Engel and Ove Sommer and Christian Ernst and
                 Thomas Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-118",
  booktitle =    "Proceedings of the International Symposium on
                 Intelligent Multimedia and Distance Education",
}

@InProceedings{EVL-1999-119,
  pages =        "192--196",
  year =         "1999",
  title =        "Interactive Direct Volume Rendering of the Inner Ear
                 for the Planning of Neurosurgery",
  author =       "P. Hastreiter and C. Rezk--Salama and B. Tomandl and
                 K. Eberhard and T. Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-119",
  booktitle =    "Proc. Worksh. Bildverarbeitung f{"}ur die Medizin
                 (BVM)",
  publisher =    "Springer",
}

@Article{EVL-1999-12,
  pages =        "77--101",
  year =         "1999",
  title =        "Knowledge-based tensor anisotropic diffusion of
                 cardiac magnetic resonance images",
  author =       "G. I. Sanchez-Ortiz and D. Rueckert and P. Burger",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-12",
  language =     "en",
  abstract =     "We present a general formulation for a new
                 knowledge-based approach to anisotropic diffusion of
                 multi-valued and multi-dimensional images, with an
                 illustrative application for the enhancement and
                 segmentation of cardiac magnetic resonance (MR) images.
                 In the proposed method all available information is
                 incorporated through a new definition of the
                 conductance function which differs form previous
                 approaches in two aspects. First, we model the
                 conductance as an explicit function of time and
                 position, and not only of the differential structure of
                 the image data. Inherent properties of the system (such
                 as geometrical features or non-homogeneous data
                 sampling) can therefore be taken into account by
                 allowing the conductance function to vary depending on
                 the location in the spatial and temporal coordinate
                 space. Secondly, by defining the conductance as a
                 second-rank tensor, the non-homogeneous diffusion
                 equation gains a truly anisotropic character which is
                 essential to emulate and handle certain aspects of
                 complex data systems. The method presented is suitable
                 for image enhancement and segmentation of single- or
                 multi-valued images. We demonstrate the efficiency of
                 the proposed framework by applying it to anatomical and
                 velocity-encoded cine volumetric (4-D) MR images of the
                 left ventricle. Spatial and temporal a priori knowledge
                 about the shape and dynamics of the heart is
                 incorporated into the diffusion process. We compare our
                 results to those obtained with other diffusion schemes
                 and exhibit the improvement in regions of the image
                 with low contrast and low signal-to-noise ratio.",
  keywords =     "anisotropic diffusion, geometry-driven diffusion,
                 left-ventricle modelling, phase-sensitive MR,
                 segmentation",
  volume =       "3",
  number =       "1",
  journal =      "Medical Image Analysis",
}

@InProceedings{EVL-1999-120,
  year =         "1999",
  title =        "A Warping-based Refinement of Lumigraphs",
  author =       "Wolfgang Heidrich and Hartmut Schirmacher and Hendrik
                 K{\"u}ck and Hans-Peter Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-120",
  editor =       "N. Thalmann and V. Skala",
  booktitle =    "Proc. WSCG '99",
}

@InProceedings{EVL-1999-121,
  year =         "1999",
  title =        "Applications of Pixel Textures in Visualization and
                 Realistic Image Synthesis",
  author =       "W. Heidrich and R. Westermann and H.-P. Seidel and T.
                 Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-121",
  organization = "ACM/Siggraph",
  booktitle =    "ACM Symposium on Interactive 3D Graphics",
}

@InProceedings{EVL-1999-122,
  year =         "1999",
  title =        "Non-linear Registration of Pre- and Intraoperative
                 Volume Data Based On Piecewise Linear Transformations",
  author =       "C. Rezk-Salama and P. Hastreit and G. Greiner and T.
                 Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-122",
  note =         "accepted for publication",
  booktitle =    "Proc. Erlangen Workshop of Vision, Modelling, and
                 Visualization (VMV)",
}

@Article{EVL-1999-123,
  pages =        "233--244",
  year =         "1999",
  title =        "An Interactive Visualization and Navigation Tool for
                 Medical Volume Data",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-123",
  author =       "O. Sommer and A. Dietz and R. Westermann and T. Ertl",
  note =         "ISSN 0097-8493",
  number =       "2",
  journal =      "Computers & Graphics",
}

@InProceedings{EVL-1999-124,
  year =         "1999",
  title =        "Virtual Labyrinthoscopy: Visualization of the Inner
                 Ear with Interactive Direct Volume Rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-124",
  author =       "B. Tomandl and P. Hastreiter and K. Eberhardt and H.
                 Greess and U. Nissen",
  note =         "accepted for publication in Jan. 2000",
  booktitle =    "RadioGraphics",
}

@InProceedings{EVL-1999-125,
  pages =        "45--56",
  year =         "1999",
  title =        "Decoupling Polygon Rendering from Geometry using
                 Rasterization Hardware",
  author =       "Ruediger Westermann and Ove Sommer and Thomas Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-125",
  organization = "Eurographics",
  note =         "ISBN 3-211-83382-X",
  editor =       "D. Lischinski and G. W. Larson",
  booktitle =    "Rendering Techniques '99",
  publisher =    "Springer-Verlag, Wien, New York",
}

@Article{EVL-1999-13,
  pages =        "63--75",
  year =         "1999",
  title =        "Evaluating a robust contour tracker on
                 echocardiographic sequences",
  author =       "G. Jacob and J. A. Noble and M. Mulet-Parada and A.
                 Blake",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-13",
  language =     "en",
  abstract =     "In this paper we present an evaluation of a robust
                 visual image tracker on echographic image sequences. We
                 show how the tracking framework can be customized to
                 define an appropriate shape space that describes heart
                 shape deformations that can be learned from a training
                 dataset. We also investigate energy-based temporal
                 boundary enhancement methods to improve image feature
                 measurement. Results are presented demonstrating
                 real-time tracking on normal heart motion data
                 sequences and abnormal synthesized and real heart
                 motion data sequences. We conclude by discussing some
                 of our current research efforts.",
  keywords =     "2-D+T ultrasound, echocardiography, energy filters,
                 heart wall motion, real-time tracking",
  volume =       "3",
  number =       "1",
  journal =      "Medical Image Analysis",
}

@TechReport{EVL-1999-14,
  year =         "1999",
  title =        "Efficient image retrieval through vantage objects",
  author =       "J. Vleugels and R. C. Veltkamp",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-14",
  abstract =     "We describe a new indexing structure for general image
                 retrieval that relies solely on a distance function
                 giving the similarity between two images. For each
                 image object in the database, its distance to a set of
                 m predetermined vantage objects is calculated; the
                 m-vector of these distances specifies a point in the
                 m-dimensional vantage space. The database objects that
                 are similar (in terms of the distance function) to a
                 given query object can be determined by means of an
                 efficient nearest-neighbor search on these points. We
                 demonstrate the viability of our approach through
                 experimental results obtained with a database of about
                 40.000 hieroglyphic polylines.",
  number =       "UU-CS-1999-01",
  institution =  "Department of Computer Science, Utrecht University",
}

@Article{EVL-1999-144,
  pages =        "817--835",
  year =         "1999",
  title =        "Using Farin Points for Rational Bezier Surfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-144",
  author =       "Holger Theisel",
  month =        sep,
  volume =       "16",
  number =       "8",
  journal =      "Computer Aided Geometric Design",
}

@InProceedings{EVL-1999-145,
  pages =        "89--102",
  year =         "1999",
  title =        "Fast Volume rendering methods for voxel-based
                 2{D}/3{D} registration - {A} comparative study",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-145",
  author =       "Roland G{\"{o}}cke and Heidrun Schumann",
  booktitle =    "Proceedings Biomedical Image Registration, WBIR´99",
}

@Article{EVL-1999-146,
  year =         "1999",
  title =        "Demand-driven Image Transmission with Levels of Detail
                 and Regions of Interest",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-146",
  author =       "Uwe Rauschenbach and Heidrun Schumann",
  volume =       "23",
  number =       "6",
  journal =      "Computers and Graphics",
}

@InProceedings{EVL-1999-147,
  year =         "1999",
  title =        "The Rectangular Fish Eye View as an Efficient Method
                 for the Transmission and Display of Large Images",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-147",
  author =       "U. Rauschenbach",
  month =        oct,
  booktitle =    "Proceedings of IEEE ICIP'99, Kobe, Japan",
}

@Conference{EVL-1999-148,
  year =         "1999",
  title =        "A fast voxel-based 2{D}/3{D} registration algorithm
                 using a volume rendering method based on the shear warp
                 factorization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-148",
  author =       "J. Weese and R. G{\"{o}}cke and G. P. Penney and P.
                 Desmedt and T. Buzug and H. Schumann",
  month =        feb,
  booktitle =    "SPIE International Symposium on Medical Imaging, San
                 Diego,USA",
}

@InProceedings{EVL-1999-149,
  year =         "1999",
  title =        "Blobworld: {A} system for region-based image indexing
                 and retrieval",
  author =       "Chad Carson and Megan Thomas and Serge Belongie and
                 Joseph M. Hellerstein and Jitendra Malik",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-149",
  abstract =     "Blobworld is a system for image retrieval based on
                 finding coherent image regions which roughly correspond
                 to objects. Each image is segmented into regions by
                 fitting a mixture of Gaussians to the pixel
                 distribution in a joint color-texture-position feature
                 space. Each region ({"}blob{"}) is then associated with
                 color and texture descriptors. Querying is based on the
                 user specifying attributes of one or two regions of
                 interest, rather than a description of the entire
                 image. In order to make large-scale retrieval feasible,
                 we index the blob descriptions using a tree. Because
                 indexing in the high-dimensional feature space is
                 computationally prohibitive, we use a lower-rank
                 approximation to the high-dimensional distance.
                 Experiments show good results for both querying and
                 indexing.",
  month =        jun,
  booktitle =    "Third International Conference on Visual Information
                 Systems",
  publisher =    "Springer",
}

@TechReport{EVL-1999-15,
  year =         "1999",
  title =        "Mesh-Oriented 3{D} Graphics Architecture",
  author =       "Tzi-cker Chiueh Tulika Mitra",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-15",
  abstract =     "Although triangle meshes are used pervasively in 3D
                 graphics applications and there exist highly efficient
                 mesh representations, almost all existing 3D graphics
                 processors are based on the assumption that individual
                 triangles are processed completely independently of one
                 another. Consequently, none of these graphics
                 processors are able to exploit triangle mesh's
                 vertex/edge sharing property. This paper describes a
                 meshoriented 3D graphics architecture called Heresy,
                 which treats meshes as first-class objects, and
                 significantly reduces the computation and communication
                 costs of rendering triangle meshes by reusing
                 intermediate computation results and eliminating data
                 redundancy. The central architectural feature of Heresy
                 is a highly efficient triangle mesh representation
                 based on breadth-first mesh traversal, which is applied
                 throughout the entire 3D graphics pipeline, from
                 geometric transformation, clipping, to rasterization,
                 and thus enables aggressive exploitation of vertex/edge
                 sharing to minimize both transformation/shading and
                 rasterization cost. Results of a detailed simulation
                 study based on a set of sophisticated 3D models
                 demonstrate the superiority of Heresy over traditional
                 triangle-based architectures and show that on the
                 average the geometric transformation/shading cost, the
                 communications overhead between CPU and the
                 rasterization hardware, and the rasterization delay are
                 reduced by 82\%, 74\% and 37\%, respectively. Moreover,
                 owing to a window-based prefetching technique, the
                 on-chip storage requirement for Heresy is shown to
                 remain small (about 128 vertices wide) and largely
                 independent of the size of the triangle mesh.",
  number =       "TR-62",
  institution =  "Experimental Computer Systems Lab, State University of
                 New York Stony Brook",
}

@InProceedings{EVL-1999-150,
  pages =        "377--395",
  year =         "1999",
  title =        "Visualisierung von Multiparameterdaten in Raum und
                 Zeit -eine Systematisierung-",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-150",
  author =       "Heidrun Schumann",
  organization = "Society for Computer Simulation International, San
                 Diego, Erlangen",
  booktitle =    "Proceedings {"}Simulation und Visualisierung '99{"}",
}

@TechReport{EVL-1999-16,
  year =         "1999",
  title =        "Modeling with {A}-patches from Rational Trivariate
                 Functions",
  author =       "Guoliang Xu and Hongci Huang and Chandrajit Bajaj",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-16",
  abstract =     "We approximate a manifold triangulation in IR3 using
                 smooth implicit algebraic surface patches, which we
                 call A-patches. Here each A-patch is a real iso-contour
                 of a trivariate rational function defined within a
                 tetrahedron. The rational trivariate function provides
                 increased degrees of freedom so that the number of
                 surface patches needed for free-form shape modeling is
                 significantly reduced compared to earlier similar
                 approaches. Furthermore, the surface patches have
                 quadratic precision, that is they exactly recover
                 quadratic surfaces. We give conditions under which a C1
                 smooth and single sheeted surface patch is isolated
                 from the multiple sheets.",
  keywords =     "Algebraic surface, rational A-patch, surface fit,
                 triangulation",
  institution =  "Department of Computer Science, Center for
                 Computational Visualization, University of Texas,
                 Austin",
}

@TechReport{EVL-1999-17,
  year =         "1999",
  title =        "Active Contouring of Images with Physical
                 {A}-splines",
  author =       "Chandrajit L. Bajaj and Valerio Pascucci and Robert J.
                 Holt and Arun N. Netravali",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-17",
  abstract =     "A-splines are implicit real algebraic curves in
                 Bernstein-B{\'e}zier (BB) form that are smooth. We use
                 these in an algorithm for active contouring of images.
                 One advantage of A-splines is that any change to the
                 controlling weights only affects the curve locally,
                 which results in fast convergence. Our active A-splines
                 are also level sets of a time-dependent function with
                 the added flexibility of a dynamic unstructured mesh.
                 Other advantages include the ability to use lower
                 degree polynomials than traditional polynomial
                 parametric B-splines. A-splines also avoid the
                 necessity of dealing with poles that can arise from
                 rational parametric B-splines (NURBS), and they also
                 allow an easy specification of orientation, so that
                 they may be driven to converge to an interior or
                 exterior contour. Our algorithm finds image contours by
                 using a level-set method to obtain an initial close
                 fitting polygon, constructing a physical A-spline
                 contour by minimizing the image energy, and then
                 minimizing the total energy by considering the energy
                 in each spline segment individually.",
  number =       "99-03",
  institution =  "Department of Computer Science, Center for
                 Computational Visualization, University of Texas,
                 Austin",
}

@InProceedings{EVL-1999-170,
  pages =        "68--77",
  year =         "1999",
  title =        "Tetrahedal Discretization of Complex Volumetric
                 Spaces: Implementation, Efficiency, Robustness and
                 Interactive Control",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-170",
  author =       "Ashwini Patgawkar and Dinesh Shikhareand and
                 Satyashree Mahapatra and S. Gopalsamy and S. P. Mudur",
  abstract =     "The problem of tetrahedal grid generation within
                 volumes bounded by triangled surfaces has received
                 considerable attention in recent years due its
                 significance in CFD analysis dealing with complex
                 geometric bodies. Tetrahedal discretization of volume
                 is complex, and even more so when the bounding surfaces
                 are complex geometric configurations of intersecting
                 doubly curved surfaces, say, represented as NURBS, as
                 is usually the case for aircraft surfaces. In addition,
                 the CFD analyst has to be provided with both
                 quantitative and qualitative controls over the grid
                 generation process in terms of the number of tetrahedal
                 elements to be generated, their size, shape, local
                 density, variation and so on. The implementation
                 related issues of efficiency, robustness, scalabitlity
                 and interactive control are thus understandably hard to
                 handle. In this paper, we describe these issues in
                 detail and present the solutions we have implemented in
                 our grid generation system - VolGrid.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
}

@InProceedings{EVL-1999-171,
  title =        "Construction of Trimmed Surface Patches from
                 Unstructured Set of Points",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "78--87",
  year =         "1999",
  author =       "N. Adhikary and B. Gurumoorthy",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-171",
  abstract =     "This paper addresses the problem of constructing a
                 smooth representation of a trimmed surface patch from a
                 given set of unstructured point data measured on the
                 surface patch. Unlike the bottom up approaches reported
                 thus far, the approach described in this paper is
                 similar to the way a trimmed surface would be
                 constructed in a CAD system. The underlying four sided
                 patch is first identified and constructed from the
                 point data and then trimmed with the boundary curves to
                 obtain the trimmed surface patch. The algorithm for
                 identifying the underlying four sided patch is
                 described and results of implementation on typical
                 point data are presented.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "CAD, surface modelling, surface fitting, reverse
                 engineering",
}

@InProceedings{EVL-1999-172,
  title =        "Virtual Reality and Augmented Reality",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "88--94",
  year =         "1999",
  author =       "Stefan M{\"{u}}ller",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-172",
  abstract =     "Virtual Reality (VR) became an important technology
                 for the daily development process. One example is the
                 use of {"}digital mock-ups{"} with the aim to reduce
                 the number of prototypes in order to minimize product
                 development times and costs. A far less known
                 technology is described by the term {"}Augmented
                 Reality{"}. As an extension of Virtual Reality, this
                 technology integrates virtual information into a real
                 environment. In this paper, an overview of existing
                 solutions in both areas and the potential for future
                 development areas are presented.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "virtual reality, augmented reality",
}

@InProceedings{EVL-1999-173,
  title =        "A Virtual Environment for Interactive Music
                 Reproduction",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "95--100",
  year =         "1999",
  author =       "Hartmut Chodura and Arnold Kaup",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-173",
  abstract =     "In this paper a new application for interactive music
                 experience is described. In contrast to the
                 conventional way of consuming music productions, the
                 user is able to change the location of sound sources,
                 like instruments or vocals and navigate through a three
                 dimensional virtual environment. Additionally, he can
                 change the visual and acoustical presentation of the
                 scene. So the virtual performance can take place in a
                 church or a chamber for instance. The functionality of
                 the system and the synchronization between the graphic
                 and sound components is described. Also discussed are
                 important aspects for audio-visual presentations in
                 virtual worlds. For the proposed system a new way of
                 recording and production is required. The multi-track
                 recording of a chamber quartet in an anechoic chamber
                 is specified. The feasibility for various music types
                 is evaluated. Because of taking advantage of 3D output
                 for graphics and sound it is highly entertaining. But
                 also for professionals, like architects and music
                 students there are useful applications, for example
                 individual {"}minus-one recordings{"}.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "virtual environments, 3D HCI, spatial audio",
}

@InProceedings{EVL-1999-174,
  title =        "Virtual Surgery: Methods and Areas of Application",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "101--105",
  year =         "1999",
  author =       "Hans-Peter Meinzer and Harald Evers and Gerald
                 Glombitza",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-174",
  abstract =     "This paper describes the current objectives, methods
                 and applications of virtual surgery. The increased use
                 of three-dimensional imaging has led to an improved
                 diagnosis and enables one to plan surgical procedures
                 in an individualized manner, focusing on the specific
                 patient. Since the conventional evaluation of the large
                 quantity of image data is a complex task,
                 computerassisted processing has gained increased
                 interest. In light of this, the range of methods
                 required for applications in virtual surgery are
                 explained in a step by step manner and are clarified by
                 selected examples. Finally, a discussion follows of the
                 current areas of application, the systems employed as
                 well as the future objectives and developments. The
                 value of systems that support surgery will continue to
                 increase. They profit from a continual improvement in
                 imaging as well as new analysis and presentation
                 techniques which simplify preperative planning and will
                 in the future do the same for the intraoperative
                 implementation of surgical objectives.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "volume visualization, segmentation, virtual surgery",
}

@InProceedings{EVL-1999-175,
  title =        "Model-Based Invariant using Simulated Motion for the
                 Recovery of Planar Objects",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "107--116",
  year =         "1999",
  author =       "Hock Soon Seah and Kok Cheong Wong",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-175",
  abstract =     "A scheme to recover 3D planar object shapes in a
                 single perspective image is presented. The model-based
                 invariant proposed to recognize planar objects is based
                 on the computation of a simulated camera motion between
                 appropriate template and the shape image. It features
                 an active template whose position and orientation are
                 determined for accurate computation of this simulated
                 motion. With the motion parameters between the template
                 and the shape image known, the surface normal of the
                 template, and the surface normal of the object are
                 estimated. If the observed shape is a view of the
                 template, the recovered surface normal of the template
                 will agree with the known value. Once the object is
                 recognized, its pose is also recovered. The recovered
                 3D surface can then be used for facilitating
                 interactions between the images and computer graphics
                 in augmented reality applications.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "structure-from-motion, planar objects recovery,
                 model-based invariant, active template",
}

@InProceedings{EVL-1999-176,
  title =        "Evolution of Autonomous Actors in an Interactive
                 Real-Time Environment",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "117--122",
  year =         "1999",
  author =       "Cedric Sanza and Christophe Destruel and Yves Duthen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-176",
  abstract =     "This paper presents a learning system based on
                 artificial life that uses short-term memory and
                 knowledge sharing. Inspired from classifier systems,
                 the model allows to generate behaviors for agents
                 integrated in a multi-task environment. The user,
                 immersed in the scene, interacts through this clone
                 with autonomous actors. By his own behavior, he
                 influences the agents' one. An agent perceives the
                 world through sensors and acts through effectors in
                 order to produce rules (called classifiers). Rewards
                 from the environment allow to adjust the strength of
                 every rule that is used in order to define the best
                 behavior. The {"}sending message{"} protocol has been
                 included to increase the performances of the system in
                 complex environment. By combining communication and
                 evolution, we then produce a real-time application (a
                 virtual soccer) where the user plays with the other
                 agents. After a short period of adaptation, the
                 simulation gives some positive results: a coherent
                 global behavior is built by the teams.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "agent, evolution, adaptation, classifier systems,
                 virtual environments",
}

@InProceedings{EVL-1999-177,
  title =        "Hierarchical Model For Complex Geometrical Objects
                 Modeling",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "123--128",
  year =         "1999",
  author =       "Oskar Guilbert and Yves Bertrand",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-177",
  abstract =     "The representation of multi-resolution geometric
                 objects can easily overflow the resources of the most
                 powerful computers. Indeed, going down the hierarchy,
                 the number of elements of an object grows exponentially
                 and it is impossible to represent it in extenso. The
                 traditional CAD tools are generally restricted to one
                 level of representation or one topological dimension.
                 That is why we define a B-rep hierarchical model for
                 geometric objects based on generalized maps, and which
                 is homogeneous at all dimensions. It allows important
                 information savings thanks to the cloning of objects.
                 Clones can be linked topologically, even when they come
                 from one representation level to another, may possibly
                 increase the number of topological dimensions, in order
                 to produce more various and richer objects, or degrease
                 the number of topological dimensions in order to
                 realize further information savings. Finally, the
                 interactive handling of this model authorizes,
                 throughout the creation process, operations both at a
                 given level of the hierarchy and on the structure of
                 the hierarchy itself.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "modeling curves / surfaces / solids / volumes,
                 geometry compression, geometry simplification,
                 ease-of-use for 3D designs",
}

@InProceedings{EVL-1999-178,
  title =        "An algorithm for Interference Detection in Cloth
                 Animation",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "129--133",
  year =         "1999",
  author =       "Torbjorn Hallgren and Chandini Wijenayake
                 Halpegamage",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-178",
  abstract =     "This paper presents an algorithm for low-level (also
                 called elementary) collision detection. This
                 encompasses accurate and efficient computation of
                 intersection and proximity parameters. The algorithm in
                 it's basic form addresses collisions between convex
                 polyhedra. However, a small extension makes it
                 attractive also for intersection and proximity
                 computations for any application involving colliding
                 surfaces modeled by means of polygonal meshes. The
                 algorithm offers an improvement in time performance by
                 a factor of about 2 or more compared to often
                 referenced algorithms. The algorithm is generally
                 applicable, but the particular application in mind is
                 cloth modeling and animation. The method is based on a
                 well established algorithm for line clipping.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "collision detection, cloth animation, virtual
                 reality",
}

@InProceedings{EVL-1999-179,
  title =        "Image-Based Rendering: {A} Survey",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "136--143",
  year =         "1999",
  author =       "Heinrich M{\"{u}}ller",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-179",
  abstract =     "Image-based rendering means to use images for modeling
                 3D scenes from which new views can be rendered, like
                 from traditional surface-based scene models. A
                 well-known step in this direction was classical
                 image-based texture mapping. Image-based rendering
                 helps to cope with complex realistic scenes which can
                 hardly be modeled traditionally, and to render such
                 scenes in significant less time. We give an overview of
                 the different approaches to image-based rendering
                 developed in the last few years, and show up the
                 achievements, but also difficulties not yet solved. We
                 do this in a systematic framework. The basis of the
                 framework is the representation of the scene in an
                 optical ray space. From that, direct image-based
                 rendering methods can be immediately deduced, like
                 light field rendering, the lumigraph, and direct view
                 interpolation. Other classes are reconstructive
                 methods, like depth images, layered depth images, dexel
                 and voxel volumes, and hybrid surface-/image-based
                 modeling and rendering. This variety of methods shows
                 the interest image-based modeling and rendering
                 currently finds, and is an example of increasing
                 interaction between the field of computer graphics and
                 the field of digital image processing.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "image-based rendering, rendering",
}

@InCollection{EVL-1999-18,
  pages =        "215--255",
  year =         "1999",
  title =        "Three-dimensional grid drawings of graphs",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-18",
  author =       "J{\'a}nos Pach and Geza T{\'o}th and Torsten Thiele",
  address =      "Mount Holyoke College",
  editor =       "Bernhard Chazelle and Jacob E. Goodman and Richard
                 Pollack",
  volume =       "223",
  booktitle =    "Advances in Discrete and Computational Geometry",
  publisher =    "American Mathematical Society",
}

@InProceedings{EVL-1999-180,
  title =        "Hybrid Shear-Warp Rendering",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "144--153",
  year =         "1999",
  author =       "M. Nordin Zakaria and Md. Yazid Md. Saman",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-180",
  abstract =     "Shear-warp rendering is a fast and efficient method
                 for visualizing a volume of sampled data based on a
                 factorization of the viewing transformation into a
                 shear and a warp. In shear-warp rendering, the volume
                 is resampled, composited and warped to obtain the final
                 image. Many applications, however, require a mixture of
                 polygonal and volumetric data to be rendered together
                 in a single image. We describe, in this paper, a new
                 approach for extending the shear-warp rendering to
                 simultaneously handle polygonal objects. We present a
                 data structure, that we call the zlist-buffer, which is
                 basically a multilayered z-buffer. With the
                 zlist-buffer, an object-based scan conversion of
                 polygons requires only a simple modification of the
                 standard polygon scan-conversion algorithm. We show in
                 this paper how the scan conversion can be integrated
                 with shear-warp rendering of run-length encoded volume
                 data to obtain quality images in real time. We discuss
                 the quality and performance of the approach using a
                 number of test renderings.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "volume rendering, polygon rendering, shear-warp
                 factorization",
}

@InProceedings{EVL-1999-181,
  pages =        "154--161",
  year =         "1999",
  title =        "Visible Space Models: 2 1/2-{D} Representation for
                 Large Virtual Environments",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-181",
  author =       "P. J. Narayanan",
  abstract =     "Traditional geometric representation of virtual
                 environments describe its component objects
                 independently in a single global reference frame. This
                 could be inefficient for large environments,
                 neccessitating representational mechanisms like
                 level-of-detail (LOD) and visibility culling to reduce
                 the adverse impact of intricate detail that has little
                 visible influence from the current viewpoint. In this
                 paper, we present an alternate representation of
                 virtual environments in terms of multiple
                 viewpoint-based (i.e. 2 1/2-D) models called Visible
                 Space Models (VSM). These are motivated by the recent
                 efforts to compute the geometric structure of a real
                 scene using range fining devices. We present the MVSM
                 representation and the associated algorithms for
                 rendering in this paper.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
}

@InProceedings{EVL-1999-182,
  pages =        "162--167",
  year =         "1999",
  title =        "{LDP}: An Image-based Representation for Interactive
                 Rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-182",
  author =       "Li Xinxiao and Zheng Nanning and Jin Li",
  abstract =     "In this paper, an efficient image-based modeling and
                 rendering approach is presented. This method, which is
                 an extension of layered depth image (LDI), uses a
                 panorama-like structure to describe the model of the
                 object or scene. The representation has the regularity
                 of pixels' position and epipolar line relationship
                 among multiple views. It is fitful to compress N sample
                 images efficiently, and generate new views in
                 interactive rate.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
}

@InProceedings{EVL-1999-183,
  title =        "Wavelets for Volume Fusion",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "168--177",
  year =         "1999",
  author =       "Lakshmish. M. R. and Swami Manohar and Vijay Chandru",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-183",
  abstract =     "Volume Fusion is a technique by which two or more
                 {"}similar{"} volumes are intermixed to yield a volume
                 which includes the significant features present in each
                 of the volumes. Volume fusion has direct applications
                 in several areas including medicine and computational
                 fluid dynamics. In this paper we describe a novel
                 waveletbased algorithm for volume fusion that is an
                 extension to three dimensions of an algorithm for image
                 fusion presented by H. Li et al. in 1995. We present a
                 simple measure for the effectiveness of volume fusion
                 and demonstrate the quality of our algorithm with
                 respect to this measure. A Gaussian-blur technique is
                 used to generate the 'similar' volume data sets from a
                 standard shell data set and these derived data sets
                 have been used to demonstrate our algorithm.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "volume graphics, wavelets, volume fusion,
                 visualization",
}

@InProceedings{EVL-1999-184,
  pages =        "178--186",
  year =         "1999",
  title =        "Motion vector interpolation using wavelets",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-184",
  author =       "S. H. Srinivasan and P. Padmashree and K. R.
                 Ramakrishnan",
  abstract =     "Motion vectors constitute an important feature for
                 object segmentation algorithms. In MPEG we have motion
                 vectors defined at the macroblock level. In order to
                 get realistic object boundaries, we need to get the
                 motion vectors at the pixel level. In other words, we
                 need to interpolate the MPEG motion vectors from the
                 block level to the pixel level. There are conflicting
                 requirements on the interpolation scheme. (1) The
                 interpolation scheme used should interpolate smoothly.
                 (2) The interpolation scheme should also preserve
                 discontinuities in order to ensure realistic
                 segmentation. The well-known linear interpolation of
                 motion vectors smoothes discontinuities in the motion
                 field. The wavelet interpolation scheme (Pentland,
                 1994) which meets these twin objectives considered in
                 this paper for motion vectors. This algorithm also has
                 other good properties. (1) It regularizes the
                 interpolation operator. (2) It runs in linear (O(n))
                 time. We cluster the interpolated motion vectors to
                 object boundaries. We compare the results of object
                 segmentation obtained using wavelet interpolation with
                 that obtained using linear interpolation. The object
                 boundaries obtained using wavelet interpolation are
                 closer to real object boundaries as expected.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  address =      "05, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
}

@InProceedings{EVL-1999-185,
  title =        "Automated Generation of Visual Elements for Image
                 Processing Algorithms",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "187--192",
  year =         "1999",
  author =       "Carlos E. Cardenas S. and Athanasios M. Demiris and
                 Hans-Peter Meinzer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-185",
  abstract =     "The development of a new image processing algorithm,
                 the {"}semantic debugging{"} and its application are
                 usually supported by a user interface. A graphical user
                 interface makes the interactive operation and
                 parameterization of the algorithms easier. The
                 development of graphical user interfaces, though, is a
                 laborious and complex process and distracts the
                 developer of image processing solutions from his/hers
                 actual work-tasks. In this paper we present a generic
                 system, which offers an extensive automation of the
                 generation process of graphical user interfaces
                 specially designed and developed for image processing
                 algorithms. The primary goal was to reach an automation
                 level such as to have the system generate user
                 interface elements with either minimal or no additional
                 information or interference by the developer. We
                 developed a framework for the generation of graphical
                 user interface elements and implemented a test-editor
                 which demonstrates the feasibility of such an approach
                 and the practical advantages of its application in the
                 realm of medical image processing.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "visual interface, image processing algorithms,
                 GUI-generation, object-oriented frameworks, rapid
                 prototyping",
}

@InProceedings{EVL-1999-186,
  title =        "A New Approach to Lighting Design",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "194--203",
  year =         "1999",
  author =       "Antonio Cardoso Costa and Antonio Augusto Sousa and
                 Fernando Nunes Ferreira",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-186",
  abstract =     "There is a need for robust and reliable lighting
                 design applications. Architects, engineers and
                 designers need working tools to define, test and
                 validate the lighting solutions for their problems.
                 Unfortunately, available tools are limited and
                 inappropriate for interactive or creative use. In this
                 paper, we present a new approach to the lighting design
                 problem based on two methodologies that include scene
                 geometry, material properties and lighting design
                 goals. It is possible to obtain luminaires
                 characteristics that maximize the attainment of the
                 lighting goals, which may include different kinds of
                 constraints or objectives (lighting, geometrical,
                 etc.). Our main goal is to shorten and improve the
                 design cycle.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "lighting design, global illumination, light
                 transport",
}

@InProceedings{EVL-1999-187,
  title =        "Building a Visualisation on the Foundation of Software
                 Ergonomics and Media Design",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "204--210",
  year =         "1999",
  author =       "Bernhard E. B{\"{u}}rdek and Maximilian Eibl and
                 J{\"{u}}rgen Krause",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-187",
  abstract =     "Boolean queries in information retrieval are still a
                 problem not solved. Users have serious trouble building
                 Boolean expressions. Although many IR systems move on
                 to probabilistic methods, there still remain domains
                 where Boolean algebra is needed. There have been made
                 some attempts introducing visualisation to solve the
                 problem but they brought a bunch of new problems with
                 them. This paper deals with an attempt to consider
                 aspects of software ergonomics and interface- and
                 mediadesign in order to find a suitable
                 visualisation.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "document retrieval, visualisation, human computer
                 interface, software ergonomics, graphics design",
}

@InProceedings{EVL-1999-188,
  title =        "Simulation of Ply Realization in Composites
                 Manufacturing",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "211--216",
  year =         "1999",
  author =       "P. V. Kiran Kumar and V. Devaraja Holla and K. G.
                 Shastry",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-188",
  abstract =     "Laminated composite structures are generally realized
                 using Uni.-directional tapes and bi-directional (woven)
                 fabric. A simulation process is a must to assess the
                 realizability of the component in terms of fiber
                 orientation and fabric deformation. It serves as a
                 manufacturing aid to the composite lay-up process. It
                 also ensures process repeatability and maximizes
                 material utilization. The present paper describes the
                 simulation software developed for realizing the
                 composite ply of any shape using bi-directional fabric
                 or uni-directional tapes. Though the fabric and tape
                 material constituents are same, the lay-up path taken
                 by fabric and tape on the tool surface with the same
                 initial conditions are different due to their differing
                 mechanical properties. The uni-directional tapes are
                 mapped onto the component surface by estimating natural
                 path of the tape. Tchebychev net-based mapping is used
                 for the draping simulation of a bi-directional fabric
                 on the tool surface contour. The user can define
                 several overlapping or non-overlapping regions on the
                 given surface, and can run the simulation on individual
                 regions separately, which facilitates handling of
                 complex surfaces.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "ply, bi-directional fabric, uni-directional tape,
                 surface allocation, draping",
}

@InProceedings{EVL-1999-189,
  title =        "Automatic Reconstruction of Shape, Surface and Solid
                 Model of Laminated Composites",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "217--222",
  year =         "1999",
  author =       "V. T. Johnson and Prabba Srinivasan and G. V. V. Ravi
                 kumar and K. G. Shastry",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-189",
  abstract =     "With the advent of powerful graphics computers,
                 Computer Aided concurrent engineering design of
                 geometrically complex objects is becoming increasingly
                 popular in the realm of design and manufacture of
                 laminated composites (laminate). The precise definition
                 of geometric data of the laminate and plies in various
                 forms such as tessellated model, surface model and
                 solid model are required for design and manufacture of
                 the laminate. This geometric data for a laminate is to
                 be computed from the geometry of the layup surface, the
                 ply boundary curves and ply thickness information. This
                 paper attempts to address these issues and presents
                 methodologies to retrieve and reconstruct precise
                 representation of the laminate in the form of
                 tessellated surface model, G1 continuous multiple-patch
                 surface model. Further research effort has evolved a
                 software system for automating the same on UNIX
                 platform using C/C++.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "laminated composites, geometrically constrained
                 triangulation, surface reconstruction, solid model",
}

@TechReport{EVL-1999-19,
  year =         "1999",
  title =        "Applying an Edit Distance to the Matching of Tree Ring
                 Sequences in Dendrochronology",
  author =       "Carola Wenk",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-19",
  abstract =     "In dendrochronology wood samples are dated according
                 to the tree rings they contain. The dating process is a
                 one dimensional matching in which the sequence of tree
                 ring widths in the sample is compared to a dated master
                 sequence. Assuming that a tree forms one ring per year
                 a consecutive piece in the master sequence is searched
                 which has the same length than and is similar to the
                 sample sequence. A brute force algorithm takes
                 $\theta(mn)$ time where n and m are the lengths of the
                 sample and the master sequences, respectively. Yet,
                 sometimes a tree produces no ring or even two rings in
                 a year. A sample sequence containing this kind of
                 inconsistencies cannot be dated correctly by the simple
                 dating algorithm mentioned above. We therefore
                 introduce a $O(\alpha^{4}(m+n)+\alpha^{2}mn)$ algorithm
                 for dating such sample sequence against an error-free
                 master sequence. Our algorithms takes into account that
                 the sample might contain up to $\alpha$ missing our
                 double rings and suggests possible positions for these
                 kind of inconsistencies. This is done by employing an
                 edit distance as the distance measure.",
  month =        jan,
  institution =  "Freie Universit{\"a}t Berlin, Department of Computer
                 Science, Germany",
}

@InProceedings{EVL-1999-190,
  title =        "A Virtual Prototyping, Simulation And Automated
                 Control System For {A} Family of Palletising Robots",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "223--228",
  year =         "1999",
  author =       "Juha Tuominen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-190",
  abstract =     "The main interest within the robotics research has
                 recently been in the highly articulated manipulators
                 and the related computationally complex problems, such
                 a automated path planning. At the same time, the
                 increased demand for flexibility, effectivity and
                 safety in the portal robot and palletising applications
                 has brought new challenges to the domain. The design
                 and implementation of a modern palletising robot system
                 targeted at applications dealing with the handling of a
                 large variety of products and orders requires the use
                 of the same technique as with the case of any other
                 type of a robot cell. With the increased demands, also
                 Virtual prototyping, simulation and off-line
                 programming have a lot to offer to the palletising
                 robot applications. This paper reports the design and
                 implementation of an integrated software system for
                 virtual prototyping, simulation and automated control
                 of a family of palletising robots.",
  organization = "ational Centre for Software Technology, Mumbai,
                 India",
  keywords =     "robotics, virtual prototyping, off-line programming,
                 palletising, graphical simulation, 3D modeling,
                 reconfigurable control system",
}

@InProceedings{EVL-1999-191,
  title =        "Applying {A}. {I}. Informed Search and Iterative
                 Methods to Solve Containment Problems for the Footwear
                 Industry",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "229--234",
  year =         "1999",
  author =       "Nuno Marques and Joao Bernardo and Pedro Capela",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-191",
  abstract =     "This paper deals with containment problems in which
                 there is a set of shapes to be allocated in another
                 shape, called the container. We are making the
                 correspondence between 2D translational or rotational
                 containment problems and generic A* informed search
                 methods for geometric reasoning. A* can be proven to be
                 complete and optimal if the heuristic has some
                 restrictions, that is, if it holds the admissibility
                 property. We formally present the 2D containment
                 problems as an A* informed search problem and give an
                 admissible heuristic, using Minkowski operators. We
                 also present an iterative containment algorithm that
                 selects a shape from a set of shapes to place at each
                 step to keep running times controlled. We mix both
                 approximations in order to get results that have low
                 time execution and still can compete with the manual
                 containment process.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "containment algorithms, heuristic search, Minkowski
                 operators",
}

@InProceedings{EVL-1999-192,
  title =        "{CAPP}: {A} 2{D} Feature Based Modeling Approach",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "235--238",
  year =         "1999",
  author =       "Milind D. Sinkar",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-192",
  abstract =     "Down stream applications such as process planning,
                 require geometric information as well as process
                 parameters. This information is captured from the
                 2d-Feature Based Component Data Model (2d-FBCDM),
                 obtained through a 2d-Feature Based Modeling System
                 (2d-FBMS). 2d-fbms works as an integrated software
                 module of the present 2d-CAD Systems. 2d-FBCDM can be
                 used to take process planning decisions to generate
                 process plan autmatically. Efforts are being made to
                 develop a process planning system that will use the
                 2d-FBCDM as one of the inputs. At present, this study
                 is confined to rotational parts only.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "2d-CAD Systems, 2d-FBMS, 2d-FBCDM, CAPP,
                 Expert-System, Knowledge Base, Frames, Rules",
}

@InProceedings{EVL-1999-193,
  title =        "Implementation of {LAN}-based Virtual Classroom and
                 Video Telephony Services",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "240--246",
  year =         "1999",
  author =       "Shailendra Sinha and Anamitra Makur",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-193",
  abstract =     "This work describes an implementation of the
                 multimedia services involving continuous media over PCs
                 connected by an ethernet LAN. The services we have
                 implemented are Virtual Classroom and Video Telephony.
                 In a Virtual Classroom scenario, there is one teacher
                 (sender) and many students (recipients), located at
                 different places. Video telephony is similar to
                 conventional telephony between two parties, except that
                 in addition to audio, video too is transmitted and
                 received by both sides. In this work, we describe
                 Conditional Replenishment with Forced Update as our
                 compression algorithm for Virtual Classroom, and simple
                 Conditional Replenishment for Video Telephony. We also
                 describe multiplexing and demultiplexing of audio/video
                 streams, simulated random packet loss in Virtual
                 Classroom scenario, experiments with various packet
                 sizes and their effect on the performance. Other
                 relevant design issues have also been described.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "computer assisted Virtual Classroom, Video
                 Encoding/Compression, Multimedia",
}

@InProceedings{EVL-1999-194,
  title =        "Virtual Reality Semiconductor Laboratory ({VRSEMLAB}):
                 an Advanced Training Tool for Teaching Complex Ideas",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "247--255",
  year =         "1999",
  author =       "Christopher Allport and Brandon Schreiner and Paul
                 Sines and Biswajit Das",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-194",
  abstract =     "Virtual Reality (VR) training systems are advanced
                 interactive self-paced learning systems that are
                 customizable and adaptable to the need of different
                 individuals. Because of their many advantages, VR
                 training systems are finding increased applications in
                 a variety of disciplines. Virtual Reality's superior 3D
                 immersive environment makes it especially suitable for
                 teaching complex ideas, in particular, principles that
                 involve spatial relationships. VR provides the user
                 with the ability to visualize and manipulate objects in
                 3 dimensions. This results in a clearer elicitation of
                 spatial relationships. As a demonstration of VR's
                 potential for teaching complex themes, we have
                 developed the Virtual Reality Semiconductor Laboratory
                 (VRSEMLAB). It is designed to aid in teaching
                 semiconductor device physics to undergraduate college
                 students. VRSEMLAB demonstrates an effective method for
                 modeling abstract and complicated 3D systems. It does
                 so by presenting semiconductor devices in an
                 experimental environment free of concerns of electrical
                 safety, equipment limitations, and device
                 limitations.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "Virtual Reality, 3D visualization, Education and
                 Training",
}

@InProceedings{EVL-1999-195,
  title =        "Authoring Courseware for Distributed Environments",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "256--263",
  year =         "1999",
  author =       "Jose Carlos Teixeira and Cesar Paris and Joao M.
                 Brisson Lopes and Ana Maria Paiva",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-195",
  abstract =     "This paper introduces a new approach to the design and
                 delivery of computer based courseware. The new approach
                 is based on modularity and adaptability. It allows
                 lower courseware production costs and leads to
                 solutions in oder to meet the new demands of the
                 learning/teaching process such as: the inclusion of new
                 educational technologies, the new emerging learning
                 scenarios and, as a consequence, a whole new learning
                 philosophy. In a global way, the concepts presented
                 supported Individual, Group Learning and Lecturing
                 scenarios, with or without Tutoring Support. In all
                 these scenarios, coursware can record student
                 performance, and even, adapt itself to student
                 understanding and mastering of the content. This
                 feature is of crucial interest for heterogeneous
                 audiences. This new approach allows the creation of
                 Virtual Learning Spaces, where learning material is
                 distributed over the different databases, which
                 constitute the system. This way all the learning
                 material is available at all places as if it was stored
                 in one. IDEALS, a European Union funded project,
                 implemented these concepts and used the transport
                 mechanism of the World Wide Web to present learning
                 material and interact with learners. The use of such
                 technology ensures future evolution of the application
                 and simplifies the authoring process.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "authoring tools and methodologies, Internet based
                 educational systems, cost effectiveness, on-demand
                 education",
}

@InProceedings{EVL-1999-196,
  title =        "Spherical harmonic surface modeling with improved
                 center location",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "265--273",
  year =         "1999",
  author =       "Sarp Ert{\"{u}}rk and Tim J. Dennis",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-196",
  abstract =     "Spherical harmonic (SH) modeling is used in the
                 representation of rigid and non-rigid object shapes.
                 The object surface is expressed in the form of a radial
                 distance from a chosen center and two spatial angle
                 co-ordinates. A linear combination of SH basis
                 functions are used for the radial distance, with
                 coefficients computed from range data sampled on the
                 object surface. This paper focuses on the selection of
                 an appropriate center location to improve the fit
                 between actual shape and the SH model. Instead of
                 pre-determined positions the co-ordinate system center
                 is chosen to be located close to areas of high detail.
                 This is accomplished by comparing the surface with its
                 low order SH approximation using a variance-based
                 technique. The technique provides a direction with
                 respect to current co-ordinate system along which the
                 center is located. This paper demonstrates that it is
                 possible to improve visual and statistical accuray of
                 SH representation for objects with concentrated local
                 detail, by improving the location of the co-ordinate
                 system center.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "shape representation, surface modeling, spherical
                 harmonics",
}

@InProceedings{EVL-1999-197,
  title =        "Implicit Surface Approximation Using Adaptive Spatial
                 Meshes",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "ntasey Typesetters Pvt. Ltd.",
  pages =        "274--280",
  year =         "1999",
  author =       "Heinrich M{\"{u}}ller and Michael Wehle",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-197",
  abstract =     "An algorithm is presented which calculates surface
                 meshes approximating an implicitly defined surface. The
                 algorithm first performs an adaptive spatial
                 tetrahedrization from which the surface is extracted by
                 an marching tetrahedra approach. Our improvement over
                 the known implementations of this approach is the
                 context-free calculation of the spatial
                 tetrahedrization which is more efficient than existing
                 context-sensitive or two-pass solutions. Furthermore,
                 an improved quality of the surface mesh is achieved by
                 setting the subdivision points on the surface so that
                 the resulting surface mesh is part of the resulting
                 spatial mesh.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "graphics and geometric algorithms, modeling surfaces",
}

@InProceedings{EVL-1999-198,
  title =        "Engineering a geometric Approach for Efficient Visible
                 Surface Determination",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "281--288",
  year =         "1999",
  author =       "Amit Shirsat and Gopat Shevare",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-198",
  abstract =     "We describe a simple, robust and efficient algorithm
                 to compose the trapezoidal decomposition of line
                 segments by endpoint rounding and exact computation of
                 the signs of determinants. We use a two pass approach
                 derived from [3] to delay the computation of
                 intersection to the second pass. Using this approach
                 and Mulmuley's [6] randomized incremental construction,
                 we give an algorithm for an efficient geometric
                 solution to visible surface determination.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "computational geometry, robustness, exact computation,
                 hidden surface removal (HSR), trapezoidal
                 decompositions.",
}

@InProceedings{EVL-1999-199,
  title =        "Modelling of the Left Ventricle with a Dynamic
                 Gaussian Blob Model",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
  pages =        "289--293",
  year =         "1999",
  author =       "Soo-Mi Choi and Myoung-Hee Kim",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-199",
  abstract =     "We present a method for modelling of the heart's left
                 ventricle (LV) wall motion during a cardiac cycle. In
                 this method, the LV is approximated by a dynamic
                 gaussian blob model, which unify a gaussian
                 interpolation basis FEM element with a superellipsoid.
                 The proposed model describes the shape of the LV and
                 the wall motion using a physics-based approach. It
                 tracks the LV due to forces exerted from 3-D points
                 corresponding to the wall of the LV in a time sequence
                 of images. We believe that our method may help
                 automatic diagnosis of heart diseases.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  keywords =     "left ventricle, finite element method, gaussian
                 interpolations, superellipsoids",
}

@Article{EVL-1999-2,
  pages =        "87--115",
  year =         "1999",
  title =        "Deformable Smooth Surface Design",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-2",
  author =       "H. Edelsbrunner",
  language =     "en",
  abstract =     "A new paradigm for designing smooth surfaces is
                 described. A finite set of points with weights
                 specifies a closed surface in space referred to as
                 skin. It consists of one or more components, each
                 tangent continuous and free of self-intersections and
                 intersections with other components. The skin varies
                 continuously with the weights and locations of the
                 points, and the variation includes the possibility of a
                 topology change facilitated by the violation of tangent
                 continuity at a single point in space and time.
                 Applications of the skin to molecular modeling and to
                 geometric deformation are discussed.",
  month =        jan,
  volume =       "21",
  number =       "1",
  journal =      "Discrete and Computational Geometry",
}

@TechReport{EVL-1999-20,
  year =         "1999",
  title =        "Introduction to Integer-Coordinate Crystalline
                 Meshes",
  author =       "Vasiliki Chatzi and Franco P. Preparata",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-20",
  abstract =     "We introduce 2D and 3D integer-coordinate crystalline
                 meshes and discuss their properties. We explain that
                 crystalline meshes give us both the flexibility of
                 variable element density in different parts of the
                 domain as well as the ability to construct the system
                 of equations quickly and accurately, which makes them
                 ideal for problems where the domain has regions of
                 interest. We also illustrate the versatility of
                 crystalline meshes in the context of dynamic mesh
                 adaptation, discussing in detail the refinement
                 procedure. Finally, we give some experimental results
                 and compare the computational behavior of crystalline
                 meshes with that of current methods.",
  month =        feb,
  number =       "CS-99-01",
  institution =  "Brown University",
}

@InProceedings{EVL-1999-200,
  pages =        "295--302",
  year =         "1999",
  title =        "ScanlineFlow Rasterization - {A} Sort-Last Algorithm
                 for Polygon Rendering on a Multicomputer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-200",
  author =       "Joao Madeiras Pereira and Mario R. Gomes",
  abstract =     "A sort-last strategy for polygon parallel rendering,
                 namely the ScanlineFlow Rasterization algorithm, is
                 described. Its main characteristic resides on the fact
                 that both steps, rendering and merging, run
                 concurrently. By taking advantage of a pipeline
                 interconnection network and since this algorithm
                 renders an image one scanline at a time, each node
                 rasterizes multiple polygons active on a given scanline
                 after synchronisation with the previous node in order
                 to read the rasterization data concerning that
                 scanline. The execution is pipelined, in the sense that
                 while a node is rasterizing a scanline (already
                 rasterized) to the next node. The algorithm makes use
                 of the full-frame merging technique because merging a
                 full frame from each node is very regular and easy to
                 implement. This solution has provided good results and
                 is a viable alternative to implement sort-last
                 algorithms on a multicomputer. Our developing platform
                 consisted of a Parsytec MultiCluster machine with
                 seventeen processors running the Helios Operating
                 System and using the CDL (Component Distributing
                 Language) parallel programming language.",
  organization = "National Centre for Software Technology, Mumbai,
                 India",
  address =      "305, Mahadkar Chambers, 127/3A, Karve Road, Kothrud,
                 Pune 29",
  editor =       "S. P. Mudur and Dinesh Shikhare",
  booktitle =    "ICVC99 - International Conference on Visual
                 Computing",
  publisher =    "Fontasey Typesetters Pvt. Ltd.",
}

@Article{EVL-1999-201,
  pages =        "20--21",
  year =         "1999",
  title =        "Visual Data Mining",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-201",
  author =       "Pak Chung Wong",
  month =        sep,
  note =         "guest editor's introduction",
  keywords =     "state of the art, data mining, visualization,
                 techniques",
  volume =       "19",
  number =       "5",
  journal =      "IEEE Computer Graphics and Applications",
}

@Article{EVL-1999-202,
  pages =        "61--65",
  year =         "1999",
  title =        "Visualizing Underwater Environments Using
                 Multifrequency Sonar",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-202",
  author =       "Paul Chapman and Derek Wills and Graham Brookes and
                 Peter Stevens",
  abstract =     "This article introduces seabed visualization by
                 describing three case studies that use a high-speed,
                 multifrequency, continuous scan sonar called the Seabed
                 Visualization System.",
  month =        sep,
  volume =       "19",
  number =       "5",
  journal =      "IEEE Computer Graphics and Applications",
}

@Article{EVL-1999-203,
  pages =        "22--31",
  year =         "1999",
  title =        "{HD}-Eye: Visual Mining of High-Dimensional Data",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-203",
  author =       "Alexander Hinnenburg and Daniel A. Keim and Markus
                 Wawryniuk",
  abstract =     "An advanced clustering algorithm combined with new
                 visualization methods interactively clusters data.
                 Experiments show these techniques improve the data
                 mining process.",
  month =        sep,
  volume =       "19",
  keywords =     "data mining, information visualization",
  number =       "5",
  journal =      "IEEE Computer Graphics and Applications",
}

@Article{EVL-1999-204,
  pages =        "32--39",
  year =         "1999",
  title =        "Discovery Visualization Using Fast Clustering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-204",
  author =       "William Ribarsky and Jochen Katz and Frank Jiang and
                 Aubrey Holland",
  abstract =     "Discovery visualization combines automated response
                 and user selection to achieve and sustain animated
                 action while users explore time-dependent data.",
  month =        sep,
  volume =       "19",
  keywords =     "data mining, information visualization, feature-based
                 hierarchy",
  number =       "5",
  journal =      "IEEE Computer Graphics and Applications",
}

@Article{EVL-1999-205,
  pages =        "40--46",
  year =         "1999",
  title =        "A Shape-based Visual Interface for Text Retrieval",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-205",
  author =       "Randall M. Rohrer and John L. Silbert and David S.
                 Ebert",
  abstract =     "Our shape-based visual interface uses procedurally
                 generated shapes coupled with an underlying
                 text-retrieval engine. The interface lets users
                 visualize multidimensional relationships among
                 documents. We use implicit surface models for
                 visualizing the query results of a text retrieval
                 engine.",
  month =        sep,
  keywords =     "data mining, information retrieval, document
                 retrieval, information visualization, text retrieval,
                 procedural shapes",
  volume =       "19",
  number =       "5",
  journal =      "IEEE Computer Graphics and Applications",
}

@Article{EVL-1999-206,
  pages =        "54--60",
  year =         "1999",
  title =        "Visualizing the Atomic Nucleus",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-206",
  author =       "Norman D. Cook and Takefumi Hayashi and Nobuaki
                 Yoshida",
  abstract =     "Developments in computer science and nuclear theory
                 make it possible to visualize the nucleus consistently
                 among the diverse models of nuclear structure. Nuclear
                 Visualization Software facilitates this goal.",
  month =        sep,
  volume =       "19",
  number =       "5",
  journal =      "IEEE Computer Graphics and Applications",
}

@Article{EVL-1999-207,
  pages =        "66--71",
  year =         "1999",
  title =        "First-Generation {ASCI} Production Visualization
                 Environment",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-207",
  author =       "Philip D. Heermann",
  abstract =     "Visualization terascale computing results requires
                 considering the entire data visualization system. The
                 systems approach presented here decreases the time
                 needed to prepare large data sets for visualization
                 four-fold.",
  month =        sep,
  volume =       "19",
  keywords =     "parallel computing, physics calculations, simulation",
  number =       "5",
  journal =      "IEEE Computer Graphics and Applications",
}

@Article{EVL-1999-208,
  pages =        "72--77",
  year =         "1999",
  title =        "Task-Specific Visualization Design",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-208",
  author =       "Lloyd A. Treinish",
  abstract =     "This case study in operational weather forecasting
                 demonstrates the principles of task-specific
                 visualization design: defining user needs, implementing
                 that definition, and establishing techniques for
                 different user goals.",
  month =        sep,
  volume =       "19",
  keywords =     "3D visualization, meteorology",
  number =       "5",
  journal =      "IEEE Computer Graphics and Applications",
}

@Article{EVL-1999-209,
  pages =        "48--49",
  year =         "1999",
  title =        "Visualizing Case Studies: Drawing a Roadmap for Future
                 Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-209",
  author =       "Kwan-Liu Ma and Frits H. Post",
  month =        sep,
  note =         "Guest editor's Introduction",
  volume =       "19",
  number =       "5",
  journal =      "IEEE Computer Graphics and Applications",
}

@InProceedings{EVL-1999-21,
  year =         "1999",
  title =        "A Mobile Augmented Reality User Interface for
                 Terrestrial Navigation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-21",
  author =       "W. Piekarski and D. Hepworth and V. Demczuk and B.
                 Thomas and B. Gunther",
  month =        jan,
  address =      "Auckland, New Zealand",
  booktitle =    "Proceedings of the Twenty Second Australasian Computer
                 Science Conference",
}

@Article{EVL-1999-210,
  pages =        "50--53",
  year =         "1999",
  title =        "Visualizing 3{D} Configuration Spaces for Mechanical
                 Design",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-210",
  author =       "Elisha Sacks and Charles Pisula and Leo Joskowicz",
  abstract =     "We describe configuration space visualization methods
                 for mechanical design. The research challenge is to
                 relate the configuration space geometry to the
                 mechanical function of the parts.",
  month =        sep,
  volume =       "19",
  keywords =     "visualization, construction, robot motion, mechanics",
  number =       "5",
  journal =      "IEEE Computer Graphics and Applications",
}

@Article{EVL-1999-211,
  pages =        "286--297",
  year =         "1999",
  title =        "Segmentation of gated Tl-{SPECT} images and
                 computation of ejection fraction: {A} different
                 approach",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-211",
  author =       "P. Brigger and S. L. Bacharach and G. Srinivasan and
                 K. A. Nour and J. A. Carson and V. Dilsizian and A.
                 Aldroubi and M. Unser",
  abstract =     "We describe a set of image processing algorithms and
                 mathematical models that can be advantageously used in
                 schemes for the segmentation of thallium-201-single
                 photon emission computed tomography (SPECT) images and
                 for computation of left ventricular ejection fraction
                 (EF). Methods: The system consists of two independent
                 blocs for image segmentation and computation of
                 function. The former is based on a multiresolution
                 elliptical coordinate transformation and dynamic
                 contour tracking. Computation of EF is formulated on
                 the basis of both the endocardial and epicardial
                 contours, and we compare this formulation with that
                 using only the endocardial border for images with low
                 signal-to-noise ratios. The accuracy of border
                 detection was validated against manual border tracing
                 on FDG-PET images, simulated Tl-201-SPECT images where
                 the true underlying borders were known, and actual
                 Tl-201-SPECT images. Finally, we compared EFs computed
                 for FDG-PET, technetium-99m-SPECT and Tl-201-SPECT with
                 those obtained from planar gated blood pool imaging.
                 Results: The automatically obtained results always were
                 within the manual uncertainty range. Agreement between
                 myocardial volumes from positron emission tomography
                 and automatically obtained values from the simulated
                 Tl-201-SPECT images was excellent (r = 0.95, n = 32),
                 agreement between EFs from planar gated blood pool
                 imaging and the other image modalities was good
                 (FDG-PET: y = 5.89 + 1.21x, r = 0.92, see = 6.24, n =
                 19, Tc-99m-SPECT: y = -3.86 + 1.06x, r = 0.88, see =
                 7.78, n = 9, Tl-201-SPECT: y = 17.8 + 0.81x, r = 0.77,
                 see = 7.44, n = 26), for noisy input data the combined
                 use of information from epicardial and endocardial
                 contours gives more accurate EF values than the
                 traditional formula on the basis of the endocardial
                 contour only. Conclusions: Alternate approaches for
                 segmentation and computation of function have been
                 presented and validated. They might also be
                 advantageously incorporated into other existing
                 techniques.",
  month =        may,
  volume =       "6",
  number =       "3",
  journal =      "J. Nuclear Cardiology",
}

@TechReport{EVL-1999-212,
  year =         "1999",
  title =        "Computing Contour Trees in All Dimensions",
  author =       "Hamish Carr and Jack Snoeyink and Ulrike Axen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-212",
  abstract =     "We show that contour trees can be computed in all
                 dimensions by a simple algorithm that merges two trees.
                 Our algorithm extends, simplifies, and improves work of
                 Tarasov and Vyalyi and of van Kreveld et al.",
  month =        aug,
  number =       "TR-99-09",
  institution =  "Department of Computer Science, University of British
                 Comlumbia",
}

@TechReport{EVL-1999-213,
  year =         "1999",
  title =        "Practical Point-in-Polygon Tests Using {CSG}
                 Representations of Polygons",
  author =       "Robert J. Walker and Jack Snoeyink",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-213",
  abstract =     "We investigate the use of a constructive solid
                 geometry (CSG) representation of polygons in testing if
                 points fall within them; this representation consists
                 of a tree whose nodes are either Boolean operators or
                 edges. By preprocessing the polygons, we seek (1) to
                 construct a space-conserving data structure that
                 supports point-in-polygon tests, (2) to prune as many
                 edges as possible while maintaining the semantics of
                 our tree, and (3) to obtain a tight inner loop to make
                 testing the remaining edges as fast as possible. We
                 utilize opportunities to optimize the pruning by
                 permuting sibling nodes. We find that this process is
                 less memory-intensive than the grid method and faster
                 than existing one-shot methods.",
  month =        nov,
  number =       "TR-99-12",
  institution =  "Department of Computer Science, University of British
                 Comlumbia",
}

@Article{EVL-1999-214,
  pages =        "52--58",
  year =         "1999",
  title =        "Solving Einstein's Equation on Supercomputers",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-214",
  author =       "Gabrielle Allen and Tom Goodale and Gerd Lanfermann
                 and Edward Seidel and Werner Benger and Hans-Christian
                 Hege and Andre Merzky and Joan Masso and John Shalf",
  abstract =     "Globally distributed scientific teams, linked to the
                 most powerful supercomputers, are running visual
                 simulations of Einstein's equations on the
                 gravitational effects of colliding black holes.",
  month =        dec,
  keywords =     "scientific visualization, visualization software,
                 theory of relativity, collaborative visualization",
  volume =       "32",
  number =       "12",
  journal =      "IEEE Computer",
}

@Article{EVL-1999-215,
  pages =        "36--43",
  year =         "1999",
  title =        "Distance Visualization: Data Exploration on the Grid",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-215",
  author =       "Ian Foster and Joseph Insley and Gregor von Laszewski
                 and Carl Kesselman and Marcus Thiebaux",
  abstract =     "We describe an online system that supports
                 three-dimensional tomographic image reconstruction --
                 and subsequent collaborative analysis -- of data from
                 remote scientific instruments.",
  month =        dec,
  volume =       "32",
  number =       "12",
  journal =      "IEEE Computer",
}

@Article{EVL-1999-216,
  pages =        "44--51",
  year =         "1999",
  title =        "Virtue: Performance Visualization of Parallel and
                 Distributed Applications",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-216",
  author =       "Eric Shaffer and Daniel A. Reed and Shannon Whitmore
                 and Benjamin Schaeffer",
  month =        dec,
  volume =       "32",
  number =       "12",
  journal =      "IEEE Computer",
}

@Article{EVL-1999-217,
  pages =        "59--65",
  year =         "1999",
  title =        "Interactive Simulation and Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-217",
  author =       "C. Johnson and S. G. Parker and C. Hansen and G. L.
                 Kindlmann and Y. Livnat",
  abstract =     "As computational science and engineering applications
                 grow in complexity, working with the data becomes
                 increasingly difficult. An emerging technology - called
                 computational steering - addressses this problem by
                 providing a mechanism to integrate modeling,
                 simulation, data analysis, and visualization.",
  month =        dec,
  volume =       "32",
  number =       "12",
  journal =      "IEEE Computer",
}

@Article{EVL-1999-218,
  pages =        "66--73",
  year =         "1999",
  title =        "Visualization in Teleimmersive Environments",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-218",
  author =       "Jason Leigh and Andrew E. Johnson and Maxime Brown and
                 Daniel J. Sandin and Thomas A. DeFanti",
  abstract =     "In teleimmersion, collaborators at remote sites share
                 the details of a virtual world that can autonomously
                 control computation, query databases, and gather
                 results. They don't meet in a room to discuss a car
                 engine. They meet in the engine itself.",
  month =        dec,
  keywords =     "visualization, virtual environments, collaborative
                 virtual reality",
  volume =       "32",
  number =       "12",
  journal =      "IEEE Computer",
}

@Conference{EVL-1999-219,
  year =         "1999",
  title =        "Fersteuerung und Fern&uuml;berwachung von verteilten
                 Anwendungen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-219",
  author =       "Andr&eacute; Merzky",
  note =         "in press",
  booktitle =    "Conference Booklet HLRN-Workshop: Wissenschaftliche
                 Anwendungen auf H&ouml;chstleistungsrechnern",
}

@Article{EVL-1999-22,
  pages =        "177--181",
  year =         "1999",
  title =        "Isoperimetric Inequalities for Densities of
                 Lattice-Periodic Sets",
  author =       "Peter Brass",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-22",
  note =         "appeared also as preprint Freie Universit{\"a}t
                 Berlin, Fachbereich Mathematik und Informatik Serie B
                 97-05",
  volume =       "127",
  journal =      "Monatshefte f{\"u}r Mathematik",
}

@Article{EVL-1999-220,
  pages =        "93--96",
  year =         "1999",
  title =        "A quantitative three-dimensional model of the
                 Drosophila optic lobes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-220",
  author =       "Karlheinz Rein and Malte Z{\"o}ckler and Martin
                 Heisenberg",
  volume =       "9",
  number =       "2",
  journal =      "Current Biology",
}

@Conference{EVL-1999-221,
  pages =        "942--946",
  year =         "1999",
  title =        "Optimized Arrangement of Osseointegrated Implants: {A}
                 Surgical Planning System for the Fixation of Facial
                 Prostheses",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-221",
  author =       "Stefan Zachow and Tim C. Lueth and Detlev Stalling and
                 Andreas Hein and Martin Klein and Horst Menneking",
  editor =       "Lenke and Vannier and Inamura and Farman",
  booktitle =    "Computer Assisted Radiology and Surgery (CARS'99)",
  publisher =    "Elsevier Science B.V.",
}

@InCollection{EVL-1999-222,
  pages =        "203--207",
  year =         "1999",
  title =        "Mehrschichtige Oberfl{\"a}chenmodelle zur
                 computergenerierten Planung in der Chirugie",
  author =       "Detlev Stalling and Martin Seeba{\ss} and Stefan
                 Zachow",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-222",
  abstract =     "Polygonale Sch{\"{a}}delmodelle bilden ein wichtiges
                 Hilfsmittel f{\"{u}}r computergest{\"{u}}tzte Planungen
                 im Bereich der plastischen Chirurgie. Wir beschreiben,
                 wie derartige Modelle automatisch aus
                 hochaufgel{\"{o}}sten CT-Datens{\"{a}}tzen erzeugt
                 werden k{\"{o}}nnen. Durch einen lokal steuerbaren
                 Simplifizierungsalgorithmus werden die Modelle so weit
                 vereinfacht, da{\ss{}} auch auf kleineren
                 Graphikcomputern interaktives Arbeiten m{\"{o}}glich
                 wird. Die Verwendung eines speziellen
                 Transparenzmodells erm{\"{o}}glicht den ungehinderten
                 Blick auf die bei der Planung relevanten
                 Knochenstrukturen und l{\"{a}}{\ss{}}t den Benutzer
                 zugleich die Kopfumrisse des Patienten erkennen.",
  editor =       "H. Evers and G. Glombitza and T. Lehmann and H.-P.
                 Meinzer",
  keywords =     "Isofl{\"{a}}chen, Simplifizierung, Transparenzen",
  booktitle =    "Bildverarbeitung f{\"u}r die Medizin 1999",
  publisher =    "Springer-Verlag",
}

@TechReport{EVL-1999-223,
  year =         "1999",
  title =        "Visualizing Quantum Mechanical Phenomena",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-223",
  author =       "J. Schmidt-Ehrenberg and H.-C. Hege",
  abstract =     "In this paper we discuss several ways to visualize
                 stationary and non-stationary quantum mechanical
                 systems. We demonstrate an approach for the
                 quantitative interpretation of probability density
                 isovalues which yields a reasonable correlation between
                 isosurfaces for different timesteps. As an intuitive
                 quantity for visualizing the momentum of a quantum
                 system we propose the probability flow density which
                 can be treated by vector field visualization
                 techniques. Finally, we discuss the visualization of
                 non-stationary systems by a sequence of single timestep
                 images.",
  keywords =     "quantum dynamics, isosurfaces, probability flow,
                 animation",
  institution =  "Konrad-Zuse-Zentrum f{\"ur} Informationstechnik (ZIB),
                 Preprint SC 99-39",
}

@InCollection{EVL-1999-224,
  pages =        "1049",
  year =         "1999",
  title =        "A 2{D} Planning System for Robot-Assisted
                 Interventions",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-224",
  author =       "A. Hein and T. C. Lueth and S. Zachow and M. Stien",
  editor =       "H. U. Lemke et al.",
  booktitle =    "Computer Assisted Radiology and Surgery",
  publisher =    "Elsevier Science B.V.",
}

@InProceedings{EVL-1999-225,
  year =         "1999",
  title =        "Visualization of groundwater flow using line integral
                 convolution",
  author =       "J{\"o}rg Gotthardt and Carola Bl{\"o}mer and Detlev
                 Stalling",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-225",
  abstract =     "Line integral convolution (LIC) is a particularly
                 elegant and powerful method for the visualisation of 2D
                 and 3D vector field data. The method is based on
                 convolving a random input texture or noise image along
                 the streamlines of a stationary flow. The resulting
                 output image clearly reveals the directional structure
                 of the flow at high spatial resolution. For an
                 efficient computing of L IC images the pixel coherence
                 along the stream should be exploited. A fast LIC
                 algorithm based on this principle has been d eveloped
                 at Konrad-Zuse-Zentrum. Additional scalar quantities
                 may be visualized using pseudo-coloring. The algorithm
                 has been integra ted into the software system SPRING
                 designed for the simulation of groundwater flow and
                 transport models.",
  booktitle =    "Proc. ModelCare'99",
}

@InProceedings{EVL-1000-1,
  pages =        "73--80",
  year =         "1999",
  title =        "Efficient Compression of Non-Manifold Polygonal
                 Meshes",
  author =       "Andr{\'{e}} P. Gu{\'{e}}ziec and Frank Bossen and
                 Gabriel Taubin and Cl{\'{a}}udio T. Silva",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-226",
  abstract =     "We present a method for compressing non-manifold
                 polygonal meshes, i.e. polygonal meshes with
                 singularities, which occur very frequently in the
                 real-world. Most efficient polygonal compression
                 methods currently available are restricted to a
                 manifold mesh: they require a conversion process, and
                 fail to retrieve the original model connectivity after
                 decompression. The present method works by converting
                 the original model to a manifold model, encoding the
                 manifold model using an existing mesh compression
                 technique, and clustering, or stitching together during
                 the decompression process vertices that were duplicated
                 earlier to faithfully recover the original
                 connectivity. This paper focuses on efficiently
                 encoding and decoding the stitching information. By
                 separating connectivity from geometry and properties,
                 the method avoids encoding vertices (and properties
                 bound to vertices) multiple times; thus a reduction of
                 the size of the bit-stream of about 10% is obtained
                 compared with encoding the model as a manifold.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "Polygonal Mesh, Geometry Compression, Non-Manifold,
                 Stitching Copyright",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-227,
  title =        "Physically Based Motion Transformation",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "11--20",
  year =         "1999",
  author =       "Zoran Popovic and Andy Witkin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-227",
  abstract =     "We introduce a novel algorithm for transforming
                 character anima-tion sequences that preserves essential
                 physical properties of the motion. By using the
                 spacetime constraints dynamics formulationour algorithm
                 maintains realism of the original motion sequence
                 without sacrificing full user control of the editing
                 process. In contrast to most physically based animation
                 techniques thatsynthesize motion from scratch, we take
                 the approach of motion transformation as the underlying
                 paradigm for generating computeranimations. In doing
                 so, we combine the expressive richness of an input
                 animation sequence with the controllability of
                 spacetime op-timization to create a wide range of
                 realistic character animations. The spacetime dynamics
                 formulation also allows editing of intu-itive,
                 high-level motion concepts such as the time and
                 placement of footprints, length and mass of various
                 extremities, number of bodyjoints and gravity.",
  organization = "Computer Graphics (SIGGRAPH) 1999",
  keywords =     "Human Body Simulation, Physically Based Animation,
                 Animation with Constraints",
}

@InProceedings{EVL-1999-228,
  title =        "Voice Puppetry",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "21--28",
  year =         "1999",
  author =       "Matthew Brand",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-228",
  abstract =     "We introduce a method for predicting a control signal
                 from anotherrelated signal, and apply it to voice
                 puppetry: Generating full facial animation from
                 expressive information in an audio track. Thevoice
                 puppet learns a facial control model from computer
                 vision of real facial behavior, automatically
                 incorporating vocal and facialdynamics such as
                 co-articulation. Animation is produced by using audio
                 to drive the model, which induces a probability
                 distributionover the manifold of possible facial
                 motions. We present a lineartime closed-form solution
                 for the most probable trajectory overthis manifold. The
                 output is a series of facial control parameters,
                 suitable for driving many different kinds of animation
                 ranging fromvideo-realistic image warps to 3D cartoon
                 characters.",
  organization = "ACM Siggraph",
  keywords =     "Facial animation, lip-syncing, control,
                 learning,computer vision and audition.",
}

@InProceedings{EVL-1999-229,
  title =        "Cognitive Modeling: Knowledge, Reasoning and Planning
                 for Intelligent Characters",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "29--38",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-229",
  author =       "John Funge and Xiaoyuan Tu and Demetri Terzopoulos",
  abstract =     "Recent work in behavioral animation has taken
                 impressive steps toward autonomous, self-animating
                 characters for use in production animation and
                 interactive games. It remains difficult, however, to
                 direct autonomous characters to perform specific tasks.
                 This paper addresses the challenge by introducing
                 cognitive modeling. Cognitive models go beyond
                 behavioral models in that they govern what a character
                 knows, how that knowledge is acquired, and how it can
                 be used to plan actions. To help build cognitive
                 models, we develop the cognitive modeling language CML.
                 Using CML, we can imbue a character with domain
                 knowledge, elegantly specified in terms of actions,
                 their preconditions and their effects, and then direct
                 the characters behavior in terms of goals. Our
                 approach allows behaviors to be specified more
                 naturally and intuitively, more succinctly and at a
                 much higher level of abstraction than would otherwise
                 be possible. With cognitively empowered characters, the
                 animator need only specify a behavior outline or
                 {"}sketch plan{"} and, through reasoning, the character
                 will automatically work out a detailed sequence of
                 actions satisfying the specification. We exploit
                 interval methods to integrate sensing into our
                 underlying theoretical framework, thus enabling our
                 autonomous characters to generate action plans even in
                 highly complex, dynamic virtual worlds. We demonstrate
                 cognitive modeling applications in advanced character
                 animation and automated cinematography.",
  organization = "ACM Siggraph",
  keywords =     "Computer Animation, Character Animation, Intelligent
                 Characters, Behavioral Animation, Cognitive Modeling,
                 Knowledge, Sensing, Action, Reasoning, Planning",
}

@TechReport{EVL-1999-23,
  year =         "1999",
  title =        "On the number of maximum-area triangles in a planar
                 pointset",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-23",
  author =       "Peter Brass and Konrad Swanepoel",
  note =         "eingereicht bei {Discrete Comput. Geom.\/}",
  number =       "B 99-07",
  institution =  "Freie Universit{\"a}t Berlin, Fachbereich Mathematik
                 und Informatik",
}

@InProceedings{EVL-1999-230,
  title =        "A Hierarchical Approach to Interactive Motion Editing
                 for Human-LikeFigures",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceddings",
  publisher =    "Addison Wesley Longman",
  pages =        "39--48",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-230",
  author =       "Jehee Lee and Sung Yong Shin",
  abstract =     "This paper presents a technique for adapting existing
                 motion of a human-like character to have the desired
                 features that are specified by a set of constraints.
                 This problem can be typically formulated as a spacetime
                 constraint problem. Our approach combines a
                 hierarchical curve fitting technique with a new inverse
                 kinematics solver. Using the kinematics solver, we can
                 adjust the configuration of an articulated figure to
                 meet the constraints in each frame. Through the fitting
                 technique, the motion displacement of every joint at
                 each constrained frame is interpolated and thus
                 smoothly propagated to frames. We are able to
                 adaptively add motion details to satisfy the
                 constraints within a specified tolerance by adopting a
                 multilevel B-spline representation which also provides
                 a speedup for the interpolation. The performance of our
                 system is further enhanced by the new inverse
                 kinematics solver. We present a closed-form solution to
                 compute the joint angles of a limb linkage. This
                 analytical method greatly reduces the burden of a
                 numerical optimization to find the solutions for full
                 degrees of freedom of a human-like articulated figure.
                 We demonstrate that the technique can be used for
                 re-targetting a motion to compensate for geometric
                 variations caused by both characters and environments.
                 Furthermore, we can also use this technique for
                 directly manipulating a motion clip through a graphical
                 interface.",
  organization = "ACM Siggraph",
  keywords =     "Motion Editing, Motion Adaptation, Spacetime
                 Constraints, Hierarchical Techniques, Inverse
                 Kinematics",
}

@InProceedings{EVL-1999-232,
  title =        "Robust Mesh Watermarking",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "49--56",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-232",
  author =       "Emil Praun and Hugues Hoppe and Adam Finkelstein",
  abstract =     "We describe a robust method for watermarking triangle
                 meshes. Watermarking provides a mechanism for copyright
                 protection of digital media by embedding information
                 identifying the owner in the data. The bulk of the
                 research on digital watermarks has focused on media
                 such as images, video, audio, and text. Robust
                 watermarks must be able to survive a variety of
                 {"}attacks,{"} including resizing, cropping, and
                 filtering. For resilience to such attacks, recent
                 watermarking schemes employ a {"}spread-spectrum{"}
                 approach - they transform the document to the frequency
                 domain and perturb the coefficients of the perceptually
                 most significant basis functions. We extend this
                 spread-spectrum approach to work for the robust
                 watermarking of arbitrary triangle meshes. Generalizing
                 spread spectrum techniques to surfaces presents two
                 major challenges. First, arbitrary surfaces lack a
                 natural parametrization for frequency-based
                 decomposition. Our solution is to construct a set of
                 scalar basis function over the mesh vertices using
                 multiresolution analysis. The watermark perturbs
                 vertices along the direction of the surface normal,
                 weighted by the basis functions. The second challenge
                 is that simplification and other attacks may modify the
                 connectivity of the mesh. We use an optimization
                 technique to resample an attacked mesh using the
                 original mesh connectivity. Results show that our
                 watermarks are resistant to common mesh operations such
                 as translation, rotation, scaling, cropping, smoothing,
                 simplification, and resampling, as well as malicious
                 attacks such as the insertion of noise, modification of
                 low-order bits, or even insertion of other
                 watermarks.",
  organization = "ACM Siggraph",
  keywords =     "copyright protection, steganography",
}

@InProceedings{EVL-1999-233,
  title =        "Interpolating Nets of Curves by Smooth Subdivision
                 Surface",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "57--64",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-233",
  author =       "Adi Levin",
  abstract =     "A subdivision algorithm is presented for the
                 computation and representation of a smooth surface of
                 arbitrary topological type interpolating a given net of
                 smooth curves. The algorithm belongs to a new class of
                 subdivision schemes called combined subdivision
                 schemes. These schemes can exactly interpolate a net of
                 curves given in any parametric representation. The
                 surfaces generated by our algorithm are G2 except at a
                 finite number of points, where the surface is G1 and
                 has bounded curvature. The algorithm is simple and easy
                 to implement, and is based on a variant of the famous
                 Catmull-Clark subdivision scheme.",
  organization = "ACM Siggraph",
  keywords =     "Subdivision, Interpolation, Combined Subdivision
                 schemes, Net of curves",
}

@InProceedings{EVL-1999-234,
  title =        "ArtDefo - Accurate Real Time Deformable Objects",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "65--72",
  year =         "1999",
  author =       "Doug L. James and Dinesh K. Pai",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-234",
  abstract =     "We present an algorithm for fast, physically accurate
                 simulation of deformable objects suitable for real time
                 animation and virtual environment interaction. We
                 describe the boundary integral equation formulation of
                 static linear elasticity as well as the related
                 Boundary Element Method (BEM) discretization technique.
                 In addition, we show how to exploit the coherence of
                 typical interactions to achieve low latency; the
                 boundary formulation lends itself well to a fast update
                 method when a few boundary conditions change. The
                 algorithms are described in detail with examples from
                 ArtDefo, our implementation.",
  organization = "ACM Siggraph",
}

@InProceedings{EVL-1999-235,
  title =        "A Perceptually Based Physical Error Metric for
                 Realistic Image Synthesis",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "73--82",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-235",
  author =       "Mahesh Ramasubramanian and Sumanta N. Pattanaik and
                 Donald P. Greenberg",
  abstract =     "We introduce a new concept for accelerating realistic
                 image synthesis algorithms. At the core of this
                 procedure is a novel physical error metric that
                 correctly predicts the perceptual threshold for
                 detecting artifacts in scene features. Built into this
                 metric is a computational model of the human visual
                 system's loss of sensitivity at high background
                 illumination levels, high spatial frequencies, and high
                 contrast levels (visual masking). An important feature
                 of our model is that it handles the luminance-dependent
                 processing and spatially-dependent processing
                 independently. This allows us to precompute the
                 expensive spatially-dependent component, making our
                 model extremely efficient. We illustrate the utility of
                 our procedure with global illumination algorithms used
                 for realistic image synthesis. The expense of global
                 illumination computations is many orders of magnitude
                 higher than the expense of direct illumination
                 computations and can greatly benefit by applying our
                 perceptually based technique. Results show our method
                 preserves visual quality while achieving significant
                 computational gains in areas of images with high
                 frequency texture patterns, geometric details, and
                 lighting variations.",
  organization = "ACM Siggraph",
  keywords =     "Realistic Image Synthesis, Global Illumination,
                 Adaptive Sampling, Perception, Visual Masking, Error
                 Metric, Visual Threshol",
}

@InProceedings{EVL-1999-236,
  title =        "Subdivision Schemes for Fluid Flow",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "111--120",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-236",
  author =       "Henrik Weimer and Joe Warren",
  abstract =     "The motion of fluids has been a topic of study for
                 hundreds of years. In its most general setting, fluid
                 flow is governed by a system of non-linear partial
                 differential equations known as the Navier-Stokes
                 equations. However, in several important settings,
                 these equations degenerate into simpler systems of
                 linear partial differential equations. This paper will
                 show that flows corresponding to these linear equations
                 can be modeled using subdivision schemes for vector
                 fields. Given an initial, coarse vector field, these
                 schemes generate an increasingly dense sequence of
                 vector fields. The limit of this sequence is a
                 continuous vector field defining a flow that follows
                 the initial vector field. The beauty of this approach
                 is that realistic flows can now be modeled and
                 manipulated in real time using their associated
                 subdivision schemes.",
  organization = "ACM Siggraph",
  keywords =     "CAD, subdivision, multi-grid, fluid simulations,
                 fractals, physically based animation, physically based
                 modeling",
}

@InProceedings{EVL-1999-237,
  title =        "{LCIS}: {A} Boundary Hierarchy for Detail-Preserving
                 Contrast Reduction",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "83--90",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-237",
  author =       "Jack Tumblin and Greg Turk",
  abstract =     "High contrast scenes are difficult to depict on low
                 contrast displays without loss of important fine
                 details and textures. Skilled artists preserve these
                 details by drawing scene contents in coarse-to-fine
                 order using a hierarchy of scene boundaries and
                 shadings. We build a similar hierarchy using multiple
                 instances of a new low curvature image simplifier
                 (LCIS), a partial differential equation inspired by
                 anisotropic diffusion. Each LCIS reduces the scene to
                 many smooth regions that are bounded by sharp gradient
                 discontinuities, and a single parameter K chosen for
                 each LCIS controls region size and boundary complexity.
                 With a few chosen K values (K1 > K2 > K3...) LCIS makes
                 a set of progressively simpler images, and image
                 differences form a hierarchy of increasingly important
                 details, boundaries and large features. We construct a
                 high detail, low contrast display image from this
                 hierarchy by compressing only the large features, then
                 adding back all small details. Unlike linear filter
                 hierarchies such as wavelets, filter banks, or image
                 pyramids, LCIS hierarchies do not smooth across scene
                 boundaries, avoiding {"}halo{"} artifacts common to
                 previous contrast reducing methods and some tone
                 reproduction operators. We demonstrate LCIS
                 effectiveness on several example images.",
  organization = "ACM Siggraph",
  keywords =     "Signal Processing, Displays, Non-Realistic Rendering,
                 Level Of Detail Algorithms, Radiosity, Weird Math",
}

@InProceedings{EVL-1999-238,
  title =        "A Practical Analytic Model for Daylight",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Grahics Procedings",
  publisher =    "Addison Wesley Longman",
  pages =        "91--100",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-238",
  author =       "A. J. Preetham and Peter Shirley and Brian E. Smits.",
  abstract =     "Sunlight and skylight are rarely rendered correctly in
                 computer graphics. A major reason for this is high
                 computational expense. Another is that precise
                 atmospheric data is rarely available. We present an
                 inexpensive analytic model that approximates full
                 spectrum daylight for various atmospheric conditions.
                 These conditions are parameterized using terms that
                 users can either measure or estimate. We also present
                 an inexpensive analytic model that approxi-mates the
                 effects of atmosphere (aerial perspective). These
                 models are fielded in a number of conditions and
                 intermediate results verified against standard
                 literature from atmospheric science. These models are
                 analytic in the sense that they are simple formulas
                 based on fits to simulated data; no explicit simulation
                 is required to use them. Our goal is to achieve as much
                 accuracy as possible without sacrificing usability.",
  organization = "ACM Siggraph",
  keywords =     "sunlight, skylight, aerial perspective, illumination",
}

@InProceedings{EVL-1999-239,
  title =        "Diffraction Shaders",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "101--110",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-239",
  author =       "Jos Stam",
  abstract =     "The reflection of light from surfaces is a fundamental
                 problem in computer graphics. Although many reflection
                 models have been proposed, few take into account the
                 wave nature of light. In this paper, we derive a new
                 class of reflection models for metallic surfaces that
                 handle the effects of diffraction. Diffraction is a
                 purely wave-like phenomenon and cannot be properly
                 modeled using the ray theory of light alone. A common
                 example of a surface which exhibits diffraction is the
                 compact disk. A characteristic of such surfaces is that
                 they reflect light in a very colorful manner. Our model
                 is also a generalization of most reflection models
                 encountered in computer graphics. In particular, we
                 extend the He-Torrance model to handle anisotropic
                 reflections. This is achieved by rederiving, in a more
                 general setting, results from surface wave physics
                 which were taken for granted by other researchers.
                 Specifically, our use of Fourier analysis has enabled
                 us to tackle the difficult task of analytically
                 computing the Kirchhoff integral of surface
                 scattering.",
  organization = "ACM Siggraph",
  keywords =     "shading models, diffraction, Fourier transform,
                 Kirchhoff theory, rough surface scattering, random
                 processes",
}

@Article{EVL-1999-24,
  year =         "1999",
  title =        "A Distributed Spatial Architecture for Bush Fire
                 Simulation",
  author =       "Peter Eklund and Stephen Kirkby and Jonathan Mann",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-24",
  abstract =     "This paper describes a spatial systems architecture
                 allowing clients to modify parameters and run their own
                 bush fire simulations via map data provided by a world
                 wide web server. The simulations conform to the model
                 of bush fire behaviors as well as operational
                 requirements. A wavelet model and fire simulation over
                 a distributed network are considered in this paper and
                 we demonstrate how the architecture is general enough
                 to accommodate other spatial simulations for emergency
                 management using the world wide web.",
  volume =       "3",
  keywords =     "Distributed spatial information systems, web-based
                 spatial simulation, bush fire modeling",
  number =       "3",
  journal =      "Transactions on GIS",
  publisher =    "Blackwell Publishers Oxford",
}

@InProceedings{EVL-1999-240,
  title =        "Stable Fluids",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "121--128",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-240",
  author =       "Jos Stam",
  abstract =     "Building animation tools for fluid-like motions is an
                 important and challenging problem with many
                 applications in computer graphics. The use of
                 physics-based models for fluid flow can greatly assist
                 in creating such tools. Physical models, unlike key
                 frame or procedural based techniques, permit an
                 animator to almost effortlessly create interesting,
                 swirling fluid-like behaviors. Also, the interaction of
                 flows with objects and virtual forces is handled
                 elegantly. Until recently, it was believed that
                 physical fluid models were too expensive to allow
                 real-time interaction. This was largely due to the fact
                 that previous models used unstable schemes to solve the
                 physical equations governing a fluid. In this paper,
                 for the first time, we propose an unconditionally
                 stable model which still produces complex fluid-like
                 flows. As well, our method is very easy to implement.
                 The stability of our model allows us to take larger
                 time steps and therefore achieve faster simulations. We
                 have used our model in conjuction with advecting solid
                 textures to create many fluid-like animations
                 interactively in two- and three-dimensions.",
  organization = "ACM Siggraph",
  keywords =     "animation of fluids, Navier-Stokes, stable solvers,
                 implicit elliptic PDE solvers, interactive modeling,
                 gaseous phenomena, advected textures",
}

@InProceedings{EVL-1999-241,
  title =        "Computational Fluid Dynamics in a Traditional
                 Animation Environment",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood.",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "129--136",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-241",
  author =       "Patrick Witting",
  abstract =     "This paper presents a system that uses computational
                 fluid dynamics to produce smoke, water, and other
                 effects for traditionally-animated films. The system
                 was used in over twenty scenes in the animated feature
                 film {"}The Prince of Egypt.{"} Animators use images
                 and animation sequences to drive two-dimensional
                 numerical simulations of the time-dependent
                 compressible Navier-Stokes equations. For instance,
                 images can be used to initialize temperature fields
                 which cause dynamic buoyancy-driven vortices to evolve.
                 In addition to being image-driven, the systemis unique
                 in allowing for compressibility of the fluid, and in
                 its use of partial differential equations for texture
                 mapping.",
  organization = "ACM Siggraph",
  keywords =     "animation, animation systems, applications, fluid
                 simulations, natural phenomena, numerical analysis,
                 physically based animation, physically based modeling,
                 scientific visualization, texture mapping",
}

@InProceedings{EVL-1999-242,
  title =        "Graphical Modeling and Animation of Brittle",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Grpahics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "137--146",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-242",
  author =       "James F. O'Brien and Jessica K. Hodgins",
  abstract =     "In this paper, we augment existing techniques for
                 simulating flexible objects to include models for crack
                 initiation and propagation in three-dimensional
                 volumes. By analyzing the stress tensors computed over
                 a finite element model, the simulation determines where
                 cracks should initiate and in what directions they
                 should propagate. We demonstrate our results with
                 animations of breaking bowls, cracking walls, and
                 objects that fracture when they collide. By varying the
                 shape of the objects, the material properties, and the
                 initial conditions of the simulations, we can create
                 strikingly different effects ranging from a wall that
                 shatters when it is hit by a wrecking ball to a bowl
                 that breaks in two when it is dropped on edge.",
  organization = "ACM Siggraph",
  keywords =     "Animation techniques, physically based modeling,
                 simulation, dynamics, fracture, cracking, deformation,
                 finite element method",
}

@InProceedings{EVL-1999-243,
  title =        "Direct Illumination With LazyVisibility Evaluation",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "147--154",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-243",
  author =       "David Hart and Philip Dutr{\'{e}} and Donald P.
                 Greenberg",
  abstract =     "In this paper we present a technique for computing the
                 direct lighting in a three-dimensional scene containing
                 area light sources. Our method correctly handles
                 partial visibility between luminaires and receivers,
                 and is able to efficiently generate accurate soft
                 shadows in scenes modeled with general bidirectional
                 reflectance distribution functions. In most current
                 algorithms, the form factor between a light source and
                 receiver is computed using a stochastic ray casting
                 approach which evaluates partial visibility. Such an
                 approach often leads to noisy artifacts or aliasing
                 problems. Generating significantly more rays is often
                 the only solution to improving image quality. Our
                 approach first stores visibility information in the
                 image plane, using lazy evaluation of the visibility
                 function. In a second phase, illumination values for
                 each pixel are generated, using analytic or stochastic
                 integration. Soft shadows and other shading effects are
                 generated with high accuracy in less time than with
                 existing shading algorithms. Coherence in specific
                 blocker-light source relationships across the image
                 plane is exploited to amortize the cost of analytic
                 form factor calculations. By storing information in the
                 image plane, our method is currently designed for
                 generating a single image, and is thus
                 view-dependent.",
  organization = "ACM Siggraph",
  keywords =     "Rendering, Illumination Effects, Monte Carlo
                 Techniques, Shadow Algorithms, Visibility
                 Determination",
}

@InProceedings{EVL-1999-244,
  title =        "Computing Exact Shadow Irradiance Using Splines",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "155--164",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-244",
  author =       "Michael M. Stark and Elaine Cohen and Tom Lyche and
                 Richard F. Riesenfeld",
  abstract =     "We present a solution to the general problem of
                 characterizing shadows in scenes involving a uniform
                 polygonal area emitter and a polygonal occluder in
                 arbitrary position by manifesting shadow irradiance as
                 a spline function. Studying generalized prism-like
                 constructions generated by the emitter and the occluder
                 in a four-dimensional (shadow) space reveals a simpler
                 intrinsic structure of the shadow as compared to the
                 more complicated 2D projection onto a receiver. A
                 closed form expression for the spline shadow irradiance
                 function is derived by twice applying Stokes' theorem
                 to reduce an evaluation over a 4D domain to an explicit
                 formula involving only 2D faces on the receiver,
                 derived the scene geometry. This leads to a
                 straightforward computational algorithm and an
                 interactive implementation. Moreover, this approach can
                 be extended to scenes involving multiple emitters and
                 occluders, as well as curved emitters, occluders, and
                 receivers. Spline functions are constructed from these
                 prism-like objects. We call them generalized polyhedral
                 splines because they extend the classical polyhedral
                 splines to include curved boundaries and a density
                 function. The approach can be applied to more general
                 problems such as some of those occurring in radiosity,
                 and other related topics.",
  organization = "ACM Siggraph",
  keywords =     "rendering, shadow algorithms, illumination, visibility
                 determination",
}

@InProceedings{EVL-1999-245,
  title =        "Reflection Space Image Based Rendering",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "165--170",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-245",
  author =       "Brian Cabral and Marc Olano and Philip Nemec",
  abstract =     "High quality, physically accurate rendering at
                 interactive rates has widespread application, but is a
                 daunting task. We attempt to bridge the gap between
                 high-quality offline and interactive rendering by using
                 existing environment mapping hardware in combination
                 with a novel Image Based Rendering (IBR) algorithm. The
                 primary contribution lies in performing IBR in
                 reflection space. This method can be applied to
                 ordinary environment maps, but for more physically
                 accurate rendering, we apply reflection space IBR to
                 radiance environment maps. A radiance environment map
                 pre-integrates a Bidirectional Reflection Distribution
                 Function (BRDF) with a lighting environment. Using the
                 reflection-space IBR algorithm on ra-diance environment
                 maps allows interactive rendering of arbitrary objects
                 with a large class of complex BRDFs in arbitrary
                 lighting environments. The ultimate simplicity of the
                 final algorithm suggests that it will be widely and
                 immediately valuable given the ready availability of
                 hardware assisted environment mapping.",
  organization = "ACM Siggraph",
  keywords =     "interactive rendering and shading, texture mapping,
                 reflection mapping, image based rendering",
}

@InProceedings{EVL-1999-246,
  title =        "Realistic, Hardware-Accelerated Shading and Lighting",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Annual Conference Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "171--178",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-246",
  author =       "Wolfgang Heidrich and Hans-Peter Seidel",
  abstract =     "With fast 3D graphics becoming more and more available
                 even on low end platforms, the focus in
                 hardware-accelerated rendering is beginning to shift
                 towards higher quality rendering and additional
                 functionality instead of simply higher performance
                 implementations based on the traditional graphics
                 pipeline. In this paper we present techniques for
                 realistic shading and lighting using computer graphics
                 hardware. In particular, we discuss multipass methods
                 for high quality local illumination using
                 physically-based reflection models, as well as
                 techniques for the interactive visualization of
                 non-diffuse global illumination solutions. These
                 results are then combined with normal mapping for
                 increasing the visual complexity of rendered images
                 Although the techniques presented in this paper work at
                 interactive frame rates on contemporary graphics
                 hardware, we also discuss some modifications of the
                 rendering pipeline that help to further improve both
                 performance and quality of the proposed methods",
  organization = "ACM Siggraph",
  keywords =     "reflectance functions, illumination effects, shading,
                 texture mapping, renderinghardware, frame buffer
                 techniques",
}

@InProceedings{EVL-1999-247,
  title =        "Tracing Ray Differential",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "179--186",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-247",
  author =       "Homan Igehy",
  abstract =     "Antialiasing of ray traced images is typically
                 performed by super-sampling the image plane. While this
                 type of filtering works well for many algorithms, it is
                 much more efficient to perform filtering locally on a
                 surface for algorithms such as texture mapping. In
                 order to perform this type of filtering, one must not
                 only trace the ray passing through the pixel, but also
                 have some approximation of the distance to neighboring
                 rays hitting the surface (i.e., a ray¹s footprint). In
                 this paper, we present a fast, simple, robust scheme
                 for tracking such a quantity based on ray
                 differentials, derivatives of the ray with respect to
                 the image plane.",
  organization = "ACM Siggraph",
  keywords =     "color, shading, shadowing, and texture; raytracing",
}

@InProceedings{EVL-1999-248,
  title =        "A Morphable Model for the Synthesis of 3{D} Faces",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "187--194",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-248",
  author =       "Volker Blanz and Thomas Vetter",
  abstract =     "In this paper, a new technique for modeling textured
                 3D faces is introduced. 3D faces can either be
                 generated automatically from one or more photographs,
                 or modeled directly through an intuitive user
                 interface. Users are assisted in two key problems of
                 computer aided face modeling. First, new face images or
                 new 3D face models can be registered automatically by
                 computing dense one-to-one correspondence to an
                 internal face model. Second, the approach regulates the
                 naturalness of modeled faces avoiding faces with an
                 {"}unlikely{"} appearance. Starting from an example set
                 of 3D face models, we derive a morphable face model by
                 transforming the shape and texture of the examples into
                 a vector space representation. New faces and
                 expressions can be modeled by forming linear
                 combinations of the prototypes. Shape and texture
                 constraints derived from the statistics of our example
                 faces are used to guide manual modeling or automated
                 matching algorithms. We show 3D face reconstructions
                 from single images and their applications for
                 photo-realistic image manipulations. We also
                 demonstrate face manipulations according to complex
                 parameters such as gender, fullness of a face or its
                 distinctiveness.",
  organization = "ACM Siggraph",
  keywords =     "facial modeling, registration, photogrammetry,
                 morphing, facial animation, computer vision",
}

@InProceedings{EVL-1999-249,
  title =        "Creating Generative Models From Range Images",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "195--204",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-249",
  author =       "Ravi Ramamoorthi and James Arvo",
  abstract =     "We describe a new approach for creating concise
                 high-level generative models from range images or other
                 approximate representations of real objects. Using data
                 from a variety of acquisition techniques and a
                 user-defined class of models, our method produces a
                 compact object representation that is intuitive and
                 easy to edit. The algorithm has two inter-related
                 phases: recognition, which chooses an appropriate model
                 within a user-specified hierarchy, and parameter
                 estimation, which adjusts the model to best fit the
                 data. Since the approach is model-based, it is
                 relatively insensitive to noise and missing data. We
                 describe practical heuristics for automatically making
                 tradeoffs between simplicity and accuracy to select the
                 best model in a given hierarchy. We also describe a
                 general and efficient technique for optimizing a model
                 by refining its constituent curves. We demonstrate our
                 approach for model recovery using both real and
                 synthetic data and several generative model
                 hierarchies.",
  organization = "ACM Siggraph",
  keywords =     "Generative Models, Range Images, Curves and Surfaces,
                 Procedural Modeling",
}

@Article{EVL-1999-25,
  pages =        "3--16",
  year =         "1999",
  title =        "Linear complexity hexahedral mesh generation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-25",
  author =       "David Eppstein",
  abstract =     "We show that any polyhedron forming a topological ball
                 with an even number of quadrilateral sides can be
                 partitioned into O (n) topological cubes, meeting face
                 to face. The result generalizes to non-simply-connected
                 polyhedra satisfying an additional bipartiteness
                 condition. The same techniques can also be used to
                 reduce the geometric version of the hexahedral mesh
                 generation problem to a finite case analysis amenable
                 to machine solution.",
  month =        feb,
  volume =       "12",
  number =       "1-2",
  journal =      "Computational Geometry",
}

@InProceedings{EVL-1999-250,
  title =        "Environment Matting and Compositing",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Sigggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "205--214",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-250",
  author =       "Douglas E. Zongker and Dawn M. Werner and Brian
                 Curless and David H. Salesin.",
  abstract =     "This paper introduces a new process, environment
                 matting,which captures not just a foreground object and
                 its traditional opacity matte froma real-world scene,
                 but also a description of how that object refracts and
                 reflects light, which we call an environment matte. The
                 foreground object can then be placed in a new
                 environment, using environment compositing, where it
                 will refract and reflect light from that scene. Objects
                 captured in this way exhibit not only specular but
                 glossy and translucent effects, as well as selective
                 attenuation and scattering of light according to
                 wavelength. Moreover, the environment compositing
                 process, which can be performed largely with texture
                 mapping operations, is fast enough to run at
                 interactive speeds on a desktop PC. We compare our
                 results to photos of the same objects in real scenes.
                 Applications of this work include the relighting of
                 objects for virtual and augmented reality, more
                 realistic 3D clip art, and interactive lighting
                 design.",
  organization = "ACM Siggraph",
  keywords =     "environment matte, refraction, reflection, image-based
                 rendering, environment map, augmented reality,
                 interactive lighting design, clip art, alpha channel,
                 blue-screen matting, blue spill, colored transparency",
}

@InProceedings{EVL-1999-251,
  title =        "Inverse Global Illumination: Recovering Reflectance
                 Models of Real Scenes From Photographs From",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph99, Annual Conference Series",
  publisher =    "Addison Wesley Longman",
  pages =        "215--224",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-251",
  author =       "Yizhou Yu and Paul Debevec and Jitendra Malik and Tim
                 Hawkins",
  abstract =     "In this paper we present a method for recovering the
                 reflectance properties of all surfaces in a real scene
                 from a sparse set of photographs, taking into account
                 both direct and indirect illumination. The result is a
                 lighting-independent model of the scene's geometry and
                 reflectance properties, which can be rendered with
                 arbitrary modifications to structure and lighting via
                 traditional rendering methods. Our technique models
                 reflectance with a low-parameter reflectance model, and
                 allows diffuse albedo to vary arbitrarily over surfaces
                 while assuming that non-diffuse characteristics remain
                 constant across particular regions. The method's input
                 is a geometric model of the scene and a set of
                 calibrated high dynamic range photographs taken with
                 known direct illumination. The algorithm hierarchically
                 partitions the scene into a polygonal mesh, and uses
                 image-based rendering to construct estimates of both
                 the radiance and irradiance of each patch from the
                 photographic data. The algorithm computes the expected
                 location of specular highlights, and then analyzes the
                 highlight areas in the images by running a novel
                 iterative optimization procedure to recover the diffuse
                 and specular reflectance parameters for each region.
                 Lastly, these parameters are used in constructing
                 high-resolution diffuse albedo maps for each surface.
                 The algorithm has been applied to both real and
                 synthetic data, including a synthetic cubical room and
                 a real meeting room. Re-renderings are produced using a
                 global illumination system under both original and
                 novel lighting, and with the addition of synthetic
                 objects. Side-by-side comparisons show success at
                 predicting the appearance of the scene under novel
                 lighting conditions.",
  organization = "ACM Siggraph",
  keywords =     "Global Illumination, Image-Based Modeling and
                 Rendering, BRDF Models, Reflectance Recovery, Albedo
                 Maps, Radiance, Radiosity, Rendering",
}

@InProceedings{EVL-1999-252,
  title =        "Modeling and Rendering of Weathered Stone",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "225--234",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-252",
  author =       "Julie Dorsey and Alan Edelman and Justin Legakis and
                 Henrik Wann Jensen and Hans Kohling Pedersen",
  abstract =     "Stone is widespread in its use as a building material
                 and artistic medium. One of its most remarkable
                 qualities is that it changes appearance as it interacts
                 with the environment. These changes are mainly confined
                 to the surface but involve complex volumetric effects
                 such as erosion and mineral dissolution. This paper
                 presents an approach for the modeling and rendering of
                 changes in the shape and appearance of stone. To
                 represent stone, we introduce a slab data structure,
                 which is a surface-aligned volume confined to a narrow
                 region around the boundary of the stone. Our weathering
                 model employs a simulation of the flow of moisture and
                 the transport, dissolution, and recrystallization of
                 minerals within the porous stone volume. In addition,
                 this model governs the erosion of material from the
                 surface. To render the optical effects of translucency
                 and coloration due to the composition of minerals near
                 the surface, we simulate the scattering of light inside
                 the stone using a general subsurface Monte Carlo ray
                 tracer. These techniques can capture many aspects of
                 the time-dependent appearance of stone. We demonstrate
                 the approach with models of granite and marble statues,
                 as well as a sandstone column.",
  organization = "ACM Siggraph",
  keywords =     "erosion, material models, natural phenomena, physical
                 simulation, ray tracing, subsurface scattering,
                 texturing, volume modeling, weathering",
}

@InProceedings{EVL-1999-253,
  title =        "Pattern-Based Texturing Revisited",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "235--242",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-253",
  author =       "Fabrice Neyret and Marie-Paule Cani",
  abstract =     "We present a texturing method that correctly maps
                 homogeneous non-periodic textures to arbitrary surfaces
                 without any of the difficulties usually encountered
                 using existing tools. Our technique requires little
                 redundant designer work, has low time and memory costs
                 during rendering and provides high texture resolution.
                 The idea is simple: a few triangular texture samples,
                 which obey specific boundary conditions, are chosen
                 from the desired pattern and mapped in a non-periodic
                 fashion onto the surface. Our mapping algorithm enables
                 us to freely tune the scale of the texture with respect
                 to the object's geometry, while minimizing distortions.
                 Moreover, it yields singularity-free texturing whatever
                 the topology of the object. The sets of texture samples
                 may be created interactively from pictures or drawings.
                 We also provide two alternative methods for
                 automatically generating them, defined as extensions of
                 Perlin's and Worley's procedural texture synthesis
                 techniques. As our results show, the method produces
                 textured objects that look reasonable from any
                 viewpoint and can be used in real-time applications.",
  organization = "ACM Siggraph",
  keywords =     "Texture Mapping, Patterns, Texture Synthesis,
                 Non-periodic Tiling",
}

@InProceedings{EVL-1999-254,
  title =        "Feline: Fast Elliptical Lines for Anisotropic Texture
                 Mapping",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999",
  publisher =    "Addison Wesley Longman",
  pages =        "243--250",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-254",
  author =       "Joel McCormack and Ronald Perry and Keith I. Farkas
                 and Norman P. Jouppi",
  abstract =     "Texture mapping using trilinearly filtered mip-mapped
                 data is efficient and looks much better than
                 point-sampled or bilinearly filtered data. But
                 trilinear filtering represents the projection of a
                 pixel filter footprint from screen space into texture
                 space as a square, when in reality the footprint may be
                 long and narrow. Consequently, trilinear filtering
                 severely blurs images on surfaces angled obliquely away
                 from the viewer This paper describes a new texture
                 filtering technique called Feline (for Fast Elliptical
                 Lines). Like other recent hardware anisotropic
                 filtering algorithms, Feline uses an underlying
                 space-invariant (isotropic) filter with mip-mapped
                 data, and so can be built on top of an existing
                 trilinear filtering engine. To texture a pixel, it uses
                 this space-invariant filter at several points along a
                 line in texture space, and combines the results. With a
                 modest increase in implementation complexity over
                 earlier techniques, Feline more accurately matches the
                 desired projection of the pixel filter in texture
                 space, resulting in images with fewer aliasing
                 artifacts. Feline s visual quality compares well
                 against Elliptical Weighted Average, the best software
                 anisotropic texture filtering algorithm known to date,
                 but Feline requires much less setup computation and far
                 fewer cycles for texel fetches. Finally, since it uses
                 standard mip-maps, Feline requires minimal extensions
                 to standard 3D interfaces like OpenGL.",
  organization = "ACM Siggraph",
  keywords =     "texture mapping, anisotropic filtering, space-variant
                 filtering",
}

@InProceedings{EVL-1999-255,
  title =        "The VolumePro Real-Time Ray-casting System",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings,",
  publisher =    "Addison Wesley Longman",
  pages =        "251--260",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-255",
  author =       "Hanspeter Pfister and Jan Hardenbergh and Jim Knittel
                 and Hugh Lauer and Larry Seiler",
  abstract =     "This paper describes VolumePro, the world's first
                 single-chip real-time volume rendering system for
                 consumer PCs. VolumePro implements ray-casting with
                 parallel slice-by-slice processing. Our discussion of
                 the architecture focuses mainly on the rendering
                 pipeline and the memory organization. VolumePro has
                 hardware for gradient estimation, classification, and
                 per-sample Phong illumination. The system does not
                 perform any pre-processing and makes parameter
                 adjustments and changes to the volume data immediately
                 visible. We describe several advanced features of
                 VolumePro, such as gradient magnitude modulation of
                 opacity and illumination, supersampling, cropping and
                 cut planes. The system renders 500 million
                 interpolated, Phong illuminated, composited samples per
                 second. This is sufficient to render volumes with up to
                 16 million voxels (e.g., 2563) at 30 frames per
                 second.",
  organization = "ACM Siggraph",
  keywords =     "Graphics Hardware, Hardware Systems, Rendering
                 Hardware, Rendering Systems, Volume Renderin",
}

@InProceedings{EVL-1999-256,
  title =        "Deep Compression for Streaming Texture Intensive
                 Animations",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "261--268",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-256",
  author =       "Daniel Cohen-Or and Yair Mann and Shachar Fleishman",
  abstract =     "This paper presents a streaming technique for
                 synthetic texture intensive 3D animation sequences.
                 There is a short latency time while downloading the
                 animation, until an initial fraction of the compressed
                 data is read by the client. As the animation is played,
                 the remainder of the data is streamed online seamlessly
                 to the client. The technique exploits frame-to-frame
                 coherence for transmitting geometric and texture
                 streams. Instead of using the original textures of the
                 model, the texture stream consists of view-dependent
                 textures which are generated by rendering offline
                 nearby views. These textures have a strong temporal
                 coherency and can thus be well compressed. As a
                 consequence, the bandwidth of the stream of the
                 view-dependent textures is narrow enough to be
                 transmitted together with the geometry stream over a
                 low bandwidth network. These two streams maintain a
                 small online cache of geometry and view-dependent
                 textures from which the client renders the walk-through
                 sequence in real-time. The overall data transmitted
                 over the network is an order of magnitude smaller than
                 an MPEG post-rendered sequence with an equivalent image
                 quality.",
  organization = "ACM Siggraph",
  keywords =     "compression, MPEG, streaming, virtual environment,
                 image-based rendering",
}

@InProceedings{EVL-1999-257,
  title =        "Optimization of Mesh Locality for Transparent Vertex
                 Caching",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "269--276",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-257",
  author =       "Hugues Hoppe",
  abstract =     "Bus traffic between the graphics subsystem and memory
                 can become a bottleneck when rendering geometrically
                 complex meshes. In this paper, we investigate the use
                 of vertex caching to transparently reduce geometry
                 bandwidth. Use of an indexed triangle strip
                 representation permits application programs to animate
                 the meshes at video rates, and provides backward
                 compatibility on legacy hardware. The efficiency of
                 vertex caching is maximized by reordering the faces in
                 the mesh during a preprocess. We present two reordering
                 techniques, a fast greedy strip-growing algorithm and a
                 local optimization algorithm. The strip-growing
                 algorithm performs lookahead simulations of the cache
                 to adapt strip lengths to the cache capacity. The local
                 optimization algorithm improves this initial result by
                 exploring a set of perturbations to the face ordering.
                 The resulting cache miss rates are comparable to the
                 efficiency of the earlier mesh buffer scheme described
                 by Deering and Chow, even though the vertex cache is
                 not actively managed.",
  organization = "ACM Siggraph",
  keywords =     "geometry compression, triangle strips",
}

@InProceedings{EVL-1999-258,
  title =        "A Real Time Low-Latency Hardware Light-Field
                 Renderer",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annula Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "287--290",
  year =         "1999",
  author =       "Matthew J. P. Regan and Gavin S. P. Miller and Steven
                 M. Rubin and Chris Kogelnik",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-258",
  abstract =     "This paper describes the design and implementation of
                 an architecture for interactively viewing static light
                 fields with very low latency. The system was
                 deliberately over engineered to specifications much
                 tighter than expected necessary to eliminate
                 perceptible latency. This allowed us to relax the
                 specifications to the point at which human users began
                 to detect latency artifacts. We found empirically that
                 when interacting with a light field, human users began
                 to notice latency artifacts when the total system
                 latency is approximately 15 ms. Although the
                 architecture may not be used in practice, this result
                 should prove fundamental for designers of future
                 interactive graphics systems.",
  organization = "ACM Siggraph",
}

@InProceedings{EVL-1999-259,
  title =        "{LDI} Tree: {A} Hierarchical Representation for
                 Image-Based Rendering",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "291--298",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-259",
  author =       "Chun-Fa Chang and Gary Bishop and Anselmo Lastra",
  abstract =     "Using multiple reference images in 3D image warping
                 has been a challenging problem. Recently, the Layered
                 Depth Image (LDI) was proposed by Shade et al. to merge
                 multiple reference images under a single center of
                 projection, while maintaining the simplicity of warping
                 a single reference image. However it does not consider
                 the issue of sampling rate. We present the LDI tree,
                 which combines a hierarchical space partitioning scheme
                 with the concept of the LDI. It preserves the sampling
                 rates of the reference images by adaptively selecting
                 an LDI in the LDI tree for each pixel. While rendering
                 from the LDI tree, we only have to traverse the LDI
                 tree to the levels that are comparable to the sampling
                 rate of the output image. We also present a progressive
                 refinement feature and a {"}gap filling{"} algorithm
                 implemented by pre-filtering the LDI tree. We show that
                 the amount of memory required has the same order of
                 growth as the 2D reference images. This also bounds the
                 complexity of rendering time to be less than directly
                 rendering from all reference images.",
  organization = "ACM Siggraph",
  keywords =     "image-based rendering, hierarchical representation",
}

@Article{EVL-1999-26,
  year =         "1999",
  title =        "Combinatorial and experimental results for randomized
                 point matching algorithms",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-26",
  author =       "Sandy Irani and Prabhakar Raghavan",
  abstract =     "The subject of this paper is the design and analysis
                 of Monte Carlo algorithms for two basic matching
                 techniques used in model-based recognition: alignment,
                 and geometric hashing. We first give analyses of our
                 Monte Carlo algorithms, showing that they are
                 asymptotically faster than their deterministic
                 counterparts while allowing failure probabilities that
                 are provably very small. We then describe experimental
                 results that bear out this speed-up, suggesting that
                 randomization results in significant improvements in
                 running time. Our theoretical analyses are not the best
                 possible; as a step to remedying this we define a
                 combinatorial measure of self-similarity for point
                 sets, and give an example of its power.",
  month =        feb,
  volume =       "12",
  keywords =     "Model-based recognition; Geometric point matching;
                 Randomized algorithms",
  number =       "1-2",
  journal =      "Computational Geometry",
}

@InProceedings{EVL-1999-260,
  title =        "Rendering with Concentric Mosaics",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999",
  publisher =    "Addison Wesley Longman",
  pages =        "299--306",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-260",
  author =       "Heung-Yeung Shum and Li-Wei He",
  abstract =     "This paper presents a novel 3D plenoptic function,
                 which we call concentric mosaics. We constrain camera
                 motion to planar concentric circles, and create
                 concentric mosaics using a manifold mosaic for each
                 circle (i.e., composing slit images taken at different
                 locations). Concentric mosaics index all input image
                 rays naturally in 3 parameters: radius, rotation angle
                 and vertical elevation. Novel views are rendered by
                 combining the appropriate captured rays in an efficient
                 manner at rendering time. Although vertical distortions
                 exist in the rendered images, they can be alleviated by
                 depth correction. Like panoramas, concentric mosaics do
                 not require recovering geometric and photometric scene
                 models. Moreover, concentric mosaics provide a much
                 richer user experience by allowing the user to move
                 freely in a circular region and observe significant
                 parallax and lighting changes. Compared with a
                 Lightfield or Lumigraph, concentric mosaics have much
                 smaller file size because only a 3D plenoptic function
                 is constructed. Concentric mosaics have good space and
                 computational efficiency, and are very easy to capture.
                 This paper discusses a complete working system from
                 capturing, construction, compression, to rendering of
                 concentric mosaics from synthetic and real
                 environments.",
  organization = "ACM Siggraph",
  keywords =     "plenoptic functions, virtual environments, image-based
                 rendering",
}

@InProceedings{EVL-1999-261,
  title =        "Implicit Fairing of Irregular Meshes Using Diffusion
                 and Curvature Flow",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999",
  publisher =    "Addison Wesley Longman",
  pages =        "317--324",
  year =         "1999",
  author =       "Mathieu Desbrun and Mark Meyer and Peter
                 Schr{\"{o}}der and Alan H. Barr",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-261",
  abstract =     "In this paper, we develop methods to rapidly remove
                 rough features from irregularly triangulated data
                 intended to portray a smooth surface. The main task is
                 to remove undesirable noise and uneven edges while
                 retaining desirable geometric features. The problem
                 arises mainly when creating high-fidelity computer
                 graphics objects using imperfectly-measured data from
                 the real world. Our approach contains three novel
                 features: an implicit integration method to achieve
                 efficiency, stability, and large time-steps; a
                 scale-dependent Laplacian operator to improve the
                 diffusion process; and finally, a robust curvature flow
                 operator that achieves a smoothing of the shape itself,
                 distinct from any parameterization. Additional features
                 of the algorithm include automatic exact volume
                 preservation, and hard and soft constraints on the
                 positions of the points in the mesh. We compare our
                 method to previous operators and related algorithms,
                 and prove that our curvature and Laplacian operators
                 have several mathematically-desirable qualities that
                 improve the appearance of the resulting surface. In
                 consequence, the user can easily select the appropriate
                 operator according to the desired type of fairing.
                 Finally, we provide a series of examples to graphically
                 and numerically demonstrate the quality of our
                 results.",
  organization = "ACM Siggraph",
}

@InProceedings{EVL-1999-262,
  title =        "Automatic Image Placement to Provide a Guaranteed
                 Frame Rate",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "307--316",
  year =         "1999",
  author =       "Daniel G. Aliaga and Anselmo Lastra",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-262",
  abstract =     "We present a preprocessing algorithm and run-time
                 system for rendering 3D geometric models at a
                 guaranteed frame rate. Our approach trades off space
                 for frame rate by using images to replace distant
                 geometry. The preprocessing algorithm automatically
                 chooses a subset of the model to display as an image so
                 as to render no more than a specified number of
                 geometric primitives. We also summarize an optimized
                 layered-depth-image warper to display images surrounded
                 by geometry at run time. Furthermore, we show the
                 results of applying our method to accelerate the
                 interactive walkthrough of several complex models.",
  organization = "ACM Siggraph",
}

@InProceedings{EVL-1999-263,
  title =        "Multiresolution Signal Processing for Meshes",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "325--334",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-263",
  author =       "Igor Guskov and Wim Sweldens and Peter
                 Schr{\"{o}}der",
  abstract =     "We generalize basic signal processing tools such as
                 downsampling, upsampling, and filters to irregular
                 connectivity triangle meshes. This is accomplished
                 through the design of a non-uniform relaxation
                 procedure whose weights depend on the geometry and we
                 show its superiority over existing schemes whose
                 weights depend only on connectivity. This is combined
                 with known mesh simplification methods to build
                 subdivision and pyramid algorithms. We demonstrate the
                 power of these algorithms through a number of
                 application examples including smoothing, enhancement,
                 editing, and texture mapping.",
  organization = "ACM Siggraph",
  keywords =     "Meshes, subdivision, irregular connectivity, surface
                 parameterization, multiresolution, wavelets, Laplacian
                 Pyramid",
}

@InProceedings{EVL-1999-264,
  title =        "Shape Transformation Using Variational Implicit
                 Functions",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999",
  publisher =    "Addison Wesley Longman",
  pages =        "1999",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-264",
  author =       "Greg Turk and James O'Brien",
  abstract =     "Traditionally, shape transformation using implicit
                 functions is performed in two distinct steps: 1)
                 creating two implicit functions, and 2) interpolating
                 between these two functions. We present a new shape
                 transformation method that combines these two tasks
                 into a single step. We create a transformation between
                 two N-dimensional objects by casting this as a
                 scattered data interpolation problem in N + 1
                 dimensions. For the case of 2D shapes, we place all of
                 our data constraints within two planes, one for each
                 shape. These planes are placed parallel to one another
                 in 3D. Zero-valued constraints specify the locations of
                 shape boundaries and positive-valued constraints are
                 placed along the normal direction in towards the center
                 of the shape. We then invoke a variational
                 interpolation technique (the 3D generalization of
                 thin-plate interpolation), and this yields a single
                 implicit function in 3D. Intermediate shapes are simply
                 the zero-valued contours of 2D slices through this 3D
                 function. Shape transformation between 3D shapes can be
                 performed similarly by solving a 4D interpolation
                 problem. To our knowledge, ours is the first shape
                 transformation method to unify the tasks of implicit
                 function creation and interpolation. The
                 transformations produced by this method appear smooth
                 and natural, even between objects of differing
                 topologies. If desired, one or more additional shapes
                 may be introduced that influence the intermediate
                 shapes in a sequence. Our method can also reconstruct
                 surfaces from multiple slices that are not restricted
                 to being parallel to one another.",
  organization = "ACM Siggraph",
  keywords =     "Shape transformation, shape morphing, contour
                 interpolation, implicit surfaces, thin-plate
                 techniques",
}

@InProceedings{EVL-1999-265,
  title =        "Multiresolution Mesh Morphing",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "343--350",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-265",
  author =       "Aaron Lee and David Dobkin and Wim Sweldens and Peter
                 Schr{\"{o}}der",
  abstract =     "We present a new method for user controlled morphing
                 of two homeomorphic triangle meshes of arbitrary
                 topology. In particular we focus on the problem of
                 establishing a correspondence map between source and
                 target meshes. Our method employs the MAPS algorithm to
                 parameterize both meshes over simple base domains and
                 an additional harmonic map bringing the latter into
                 correspondence. To control the mapping the user
                 specifies any number of feature pairs, which control
                 the parameterizations produced by the MAPS algorithm.
                 Additional controls are provided through a direct
                 manipulation interface allowing the user to tune the
                 mapping between the base domains. We give several
                 examples of aesthetically pleasing morphs which can be
                 created in this manner with little user input.
                 Additionally we demonstrate examples of temporal and
                 spatial control over the morph.",
  organization = "ACM Siggraph",
  keywords =     "Meshes, surface parameterization, mesh simplification,
                 multiresolution, interpolation, morphing",
}

@InProceedings{EVL-1999-266,
  title =        "Balancing Fusion, Image Depth and Distortion in
                 Stereoscopic Head-Tracked Displays",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "351--358",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-266",
  author =       "Zachary Wartell and Larry F. Hodges and William
                 Ribarsky",
  abstract =     "Stereoscopic display is a fundamental part of virtual
                 reality HMD systems and HTD (head-tracked display)
                 systems such as the virtual workbench and the CAVE. A
                 common practice in stereoscopic systems is deliberate
                 incorrect modeling of user eye separation.
                 Underestimating eye separation is frequently necessary
                 for the human visual system to fuse stereo image pairs
                 into single 3D images, while overestimating eye
                 separation enhances image depth. Unfortunately, false
                 eye separation modeling also distorts the perceived 3D
                 image in undesirable ways. This paper makes three
                 fundamental contributions to understanding and
                 controlling this stereo distortion. (1) We analyze the
                 distortion using a new analytic description. This
                 analysis shows that even with perfect head tracking, a
                 user will perceive virtual objects to warp and shift as
                 she moves her head. (2) We present a new technique for
                 counteracting the shearing component of the distortion.
                 (3) We present improved methods for managing image
                 fusion problems for distant objects and for enhancing
                 the depth of flat scenes.",
  organization = "ACM Siggraph",
  keywords =     "virtual reality, stereoscopic display, head-tracking,
                 image distortion",
}

@InProceedings{EVL-1999-267,
  title =        "Walking > Walking-in-Place > Flying, in Virtual
                 Environments",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "359--364",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-267",
  author =       "Martin Usoh and Kevin Arthur and Mary C. Whitton and
                 Rui Bastos and Anthony Steed and Mel Slater and Jr.
                 Frederick P. Brooks",
  abstract =     "A study by Slater, et al., [1995] indicated that naive
                 subjects in an immersive virtual environment experience
                 a higher subjective sense of presence when they
                 locomote by walking-in-place (virtual walking) than
                 when they push-button-fly (along the floor plane). We
                 replicated their study, adding real walking as a third
                 condition. Our study confirmed their findings. We also
                 found that real walking is significantly better than
                 both virtual walking and flying in ease (simplicity,
                 straightforwardness, naturalness) as a mode of
                 locomotion. The greatest difference in subjective
                 presence was between flyers and both kinds of walkers.
                 In addition, subjective presence was higher for real
                 walkers than virtual walkers, but the difference was
                 statistically significant only in some models.
                 Follow-on studies show virtual walking can be
                 substantially improved by detecting footfalls with a
                 head accelerometer. As in the Slater study, subjective
                 presence significantly correlated with subjects' degree
                 of association with their virtual bodies (avatars).
                 This, our strongest statistical result, suggests that
                 substantial potential presence gains can be had from
                 tracking all limbs and customizing avatar appearance.
                 An unexpected by-product was that real walking through
                 our enhanced version of Slater's visual-cliff virtual
                 environment (Figure 1) yielded a strikingly compelling
                 virtual experiencethe strongest we and most of our
                 visitors have yet experienced. The most needed system
                 improvement is the substitution of wireless technology
                 for all links to the user.",
  organization = "ACM Siggraph",
  keywords =     "Presence, Locomotion, Virtual Reality, Virtual
                 Walking, Human Factors, Neural Networks, Visual Cliff",
}

@InProceedings{EVL-1999-268,
  title =        "Real-Time Acoustic Modeling for Distributed Virtual
                 Environments",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longma",
  pages =        "365--374",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-268",
  author =       "Thomas A. Funkhouser and Patrick Min and Ingrid
                 Carlbom",
  abstract =     "Realistic acoustic modeling is essential for
                 spatializing sound in distributed virtual environments
                 where multiple networked users move around and interact
                 visually and aurally in a shared virtual world.
                 Unfortunately, current methods for computing accurate
                 acoustical models are not fast enough for real-time
                 auralization of sounds for simultaneously moving
                 sources and receivers. In this paper, we present three
                 new beam tracing algorithms that greatly accelerate
                 computation of reverberation paths in a distributed
                 virtual environment by taking advantage of the fact
                 that sounds can only be generated or heard at the
                 positions of {"}avatars{"} representing the users. The
                 priority-driven beam tracing algorithm performs a
                 best-first search of a cell adjacency graph, and thus
                 enables new termination criteria with which all early
                 reflection paths can be found very efficiently. The
                 bidirectional beam tracing algorithm combines sets of
                 beams traced from pairs of avatar locations to find
                 reverberation paths between them while requiring
                 significantly less computation than previous
                 unidirectional algorithms. The amortized beam tracing
                 algorithm computes beams emanating from box-shaped
                 regions of space containing predicted avatar locations
                 and re-uses those beams multiple times to compute
                 reflections paths as each avatar moves inside the box.
                 Cumulatively, these algorithms enable speedups of
                 approximately two orders of magnitude over previous
                 methods. They are incorporated into a time-critical
                 multiprocessing system that allocates its computational
                 resources dynamically in order to compute the highest
                 priority reverberation paths between moving avatar
                 locations in real-time with graceful degradation and
                 adaptive refinement.",
  organization = "ACM Siggraph",
  keywords =     "Virtual environment systems, virtual reality, acoustic
                 modeling, auralization, beam tracing",
}

@InProceedings{EVL-1999-269,
  title =        "Six Degrees-of-Freedom Haptic Rendering Using Voxel
                 Sampling",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999",
  publisher =    "Addison Wesley Longman",
  pages =        "401--408",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-269",
  author =       "William A. McNeely and Kevin D. Puterbaugh and James
                 J. Troy",
  abstract =     "A simple, fast, and approximate voxel-based approach
                 to 6-DOF haptic rendering is presented. It can reliably
                 sustain a 1000 Hz haptic refresh rate without resorting
                 to asynchronous physics and haptic rendering loops. It
                 enables the manipulation of a modestly complex rigid
                 object within an arbitrarily complex environment of
                 static rigid objects. It renders a short-range force
                 field surrounding the static objects, which repels the
                 manipulated object and strives to maintain a
                 voxel-scale minimum separation distance that is known
                 to preclude exact surface interpenetration. Force
                 discontinuities arising from the use of a simple
                 penalty force model are mitigated by a dynamic
                 simulation based on virtual coupling. A generalization
                 of octree improves voxel memory efficiency. In a
                 preliminary implementation, a commercially available
                 6-DOF haptic prototype device is driven at a constant
                 1000 Hz haptic refresh rate from one dedicated haptic
                 processor, with a separate processor for graphics. This
                 system yields stable and convincing force feedback for
                 a wide range of user controlled motion inside a large,
                 complex virtual environment, with very few surface
                 inter-penetration events. This level of performance
                 appears suited to applications such as certain
                 maintenance and assembly task simulations that can
                 tolerate voxel-scale minimum separation distances.",
  organization = "ACM Siggraph",
  keywords =     "force feedback, voxel representations, virtual
                 environments",
}

@Article{EVL-1999-27,
  pages =        "33--44",
  year =         "1999",
  title =        "On the bit complexity of minimum link paths:
                 Superquadratic algorithms for problem solvable in
                 linear time",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-27",
  author =       "Simon Kahan and Jack Snoeyink",
  abstract =     "All of the linear-time algorithms that have been
                 developed for minimum-link paths use the real RAM model
                 of computation. If one considers bit complexity,
                 however, merely representing a minimum-link path may
                 require a superquadratic number of bits. This paper
                 considers bounds on the number of links (segments)
                 needed by limited-precision approximations of
                 minimum-link paths: When vertices are restricted to
                 {"}first-derived{"} points, the number of links can
                 increase by a constant factor; when they are restricted
                 to points of an N×N grid, the number of links can
                 increase by $\Theta(log N)$.",
  month =        feb,
  volume =       "12",
  keywords =     "Simple polygons; Link paths; Exact geometric
                 computation; Grid geometry",
  number =       "1-2",
  journal =      "Computational Geometry",
}

@InProceedings{EVL-1999-270,
  title =        "Creating a Live Broadcast from a Virtual Environment",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "375--384",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-270",
  author =       "Chris Greenhalgh and Steve Benford and Ian Taylor and
                 John Bowers and Graham Walker and John Wyver",
  abstract =     "{"}Inhabited Television{"} combines multiuser virtual
                 environments with television, so that online
                 audience-members can participate in TV shows staged in
                 a virtual world. It is presented simultaneously both to
                 conventional passive viewers and to online
                 participants. In many cases it benefits from being
                 broadcast live. This paper is based on our fourth major
                 experiment with Inhabited TV, a live virtual game show
                 called {"}Out Of This World.{"} For this event we
                 adopted non-automated approaches to camera control and
                 mixing to allow an exploration of appropriate forms of
                 presentation for inhabited television. We describe the
                 techniques which were used to create and enhance the
                 live video output which was produced during the show:
                 appropriate world design; dynamic constraints on
                 participant movements; and a performance-oriented
                 virtual camera control interface. This camera control
                 interface includes explicit support for a range of
                 spatial and temporal control styles. We also give
                 evaluative feedback on the camera control interface and
                 the event's (television-based) approach to mixing and
                 directing, drawing on a social scientific field study
                 conducted on-site during the preparation for, and
                 performances of, the show.",
  organization = "ACM Siggraph",
  keywords =     "Multi-User, Networked Apps, Video, Viewpoint control",
}

@InProceedings{EVL-1999-271,
  title =        "Emancipated Pixels: Real-World Graphics in the
                 Luminous Room",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "385--392",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-271",
  author =       "John Underkoffler and Brygg Ullmer and Hiroshi Ishii",
  abstract =     "We describe a conceptual infrastructure - the Luminous
                 Room - for providing graphical display and interaction
                 at each of an interior architectural space's various
                 surfaces, arguing that pervasive environmental output
                 and input is one natural heir to today's rather more
                 limited notion of spatially-confined, output-only
                 display (the CRT). We discuss the requirements of such
                 real-world graphics, including computational &
                 networking demands; schemes for spatially omnipresent
                 capture and display; and issues of design and
                 interaction that emerge under these new circumstances.
                 These discussions are both illustrated and motivated by
                 five particular applications that have been built for a
                 real, experimental Luminous Room space, and by details
                 of the current technical approach to its construction
                 (involving a two-way optical transducer called an I/O
                 Bulb that projects and captures pixels).",
  organization = "ACM Siggraph",
  keywords =     "real-world graphics, luminous-tangible interfaces,
                 projection, computer vision, architectural space, CAD",
}

@InProceedings{EVL-1999-272,
  title =        "Skin: {A} Constructive Approach to Modeling Free-form
                 Shapes",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "393--400",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-272",
  author =       "Lee Markosian and Jonathan M. Cohen and Thomas Crulli
                 and John F. Hughes",
  abstract =     "We present a new particle-based surface representation
                 with which a user can interactively sculpt free-form
                 surfaces. The particles maintain mesh connectivity and
                 operate under rules that lead them to form
                 triangulations with properties that make them suitable
                 for use in subdivision. A user interactively guides the
                 particles, which we call skin, to grow over a given
                 collection of polyhedral elements (or skeletons),
                 yielding a smooth surface (through subdivision) that
                 approximates the underlying skeletal shapes. Skin
                 resembles blobby modeling in the constructive approach
                 to modeling it supports, but allows a richer vocabulary
                 of skeleton shapes, supports sharp creases where
                 desired, and provides a convenient mechanism for adding
                 multiresolution surface detail.",
  organization = "ACM Siggraph",
  keywords =     "Free-form modeling, meshes, subdivision,
                 multiresolution",
}

@InProceedings{EVL-1999-273,
  title =        "Teddy: {A} Sketching Interface for 3{D} Freeform
                 Design",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Srries",
  booktitle =    "Siggraph 1999",
  publisher =    "Addison Wesley Longman",
  pages =        "409--416",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-273",
  author =       "Takeo Igarashi and Satoshi Matsuoka and Hidehiko
                 Tanaka",
  abstract =     "We present a sketching interface for quickly and
                 easily designing freeform models such as stuffed
                 animals and other rotund objects. The user draws
                 several 2D freeform strokes interactively on the screen
                 and the system automatically constructs plausible 3D
                 polygonal surfaces. Our system supports several
                 modeling operations, including the operation to
                 construct a 3D polygonal surface from a 2D silhouette
                 drawn by the user: it inflates the region surrounded by
                 the silhouette making wide areas fat, and narrow areas
                 thin. Teddy, our prototype system, is implemented as a
                 Java program, and the mesh construction is done in
                 real-time on a standard PC. Our informal user study
                 showed that a first-time user typically masters the
                 operations within 10 minutes, and can construct
                 interesting 3D models within minutes.",
  organization = "ACM Siggraph",
  keywords =     "3D modeling, sketching, pen-based systems, gestures,
                 design, chordal axes, inflation",
}

@InProceedings{EVL-1999-274,
  title =        "Digital Facial Engraving",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer graphics Proceedings",
  publisher =    "Los Angeles",
  pages =        "417--424",
  year =         "1999",
  author =       "Victor Ostromoukhov",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-274",
  abstract =     "This contribution introduces the basic techniques for
                 digital facial engraving, which imitates traditional
                 copperplate engraving. Inspired by traditional
                 techniques, we first establish a set of basic rules
                 thanks to which separate engraving layers are built on
                 the top of the original photo. Separate layers are
                 merged according to simple merging rules and according
                 to range shift/scale masks specially introduced for
                 this purpose. We illustrate the introduced technique by
                 a set of black/white and color engravings, showing
                 different features such as engraving-specific image
                 enhancements, mixing different regular engraving lines
                 with mezzotint, irregular perturbations of engraving
                 lines etc. We introduce the notion of engraving style
                 which comprises a set of separate engraving layers
                 together with a set of associated range shift/scale
                 masks. The engraving style helps to port the look and
                 feel of one engraving to another. Once different
                 libraries of pre-defined mappable engraving styles and
                 an appropriate user interface are added to the basic
                 system, producing a decent gravure starting from a
                 simple digital photo will be a matter of seconds. The
                 engraving technique described in this contribution
                 opens new perspectives for digital art, adding
                 unprecedented power and precision to the engraver's
                 work.",
  organization = "ACM Siggraph",
  keywords =     "photorealistic rendering, nonphotorealistic rendering,
                 halftoning, dithering, digital engraving",
}

@InProceedings{EVL-1999-275,
  title =        "Multi-Color and Artistic Dithering",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "425--432",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-275",
  author =       "Victor Ostromoukhov and Roger D. Hersch",
  abstract =     "A multi-color dithering algorithm is proposed, which
                 converts a barycentric combination of color intensities
                 into a multi-color non-overlapping surface coverage.
                 Multi-color dithering is a generalization of standard
                 bi-level dithering. Combined with tetrahedral color
                 separation, multi-color dithering makes it possible to
                 print images made of a set of non-standard inks. In
                 contrast to most previous color halftoning methods,
                 multi-color dithering ensures by construction that the
                 different selected basic colors are printed side by
                 side. Multi-color dithering is applied to generate
                 color images whose screen dots are made of artistic
                 shapes (letters, symbols, ornaments, etc.). Two dither
                 matrix postprocessing techniques are developed, one for
                 enhancing the visibility of screen motives and one for
                 the local equilibration of large dither matrices. The
                 dither matrix equilibration process corrects disturbing
                 local intensity variations by taking dot gain and the
                 human visual system transfer function into account.
                 Thanks to the combination of the presented techniques,
                 high quality images can be produced, which incorporate
                 at the micro level the desired artistic screens and at
                 the macro level the full color image. Applications
                 include designs for advertisements and posters as well
                 as security printing. Multi-color dithering also offers
                 new perspectives for printing with special inks, such
                 as fluorescent and metallic inks.",
  organization = "ACM Siggraph",
  keywords =     "color halftoning, artistic dithering, dither matrix
                 equilibration, non-standard inks, side by side
                 printing",
}

@InProceedings{EVL-1999-276,
  title =        "Art-Based Rendering of Fur, Grass, and Trees",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "433--438",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-276",
  author =       "Michael A. Kowalski and Lee Markosian and J. D.
                 Northrup and Lubomir Bourdev and Ronen Barzel and
                 Loring S. Holden and John Hughes",
  abstract =     "Artists and illustrators can evoke the complexity of
                 fur or vegetation with relatively few well-placed
                 strokes. We present an algorithm that uses strokes to
                 render 3D computer graphics scenes in a stylized manner
                 suggesting the complexity of the scene without
                 representing it explicitly. The basic algorithm is
                 customizable to produce a range of effects including
                 fur, grass and trees, as we demonstrate in this paper
                 and accompanying video. The algorithm is implemented
                 within a broader framework that supports procedural
                 stroke-based textures on polyhedral models. It renders
                 mod-erately complex scenes at multiple frames per
                 second on current graphics workstations, and provides
                 some interframe coherence.",
  organization = "ACM Siggraph",
  keywords =     "Non-photorealistic rendering, graftals, procedural
                 textures",
}

@InProceedings{EVL-1999-277,
  title =        "View-Dependent Geometry",
  address =      "Los Angeles",
  editor =       "Alyn Rockwood",
  series =       "Annual Conference Series",
  booktitle =    "Siggraph 1999, Computer Graphics Proceedings",
  publisher =    "Addison Wesley Longman",
  pages =        "439--446",
  year =         "1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-277",
  author =       "Paul Rademacher",
  abstract =     "When constructing 3D geometry for use in cel
                 animation, the reference drawings of the object or
                 character often contain various view-specific
                 distortions, which cannot be captured with conventional
                 3D models. In this work we present a technique called
                 View-Dependent Geometry, wherein a 3D model changes
                 shape based on the direction it is viewed from. A
                 view-dependent model consists of a base model, a set of
                 key deformations (deformed versions of the base model),
                 and a set of corresponding key viewpoints (which relate
                 each 2D reference drawing to the 3D base model). Given
                 an arbitrary viewpoint, our method interpolates the key
                 deformations to generate a 3D model that is specific to
                 the new viewpoint, thereby capturing the view-dependent
                 distortions of the reference drawings.",
  organization = "ACM Siggraph",
  keywords =     "Cartoon animation, 3D animation, rendering, animation
                 systems, non-photorealistic rendering, 3D blending",
}

@InProceedings{EVL-1999-278,
  pages =        "19--26",
  year =         "1999",
  title =        "Construction of Vector Field Hierarchies",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-278",
  author =       "Bjoern Heckel and Gunther H. Weber and Bernd Hamann
                 and Kenneth I. Joy",
  abstract =     "We present a method for the hierarchical
                 representation of vector fields. Our approach is based
                 on iterative refinement using clustering and principal
                 component analysis. The input to our algorithm is a
                 discrete set of points with associated vectors. The
                 algorithm generates a top-down segmentation of the
                 discrete field by splitting clusters of points. We
                 measure the error of the various approximation levels
                 by measuring the discrepancy between streamlines
                 generated by the original discrete field and its
                 approximations based on much smaller discrete data
                 sets. Our method assumes no particular structure of the
                 field, nor does it require any topological connectivity
                 information. It is possible to generate multiresolution
                 representations of vector fields using this approach.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "vector field visualization; Hardy's multiquadric
                 method; binary-space partitioning; data
                 simplification",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-279,
  pages =        "27--34",
  year =         "1999",
  title =        "Large Field Visualization With Demand-Driven
                 Calculation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-279",
  author =       "Patrick J. Moran and Chris Henze",
  abstract =     "We present a system designed for the interactive
                 definition and visualization of fields derived from
                 large data sets: the Demand-Driven Visualizer (DDV).
                 The system allows the user to write arbitrary
                 expressions to define new fields, and then apply a
                 variety of visualization techniques to the result.
                 Expressions can include differential operators and
                 numerous other built-in functions. Determination of
                 field values, both in space and in time, is directed
                 automatically by the demands of the visualization
                 techniques. The payoff of following a demand-driven
                 design philosophy throughout the visualization system
                 becomes particularly evident when working with large
                 time-series data, where the costs of eager evaluation
                 alternatives can be prohibitive.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "Edited by David Ebert and Markus Gross and Bernd
                 Hamann",
  keywords =     "arge scale visualization, scientific visualization,
                 interactive visualization, demand-driven evaluation,
                 lazy evaluation, interpreted systems, Python",
  booktitle =    "IEEE Visualization '99",
}

@Article{EVL-1999-28,
  pages =        "45--62",
  year =         "1999",
  title =        "Partial surface matching by using directed
                 footprints",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-28",
  author =       "Gill Barequet and Micha Sharir",
  abstract =     "In this paper we present a new technique for partial
                 surface and volume matching of images in three
                 dimensions. In this problem, we are given two objects
                 in 3-space, each represented as a set of points,
                 scattered uniformly along its boundary or inside its
                 volume. The goal is to find a rigid motion of one
                 object which makes a sufficiently large portion of its
                 boundary lying sufficiently close to a corresponding
                 portion of the boundary of the second object. This is
                 an important problem in pattern recognition and in
                 computer vision, with many industrial, medical, and
                 chemical applications. Our algorithm is based on
                 assigning a directed footprint to every point of the
                 two sets, and locating all the pairs of points (one of
                 each set) whose undirected components of the footprints
                 are sufficiently similar. The algorithm then computes
                 for each such pair of points all the rigid
                 transformations that map the first point to the second,
                 while making the respective direction components of
                 their footprints coincide. A voting scheme is employed
                 for computing transformations which map significantly
                 large number of points of the first set to points of
                 the second set. Experimental results on various
                 examples are presented and show the accurate and robust
                 performance of our algorithm.",
  month =        feb,
  volume =       "12",
  keywords =     "Geometric hashing; Computer vision; Pattern
                 recognition; Partial surface matching",
  number =       "1-2",
  journal =      "Computational Geometry",
}

@InProceedings{EVL-1999-280,
  pages =        "35--42",
  year =         "1999",
  title =        "Simplified Representation of Vector Fields",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-280",
  author =       "Alexandru Telea and Jarke J. van Wijk",
  abstract =     "Vector field visualization remains a difficult task.
                 Although many local and global visualization methods
                 for vector fields such as flow data exist, they usually
                 require extensive user experience on setting the
                 visualization parameters in order to produce images
                 communicating the desired insight. We present a
                 visualization method that produces simplified but
                 suggestive images of the vector field automatically,
                 based on a hierarchical clustering of the input data.
                 The resulting clusters are then visualized with
                 straight or curved arrow icons. The presented method
                 has a few parameters with which users can produce
                 various simplified vector field visualizations that
                 communicate different insights on the vector data",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "Flow Visualization, Simplification, Clustering",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-281,
  pages =        "43--50",
  year =         "1999",
  title =        "Hierarchical Parallel Coordinates for Exploration of
                 Large Datasets",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-281",
  author =       "Ying-Huey Fua and Matthew O. Ward and Elke A.
                 Rundensteiner",
  abstract =     "Our ability to accumulate large, complex
                 (multivariate) data sets has far exceeded our ability
                 to effectively process them in search of patterns,
                 anomalies, and other interesting features. Conventional
                 multivariate visualization techniques generally do not
                 scale well with respect to the size of the data set.
                 The focus of this paper is on the interactive
                 visualization of large multivariate data sets based on
                 a number of novel extensions to the parallel
                 coordinates display technique. We develop a
                 multiresolutional view of the data via hierarchical
                 clustering, and use a variation on parallel coordinates
                 to convey aggregation information for the resulting
                 clusters. Users can then navigate the resulting
                 structure until the desired focus region and level of
                 detail is reached, using our suite of navigational and
                 filtering tools. We describe the design and
                 implementation of our hierarchical parallel coordinates
                 system which is based on extending the XmdvTool system.
                 Lastly, we show examples of the tools and techniques
                 applied to large (hundreds of thousands of records)
                 multivariate data sets.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "Large-scale multivariate data visualization,
                 hierarchical data exploration, parallel coordinates",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-282,
  pages =        "67--72",
  year =         "1999",
  title =        "Progressive Compression of Arbitrary Triangular
                 Meshes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-282",
  author =       "Daniel Cohen-Or and David Levin and Offir Remez",
  abstract =     "In this paper we present a mesh compression method
                 based on a multiresolution decomposition whose detail
                 coefficients have a compact representation and thus
                 smaller entropy than the original mesh. Given an
                 arbitrary triangular mesh with an irregular
                 connectivity, we use a hierarchical simplification
                 scheme, which generates a multiresolution model. By
                 reversing the process we define a hierarchical
                 progressive refinement process, where a simple
                 prediction plus a correction is used for inserting
                 vertices to form a finer level. We show how the
                 connectivity of an arbitrary triangulation can be
                 encoded efficiently by a coloring technique, and
                 recovered incrementally during the progressive
                 reconstruction of the original mesh.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "compression, streaming, progressive meshes,
                 simplification",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-283,
  pages =        "51--58",
  year =         "1999",
  title =        "Tetrahedral Mesh Compression with the Cut-Border
                 Machine",
  author =       "Stefan Gumhold and Stefan Guthe and Wolfgang
                 Stra{\ss{}}er",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-283",
  abstract =     "In recent years, substantial progress has been
                 achieved in the area of volume visualization on
                 irregular grids, which is mainly based on tetrahedral
                 meshes. Even moderately fine tetrahedral meshes consume
                 several mega-bytes of storage. For archivation and
                 transmission compression algorithms are essential. In
                 scientific applications lossless compression schemes
                 are of primary interest. This paper introduces a new
                 lossless compression scheme for the connectivity of
                 tetrahedral meshes. Our technique can handle all
                 tetrahedral meshes in three dimensional euclidean space
                 even with non manifold border. We present compression
                 and decompression algorithms which consume for
                 reasonable meshes linear time in the number of
                 tetrahedra. The connectivity is compressed to less than
                 2.4 bits per tetrahedron for all measured meshes. Thus
                 a tetrahedral mesh can almost be reduced to the vertex
                 coordinates, which consume in a common representation
                 about one quarter of the total storage space. We
                 complete our work with solutions for the compression of
                 vertex coordinates and additional attributes, which
                 might be attached to the mesh.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "compression algorithms, solid modeling, scientific
                 visualization, volume rendering",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-284,
  pages =        "59--66",
  year =         "1999",
  title =        "New Quadric Metric for Simplifying Meshes with
                 Appearance Attributes",
  author =       "Hugues H. Hoppe",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-284",
  abstract =     "Complex triangle meshes arise naturally in many areas
                 of computer graphics and visualization. Previous work
                 has shown that a quadric error metric allows fast and
                 accurate geometric simplification of meshes. This
                 quadric approach was recently generalized to handle
                 meshes with appearance attributes. In this paper we
                 present an improved quadric error metric for
                 simplifying meshes with attributes. The new metric,
                 based on geometric correspondence in 3D, requires less
                 storage, evaluates more quickly, and results in more
                 accurate simplified meshes. Meshes often have attribute
                 discontinuities, such as surface creases and material
                 boundaries, which require multiple attribute vectors
                 per vertex. We show that a wedge-based mesh data
                 structure captures such discontinuities efficiently and
                 permits simultaneous optimization of these multiple
                 attribute vectors. In addition to the new quadric
                 metric, we experiment with two techniques proposed in
                 geometric simplification, memoryless simplification and
                 volume preservation, and show that both of these are
                 beneficial within the quadric framework. The new scheme
                 is demonstrated on a variety of meshes with colors and
                 normals.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "level of detail, mesh decimation, multiresolution",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-285,
  pages =        "81--88",
  year =         "1999",
  title =        "Image Graphs - {A} Novel Approach to Visual Data
                 Exploration",
  author =       "Kwan-Liu Ma",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-285",
  abstract =     "For types of data visualization where the cost of
                 producing images is high, and the relationship between
                 the rendering parameters and the image produced is less
                 than obvious, a visual representation of the
                 exploration process can make the process more effcient
                 and effective. Image graphs represent not only the
                 results but also the process of data visualization.
                 Each node in an image graph consists of an image and
                 the corresponding visualization parameters used to
                 produce it. Each edge in a graph shows the change in
                 rendering parameters between the two nodes it connects.
                 Image graphs are not just static representations: users
                 can interact with a graph to review a previous
                 visualization session or to perform new rendering.
                 Operations which cause changes in rendering parameters
                 can propagate through the graph. The user can take
                 advantage of the information in image graphs to
                 understand how certain parameter changes affect
                 visualization results. Users can also share image
                 graphs to streamline the process of collaborative
                 visualization. We have implemented a volume
                 visualization system using the image graph interface,
                 and our examples in the paper come from this
                 application.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "knowledge representations, scientific visualization,
                 visualization systems, volume renderin",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-286,
  pages =        "89--96",
  year =         "1999",
  title =        "Forward Image Warping",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-286",
  author =       "Baoquan Chen and Frank Dachille and Arie E. Kaufman",
  abstract =     "We present a new forward image mapping algorithm,
                 which speeds up perspective warping - as in texture
                 mapping. It processes the source image in a special
                 scanline order instead of the normal raster scanline
                 order. This special scanline has the property of
                 preserving parallelism when projecting to the target
                 image. The algorithm reduces the complexity of
                 perspective-correct image warping by eliminating the
                 division per pixel and replacing it with a division per
                 scanline. The method also corrects the perspective
                 distortion in Gouraud shading with negligible overhead.
                 Furthermore, the special scanline order is suitable for
                 antialiasing using a more accurate antialiasing conic
                 filter, with minimum additional cost. The algorithm is
                 highlighted by incremental calculations and optimized
                 memory bandwidth by reading each source pixel only
                 once, suggesting a potential hardware implementation.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "image warping, forward mapping, texture mapping,
                 antialiasing, anisotropic filtering, Gouraud shading,
                 hardware",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-287,
  pages =        "97--104",
  year =         "1999",
  title =        "Structured Spatial Domain Image and Data Comparison
                 Metrics",
  author =       "Nivedita Sahasrabudhe and John E. West and Raghu
                 Machiraju and Mark Janus",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-287",
  abstract =     "Often, images or datasets have to be compared, to
                 facilitate choices of visualization and simulation
                 parameters respectively. Common comparison techniques
                 include side-by-side viewing and juxtaposition, in
                 order to facilitate visual verification of
                 verisimilitude. In this paper, we propose quantitative
                 techniques which accentuate differences in images and
                 datasets. The comparison is enabled through a
                 collection of partial metrics which, essentially,
                 measure the lack of correlation between the datasets or
                 images being compared. That is, they attempt to expose
                 and measure the extent of the inherent structures in
                 the difference between images or datasets. Besides
                 yielding numerical attributes, the metrics also produce
                 images, which can visually highlight differences. Our
                 metrics are simple to compute and operate in the
                 spatial domain. We demonstrate the effectiveness of our
                 metrics through examples for comparing images and
                 datasets.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "metrics, steering, rendering, correlation measure",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-288,
  pages =        "105--114",
  year =         "1999",
  title =        "Feature Comparisons Of 3-{D} Vector Fields Using Earth
                 Mover's Distance",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-288",
  author =       "Rajesh K. Batra and Lambertus Hesselink",
  abstract =     "A method for comparing three-dimensional vector fields
                 constructed from simple critical points is described.
                 This method is a natural extension of the previous work
                 [1] which defined a distance metric for comparing
                 two-dimensional fields The extension to
                 three-dimensions follows the path of our previous work,
                 rethinking the representation of a critical point
                 signature and the distance measure between the points.
                 Since the method relies on topologically based
                 information, problems such as grid matching and vector
                 alignment which often complicate other comparison
                 techniques are avoided. In addition, since only feature
                 information is used to represent, and therefore stored
                 for each field, a significant amount of compression
                 occurs.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-289,
  pages =        "115--122",
  year =         "1999",
  title =        "Rendering on a Budget: {A} Framework for Time-Critical
                 Rendering",
  author =       "James T. Klosowski and Cl{\'{a}}udio T. Silva",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-289",
  abstract =     "We present a technique for optimizing the rendering of
                 high-depth complexity scenes. Prioritized-Layered
                 Projection (PLP) does this by rendering an estimation
                 of the visible set for each frame. The novelty in our
                 work lies in the fact that we do not explicitly compute
                 visible sets. Instead, our work is based on computing
                 on demand a priority order for the polygons that
                 maximizes the likelihood of rendering visible polygons
                 before occluded ones for any given scene. Given a fixed
                 budget, e.g. time or number of triangles, our rendering
                 algorithm makes sure to render geometry respecting the
                 computed priority. There are two main steps to our
                 technique: (1) an occupancy-based tessellation of
                 space; and (2) a solidity-based traversal algorithm.
                 PLP works by computing an occupancy-based tessellation
                 of space, which tends to have smaller cells where there
                 are more geometric primitives, e.g., polygons. In this
                 spatial tessellation, each cell is assigned a solidity
                 value, which is directly proportional to its likelihood
                 of occluding other cells. In its simplest form, a
                 cell's solidity value is directly proportional to the
                 number of polygons contained within it. During our
                 traversal algorithm, cells are marked for projection,
                 and the geometric primitives contained within them
                 actually rendered. The traversal algorithm makes use of
                 the cells' solidity, and other view-dependent
                 information to determine the ordering in which to
                 project cells. By tailoring the traversal algorithm to
                 the occupancy-based tessellation, we can achieve very
                 good frame rates with low preprocessing and rendering
                 costs In this paper, we describe our technique and its
                 implementation in detail. We also provide experimental
                 evidence of its performance and briefly discuss
                 extensions of our algorithm.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "Polygon rendering, visibility ordering, occlusion
                 culling",
  booktitle =    "IEEE Visualization '99",
}

@Article{EVL-1999-29,
  pages =        "155--176",
  year =         "1999",
  title =        "Multicolor Combination Lemma",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-29",
  author =       "Sariel Har-Peled",
  abstract =     "We present an extension of the Combination Lemma of
                 Guibas et al. (1983) that expresses the complexity of
                 one or several faces in the overlay of many
                 arrangements (as opposed to just two arrangements in
                 (Guibas et al. 1989)), as a function of the number of
                 arrangements, the number of faces, and the complexities
                 of these faces in the separate arrangements. Several
                 applications of the new Combination Lemma are
                 presented. We first show that the complexity of a
                 single face in an arrangement of k simple polygons with
                 a total of n sides is $\Theta(n \alpha(k) )$, where
                 $\alpha() is the inverse of Ackermann's function. We
                 also give a new and simpler proof of the bound $
                 O(ROOT(m) \lambda_{s+2} (n))$ on the total number of
                 edges of m faces in an arrangement of n Jordan arcs,
                 each pair of which intersect in at most s points, where
                 $\lambda_s(n)$ is the maximum length of a
                 Davenport--Schinzel sequence of order s with n symbols.
                 We extend this result, showing that the total number of
                 edges of m faces in a sparse arrangement of n Jordan
                 arcs is $ O( (n + ROOT(m)ROOT(w)) \lambda_{s+2} (n)/n )
                 , where w is the total complexity of the arrangement.
                 Several other related results are also obtained.",
  month =        apr,
  volume =       "12",
  keywords =     "Arrangements; Combination lemma",
  number =       "3-4",
  journal =      "Computational Geometry",
}

@InProceedings{EVL-1999-290,
  pages =        "123--130",
  year =         "1999",
  title =        "Time-critical Multiresolution Scene Rendering",
  author =       "Enrico Gobbetti and Eric Bouvier",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-290",
  abstract =     "We describe a framework for time-critical rendering of
                 graphics scenes composed of a large number of objects
                 having complex geometric descriptions. Our technique
                 relies upon a scene description in which objects are
                 represented as multiresolution meshes. We perform a
                 constrained optimization at each frame to choose the
                 resolution of each potentially visible object that
                 generates the best quality image while meeting timing
                 constraints. The technique provides smooth
                 level-of-detail control and aims at guaranteeing a
                 uniform, bounded frame rate even for widely changing
                 viewing conditions. The optimization algorithm is
                 independent from the particular data structure used to
                 represent multiresolution meshes. The only requirements
                 are the ability to represent a mesh with an arbitrary
                 number of triangles and to traverse a mesh structure at
                 an arbitrary resolution in a short predictable time. A
                 data structure satisfying these criteria is described
                 and experimental results are discussed.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "multiresolution modeling, level of detail, adaptive
                 rendering, time-critical graphics",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-291,
  pages =        "131--138",
  year =         "1999",
  title =        "Skip Strips: Maintaining Triangle Strips for
                 View-dependent Rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-291",
  author =       "Jihad A. El-Sana and Elvir Azanli and Amitabh
                 Varshney",
  abstract =     "View-dependent simplification has emerged as a
                 powerful tool for graphics acceleration in
                 visualization of complex environments. However,
                 view-dependent simplification techniques have not been
                 able to take full advantage of the underlying graphics
                 hardware. Specifically, triangle strips are a widely
                 used hardware-supported mechanism to compactly
                 represent and efficiently render static triangle
                 meshes. However, in a view-dependent framework, the
                 triangle mesh connectivity changes at every frame
                 making it difficult to use triangle strips. In this
                 paper we present a novel data-structure, Skip Strip,
                 that efficiently maintains triangle strips during such
                 view-dependent changes. A Skip Strip stores the vertex
                 hierarchy nodes in a skip-list-like manner with path
                 compression. We anticipate that Skip Strips will
                 provide a roadmap to combine rendering acceleration
                 techniques for static datasets, typical of
                 retained-mode graphics applications, with those for
                 dynamic datasets found in immediate-mode
                 applications.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-292,
  pages =        "139--146",
  year =         "1999",
  title =        "Isosurface Extraction Techniques for Web-based Volume
                 Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-292",
  author =       "Klaus D. Engel and R{\"{u}}diger Westermann and Thomas
                 Ertl",
  abstract =     "The reconstruction of isosurfaces from scalar volume
                 data has positioned itself as a fundamental
                 visualization technique in many different applications.
                 But the dramatically increasing size of volumetric data
                 sets often prohibits the handling of these models on
                 affordable low-end single processor architectures.
                 Distributed client-server systems integrating
                 high-bandwidth transmission channels and Web-based
                 visualization tools are one alternative to attack this
                 particular problem, but therefore new approaches to
                 reduce the load of numerical processing and the number
                 of generated primitives are required. In this paper we
                 outline different scenarios for distributed isosurface
                 reconstruction from large-scale volumetric data sets.
                 We demonstrate how to directly generate stripped
                 surface representations and we introduce adaptive and
                 hierarchical concepts to minimize the number of
                 vertices that have to be reconstructed, transmitted and
                 rendered. Furthermore, we propose a novel computation
                 scheme, which allows the user to flexibly exploit
                 locally available resources. The proposed algorithms
                 have been merged together in order to build a
                 platform-independent Web-based application. Extensive
                 use of VRML and Java OpenGL-bindings allows for the
                 exploration of large-scale volume data quite
                 efficiently.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "Volume visualization, Isosurface reconstruction,
                 Distributed Systems, Web-based Applications",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-293,
  pages =        "147--154",
  year =         "1999",
  title =        "Isosurface Extraction in Time-varying Fields Using a
                 Temporal Branch-on-Need Tree ({T}-{BON})",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-293",
  author =       "Philip M. Sutton and Charles D. Hansen",
  abstract =     "The Temporal Branch-on-Need Tree (T-BON) extends the
                 three-dimensional branch-on-need octree for
                 time-varying isosurface extraction. At each time step,
                 only those portions of the tree and data necessary to
                 construct the current isosurface are read from disk.
                 This algorithm can thus exploit the temporal locality
                 of the isosurface and, as a geometric technique,
                 spatial locality between cells in order to improve
                 performance. Experimental results demonstrate the
                 performance gained and memory overhead saved using this
                 technique.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "isosurface, time-dependent scalar field visualization,
                 multiresolution methods, octree",
  booktitle =    "EEE Visualization '99",
}

@InProceedings{EVL-1999-294,
  pages =        "155--160",
  year =         "1999",
  title =        "Interactive Lens Visualization Techniques",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-294",
  author =       "Christopher D. Shaw and James A. Hall and David S.
                 Ebert and Aaron Roberts",
  abstract =     "This paper describes new techniques for minimally
                 immersive visualization of 3D scalar and vector fields,
                 and visualization of document corpora. In our
                 glyph-based visualization system, the user interacts
                 with the 3D volume of glyphs using a pair of
                 button-enhanced 3D position and orientation trackers.
                 The user may also examine the volume using an
                 interactive lens, which is a rectangle that slices
                 through the 3Dvolume and displaysscalar information on
                 its surface. Alens allows the display of scalar data in
                 the 3Dvolume using a contour diagram, and a
                 texture-based volume rendering.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "Volumetric Data, Glyphs, Two-Handed Interfaces,
                 Interactive Volume Rendering, Contour Diagrams,
                 Stereoscopic Field Analyzer SFA, Seed Fill, Over
                 Blending",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-295,
  pages =        "169--174",
  year =         "1999",
  title =        "Exploring Geo-Scientific Data in Virtual
                 Environments",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-295",
  author =       "Bernd Fr{\"{o}}hlich and Stephen Barrass and
                 Bj{\"{o}}rn Zehner and John Plate and Martin
                 G{\"{o}}bel",
  abstract =     "This paper describes tools and techniques for the
                 exploration of geo-scientific data from the oil and gas
                 domain in stereoscopic virtual environments. The two
                 main sources of data in the exploration task are
                 seismic volumes and multivariate well logs of physical
                 properties down a bore hole. We have developed a
                 props-based interaction device called the cubic mouse
                 to allow more direct and intuitive interaction with a
                 cubic seismic volume. This device effectively places
                 the seismic cube in the user's hand. Geologists who
                 have tried this device have been enthusiastic about the
                 ease of use, and were adept only a few moments after
                 picking it up. We have also developed a multi-modal
                 visualisation and sonification technique for the dense,
                 multivariate well log data. The visualisation can show
                 two well log variables mapped along the well geometry
                 in a bivariate colour scheme, and another variable on a
                 sliding lens. A sonification probe is attached to the
                 lens so that other variables can be heard. The
                 sonification is based on a Geiger-counter metaphor that
                 is widely understood and which makes it easy to
                 explain. The data is sonified at higher or lower
                 resolutions depending on the speed of the lens. Sweeps
                 can be made at slower rates and over smaller intervals
                 to home in on peaks, boundaries or other features in
                 the full resolution data set.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-296,
  pages =        "175--182",
  year =         "1999",
  title =        "Animating Wrinkles on Clothes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-296",
  author =       "Sunil Hadap and Endre Bangarter and Pascal Volino and
                 Nadia Magnenat-Thalmann",
  abstract =     "This paper describes a method to simulate realistic
                 wrinkles on clothes without fine mesh and large
                 computational overheads. Cloth has very little in-plane
                 deformations, as most of the deformations come from
                 buckling. This can be looked at as area conservation
                 property of cloth. The area conservation formulation of
                 the method modulates the user defined wrinkle pattern,
                 based on deformation of individual triangle. The
                 methodology facilitates use of small in-plane
                 deformation stiffnesses and a coarse mesh for the
                 numerical simulation, this makes cloth simulation fast
                 and robust. Moreover, the ability to design wrinkles
                 (even on generalized deformable models) makes this
                 method versatile for synthetic image generation. The
                 method inspired from cloth wrinkling problem, being
                 geometric in nature, can be extended to other wrinkling
                 phenomena.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "cloth modeling, wrinkle modeling, deformable models",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-297,
  pages =        "191--198",
  year =         "1999",
  title =        "Mixing Translucent Polygons with Volumes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-297",
  author =       "Kevin A. Kreeger and Arie E. Kaufman",
  abstract =     "We present an algorithm which renders opaque and/or
                 translucent polygons embedded within volumetric data.
                 The processing occurs such that all objects are
                 composited in the correct order, by rendering thin
                 slabs of the translucent polygons between volume slices
                 using slice-order volume rendering. We implemented our
                 algorithm with OpenGL on current general-purpose
                 graphics systems. We discuss our system implementation,
                 speed and image quality, as well as the renderings of
                 several mixed scenes.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "Mixing polygons and volumes, Translucent Polygon
                 Rendering, Volume rendering, Ray casting,
                 Voxelization",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-298,
  pages =        "215--224",
  year =         "1999",
  title =        "A Distributed Graphics System for Large Tiled
                 Displays",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-298",
  author =       "Greg Humphreys and Pat Hanrahan",
  abstract =     "Recent interest in large displays has led to renewed
                 development of tiled displays, which are comprised of
                 several individual displays arranged in an array and
                 used as one large logical display. Stanford's
                 'Interactive Mural' is an example of such a display,
                 using an overlapping four by two array of projectors
                 that back-project onto a diffuse screen to form a 6' by
                 2' display area with a resolution of over 60 dpi.
                 Writing software to make effective use of the large
                 display space is a challenge because normal window
                 system interaction metaphors break down. One promising
                 approach is to switch to immersive applications;
                 another approach, the one we are investigating, is to
                 emulate office, conference room or studio environments
                 which use the space to display a collection of visual
                 material to support group activities. In this paper we
                 describe a virtual graphics system that is designed to
                 support multiple simultaneous rendering streams from
                 both local and remote sites. The system abstracts the
                 physical number of computers, graphics subsystems and
                 projectors used to create the display. We provide
                 performance measurements to show that the system scales
                 well and thus supports a variety of different hardware
                 configurations. The system is also interesting because
                 it uses transparent 'layers,' instead of windows, to
                 manage the screen",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99,",
}

@InProceedings{EVL-1999-299,
  pages =        "199--206",
  year =         "1999",
  title =        "Multi-resolution Multi-field Ray Tracing: {A}
                 mathematical overview",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-299",
  author =       "Charidimos E. Gasparakis",
  abstract =     "A rigorous mathematical review of ray tracing is
                 presented. The concept of a generic voxel decoder
                 acting on flexible voxel formats is introduced. The
                 necessity of interpolating opacity weighted colors is
                 proved, using a new definition of the blending process
                 in terms of functional integrals. The continuum limit
                 of the discrete opacity accumulation formula is
                 presented, and its convexity properties are
                 investigated. The issues pertaining to interpolation/
                 classification order are discussed. The lighting
                 equation is expressed in terms of opacity weighted
                 colors. The multi-resolution (along the ray) correction
                 of the opacity-weighted color is derived. The
                 mathematics of filtering on the image plane are
                 studied, and an upper limit of the local pixel size on
                 the image plane is obtained. Interpolation of pixel
                 values on the image plane is shown to be inequivalent
                 to blending of interpolated samples.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@Article{EVL-1999-3,
  pages =        "1--16",
  year =         "1999",
  title =        "Software Visualization of {LR} Parsing and Synthesized
                 Attribute Evaluation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-3",
  author =       "Elizabeth L. White and Jeffrey Ruby and Laura Denise
                 Deddens",
  language =     "en",
  abstract =     "Visual YACC is a tool that automatically creates
                 visualizations of the YACC LR parsing process and
                 synthesized attribute computation. The Visual YACC tool
                 works by instrumenting a standard YACC grammar with
                 graphics calls that draw the appropriate data
                 structures given the current actions by the parser. The
                 new grammar is processed by the YACC tools and the
                 resulting parser displays the parse stack and parse
                 tree for every step of the parsing process of a given
                 input string. Visual YACC was initially designed to be
                 used in compiler construction courses to supplement the
                 teaching of parsing and syntax directed evaluation. We
                 have also found it to be useful in the difficult task
                 of debugging YACC grammars. In this paper, we describe
                 this tool and how it is used in both contexts. We also
                 detail two different implementations of this tool: one
                 that produces a parser written in C with calls to
                 Motif; and a second implementation that generates Java
                 source code.",
  volume =       "29",
  copyright =    "John Wiley and Sons",
  number =       "1",
  journal =      "Software -- Practice and Experience",
}

@Article{EVL-1999-30,
  pages =        "177--217",
  year =         "1999",
  title =        "The mixed volume optimization problem",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-30",
  author =       "Mark J. Kaiser",
  abstract =     "The mixed volume optimization problem is to determine
                 the point of duality Q for a given convex set K that
                 minimizes the {"}mixed volume{"} of the associated
                 polar set $(K^* Q)$. In the plane, the mixed volumes
                 translate as the area and length; in space, the mixed
                 volumes include the volume, surface area, and mean
                 width. In this paper, the geometric optimization
                 problems associated with minimizing mixed volumes are
                 examined from two perspectives: enumerative search and
                 symbolic computation. The problem of minimizing the
                 polar area through an enumerative search is first
                 considered. The dual polygon $(Pm^* Q)$ is constructed
                 for an arbitrary point of duality $Q \sin P_m^o$ by
                 using an algebraic correspondence between the edges of
                 Pm and the vertices of $(P_m^* Q)$ , and the area of
                 $(P_m^* Q)$, A(P^*_m Q)$, is calculated and minimized
                 using naive search techniques. A result due to
                 Santal{\'{o}} is applied to verify the minimizing
                 solution, and computational tests are described for
                 various classes of randomly generated polygons.
                 Statistical evidence indicates that a {"}good{"}
                 approximation to the minimum area polar polygon occurs
                 when the duality point is located at the
                 center-of-gravity of Pm . The polar area problem is
                 then investigated using symbolic procedures. Explicit
                 symbolic expressions for the polar area and length
                 functionals are computed and solved directly using the
                 differential optimality conditions and Newton's
                 iterative method of solution. The mixed volume and
                 surface area functionals are formulated and solved
                 using numerical products, and the mean width functional
                 is described. Examples are used throughout to
                 illustrate the methodology.",
  month =        apr,
  volume =       "12",
  keywords =     "Mixed volumes; Geometric optimization; Constructive
                 convex geometry; Symbolic computation; Polar figures;
                 Santal{\'{o}} point",
  number =       "3-4",
  journal =      "Computational Geometry",
}

@InProceedings{EVL-1999-300,
  pages =        "207--214",
  year =         "1999",
  title =        "Enabling Classification and Shading for 3{D} Texture
                 Mapping based Volume Rendering using Open{GL} and
                 Extensions",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-300",
  author =       "Michael Mei{\ss{}}ner and Ulrich Hoffmann and Wolfgang
                 Stra{\ss{}}er",
  abstract =     "We present a new technique which enables direct volume
                 rendering based on 3D texture mapping hardware,
                 enabling shading as well as classification of the
                 interpolated data. Our technique supports accurate
                 lighting for a one directional light source,
                 semi-transparent classification, and correct blending.
                 To circumvent the limitations of one general
                 classification, we introduce multiple classification
                 spaces which are very valuable to understand the
                 visualized data, and even mandatory to comprehensively
                 grasp the 3D relationship of different materials
                 present in the volumetric data. Furthermore, we
                 illustrate how multiple classification spaces can be
                 realized using existing graphics hardware. In contrast
                 to previously reported algorithms, our technique is
                 capable of performing all the above mentioned tasks
                 within the graphics pipeline. Therefore, it is very
                 efficient: The three dimensional texture needs to be
                 stored only once and no load is put onto the CPU.
                 Besides using standard OpenGL functionality, we exploit
                 advanced per pixel operations and make use of available
                 OpenGL extensions.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-301,
  pages =        "225--232",
  year =         "1999",
  title =        "A Multi-Threaded Streaming Pipeline Architecture for
                 Large Structured Data Sets",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-301",
  author =       "C. Charles Law and Kenneth M. Martin and William J.
                 Schroeder and Joshua Temkin",
  abstract =     "Computer simulation and digital measuring systems are
                 now generating data of unprecedented size. The size of
                 data is becoming so large that conventional
                 visualization tools are incapable of processing it,
                 which is in turn is impacting the effectiveness of
                 computational tools. In this paper we describe an
                 object-oriented architecture that addresses this
                 problem by automatically breaking data into pieces, and
                 then processes the data piece-by-piece within a
                 pipeline of filters. The piece size is user specified
                 and can be controlled to eliminate the need for
                 swapping (i.e., relying on virtual memory). In
                 addition, because piece size can be controlled, any
                 size problem can be run on any size computer, at the
                 expense of extra computational time. Furthermore,
                 pieces are automatically broken into sub-pieces and
                 each piece assigned to a different thread for parallel
                 processing. This paper includes numerical performance
                 studies and references to the source code which is
                 freely available on the Web.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-302,
  pages =        "233--240",
  year =         "1999",
  title =        "Interactive Exploration of Volume Line Integral
                 Convolution Based on 3{D}-Texture Mapping",
  author =       "Christof Rezk-Salama and Peter Hastreiter and Teitzel
                 Christian and Thomas Ertl",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-302",
  abstract =     "Line integral convolution (LIC) is an effective
                 technique for visualizing vector fields. The
                 application of LIC to 3D flow fields has yet been
                 limited by difficulties to efficiently display and
                 animate the resulting 3D-images. Texture-based volume
                 rendering allows interactive visualization and
                 manipulation of 3D-LIC textures. In order to ensure the
                 comprehensive and convenient exploration of flow
                 fields, we suggest interactive functionality including
                 transfer functions and different clipping mechanisms.
                 Thereby, we efficiently substitute the calculation of
                 LIC based on sparse noise textures and show the
                 convenient visual access of interior structures.
                 Further on, we introduce two approaches for animating
                 static 3D-flow fields without the computational expense
                 and the immense memory requirements for pre-computed
                 3D-textures and without loss of interactivity. This is
                 achieved by using a single 3D-LIC texture and a set of
                 time surfaces as clipping geometries. In our first
                 approach we use the clipping geometry to pre-compute a
                 special 3D-LIC texture that can be animated by
                 time-dependent color tables. Our second approach uses
                 time volumes to actually clip the 3D-LIC volume
                 interactively during rasterization. Additionally,
                 several examples demonstrate the value of our strategy
                 in practice.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "Flow Visualization, Animated LIC, Direct Volume
                 Rendering, 3D-Textures Mapping, Interactive Volume
                 Exploration",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-303,
  pages =        "241--248",
  year =         "1999",
  title =        "A Framework for Assisted Exploration with
                 Collaboration",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-303",
  author =       "Eric A. Wernert and Andrew J. Hanson",
  abstract =     "We approach the problem of exploring a virtual space
                 by exploiting positional and camera-model constraints
                 on navigation to provide extra assistance that focuses
                 the user's explorational wanderings on the task
                 objectives. Our specific design incorporates not only
                 task-based constraints on the viewer's location, gaze,
                 and viewing parameters, but also a personal 'guide'
                 that serves two important functions: keeping the user
                 oriented in the navigation space, and 'pointing' to
                 interesting subject areas as they are approached. The
                 guide's cues may be ignored by continuing in motion,
                 but if the user stops, the gaze shifts automatically
                 toward whatever the guide was interested in. This
                 design has the serendipitous feature that it
                 automatically incorporates a nested collaborative
                 paradigm simply by allowing any given viewer to be seen
                 as the 'guide' of one or more viewers following behind;
                 the leading automated guide (we tend to select a guide
                 dog for this avatar) can remind the leading live human
                 guide of interesting sites to point out, while each
                 real human collaborator down the chain has some choices
                 about whether to follow the local leader's hints. We
                 have chosen VRML as our initial development medium
                 primarily because of its portability, and we have
                 implemented a variety of natural modes for leading and
                 collaborating, including ways for collaborators to
                 attach to and detach from a particular leader.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "wayfinding, locomotion, navigation, exploration,
                 collaboration, virtual reality, VRML",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-304,
  pages =        "249--254",
  year =         "1999",
  title =        "Tensorlines: Advection-Diffusion based Propagation
                 through Diffusion Tensor Fields",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-304",
  author =       "David M. Weinstein and Gordon L. Kindlmann and Eric C.
                 Lundberg",
  abstract =     "Tracking linear features through tensor field datasets
                 is an open research problem with widespread utility in
                 medical and engineering disciplines. Existing tracking
                 methods, which consider only the preferred local
                 diffusion direction as they propagate, fail to
                 accurately follow features as they enter regions of
                 local complexity. This shortcoming is a result of
                 partial voluming; that is, voxels in these regions
                 often contain contributions from multiple features.
                 These combined contributions result in ambiguities when
                 deciding local primary feature orientation based solely
                 on the preferred diffusion direction. In this paper, we
                 introduce a novel feature extraction method, which we
                 term tensorline propagation. Our method resolves the
                 above ambiguity by incorporating information about the
                 nearby orientation of the feature, as well as the
                 anisotropic classification of the local tensor. The
                 nearby orientation information is added in the spirit
                 of an advection term in a standard diffusion-based
                 propagation technique, and has the effect of
                 stabilizing the tracking. To demonstrate the efficacy
                 of tensorlines, we apply this method to the
                 neuroscience problem of tracking white-matter bundles
                 within the brain.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-305,
  pages =        "255--262",
  year =         "1999",
  title =        "Visualizing Planar Vector Fields with Normal Component
                 Using Line Integral Convolution",
  author =       "Gerik Scheuermann and Holger Burbach and Hans Hagen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-305",
  abstract =     "We present a method for visualizing three dimensional
                 vector fields which are defined on a two dimensional
                 manifold only. These vector fields do exist in real
                 application, as we show by an example of an optical
                 measuring instrument which can gauge the displacement
                 at the surface of a mechanical part. The general idea
                 is to compute LIC textures in the manifold's tangent
                 space and to deform the manifold according to the
                 normal information. The resulting LIC texture is mapped
                 onto the deformed manifold and is rendered as a three
                 dimensional scene. Due to the light's reflection on the
                 deformed manifold, one can interactively explore the
                 result of the deformation.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "LIC, vector field visualization, deformation",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-306,
  pages =        "263--270",
  year =         "1999",
  title =        "The {"}Parallel Vectors{"} Operator - {A} Vector Field
                 Visualization Primitive",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-306",
  author =       "Ronald Peikert and Martin Roth",
  abstract =     "In this paper we propose an elementary operation on a
                 pair of vector fields as a building block for defining
                 and computing global line-type features of vector or
                 scalar fields. While usual feature definitions often
                 are procedural and therefore implicit, our operator
                 allows precise mathematical definitions. It can serve
                 as a basis for comparing feature definitions and for
                 reuse of algorithms and implementations. Applications
                 focus on vortex core methods.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-307,
  pages =        "271--278",
  year =         "1999",
  title =        "{C1}-Interpolation for Vector Field Topology
                 Visualization",
  author =       "Gerik Scheuermann and Xavier Tricoche and Hans Hagen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-307",
  abstract =     "An application of C1 scalar interpolation for 2D
                 vector field topology visualization is presented.
                 Powell-Sabin and Nielson interpolants are considered
                 which both make use of Nielson's Minimum Norm Network
                 for the precomputation of the derivatives in our
                 implementation. A comparison of their results to the
                 commonly used linear interpolant underlines their
                 significant improvement of singularity location and
                 topological skeleton depiction. Evalution is based upon
                 the processing of polynomial vector fields with known
                 topology containing higher order singularities.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "vector field visualization, topology, critical point
                 theory, C1-interpolation",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-308,
  pages =        "279--284",
  year =         "1999",
  title =        "Optimal Triangular Haar Bases for Spherical Data",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-308",
  author =       "Georges-Pierre Bonneau",
  abstract =     "Multiresolution analysis based on FWT (Fast Wavelet
                 Transform) is now widely used in Scientific
                 Visualization. Spherical bi-orthogonal wavelets for
                 spherical triangular grids where introduced in [5]. In
                 order to improve on the orthogonality of the wavelets,
                 the concept of nearly orthogonality, and two new
                 piecewise-constant (Haar) bases were introduced in [4].
                 In our paper, we extend the results of [4]. First we
                 give two one-parameter families of triangular Haar
                 wavelet bases that are nearly orthogonal in the sense
                 of [4]. Then we introduce a measure of orthogonality.
                 This measure vanishes for orthogonal bases. Eventually,
                 we show that we can find an optimal parameter of our
                 wavelet families, for which the measure of
                 orthogonality is minimized. Several numerical and
                 visual examples for a spherical topographic data set
                 illustrates our results.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "visualization, multiresolution, wavelets,
                 orthogonality",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-309,
  pages =        "285--290",
  year =         "1999",
  title =        "Cracking the Cracking Problem with Coons Patches",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-309",
  author =       "Gregory M. Nielson and Dave Holliday and Tom
                 Roxborough",
  abstract =     "We present a new approach to solving the cracking
                 problem. The cracking problem arises in many contexts
                 in scientific visualization and computer graphics
                 modeling where there is need for an approximation based
                 upon domain decomposition that is fine in certain
                 regions and coarse in others. This includes surface
                 rendering, approximation of images and multiresolution
                 terrain visualization. In general, algorithms based
                 upon adaptive refinement strategies must deal with this
                 problem. The new approach presented here is simple and
                 general. It is based upon the use of a triangular Coons
                 patch. Both the basic idea of using a triangular Coons
                 patch in this context and the particular Coons patch
                 that is used constitute the novel contributions of this
                 paper.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@Article{EVL-1999-31,
  pages =        "219--239",
  year =         "1999",
  title =        "On some geometric optimization problems in layered
                 manufacturing",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-31",
  author =       "Jayanth Majhi and Ravi Janardan and Michiel Smid and
                 Prosenjit Gupta",
  abstract =     "Efficient geometric algorithms are given for
                 optimization problems arising in layered manufacturing,
                 where a 3D object is built by slicing its CAD model
                 into layers and manufacturing the layers successively.
                 The problems considered include minimizing the
                 stair-step error on the surfaces of the manufactured
                 object under various formulations, minimizing the
                 volume of the so-called support structures used, and
                 minimizing the contact area between the supports and
                 the manufactured object---all of which are factors that
                 affect the speed and accuracy of the process. The
                 stair-step minimization algorithm is valid for any
                 polyhedron, while the support minimization algorithms
                 are applicable only to convex polyhedra. The techniques
                 used to obtain these results include construction and
                 searching of certain arrangements on the sphere, 3D
                 convex hulls, halfplane range searching, and
                 constrained optimization.",
  month =        apr,
  volume =       "12",
  keywords =     "Layered manufacturing; Computational geometry;
                 Optimization",
  number =       "3-4",
  journal =      "Computational Geometry",
}

@InProceedings{EVL-1999-310,
  pages =        "307--316",
  year =         "1999",
  title =        "Progressive Compression and Transmission of Arbitrary
                 Triangular Meshes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-310",
  author =       "Chandrajit L. Bajaj and Valerio Pascucci and Guozhong
                 Zhuang",
  abstract =     "The recent growth in the size and availability of
                 large triangular surface models has generated interest
                 in compact multi-resolution progressive representation
                 and data transmission. An ongoing challenge is to
                 design an efficient data structure that encompasses
                 both compactness of geometric representations and
                 visual quality of progressive representations. In this
                 paper we introduce a topological layering based
                 data-structure and an encoding scheme to build a
                 compact progressive representation of an arbitrary
                 triangular mesh (a 2D simplicial complex in 3D) with
                 attached attribute data. This compact representation is
                 composed of multiple levels of detail that can be
                 progressively transmitted and displayed. The global
                 topology, which is the number of holes and connected
                 components, can be flexibly changed among successive
                 levels while still achieving guaranteed size of the
                 coarsest level mesh for very complex models. The
                 flexibility in our encoding scheme also allows topology
                 preserving progressivity.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-311,
  pages =        "291--298",
  year =         "1999",
  title =        "{LOD}-Sprite Technique for Accelerated Terrain
                 Rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-311",
  author =       "Baoquan Chen and J. Edward Swan II and Eddy Kuo and
                 Arie E. Kaufman",
  abstract =     "We present a new rendering technique, termed
                 LOD-sprite rendering, which uses a combination of a
                 level-of-detail (LOD) represen- tation of the scene
                 together with reusing image sprites (previously
                 rendered images). Our primary application is
                 accelerating terrain rendering. The LOD-sprite
                 technique renders an initial frame using a
                 high-resolution model of the scene geometry. It renders
                 subsequent frames with a much lower-resolution model of
                 the scene geometry and texture-maps each polygon with
                 the image sprite from the initial high-resolution
                 frame. As it renders these subsequent frames the
                 technique measures the error associated with the
                 divergence of the view position from the position where
                 the initial frame was rendered. Once this error exceeds
                 a user-defined threshold, the technique re-renders the
                 scene from the high-resolution model. We have
                 efficiently implemented the LOD-sprite technique with
                 texture-mapping graphics hardware. Although to date we
                 have only applied LOD-sprite to terrain rendering, it
                 could easily be extended to other applications. We feel
                 LOD-sprite holds particular promise for real-time
                 rendering systems.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-312,
  pages =        "317--324",
  year =         "1999",
  title =        "Spiraling Edge: Fast Surface Reconstruction from
                 Partially Organized Sample Points",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-312",
  author =       "Patricia J. Crossno and Edward S. Angel",
  abstract =     "Many applications produce three-dimensional points
                 that must be further processed to generate a surface.
                 Surface reconstruction algorithms that start with a set
                 of unorganized points are extremely time-consuming.
                 Sometimes, however, points are generated such that
                 there is additional information available to the
                 reconstruction algorithm. We present Spiraling Edge, a
                 specialized algorithm for surface reconstruction that
                 is three orders of magnitude faster than algorithms for
                 the general case. In addition to sample point
                 locations, our algorithm starts with normal information
                 and knowledge of each point's neighbors. Our algorithm
                 produces a localized approximation to the surface by
                 creating a star-shaped triangulation between a point
                 and a subset of its nearest neighbors. This surface
                 patch is extended by locally triangulating each of the
                 points along the edge of the patch. As each edge point
                 is triangulated, it is removed from the edge and new
                 edge points along the patch's edge are inserted in its
                 place. The updated edge spirals out over the surface
                 until the edge encounters a surface boundary and stops
                 growing in that direction, or until the edge reduces to
                 a small hole that is filled by the final triangle.",
  organization = "IEEE",
  address =      "San Francisco",
  keywords =     "Surface reconstruction, advancing front,
                 triangulation",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-313,
  pages =        "325--332",
  year =         "1999",
  title =        "Anisotropic Nonlinear Diffusion in Flow
                 Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-313",
  author =       "Tobias Preu{\ss{}}er and Martin Rumpf",
  abstract =     "Vector field visualization is an important topic in
                 scientific visualization. Its aim is to graphically
                 represent field data in an intuitively understandable
                 and precise way. Here a new approach based on
                 anisotropic nonlinear diffusion is introduced. It
                 enables an easy perception of flowdata and serves as an
                 appropriate scale space method for the visualization of
                 complicated flow pattern. The approach is closely
                 related to nonlinear diffusion methods in image
                 analysis where images are smoothed while still
                 retaining and enhancing edges. Here an initial noisy
                 image is smoothed along streamlines, whereas the image
                 is sharpened in the orthogonal direction. The method is
                 based on a continuous model and requires the solution
                 of a parabolic PDE problem. It is discretized only in
                 the final implementational step. Therefore, many
                 important qualitative aspects can already be discussed
                 on a continuous level. Applications are shown in 2D and
                 3D and the provisions for flow segmentation are
                 outlined.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-314,
  pages =        "299--306",
  year =         "1999",
  title =        "Implant Sprays: Compression of Progressive Tetrahedral
                 Mesh Connectivity",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-314",
  author =       "Renato B. Pajarola and Jarek Rossignac and Andrzej
                 Szymczak",
  abstract =     "Irregular tetrahedral meshes, which are popular in
                 many engineering and scientific applications, often
                 contain a large number of vertices. A mesh of V
                 vertices and T tetrahedra requires 48*V bits or less to
                 store the vertex coordinates, 4T log2 (V) bits to store
                 the tetrahedra-vertex incidence relations, also called
                 connectivity information, and k*V bits to store the
                 k-bit value samples associated with the vertices. Given
                 that T is 5 to 7 times larger than V and that V often
                 exceeds 323 , the storage space required for the
                 connectivity is larger than 300*V bits and thus
                 dominates the overall storage cost. Our 'implants
                 spray' compression approach introduced in this paper
                 reduces this cost to about 30*V bits or less - a 10:1
                 compression ratio. Furthermore, implant spray supports
                 the progressive refinement of a crude model through a
                 series of vertex-splits operations.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "tetrahedral meshes, compression, multiresolution
                 models, progressive incremental reconstruction",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-315,
  pages =        "363--370",
  year =         "1999",
  title =        "Splatting Without The Blur",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-315",
  author =       "Klaus Mueller and Torsten M{\"{o}}ller and Roger
                 Crawfis",
  abstract =     "Splatting is a volume rendering algorithm that
                 combines efficient volume projection with a sparse data
                 representation: Only voxels that have values inside the
                 iso-range need to be considered, and these voxels can
                 be projected via efficient rasterization schemes. In
                 splatting, each projected voxel is represented as a
                 radially symmetric interpolation kernel, equivalent to
                 a fuzzy ball. Projecting such a basis function leaves a
                 fuzzy impression, called a footprint or splat, on the
                 screen. Splatting traditionally classifies and shades
                 the voxels prior to projection, and thus each voxel
                 foot-print is weighted by the assigned voxel color and
                 opacity. Projecting these fuzzy color balls provides a
                 uniform screen image for homogeneous object regions,
                 but leads to a blurry appearance of object edges. The
                 latter is clearly undesirable, especially when the view
                 is zoomed on the object. In this work, we manipulate
                 the rendering pipeline of splatting by performing the
                 classification and shading process after the voxels
                 have been projected onto the screen. In this way,
                 volume contributions outside the iso-range never affect
                 the image. Since shading requires gradients, we not
                 only splat the density volume, using regular splats,
                 but we also project the gradient volume, using gradient
                 splats. However, alternative to gradient splats, we can
                 also compute the gradients on the projection plane,
                 using central differencing. This latter scheme cuts the
                 number of footprint rasterization by a factor of four,
                 since only the voxel densities have to be projected.
                 Our new method renders objects with crisp edges and
                 well-preserved surface detail. Added overhead is the
                 calculation of the screen gradients and the per-pixel
                 shading. Both of these operations, however, may be
                 performed using fast techniques employing lookup
                 tables",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-316,
  pages =        "333--340",
  year =         "1999",
  title =        "Visualizing Multivalued Data from 2{D} Incompressible
                 Flows Using Concepts from Painting",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-316",
  author =       "R. M. Kirby and H. Marmanis and David H. Laidlaw",
  abstract =     "We present a new visualization method for 2d flows
                 which allows us to combine multiple data values in an
                 image for simultaneous viewing. We utilize concepts
                 from oil painting, art, and design as introduced in [1]
                 to examine problems within fluid mechanics. We use a
                 combination of discrete and continuous visual elements
                 arranged in multiple layers to visually represent the
                 data. The representations are inspired by the brush
                 strokes artists apply in layers to create an oil
                 painting. We display commonly visualized quantities
                 such as velocity and vorticity together with three
                 additional mathematically derived quantities: the rate
                 of strain tensor (defined in section 4), and the
                 turbulent charge and turbulent current (defined in
                 section 5). We describe the motivation for
                 simultaneously examining these quantities and use the
                 motivation to guide our choice of visual representation
                 for each particular quantity. We present visualizations
                 of three flow examples and observations concerning some
                 of the physical relationships made apparent by the
                 simultaneous display technique that we employed",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-317,
  pages =        "341--348",
  year =         "1999",
  title =        "{PLIC}: Bridging the Gap Between Streamlines and
                 {LIC}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-317",
  author =       "Vivek Verma and David Kao and Alex Pang",
  abstract =     "This paper explores mapping strategies for generating
                 LIC-like images from streamlines and streamline-like
                 images from LIC. The main contribution of this paper is
                 a technique which we call pseudo-LIC or PLIC. By
                 adjusting a small set of key parameters, PLIC can
                 generate flow visualizations that span the spectrum of
                 streamline-like to LIC-like images. Among the
                 advantages of PLIC are: image quality comparable with
                 LIC, performance speedup over LIC, use of a template
                 texture that is independent of the size of the flow
                 field, handles the problem of multiple streamlines
                 occupying the same pixel in image space, reduced
                 aliasing, applicability to time varying data sets, and
                 variable speed animation.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "unsteady flow, variable speed animation, jitter,
                 texture mapping, comparative visualization",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-318,
  pages =        "349--354",
  year =         "1999",
  title =        "Collapsing Flow Topology Using Area Metrics",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-318",
  author =       "Wim C. de Leeuw and Robert van Liere",
  abstract =     "Visualization of topological information of a vector
                 field can provide useful information on the structure
                 of the field. However, in turbulent flows standard
                 critical point visualization will result in a cluttered
                 image which is difficult to interpret. This paper
                 presents a technique for collapsing topologies. The
                 governing idea is to classify the importance of the
                 critical points in the topology. By only displaying the
                 more important critical points, a simplified depiction
                 of the topology can be provided. Flow consistency is
                 maintained when collapsing the topology, resulting in a
                 visualization which is consistent with the original
                 topology. We apply the collapsing topology technique to
                 a turbulent flow field.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "multi-level visualization techniques, flow
                 visualization, flow topology",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-319,
  pages =        "355--362",
  year =         "1999",
  title =        "Multiresolution Techniques for Interactive
                 Texture-based Volume Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-319",
  author =       "Eric C. LaMar and Bernd Hamann and Kenneth I. Joy",
  abstract =     "We present a multiresolution technique for interactive
                 texture-based volume visualization of very large data
                 sets. This method uses an adaptive scheme that renders
                 the volume in a region-of-interest at a high resolution
                 and the volume away from this region at progressively
                 lower resolutions. The algorithm is based on the
                 segmentation of texture space into an octree, where the
                 leaves of the tree define the original data and the
                 internal nodes define lower-resolution versions.
                 Rendering is done adaptively by selecting
                 high-resolution cells close to a center of attention
                 and low-resolution cells away from this area. We limit
                 the artifacts introduced by this method by modifying
                 the transfer functions in the lower-resolution data
                 sets and utilizing spherical shells as a proxy
                 geometry. It is possible to use this technique to
                 produce viewpoint-dependent renderings of very large
                 data sets.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "multiresolution rendering, volume visualization,
                 hardware texture",
  booktitle =    "IEEE Visualization '99",
}

@Article{EVL-1999-32,
  pages =        "63--83",
  year =         "1999",
  title =        "Fast randomized point location without preprocessing
                 in two- and three-dimensional Delaunay triangulations",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-32",
  author =       "Ernst P. M{\"u}cke and Isaac Saias and Binhai Zhu",
  abstract =     "This paper studies the point location problem in
                 Delaunay triangulations without preprocessing and
                 additional storage. The proposed procedure finds the
                 query point by simply {"}walking through{"} the
                 triangulation, after selecting a {"}good starting
                 point{"} by random sampling. The analysis generalizes
                 and extends a recent result for d=2 dimensions by
                 proving this procedure takes expected time close to
                 $O(n^{1/(d+1)}) for point location in Delaunay
                 triangulations of n random points in d=3 dimensions.
                 Empirical results in both two and three dimensions show
                 that this procedure is efficient in practice.",
  month =        feb,
  volume =       "12",
  keywords =     "Computational geometry; Geometric computing;
                 Randomized algorithms; Delaunay triangulations; Point
                 location; Three dimensional",
  number =       "1-2",
  journal =      "Computational Geometry",
}

@InProceedings{EVL-1999-320,
  pages =        "371--378",
  year =         "1999",
  title =        "A Fast Volume Rendering Algorithm for Time-Varying
                 Fields Using a Time-Space Partitioning ({TSP}) Tree",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-320",
  author =       "Han-Wei Shen and Ling-Jan Chiang and Kwan-Liu Ma",
  abstract =     "This paper presents a fast volume rendering algorithm
                 for time-varying fields. We propose a new data
                 structure, called Time-Space Partitioning (TSP) tree,
                 that can effectively capture both the spatial and the
                 temporal coherence from a time-varying field. Using the
                 proposed data structure, the rendering speed is
                 substantially improved. In addition, our data structure
                 helps to maintain the memory access locality and to
                 provide the sparse data traversal so that our algorithm
                 becomes suitable for large-scale out-of-core
                 applications. Finally, our algorithm allows flexible
                 error control for both the temporal and the spatial
                 coherence so that a trade-off between image quality and
                 rendering speed is possible. We demonstrate the utility
                 and speed of our algorithm with data from several
                 time-varying CFD simulations. Our rendering algorithm
                 can achieve substantial speedup while the storage space
                 overhead for the TSP tree is kept at a minimum.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "scalar field visualization, volume visualization,
                 volume rendering, time-varying fields",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-321,
  pages =        "379--388",
  year =         "1999",
  title =        "High Performance Presence-Accelerated Ray Casting",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-321",
  author =       "Ming Wan and Arie E. Kaufman and Steve Bryson",
  abstract =     "We present a novel presence acceleration for
                 volumetric ray casting. A highly accurate estimation
                 for object presence is obtained by projecting all grid
                 cells associated with the object boundary on the image
                 plane. Memory space and access time are reduced by
                 run-length encoding of the boundary cells, while
                 boundary cell projection time is reduced by exploiting
                 projection templates and multiresolution volumes.
                 Efforts have also been made towards a fast perspective
                 projection as well as interactive classification. We
                 further present task partitioning schemes for effective
                 parallelization of both boundary cell projection and
                 ray traversal procedures. Good load balancing has been
                 reached by taking full advantage of both the
                 optimizations in the serial rendering algorithm and
                 shared-memory architecture. Our experimental results on
                 a 16-processor SGI Power Challenge have shown
                 interactive rendering rates for 2563/sup> volumetric
                 data sets at 10 - 30 Hz. This paper describes the
                 theory and implementation of our algorithm, and shows
                 its superiority over the shear-warp factorization
                 approach.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "Volume rendering, presence acceleration, run-length
                 encoding, projection template, multiresolution volumes,
                 interactive classification, parallel processing",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-322,
  pages =        "393--396",
  year =         "1999",
  title =        "Digital Design of a Surgical Simulator for
                 Interventional Magnetic Resonance Imaging",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-322",
  author =       "Terry S. Yoo and Penny Rheingans",
  abstract =     "We present the design of a simulator for a prototype
                 interventional magnetic resonance imaging scanner. This
                 MRI scanner is integrated with an operating theater,
                 enabling new techniques in minimally invasive surgery.
                 The simulator is designed with a threefold purpose: (1)
                 to provide a rehearsal apparatus for practicing and
                 modifying conventional procedures for use in the
                 magnetic environment, (2) to serve as a visualization
                 workstation for procedure planning and previewing as
                 well as a post-operative review, and (3) to form the
                 foundation of a laboratory workbench for the
                 development of new surgical tools and procedures for
                 minimally invasive surgery. The simulator incorporates
                 pre-operative data, either MRI or CT exams, as well as
                 data from commercial surgical planning systems. Dynamic
                 control of the simulation and interactive display of
                 preoperative data in lieu of intra-operative data is
                 handled via an opto-electronic tracking system. The
                 resulting system is contributing insights into how best
                 to perform visualization for this new surgical
                 environment.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-323,
  pages =        "397--400",
  year =         "1999",
  title =        "Volume Rendering Based Interactive Navigation within
                 the Human Colon",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-323",
  author =       "Ming Wan and Qingyu Tang and Arie E. Kaufman and
                 Zhengrong Liang and Mark Wax",
  abstract =     "We present an interactive navigation system for
                 virtual colonoscopy, which is based solely on high
                 performance volume rendering. Previous colonic
                 navigation systems have employed either a surface
                 rendering or a Z-buffer-assisted volume rendering
                 method that depends on the surface rendering results.
                 Our method is a fast direct volume rendering technique
                 that exploits distance information stored in the
                 potential field of the camera control model, and is
                 parallelized on a multiprocessor. Experiments have been
                 conducted on both a simulated pipe and patients' data
                 sets acquired with a CT scanner.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-324,
  pages =        "401--404",
  year =         "1999",
  title =        "A Computer Animation Representing the Molecular Events
                 of {G} protein-coupled Receptor Activation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-324",
  author =       "Zoya Maslak and Douglas J. Steel and Robert J.
                 McDermott",
  abstract =     "The molecular events involved in the activation of G
                 protein-coupled receptors, represent a fundamental
                 biochemical process. These events were selected for
                 animation because the mechanism involves both a
                 ligand-receptor conformational shape change, and an
                 enzyme-substrate conformational shape change.
                 Expository animation brought this biochemical process
                 to life.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-325,
  pages =        "405--408",
  year =         "1999",
  title =        "Visualizing Gridded Datasets with Large Number of
                 Missing Values",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-325",
  author =       "Suzana Djurcilov and Alex Pang",
  abstract =     "Much of the research in scientific visualization has
                 focused on complete sets of gridded data. This paper
                 presents our experience dealing with gridded data sets
                 with large number of missing or invalid data, and some
                 of our experiments in addressing the shortcomings of
                 standard off-the-shelf visualization algorithms. In
                 particular, we discuss the options in modifying known
                 algorithms to adjust to the specifics of sparse
                 datasets, and provide a new technique to smooth out the
                 side-effects of the operations. We apply our findings
                 to data acquired from NEXRAD (NEXt generation RADars)
                 weather radars, which usually have no more than 3 to 4
                 percent of all possible cell points filled.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "curvilinear grid, sparse data visualization, Delauney
                 triangulation, isosurface, NEXRAD",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-326,
  pages =        "389--392",
  year =         "1999",
  title =        "Interactive Exploration of Extra- and Interacranial
                 Blood Vessels",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-326",
  author =       "Dirk Bartz and Wolfgang Stra{\ss{}}er and Martin
                 Skalej and Dorothea Welte",
  abstract =     "We present a system for interactive explorations of
                 extra-and intracranial blood vessels. Starting with a
                 stack of images from 3D angiography, we use virtual
                 clips to limit the segmentation of the vessel tree to
                 the parts the neuroradiologists are interested in.
                 Furthermore, methods of interactive virtual endoscopy
                 are applied in order to provide an interior view of the
                 blood vessels",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "Virtual Environments, 3D Angiography, Virtual
                 Angioscopy, Interventional Neuroradiology, Selective
                 Segmentation, Computer Assisted Diagnosis",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-328,
  pages =        "409--412",
  year =         "1999",
  title =        "Detecting Vortical Phenomena in Vector Data by
                 Medium-Scale Correlation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-328",
  author =       "H.-G. Pagendarm and B. Henne and M. Rutten",
  abstract =     "The detection of vortical phenomena in vector data is
                 one of the key issues in many technical applications,
                 in particular in flow visualization. Many existing
                 approaches rely on purely local evaluation of the
                 vector data. In order to overcome the limits of a local
                 approach we choose to combine a local method with a
                 correlation of a pre-defined generic vortex with the
                 data in a medium-scale region. Two different concepts
                 of a generic vortices were tested on various sets of
                 flow velocity vector data. The approach is not limited
                 to the two generic patterns suggested here. The method
                 was found to successfully detect vortices in cases were
                 other methods fail.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-329,
  pages =        "417--420",
  year =         "1999",
  title =        "Visual Debugging of Visualization Software: {A} Case
                 Study for Particle Systems",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-329",
  author =       "Patricia J. Crossno and Edward S. Angel",
  abstract =     "Visualization systems are complex dynamic software
                 systems. Debugging such systems is difficult using
                 conventional debuggers because the programmer must try
                 to imagine the three-dimensional geometry based on a
                 list of positions and attributes. In addition, the
                 programmer must be able to mentally animate changes in
                 those positions and attributes to grasp dynamic
                 behaviors within the algorithm. In this paper we shall
                 show that representing geometry, attributes, and
                 relationships graphically permits visual pattern
                 recognition skills to be applied to the debugging
                 problem. The particular application is a particle
                 system used for isosurface extraction from volumetric
                 data. Coloring particles based on individual attributes
                 is especially helpful when these colorings are viewed
                 as animations over successive iterations in the
                 program. Although we describe a particular application,
                 the types of tools that we discuss can be applied to a
                 variety of problems",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "Visual debugging, algorithm animation, program
                 animation, program visualization, particle systems",
  booktitle =    "IEEE Visualization '99",
}

@Article{EVL-1999-33,
  pages =        "85--103",
  year =         "1999",
  title =        "Checking geometric programs or verification of
                 geometric structures",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-33",
  author =       "Kurt Mehlhorn and Stefan N{\"a}her and Michael Seel
                 and Raimund Seidel and Thomas Schilz and Stefan Schirra
                 and Christian Uhrig",
  abstract =     "A program checker verifies that a particular program
                 execution is correct. We give simple and efficient
                 program checkers for some basic geometric tasks. We
                 report about our experiences with program checking in
                 the context of the LEDA system. We discuss program
                 checking for data structures that have to rely on
                 user-provided functions.",
  month =        feb,
  volume =       "12",
  number =       "1-2",
  journal =      "Computational Geometry",
}

@InProceedings{EVL-1999-330,
  pages =        "413--416",
  year =         "1999",
  title =        "Interactive Visualization of Fluid Dynamics
                 Simulations in Locally Refined Cartesian Grids",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-330",
  author =       "Martin Schultz and Frank Recks and Wolf Bartelheimer
                 and Thomas Ertl",
  abstract =     "This work presents interactive flow visualization
                 techniques specifically adapted for PowerFLOW(tm), a
                 lattice-based CFD code from the EXA corporation. Their
                 Digital Physics(tm) fluid simulation technique is
                 performed on a hierarchy of locally refined cartesian
                 grids with a fine voxel resolution in areas of
                 interesting flow features. Among other applications the
                 PowerFLOW solver is used for aerodynamic simulations in
                 car body development where the advantages of automatic
                 grid generation from CAD models is of great interest.
                 In a joint project with BMW and EXA we are developing a
                 visualization tool which incorporates virtual reality
                 techniques for the interactive exploration of the large
                 scalar and vector data sets. In this paper we describe
                 the specific data structures and interpolation
                 techniques and we report on fast particle tracing
                 taking into account collisions with the car body
                 geometry. An OpenGL Optimizer based implementation
                 allows for the inspection of the flow with particle
                 probes and slice probes at interactive frame rates.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-333,
  pages =        "421--424",
  year =         "1999",
  title =        "{DELTA}'s Virtual Physics Laboratory {A} Comprehensive
                 Learning Platform on Physics & Astronomy",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-333",
  author =       "Sepideh Chakaveh and Udo Zlender and Detlef Skaley and
                 Konstantinos Fostiropoulos and Dieter Breitschwerdt",
  abstract =     "Perhaps the most effective instrument to simplify and
                 to clarify the comprehension of any complex
                 mathematical or scientific theory is through
                 visualisation. Moreover using interactivity & 3D real
                 time representations, one can easily explore and hence
                 learn quickly in the virtual environments. The concept
                 of virtual and safe laboratories has vast potentials in
                 education. With the aid of computer simulations & 3D
                 visualisations, many dangerous or cumbersome
                 experiments may be implemented in the virtual
                 environments, with rather small effort. Nonetheless
                 visualisation alone is of little use if the respective
                 simulation is not scientifically accurate. Hence a
                 rigours combination of precise computation as well as
                 sophisticated visualisation, presented through some
                 intuitive user interface is required to realise a
                 virtual laboratory for education. Here we introduce
                 Delta's Virtual Physics Laboratory, comprising of a
                 wide range of applications in the field of Physics &
                 Astronomy, which can be implemented and used as an
                 interactive learning tool on the World Wide Web.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-334,
  pages =        "433--436",
  year =         "1999",
  title =        "Visualization of Conflicts and Resolutions in a
                 {"}Free Flight{"} Scenario",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-334",
  author =       "Ronald Azuma and Howard Neely III and Mike Daily and
                 Mario Correa",
  abstract =     "'Free Flight' will change todays air traffic control
                 system by giving pilots increased flexibility to choose
                 and modify their routes in real time, reducing costs
                 and increasing system capacity. This increased
                 flexibility comes at the price of increased complexity.
                 If Free Flight is to become a reality, future air
                 traffic controllers, pilots, and airline managers will
                 require new conflict detection, resolution and
                 visualization decision support tools. This paper
                 describes a testbed system for building and evaluating
                 such tools, including its current capabilities, lessons
                 we learned, and feedback received from expert users.
                 The visualization system provides an overall plan view
                 supplemented with a detailed perspective view, allowing
                 a user to examine highlighted conflicts and select from
                 a list of proposed solutions, as the scenario runs in
                 real time. Future steps needed to improve this system
                 are described.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "air traffic control, collision detection, aircraft,
                 decision support tools, interactive 3D graphics",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-335,
  pages =        "425--428",
  year =         "1999",
  title =        "VizCraft: {A} Multidimensional Visualization Tool for
                 Aircraft Configuration Design",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-335",
  author =       "A. Goel and C. Baker and C. A. Shaffer and B. Grossman
                 and R. T. Haftka and W. H. Mason and L. T. Watson",
  abstract =     "We describe a visualization tool to aid aircraft
                 designers during the conceptual design stage. The
                 conceptual design for an aircraft is defined by a
                 vector of 10-30 parameters. The goal is to find a
                 vector that minimizes an objective function while
                 meeting a series of constraints. VizCraft integrates
                 the simulation code that evaluates the design with
                 visualizations for analyzing the design individually or
                 in contrast to other designs. VizCraft allows the
                 designer to easily switch between the view of a design
                 in the form of a parameter set, and a visualization of
                 the corresponding aircraft. The user can easily see
                 which, if any, constraints are violated. VizCraft also
                 allows the user to view a database of designs using
                 parallel coordinates.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "Scientific data visualization, aircraft design,
                 multidisciplinary design optimization, multidimensional
                 visualization",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-336,
  pages =        "429--432",
  year =         "1999",
  title =        "Design and Implementation of an Immersive Geoscience
                 Toolkit",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-336",
  author =       "Christophe Winkler and Fabien Bosquet and Xavier Cavin
                 and Jean-Claude Paul",
  abstract =     "Having a better way to represent and to interact with
                 large geological models are topics of high interest in
                 geoscience, and especially for oil and gas companies.
                 We present in this paper the design and implementation
                 of a visualization program that involves two main
                 features. It is based on the central data model, in
                 order to display in real time the modifications caused
                 by the modeler. Furthermore, it benefits from the
                 different immersive environments which give the user a
                 much more accurate insight of the model than a regular
                 computer screen. Then, we focus on the difficulties
                 that come in the way of performance.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-337,
  pages =        "437--440",
  year =         "1999",
  title =        "Real-Time Visualization of Scalably Large Collections
                 of Heterogeneous Objects",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-337",
  author =       "Douglass Davis and William Ribarsky and T. Y. Jiang
                 and Nickolas Faust and Sean Ho",
  abstract =     "This paper presents results for real-time
                 visualization of out-of-core collections of 3D objects.
                 This is a significant extension of previous methods and
                 shows the generality of hierarchical paging procedures
                 applied both to global terrain and any objects that
                 reside on it. Applied to buildings, the procedure shows
                 the effectiveness of using a screen-based paging and
                 display criterion within a hierarchical framework. The
                 results demonstrate that the method is scalable since
                 it is able to handle multiple collections of buildings
                 (e.g., cities) placed around the earth with full
                 interactivity and without extensive memory load.
                 Further the method shows efficient handling of culling
                 and is applicable to larger, extended collections of
                 buildings. Finally, the method shows that levels of
                 detail can be incorporated to provide improved detail
                 management.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-338,
  pages =        "441--444",
  year =         "1999",
  title =        "Geo-Spatial Visualization for Situational Awareness",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-338",
  author =       "Eliot Feibush and Nikhil Gagvani and Daniel Williams",
  abstract =     "Situational awareness applications require a highly
                 detailed geo-spatial visualization covering a large
                 geographic area. Conventional polygon based terrain
                 modeling would exceed the capacity of current computer
                 rendering. Terrain visualization techniques for a
                 situational awareness application are described in this
                 case study. Visualizing large amounts of terrain data
                 has been achieved using very large texture maps. Sun
                 shading is applied to the terrain texture map to
                 enhance perception of relief features. Perception of
                 submarine positions has been enhanced using a
                 translucent, textured water surface. Each visualization
                 technique is illustrated in the accompanying video
                 tape.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-339,
  pages =        "445--448",
  year =         "1999",
  title =        "{"}Whole Field Modelling{"} - Effective Real-time and
                 Post-survey Visualization of Underwater Pipelines",
  author =       "Paul Chapman and Derek Wills and Peter Stevens and
                 Graham Brookes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-339",
  abstract =     "The detailed underwater bathymetric data provided by
                 Sonar Research and Development's high speed
                 multi-frequency sonar transducer system provides new
                 challenges in the development of interactive seabed
                 visualizing tools. This paper introduces a 'Whole Field
                 Modelling' system developed at Sonar Research and
                 Development Ltd and The Department of Computer Science,
                 University of Hull. This system provides the viewer
                 with a new 3D underwater visualization environment that
                 allows the user to pilot a virtual underwater vehicle
                 around an accurate seabed model. In this paper we
                 consider two example case studies that use the Whole
                 Field Modelling System for visualizing sonar data. Both
                 case studies, visualizing real-time pipeline dredging
                 and pipe restoration visualization, are implemented
                 using real survey data.",
  organization = "IEEE",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@Article{EVL-1999-34,
  pages =        "105--124",
  year =         "1999",
  title =        "Temporally coherent conservative visibility",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-34",
  author =       "Satyan Coorg and Seth Teller",
  abstract =     "Efficiently identifying polygons that are visible from
                 a changing synthetic viewpoint is an important problem
                 in computer graphics. Even with hardware support,
                 simple algorithms like depth-buffering cannot achieve
                 interactive frame rates when applied to geometric
                 models with many polygons. However, a visibility
                 algorithm that exploits the occlusion properties of the
                 scene to identify a superset of visible polygons,
                 without touching most invisible polygons, could achieve
                 fast frame rates while viewing such models. In this
                 paper, we present a new approach to the visibility
                 problem. The novel aspects of our algorithm are that it
                 is temporally coherent and conservative; for all
                 viewpoints the algorithm overestimates the set of
                 visible polygons. As the synthetic viewpoint moves, the
                 algorithm reuses visibility information computed for
                 previous viewpoints. It does so by computing visual
                 events at which visibility changes occur, and
                 efficiently identifying and discarding these events as
                 the viewpoint changes. In essence, the algorithm
                 implicitly constructs and maintains a linearized
                 portion of an aspect graph, a data structure for
                 representing visual events. We demonstrate that the
                 visibility algorithm significantly accelerates
                 rendering of several test models.",
  month =        feb,
  volume =       "12",
  keywords =     "Conservative visibility; Temporal coherence;
                 Hierarchical representations; kD-trees; Visual events;
                 Linearized dynamic aspect graphs",
  number =       "1-2",
  journal =      "Computational Geometry",
}

@InProceedings{EVL-1999-340,
  pages =        "449--452",
  year =         "1999",
  title =        "Visualizing the Evolution of a Subject Domain: {A}
                 Case Study",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-340",
  author =       "Chaomei Chen and Leslie Carr",
  abstract =     "We explore the potential of information visualization
                 techniques in enhancing existing methodologies for
                 domain analysis and modeling. In this case study, we
                 particularly focus on visualizing the evolution of the
                 hypertext field based on author co-citation patterns,
                 including the use of a sliding-window scheme to
                 generate a series of annual snapshots of the domain
                 structure, and a factor-referenced color-coding scheme
                 to highlight predominant specialties in the field.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "applications of visualization, domain analysis,
                 citation analysis, visualization of literature",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-341,
  pages =        "453--456",
  year =         "1999",
  title =        "An Interactive Framework for Visualizing Foreign
                 Currency Exchange Options",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-341",
  author =       "D. L. Gresh and B. E. Rogowitz and M. S. Tignor and E.
                 J. Maryland",
  abstract =     "Analyzing options is a complex, multi-variate process.
                 Option behavior depends on a variety of market
                 conditions which vary over the time course of the
                 option. The goal of this project is to provide an
                 interactive visual environment which allows the analyst
                 to explore these complex interactions, and to select
                 and construct specific views for communicating
                 information to non-analysts (e.g., marketing managers
                 and customers). In this paper we describe an
                 environment for exploring 2- and 3-dimensional
                 representations of options data, dynamically varying
                 parameters, examining how multi-variate relationships
                 develop over time, and exploring the likelihood of the
                 development of different outcomes over the life of the
                 option. We also demonstrate how this tool has been used
                 by analysts to communicate to non-analysts how
                 particular options no longer deliver the behavior they
                 were originally intended to provide.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "visualization, foreign currency options, interactive
                 applications",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-342,
  pages =        "457--462",
  year =         "1999",
  title =        "Visualizing Large-Scale Telecommunication Networks and
                 Services",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-342",
  author =       "Eleftherios E. Koutsofios and Stephen C. North and
                 Russell Truscott and Daniel A. Keim",
  abstract =     "Visual exploration of massive data sets arising from
                 telecommunication networks and services is a challenge.
                 This paper describes SWIFT-3D, an integrated data
                 visualization and exploration system created at AT&T
                 Labs for large scale network analysis. SWIFT-3D
                 integrates a collection of interactive tools that
                 includes pixel-oriented 2D maps, interactive 3D maps,
                 statistical displays, network topology diagrams and an
                 interactive drill-down query interface. Example
                 applications are described, demonstrating a successful
                 application to analyze unexpected network events (high
                 volumes of unanswered calls), and comparison of usage
                 of an Internet service with voice network traffic and
                 local access coverage.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-343,
  pages =        "463--466",
  year =         "1999",
  title =        "Detecting Null Alleles with Vasarely Charts",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-343",
  author =       "Carl Manaster and Elizabeth Nanthakumar and Phillip
                 Morin",
  abstract =     "Microsatellite genotypes can have problems that are
                 difficult to detect with existing tools. One such
                 problem is null alleles. This paper presents a new
                 visualization tool that helps to find and characterize
                 these errors. The paper explains how the tool is used
                 to analyze groups of genotypes and proposes other
                 possible uses.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "Visualization, null alleles, Vasarely chart",
  booktitle =    "EEE Visualization '99",
}

@InProceedings{EVL-1999-344,
  pages =        "467--470",
  year =         "1999",
  title =        "Automating Transfer Function Design for Comprehensible
                 Volume Rendering Based on 3{D} Field Topology
                 Analysis",
  author =       "Issei Fujishiro and Taeko Azuma and Yuriko Takeshima",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-344",
  abstract =     "This paper describes initial results of a 3D field
                 topology analysis for automating transfer function
                 design aiming at comprehensible volume rendering. The
                 conventional Reeb graph-based approach to describing
                 topological features of 3D surfaces is extended to
                 capture the topological skeleton of a volumetric field.
                 Based on the analysis result, which is represented in
                 the form of a hyper Reeb graph, a procedure is proposed
                 for designing appropriate color/opacity transfer
                 functions. Two analytic volume datasets are used to
                 preliminarily prove the feasibility of the present
                 design methodology.",
  organization = "IEEE",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "Volume visualization, direct volume ren-dering,
                 comprehensible rendering, transfer function,
                 isosurface, Reeb graph, critical surfac",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-345,
  pages =        "471--474",
  year =         "1999",
  title =        "Accelerating 3{D} Convolution using Graphics
                 Hardware",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-345",
  author =       "Matthias Hopf and Thomas Ertl",
  abstract =     "Many volume filtering operations used for image
                 enhancement, data processing or feature detection can
                 be written in terms of three-dimensional convolutions.
                 It is not possible to yield interactive frame rates on
                 todays hardware when applying such convolutions on
                 volume data using software filter routines. As modern
                 graphics workstations have the ability to render
                 two-dimensional convoluted images to the frame buffer,
                 this feature can be used to accelerate the process
                 significantly. This way generic 3D convolution can be
                 added as a powerful tool in interactive volume
                 visualization toolkits.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "Convolution, Hardware Acceleration, Volume
                 Visualization",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-346,
  pages =        "475--478",
  year =         "1999",
  title =        "Visualizing Simulated Room Fires",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-346",
  author =       "Jayesh Govindarajan and Matthew Ward and Jonathan
                 Barnett",
  abstract =     "Recent advances in fire science and computer modeling
                 of fires allow scientists to predict fire growth and
                 spread through structures. In this paper we describe a
                 variety of visualizations of simulated room fires for
                 use by both fire protection engineers and fire
                 suppression personnel. We also introduce the concept of
                 fuzzy visualization, which results from the
                 superposition of data from several separate simulations
                 into a single visualization.",
  organization = "IEEE",
  address =      "San Francisco",
  editor =       "David Ebert and Markus Gross and Bernd Hamann",
  keywords =     "Scientific visualization, simulation, fire modeling",
  booktitle =    "IEEE Visualization '99",
}

@InProceedings{EVL-1999-347,
  pages =        "1--12",
  year =         "1999",
  title =        "Interactive Line Art Rendering of FreeformSurfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-347",
  author =       "Gershon Elber",
  abstract =     "In recent years, synthetically created line art
                 renderings have reached quality levels that are
                 aesthetically pleasing. Moreover, the sketch based
                 approach was found to be quite capable at conveying
                 geometrical information in an intuitive manner. While a
                 growing interest in this type of rendering method has
                 yielded successful and appealing results, the developed
                 techniques were, for the most part, too slow to be
                 embedded in real time interactive display. This paper
                 presents a line art rendering method for freeform
                 polynomial and rational surfaces that is capable of
                 achieving real time and interactive display. A careful
                 preprocessing stage that combines an a-priori
                 construction of line art strokes with proper
                 classification of these strokes, allows one to
                 significantly alleviate the computational cost, in real
                 time, of the sketch based rendering, and enables
                 interactive line art display.",
  editor =       "P. Brunet and R. Scopigno",
  keywords =     "Illustrations, Line Art Drawings, Freeform Surfaces,
                 NURBS, Surface Coverage, Real Time, Interaction.",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-348,
  pages =        "13--22",
  year =         "1999",
  title =        "Comprehensive Halftoning of 3{D} Scenes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-348",
  author =       "O. Veryovka and J. Buchanan",
  abstract =     "The display of images on binary output hardware
                 requires a halftoning step. Conventional halftoning
                 algorithms approximate image values independently from
                 the image content and often introduce artificial
                 texture that obscures fine details. The objective of
                 this research is to adapt a halftoning technique to 3D
                 scene information and thus to enhance the display of
                 computer generated 3D scenes. Our approach is based on
                 the control of halftoning texture by the combination of
                 ordered dithering and error diffusion techniques. We
                 extend our previous work and enable a user to specify
                 the shape, scale, direction, and contrast of the
                 halftoning texture using an external buffer. We control
                 texture shape by constructing a dither matrix from an
                 arbitrary image or a procedural texture. Texture
                 direction and scale are adapted to the external
                 information by the mapping function. Texture contrast
                 and the accuracy of tone reproduction are varied across
                 the image using the error diffusion process. We
                 halftone images of 3D scenes by using the geometry,
                 position, and illumination information to control the
                 halftoning texture. Thus, the texture provides visual
                 cues and can be used to enhance the viewer's
                 comprehension of the display.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  keywords =     "Illustrations, Line Art Drawings, Freeform Surfaces,
                 NURBS, Surface Coverage, Real Time, Interaction.",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-349,
  pages =        "23--30",
  year =         "1999",
  title =        "An Efficient and Flexible Perception Pipeline for
                 Autonomous Agents",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-349",
  author =       "Christophe Bordeux and Ronan Boulic and Daniel
                 Thalmann",
  abstract =     "Agents in virtual environments require a combination
                 of perception and action to behave in an autonomous
                 way. We extend a software architecture for the
                 management of actions blending, called AGENTlib, with a
                 perception mechanism. The perception system provides a
                 uniform interface to various techniques in the field of
                 virtual perception, including synthetic vision,
                 database access and perception persistency. We describe
                 the framework we designed to efficiently filter
                 valuable information from the scene and we address
                 concerns about computation redundancy and data
                 propagation through multiple filtering modules.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@Article{EVL-1999-35,
  pages =        "125--152",
  year =         "1999",
  title =        "Visualizing geometric algorithms over the Web",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-35",
  author =       "James E. Baker and Isabel F. Cruz and Giuseppe Liotta
                 and Roberto Tamassia",
  abstract =     "The visual nature of geometry applications makes them
                 a natural area where visualization can be an effective
                 tool for demonstrating algorithms. In this paper we
                 propose a new model, called Mocha, for interactive
                 visualization of algorithms over the World Wide Web.
                 Mocha is a distributed model with a client-server
                 architecture that optimally partitions the software
                 components of a typical algorithm execution and
                 visualization system, and leverages the power of the
                 Java language, which has become the standard for
                 distributing interactive platform-independent
                 applications across the Web. Mocha provides high levels
                 of security, protects the algorithm code, places a
                 light communication load on the Internet, and allows
                 users with limited computing resources to access
                 executions of computationally expensive algorithms. The
                 user interface combines fast responsiveness with the
                 powerful authoring capabilities of hypertext
                 narratives. We describe the architecture of Mocha, show
                 its advantages over previous methods, and present a
                 prototype that can be accessed by any user with a
                 Java-enabled Web browser. The Mocha prototype has been
                 widely accessed over the Web, as demonstrated by the
                 statistics that we have collected, and the Mocha model
                 has been adopted by other research groups. Mocha is
                 currently part of a broader system, called GeomNet,
                 which performs distributed geometric computing over the
                 Internet.",
  month =        feb,
  volume =       "12",
  keywords =     "Geometric algorithm visualization; World Wide Web;
                 Client-server architecture; Visual interface; Java;
                 Multimedia",
  number =       "1-2",
  journal =      "Computational Geometry",
}

@InProceedings{EVL-1999-350,
  pages =        "31--38",
  year =         "1999",
  title =        "Interactive Cuts through 3-Dimensional Soft Tissue",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-350",
  author =       "Daniel Bielser and Volker A. Maiwald and Markus H.
                 Gross",
  abstract =     "We describe a physically based framework for
                 interactive modeling and cutting of 3-dimensional soft
                 tissue that can be used for surgery simulation. Unlike
                 existing approaches which are mostly designed for
                 tensorproduct grids our methods operate on tetrahedral
                 decompositions giving more topological and geometric
                 flexibility for the efficient modeling of complex
                 anatomical structures. We start from an initial
                 tetrahedralization such as being provided by any
                 conventional meshing method. In order to track
                 topological changes tetrahedra intersected by the
                 virtual scalpel are split into substructures whose
                 connectivity follows the trajectory of the cut, which
                 can be arbitrary. For the efficient computation of
                 collisions between the scalpel and individual
                 tetrahedra we devised a local collision detection
                 algorithm. The underlying physics is approximated
                 through masses and springs attached to each tetrahedral
                 vertex and edge. A hierarchical Runge-Kutta iteration
                 computes the relaxation of the system by traversing the
                 designed data structures in a breadth-first order. The
                 framework includes a force-feedback interface and uses
                 real-time texture mapping to enhance the visual
                 realism.",
  editor =       "P. Brunet and R. Scopigno",
  keywords =     "Physically Based Modeling, Surgery Simulation, Soft
                 Tissue, Tetrahedralization, Interactive Cut, Virtual
                 Scalpel, Runge-Kutta Method",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-351,
  pages =        "39--50",
  year =         "1999",
  title =        "Creating Architectural Models from Images",
  author =       "David Liebowitz and Antonio Criminisi and Andrew
                 Zisserman",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-351",
  abstract =     "We present methods for creating 3D graphical models of
                 scenes from a limited numbers of images, i.e. one or
                 two, in situations where no scene co-ordinate
                 measurements are available. The methods employ
                 constraints available from geometric relationships that
                 are common in architectural scenes - such as
                 parallelism and orthogonality - together with
                 constraints available from the camera. In particular,
                 by using the circular points of a plane simple, linear
                 algorithms are given for computing plane rectification,
                 plane orientation and camera calibration from a single
                 image. Examples of image based 3D modelling are given
                 for both single images and image pairs.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-352,
  pages =        "51--60",
  year =         "1999",
  title =        "Occluder Shadows for Fast Walkthroughs of Urban
                 Environments",
  author =       "Peter Wonka and Dieter Schmalstieg",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-352",
  abstract =     "This paper describes a new algorithm that employs
                 image-based rendering for fast occlusion culling in
                 complex urban environments. It exploits graphics
                 hardware to render and automatically combine a
                 relatively large set of occluders. The algorithm is
                 fast to calculate and therefore also useful for scenes
                 of moderate complexity and walkthroughs with over 20
                 frames per second. Occlusion is calculated dynamically
                 and does not rely on any visibility precalculation or
                 occluder preselection. Speed-ups of one order of
                 magnitude can be obtained.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-353,
  pages =        "61--73",
  year =         "1999",
  title =        "Improved image-based impostors for accelerated
                 rendering",
  author =       "Xavier Decoret and Gernot Schaufler and Francois
                 Sillion and Julie Dorsey",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-353",
  abstract =     "This paper describes the successful combination of
                 pre-generated and dynamically updated image-based
                 representations to accelerate the visualization of
                 complex virtual environments. We introduce a new type
                 of impostor, which has the desirable property of
                 limiting de-occlusion errors to a user-specified
                 amount. This impostor, composed of multiple layers of
                 textured meshes, replaces the distant geometry and is
                 much faster to draw. It captures the relevant depth
                 complexity in the model without resorting to a complete
                 sampling of the scene. We show that layers can be
                 dynamically updated during visualization. This
                 guarantees bounded scene complexity in each frame and
                 also exploits temporal coherence to improve image
                 quality when possible. We demonstrate the strengths of
                 this approach in the context of city walkthroughs.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-354,
  pages =        "73--82",
  year =         "1999",
  title =        "Interactive Multiresolution Editing of Arbitrary
                 Meshes",
  author =       "Seungyong Lee",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-354",
  abstract =     "This paper presents a novel approach to
                 multiresolution editing of a triangular mesh. The basic
                 idea is to embed an editing area of a mesh onto a 2D
                 rectangle and interpolate the user-specified editing
                 information over the 2D rectangle. The result of the
                 interpolation is mapped back to the editing area and
                 then used to update the mesh. We adopt harmonic maps
                 for the embedding and multilevel B-splines for the
                 interpolation. The proposed mesh editing technique can
                 handle an arbitrary mesh without any preprocessing such
                 as remeshing. It runs fast enough to support
                 interactive editing and produces intuitive editing
                 results.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-355,
  pages =        "83--94",
  year =         "1999",
  title =        "Generalized View-Dependent Simplification",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-355",
  author =       "Jihad El-Sana and Amitabh Varshney",
  abstract =     "We propose a technique for performing view-dependent
                 geometry and topology simplifications for
                 level-of-detail-based renderings of large models. The
                 algorithm proceeds by preprocessing the input dataset
                 into a binary tree, the view-dependence tree of general
                 vertex-pair collapses. A subset of the Delaunay edges
                 is used to limit the number of vertex pairs considered
                 for topology simplification. Dependencies to avoid mesh
                 foldovers in manifold regions of the input object are
                 stored in the view-dependence tree in an implicit
                 fashion. We have observed that this not only reduces
                 the space requirements by a factor of two, it also
                 highly localizes the memory accesses at run time. The
                 view-dependence tree is used at run time to generate
                 the triangles for display. We also propose a
                 cubic-spline-based distance metric that can be used to
                 unify the geometry and topology simplifications by
                 considering the vertex positions and normals in an
                 integrated manner.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-356,
  pages =        "95--106",
  year =         "1999",
  title =        "An information theory framework for the analysis of
                 scene complexity",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-356",
  author =       "Miquel Feixas and Esteve del Acebo and Philippe
                 Bekaert and Mateu Sber",
  abstract =     "In this paper we present a new framework for the
                 analysis of scene visibility and radiosity complexity.
                 We introduce a number of complexity measures from
                 information theory quantifying how difficult it is to
                 compute with accuracy the visibility and radiosity in a
                 scene. We define the continuous mutual information as a
                 complexity measure of a scene, independent of whatever
                 discretisation, and discrete mutual information as the
                 complexity of a discretised scene. Mutual information
                 can be understood as the degree of correlation or
                 dependence between all the points or patches of a
                 scene. Thus, low complexity corresponds to low
                 correlation and vice versa. Experiments illustrating
                 that the best mesh of a given scene among a number of
                 alternatives corresponds to the one with the highest
                 discrete mutual information, indicate the feasibility
                 of the approach. Unlike continuous mutual information,
                 which is very cheap to compute, the computation of
                 discrete mutual information can however be quite
                 demanding. We will develop cheap complexity measure
                 estimates and derive practical algorithms from this
                 framework in future work.",
  editor =       "P. Brunet and R. Scopigno",
  keywords =     "Rendering, radiosity, Monte Carlo, information theory,
                 entropy, mutual information",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-357,
  pages =        "107--118",
  year =         "1999",
  title =        "A Free Form Feature Taxonomy",
  author =       "M. Fontana and F. Giannini and M. Meirana",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-357",
  abstract =     "In this paper the notion of free form feature for
                 aesthetic design is presented. The design of industrial
                 products constituted by free form surfaces is done by
                 using CAD systems representing curves and surfaces by
                 means of NURBS functions, which are usually defined by
                 low level entities that are not intuitive and require
                 some knowledge of the mathematical language. Similarly
                 to the feature-based approach adopted by CAD systems
                 for classical mechanical design, a set of high level
                 modelling entities which provides commonly performed
                 shape modifications has been identified. Particularly,
                 the paper suggests a classification of the so-called
                 detail features for an aesthetic and/or functional
                 characterization of predefined free form surfaces.
                 Feature types are formally described by means of an
                 analytical definition of the surface modification
                 through deformation and elimination laws. A topological
                 classification is then given according to the
                 application domain of such laws. A further
                 sub-classification of morphological types is then
                 suggested according to geometric properties of weak
                 convexity and concavity for the resulting modified
                 shape, leading to a taxonomy of simple free form
                 features meaningful for aesthetic design.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-358,
  pages =        "119--130",
  year =         "1999",
  title =        "A Shrink Wrapping Approach to Remeshing Polygonal
                 Surface",
  author =       "Leif P. Kobbelt and Jens Vorsatz and Ulf Labsik and
                 Hans-Peter Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-358",
  abstract =     "Due to their simplicity and flexibility, polygonal
                 meshes are about to become the standard representation
                 for surface geometry in computer graphics applications.
                 Some algorithms in the context of multiresolution
                 representation and modeling can be performed much more
                 efficiently and robustly if the underlying surface
                 tesselations have the special subdivision connectivity.
                 In this paper, we propose a new algorithm for
                 converting a given unstructured triangle mesh into one
                 having subdivision connectivity. The basic idea is to
                 simulate the shrink wrapping process by adapting the
                 deformable surface technique known from image
                 processing. The resulting algorithm generates
                 subdivision connectivity meshes whose base meshes only
                 have a very small number of triangles. The iterative
                 optimization process that distributes the mesh vertices
                 over the given surface geometry guarantees low local
                 distortion of the triangular faces. We show several
                 examples and applications including the progressive
                 transmission of subdivision surfaces.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-359,
  pages =        "131--138",
  year =         "1999",
  title =        "Improved Laplacian Smoothing of Noisy Surface Meshes",
  author =       "J. Vollmer and R. Mencl and and H. M{\"o}ller",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-359",
  abstract =     "This paper presents a technique for smoothing
                 polygonal surface meshes that avoids the well-known
                 problem of deformation and shrinkage caused by many
                 smoothing methods, like e.g. the Laplacian algorithm.
                 The basic idea is to push the vertices of the smoothed
                 mesh back towards their previous locations. This
                 technique can be also used in order to smooth
                 unstructured point sets, by reconstructing a temporary
                 surface mesh to which the smoothing technique is
                 applied. The key observation is that a surface mesh
                 which is not necessarily topologically correct, but
                 which can efficiently be reconstructed, is sufficient
                 for that purpose.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@Article{EVL-1999-36,
  pages =        "241--267",
  year =         "1999",
  title =        "Minimizing support structures and trapped area in
                 two-dimensional layered manufacturing",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-36",
  author =       "Jayanth Majhi and Ravi Janardan and J{\"o}rg Schwerdt
                 and Michiel Smid and Prosenjit Gupta",
  abstract =     "Algorithms are given for the two-dimensional versions
                 of optimization problems arising in layered
                 manufacturing, where a polygonal object is built by
                 slicing its CAD model and manufacturing the slices
                 successively. The problems considered are minimizing
                 (i) the contact-length between the supports and the
                 manufactured object, (ii) the area of the support
                 structures used, and (iii) the area of the so-called
                 trapped regions---factors that affect the cost and
                 quality of the process.",
  month =        apr,
  volume =       "12",
  keywords =     "Layered manufacturing; Computational geometry;
                 Optimization",
  number =       "3-4",
  journal =      "Computational Geometry",
}

@InProceedings{EVL-1999-360,
  pages =        "139--150",
  year =         "1999",
  title =        "An Adaptive Method for Area Light Sources and Daylight
                 in Ray Tracing",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-360",
  author =       "Jacques Zaninetti and Pierre Boy and Bernard Peroche",
  abstract =     "This paper proposes an adaptive method for taking both
                 (diffuse or not) planar area light sources and daylight
                 into account in a ray tracing environment which
                 separates the calculation of direct and indirect
                 illumination. In a given point, direct illumination due
                 to a light source or to natural light is represented by
                 a vector, the direction and magnitude of which being
                 computed through an adaptive area approach, which is
                 driven by the solid angle according to which a part of
                 the source is seen from the current point. In the case
                 of unoccluded diffuse polygonal sources, an analytical
                 formula is used which gives an exact value for this
                 vector.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-361,
  pages =        "151--160",
  year =         "1999",
  title =        "Adaptive Acquisition of Lumigraphs from Synthetic
                 Scenes",
  author =       "Hartmut Schirmacher and Wolfgang Heidrich and
                 Hans-Peter Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-361",
  abstract =     "Light fields and Lumigraphs are capable of rendering
                 scenes of arbitrary geometrical or illumination
                 complexity in real time. They are thus interesting ways
                 of interacting with both recorded real-world and
                 high-quality synthetic scenes. Unfortunately, both
                 light fields and Lumigraph rely on a dense sampling of
                 the illumination to provide a good rendering quality.
                 This induces high costs both in terms of storage
                 requirements and computational resources for the image
                 acquisition. Techniques for acquiring adaptive light
                 field and Lumigraph representations are thus mandatory
                 for practical applications. In this paper we present a
                 method for the adaptive acquisition of images for
                 Lumigraphs from synthetic scenes. Using image warping
                 to predict the potential improvement in image quality
                 when adding a certain view, we decide which new views
                 of the scene should be rendered and added to the light
                 field. This a-priori error estimator accounts for both
                 visibility problems and illumination effects such as
                 specular highlights.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-362,
  pages =        "161--172",
  year =         "1999",
  title =        "Compact Metallic Reflectance Models",
  author =       "Laszlo Neumann and Attila Neumann and Laszlo
                 Szirmay-Kalos",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-362",
  abstract =     "The paper presents simple, physically plausible, but
                 not physically based reflectance models for metals and
                 other specular materials. So far there has been no
                 metallic BRDF model that is easy to compute, suitable
                 for fast importance sampling and is physically
                 plausible. This gap is filled by appropriate
                 modifications of the Phong, Blinn and the Ward models.
                 The Phong and the Blinn models are known not to have
                 metallic characteristics. On the other hand, this paper
                 also shows that the Cook-Torrance and the Ward models
                 are not physically plausible, because of their behavior
                 at grazing angles. We also compare the previous and the
                 newly proposed models. Finally, the generated images
                 demonstrate how the metallic impression can be provided
                 by the new models.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  keywords =     "Reflectance function, BRDF representation, albedo
                 function, importance sampling.",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-363,
  pages =        "173--182",
  year =         "1999",
  title =        "Capturing and Re-Using Rendition Styles for
                 Non-Photorealistic Rendering",
  author =       "J. Hamel and T. Strothotte",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-363",
  abstract =     "Rendering high-quality non-photorealistic images of a
                 given geometric model is often associated with a
                 considerable amount of effort on the part of a user to
                 fine-tune the rendition. In this paper we introduce a
                 method and tools for re-using the user's effort
                 invested in one model for the rendering of other
                 models. Our method uses templates to describe rendition
                 styles. The paper gives a number of examples of the
                 successful transfer of styles from one model to
                 another.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-364,
  pages =        "183--194",
  year =         "1999",
  title =        "An Interactive Designing System with Virtual Sculpting
                 and Virtual Woodcut Printing",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-364",
  author =       "S. Mizuno and M. Okada and J. Toriwak",
  abstract =     "In this paper, we propose an interactive designing
                 method and a system based on it to create 3D objects
                 and 2D images. This system consists of two subsystems
                 for virtual sculpting to create a 3D shape and virtual
                 printing to produce a picture with a printing block. In
                 the virtual sculpting subsystem, a user can form solid
                 objects with curved surfaces as if sculpting them. The
                 user operates virtual chisels, and can remove or attach
                 arbitrary shapes of ellipsoids or cubes from or to the
                 workpiece. A 3D object generated by virtual sculpting
                 looks like a real wooden sculpture. If using a board as
                 a workpiece, a user can generate a virtual printing
                 block. In the virtual printing subsystem, a user can
                 synthesize a woodcut printing image from the virtual
                 printing block mentioned above, a virtual paper sheet,
                 and a printing brush. The user can synthesize a
                 realistic woodcut print with a procedure similar to the
                 actual woodcut printing.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-365,
  pages =        "195--208",
  year =         "1999",
  title =        "Computer-Generated Graphite Pencil Rendering of 3{D}
                 Polygonal Models",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-365",
  author =       "Mario C. Sousa and John W. Buchanan",
  abstract =     "Researchers in non-photorealistic rendering have
                 investigated the display of three-dimensional worlds
                 using various display models. In particular, recent
                 work has focused on the modeling of traditional
                 artistic media and styles such as pen-and-ink
                 illustration and watercolor painting. By providing 3D
                 rendering systems that use these alternative display
                 models users can generate traditional illustration
                 renderings of their three-dimensional worlds. In this
                 paper we present our graphite pencil 3D renderer. We
                 have broken the problem of simulating pencil drawing
                 down into four fundamental parts: (1) simulating the
                 drawing materials (graphite pencil and drawing paper,
                 blenders and kneaded eraser), (2) modeling the drawing
                 primitives (individual pencil strokes and mark-making
                 to create tones and textures), (3) simulating the basic
                 rendering techniques used by artists and illustrators
                 familiar with pencil rendering, and (4) modeling the
                 control of the drawing composition. Each part builds
                 upon the others and is essential to developing the
                 framework for higher-level rendering methods and tools.
                 In this paper we present parts 2, 3, and 4 of our
                 research. We present non-photorealistic graphite pencil
                 rendering methods for outlining and shading. We also
                 present the control of drawing steps from preparatory
                 sketches to finished rendering results. We demonstrate
                 the capabilities of our approach with a variety of
                 images generated from 3D models.",
  editor =       "P. Brunet and R. Scopigno",
  keywords =     "Nonrealistic rendering, rendering systems, natural
                 media simulation, paint systems.",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-366,
  pages =        "209--220",
  year =         "1999",
  title =        "Weighted Multipass Methods for Global Illumination",
  author =       "Frank Suykens and Yves D. Willems",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-366",
  abstract =     "In multipass rendering, care has to be taken to
                 include all light transport only once in the final
                 solution. Therefore the different methods in current
                 multipass configurations handle a perfectly disjunct
                 part of the light transport. In this paper a Monte
                 Carlo variance reduction technique is presented that
                 probabilistically weights overlapping transport between
                 different methods. A good heuristic for the weights is
                 derived so that strengths of the respective methods are
                 retained. The technique is applied to a combination of
                 radiosity and bidirectional path tracing and
                 significant improvement is obtained over the
                 non-weighted combination. This method promises to be a
                 very useful extension to other multipass algorithms as
                 well.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-367,
  pages =        "221--232",
  year =         "1999",
  title =        "A Practical Analysis of Clustering Strategies for
                 Hierarchical Radiosity",
  author =       "Jean-Marc Hasenfratz and Cyrille Damez and Francois
                 Sillion and George Drettakis",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-367",
  abstract =     "The calculation of radiant energy balance in complex
                 scenes has been made possible by hierarchical radiosity
                 methods based on clustering mechanisms. Although
                 clustering offers an elegant theoretical solution by
                 reducing the asymptotic complexity of the algorithm,
                 its practical use raises many difficulties, and may
                 result in image artifacts or unexpected behavior. This
                 paper proposes a detailed analysis of the expectations
                 placed on clustering and compares the relative merits
                 of existing, as well as newly introduced, clustering
                 algorithms. This comparison starts from the precise
                 definition of various clustering strategies based on a
                 taxonomy of data structures and construction
                 algorithms, and proceeds to an experimental study of
                 the clustering behavior for real-world scenes.
                 Interestingly, we observe that for some scenes light is
                 difficult to simulate even with clustering. Our results
                 lead to a series of observations characterizing the
                 adequacy of clustering methods for meeting such diverse
                 goals as progressive solution improvement, efficient
                 ray casting acceleration, and faithful representation
                 of object density for approximate visibility
                 calculations.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-368,
  pages =        "233--244",
  year =         "1999",
  title =        "Stochastic Iteration for Non-diffuse Global
                 Illumination",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-368",
  author =       "Laszlo Szirmay-Kalos",
  abstract =     "This paper presents a single-pass, view-dependent
                 method to solve the rendering equation, using a
                 stochastic iterational scheme where the transport
                 operator is selected randomly in each iteration. The
                 requirements of convergence are given for the general
                 case. To demonstrate the basic idea, a very
                 simple,continuous random transport operator is
                 examined, which gives back the light tracing algorithm
                 incorporating Russian roulette. Then, a new mixed
                 continuous and finite-element based iteration method is
                 proposed, which uses ray-bundles to transfer the
                 radiance in a single random direction. The resulting
                 algorithm is fast, it provides initial results in
                 seconds and accurate solutions in minutes and does not
                 suffer from the error accumulation problem and the high
                 memory demand of other finite-element and hierarchical
                 approaches.",
  editor =       "P. Brunet and R. Scopigno",
  keywords =     "Rendering equation, global radiance, Monte-Carlo
                 integration, light-tracing, global ray-bundle
                 tracing.",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-369,
  pages =        "245--256",
  year =         "1999",
  title =        "Texturing 3d Models of Real World Objects from
                 Multiple Unregistered Photographic Views",
  author =       "Peter J. Neugebauer and Konrad Klein",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-369",
  abstract =     "As the efficiency of computer graphic rendering
                 methods is increasing, generating realistic models is
                 now becoming a limiting factor. In this paper we
                 present a new technique to enhance already existing
                 geometry models of real world objects with textures
                 reconstructed from a sparse set of unregistered still
                 photographs. The aim of the proposed technique is the
                 generation of nearly photo-realistic models of
                 arbitrarily shaped objects with minimal effort. In our
                 approach, we require neither a prior calibration of the
                 camera nor a high precision of the user's interaction.
                 Two main problems have to be addressed of which the
                 first is the recovery of the unknown positions and
                 parameters of the camera. An initial estimate of the
                 orientation is calculated from interactively selected
                 point correspondences. Subsequently, the unknown
                 parameters are accurately calculated by minimising a
                 blend of objective functions in a 3D-2D projective
                 registration approach. The key point of the proposed
                 method of registration is a novel filtering approach
                 which utilises the spatial information provided by the
                 geometry model. Second, the individual images have to
                 be combined yielding a set of consistent texture maps.
                 We present a robust method to recover the texture from
                 the photographs thereby preserving high spatial
                 frequencies and eliminating artifacts, particularly
                 specular highlights. Parts of the object not seen in
                 any of the photographs are interpolated in the textured
                 model. Results are shown for three complex example
                 objects with different materials and numerous
                 self-occlusions.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@Article{EVL-1999-37,
  pages =        "3--19",
  year =         "1999",
  title =        "Rotational polygon containment and minimum enclosure
                 using only robust 2{D} constructions",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-37",
  author =       "Victor J. Milenkovic",
  abstract =     "An algorithm and a robust floating point
                 implementation is given for rotational polygon
                 containment: given polygons P1,P2,P3,...,Pk and a
                 container polygon C , find rotations and translations
                 for the k polygons that place them into the container
                 without overlapping. A version of the algorithm and
                 implementation also solves rotational minimum
                 enclosure: given a class C of container polygons, find
                 a container C&isin;C of minimum area for which
                 containment has a solution. The minimum enclosure is
                 approximate: it bounds the minimum area between
                 $(1-\epsilon) A$ and A. Experiments indicate that
                 finding the minimum enclosure is practical for k=2,3
                 but not larger unless optimality is sacrificed or
                 angles ranges are limited (although these solutions can
                 still be useful). Important applications for these
                 algorithm to industrial problems are discussed. The
                 paper also gives practical algorithms and numerical
                 techniques for robustly calculating polygon set
                 intersection, Minkowski sum, and range intersection:
                 the intersection of a polygon with itself as it rotates
                 through a range of angles. In particular, it introduces
                 nearest pair rounding, which allows all these
                 calculations to be carried out in rounded floating
                 point arithmetic.",
  month =        may,
  volume =       "13",
  keywords =     "Layout; Packing or nesting of irregular polygons;
                 Containment; Minimum enclosure; Robust geometry;
                 Geometric rounding",
  number =       "1",
  journal =      "Computational Geometry",
}

@InProceedings{EVL-1999-370,
  pages =        "257--264",
  year =         "1999",
  title =        "Virtual Dunhuang Art Cave: {A} Cave within a {CAVE}",
  author =       "B. Lutz and M. Weintke",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-370",
  abstract =     "Virtual Reality can present historical places in a
                 three-dimensional and interactive way, giving visitors
                 a photorealistic impression of objects. Not only
                 existing scenarios can be shown, but VR can also be
                 used to rebuild scenarios that were damaged or
                 destroyed a long time ago, giving new life to the
                 cultural heritage. We used Virtual Reality to present
                 the Mogao Grottos in Dunhuang. This cave site is one of
                 the most important cultural and religious places by the
                 ancient Silk Road. The presentation is to give visitors
                 the impression of visiting the cave site and provide
                 information about the caves, paintings and statues in
                 an interesting way. To achieve this, we developed a
                 new, intuitive interaction paradigm, which enables the
                 user to explore the caves. To give observers a
                 photorealistic impression of the caves and to create a
                 feeling of immersion, innovative rendering techniques
                 were integrated. The resulting presentation combines
                 Virtual Reality and archaeology to give tourists a
                 realistic experience of this cave site and to support
                 scientists in their research work.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-371,
  pages =        "265--276",
  year =         "1999",
  title =        "A New Method of Image Mosaicking and Its Application
                 to Cultural Heritage Representation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-371",
  author =       "G. M. Cortelazzo and L. Lucchese",
  abstract =     "This paper presents an original two-step procedure for
                 estimating projective transformations between pairs of
                 images: first, the transformation between the images is
                 approximated as an affine transformation; second, this
                 estimate is refined into that of a projective
                 transformation. This strategy for matching projective
                 views is computationally very efficient. The proposed
                 method can be applied both to mosaicking of high
                 resolution images of planar textured objects (e.g.,
                 frescoes and paintings), with subpixel accuracy, and to
                 construction of panoramic images. Practical examples of
                 mosaicking of cultural heritage imagery obtained by
                 using the presented procedure are discussed in the
                 paper.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-372,
  pages =        "277--286",
  year =         "1999",
  title =        "A Translucent Sketchpad for the Virtual Table
                 Exploring Motion-based Gesture Recognition",
  author =       "L. M. Encarnacao and O. Bimber and D. Schmalstieg and
                 S. D. Chandler",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-372",
  abstract =     "The Virtual Table presents stereoscopic graphics to a
                 user in a workbench-like setting. For this device, a
                 user interface and new interaction techniques have been
                 developed based on transparent props -a tracked
                 hand-held pen and a pad. These props, particularly the
                 pad, are augmented with 3D graphics from the Virtual
                 Table&rsquo;s display that can serve as a palette for
                 tools and controls as well as a window-like see-through
                 interface, a plane-shaped and through-the-plane tool,
                 supporting a variety of new interaction techniques.
                 This paper reports on an extension of this
                 user-interface design space which uses gestural input
                 to create and control solid geometries for CAD and
                 conceptual design. The application of gestural
                 interfaces is a common method for interacting with
                 virtual environments on a habitual and natural basis.
                 The motion-based gesture recognition presented here
                 uses Fuzzy Logic to support a predictable, flexible,
                 and efficient learning process. This new interaction
                 paradigm greatly increases the Virtual Table&rsquo;s
                 suitability for design tasks. Traditional CAD dialogue
                 can be combined with intuitive rapid sketching of
                 geometry on the pad. Additionally, the resulting events
                 and objects can be associated with scene details below
                 the translucent tablet.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-373,
  pages =        "287--296",
  year =         "1999",
  title =        "Interactive Mechanical Design Variation for Haptics
                 and {CAD}",
  author =       "Donald D. Nelson and Elaine Cohen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-373",
  abstract =     "A fast design variation technique for mechanical
                 systems is presented. It is used to interactively
                 optimize mechanical characteristics while
                 'self-assembling' or satisfying large systems of
                 mechanical constraints. The high speed method is
                 central to providing inverse dynamics force feedback in
                 haptics and control applications. Performance
                 advantages with the use of augmented coordinates for
                 inverse dynamics of closed loop topologies are also
                 noted. The interaction framework allows manipulation of
                 complex assemblies while maintaining kinematically
                 admissible configurations though linkage and joint
                 limit constraints. Furthermore, design variables such
                 as link length can be treated as free variables and
                 optimized to meet design criteria such as assembly
                 dexterity. Assemblies with flexible bodies fit
                 naturally within this framework. Thus, the contribution
                 of this paper is the advancement of techniques in
                 augmented coordinates for the kinematic and force
                 feedback interaction with virtual mechanical assembly
                 design optimization at force control rates.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-374,
  pages =        "297--308",
  year =         "1999",
  title =        "The Hybrid World of Virtual Environments",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-374",
  author =       "Shamus Smith and David Duke",
  abstract =     "Much of the work concerned with virtual environments
                 has addressed the development of new rendering
                 technologies or interaction techniques. As the
                 technology matures and becomes adopted in a wider range
                 of applications, there is, however, a need to better
                 understand how this technology can be accommodated in
                 software engineering practice. A particular challenge
                 presented by virtual environments is the complexity of
                 the interaction that is supported, and sometimes
                 necessary, for a particular task. Methods such as
                 finite-state automata which are used to represent and
                 design dialogue components for more conventional
                 interfaces, e.g. using direct manipulation within a
                 desktop model, do not seem to capture adequately the
                 style of interaction that is afforded by richer input
                 devices and graphical models. In this paper, we suggest
                 that virtual environments are, fundamentally, what are
                 known as hybrid systems. Building on this insight, we
                 demonstrate how techniques developed for modelling
                 hybrid systems can be used to represent and understand
                 virtual interaction in a way that can be used in the
                 specification and design phases of software
                 development, and which have the potential to support
                 prototyping and analysis of virtual interfaces.",
  editor =       "P. Brunet and R. Scopigno",
  keywords =     "Virtual Environments, hybrid systems, interaction
                 techniques, VE design, HyNet.",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-375,
  pages =        "309--318",
  year =         "1999",
  title =        "An Informed Environment dedicated to the simulation of
                 virtual humans in urban context",
  author =       "Nathalie Farenc and Ronan Boulic and Daniel Thalmann",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-375",
  abstract =     "In this paper, we outline the creation of an Informed
                 Environment, dedicated to urban life simulation.We
                 propose methods and tools for creating and providing
                 the information necessary for animating virtual humans
                 in a city using an Informed Environment. The Informed
                 Environment is based on a hierarchical decomposition of
                 a urban scene into Environment Entities providing
                 geometrical information as well as semantic notions,
                 thus allowing a more realistic simulation of human
                 behaviour. In this manner, virtual humans can integrate
                 with a certain kind of urban knowledge.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-376,
  pages =        "319--330",
  year =         "1999",
  title =        "Partitioning and Handling Massive Models for
                 Interactive Collision Detection",
  author =       "A. Wilson and E. Larsen and D. Manocha and M. C. Lin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-376",
  abstract =     "We describe an approach for interactive collision
                 detection and proximity computations on massive models
                 composed of millions of geometric primitives. We
                 address issues related to interactive data access and
                 processing in a large geometric database, which may not
                 fit into main memory of typical desktop workstations or
                 computers. We present a new algorithm using overlap
                 graphs for localizing the 'regions of interest' within
                 a massive model, thereby reducing runtime memory
                 requirements. The overlap graph is computed off-line,
                 pre-processed using graph partitioning algorithms, and
                 modified on the fly as needed. At run time, we traverse
                 localized sub-graphs to check the corresponding
                 geometry for proximity and pre-fetch geometry and
                 auxiliary data structures. To perform interactive
                 proximity queries, we use bounding-volume hierarchies
                 and take advantage of spatial and temporal coherence.
                 Based on the proposed algorithms, we have developed a
                 system called IMMPACT and used it for interaction with
                 a CAD model of a power plant consisting of over 15
                 million triangles. We are able to perform a number of
                 proximity queries in real-time on such a model. In
                 terms of model complexity and application to large
                 models, we have improved the performance of interactive
                 collision detection and proximity computation
                 algorithms by an order of magnitude.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-377,
  pages =        "331--338",
  year =         "1999",
  title =        "Seamless Integration of Databases in {VR} for
                 Constructing Virtual Environments",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-377",
  author =       "Ingo Soetebier and Ralf Durner and Norbert Braun",
  abstract =     "An approach for authoring virtual environments within
                 the virtual environments themselves is presented,
                 integrating a database containing arbitrary components
                 that are used to construct the 3D scene. The issues
                 important to a seamless integration of the database,
                 such as multimedia data storage and database linkage
                 are discussed, with the focus on the human-computer
                 interaction component. A concept for a 3D database
                 interface is described for query, presentation and
                 usage of query results in the virtual environment.
                 Finally, an implementation of the concept using VRML
                 and Java is presented.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-378,
  pages =        "339--348",
  year =         "1999",
  title =        "Image Morphing with Feature Preserving Texture",
  author =       "Ayellet Tal and Gershon Elber",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-378",
  abstract =     "Image metamorphosis as an animation tool has mostly
                 been employed in the context of the entire image. This
                 work explores the use of isolated and focused image
                 based metamorphosis between two-dimensional objects,
                 while capturing the features, colors, and textures of
                 the objects. This pinpointed approach allows one to
                 independently overlay several such dynamic shapes,
                 without any bleeding of one shape into another. Hence,
                 shape blending and metamorphosis of two-dimensional
                 objects can be exploited as animated sequences of clip
                 arts.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-379,
  pages =        "349--358",
  year =         "1999",
  title =        "Efficient and Handy Texture Mapping on 3{D} Surfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-379",
  author =       "Kenji Matsushita and Toyohisa Kaneko",
  abstract =     "There has been a rapid technical progress in
                 three-dimensional (3D) computer graphics. But gathering
                 surface and texture data is yet a laborious task. This
                 paper addresses the problem of mapping photographic
                 images on the surface of a 3D object whose geometric
                 data are already known. We propose an efficient and
                 handy method for acquiring textures and mapping them
                 precisely on the surface, employing a digital camera
                 alone. We describe an algorithm for selecting a minimal
                 number of camera positions that can cover the entire
                 surface of a given object and also an algorithm to
                 determine camera's position and direction for each
                 photograph taken so as to paste it to the corresponding
                 surfaces precisely. We obtained a matching accuracy
                 within a pixel on a surface through three experimental
                 examples, by which the practicability of our method is
                 demonstrated.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@Article{EVL-1999-38,
  pages =        "21--47",
  year =         "1999",
  title =        "Point labeling with sliding labels",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-38",
  author =       "Marc van Kreveld and Tycho Strijk and Alexander
                 Wolff",
  abstract =     "This paper discusses algorithms for labeling sets of
                 points in the plane, where labels are not restricted to
                 some finite number of positions. We show that
                 continuously sliding labels allow more points to be
                 labeled both in theory and in practice. We define six
                 different models of labeling. We compare models by
                 analyzing how many more points can receive labels under
                 one model than another. We show that maximizing the
                 number of labeled points is NP-hard in the most general
                 of the new models. Nevertheless, we give a
                 polynomial-time approximation scheme and a simple and
                 efficient factor- (1)/(2) approximation algorithm for
                 each of the new models. Finally, we give experimental
                 results based on the factor- (1)/(2) approximation
                 algorithm to compare the models in practice. We also
                 compare this algorithm experimentally to other
                 algorithms suggested in the literature.",
  month =        may,
  volume =       "13",
  keywords =     "Map labeling; Point annotation; Greedy approximation
                 algorithm; Approximation scheme",
  number =       "1",
  journal =      "Computational Geometry",
}

@InProceedings{EVL-1999-380,
  pages =        "359--368",
  year =         "1999",
  title =        "Data Intermixing and Multi-Volume Rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-380",
  author =       "Wenli Cai and Georgios Sakas",
  abstract =     "The main difference between multi-volume rendering and
                 mono-volume rendering is data intermixing. In this
                 paper, we present three levels of data intermixing and
                 their rendering pipelines in direct multi-volume
                 rendering, which discriminate image level intensity
                 intermixing, accumulation level opacity intermixing,
                 and illumination model level parameter intermixing. In
                 the context of radiotherapy treatment planning,
                 different data intermixing methods are applied to three
                 volumes, including CT volume, Dose volume, and
                 Segmentation volume, to compare the features of
                 different data intermixing methods.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-381,
  pages =        "369--376",
  year =         "1999",
  title =        "Fast Polyhedral Cell Sorting for Interactive Rendering
                 of UnstructuredGrids",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-381",
  author =       "Jo{\~{a}}o Comba and James T. Klosowski and Nelson Max
                 and Joseph S. B. Mitchell Claudio T. Silva and Peter L.
                 Williams",
  abstract =     "Direct volume rendering based on projective methods
                 works by projecting, in visibility order, the
                 polyhedral cells of a mesh onto the image plane, and
                 incrementally compositing the cell's color and opacity
                 into the final image. Crucial to this method is the
                 computation of a visibility ordering of the cells. If
                 the mesh is 'well-behaved' (acyclic and convex), then
                 the MPVO method of Williams provides a very fast
                 sorting algorithm; however, this method only computes
                 an approximate ordering in general datasets, resulting
                 in visual artifacts when rendered. A recent method of
                 Silva et al. removed the assumption that the mesh is
                 convex, by means of a sweep algorithm used in
                 conjunction with the MPVO method; their algorithm is
                 substantially faster than previous exact methods for
                 general meshes. In this paper we propose a new
                 technique, which we call BSP-XMPVO, which is based on a
                 fast and simple way of using binary space partitions on
                 the boundary elements of the mesh to augment the
                 ordering produced by MPVO. Our results are shown to be
                 orders of magnitude better than previous exact methods
                 of sorting cells.",
  editor =       "P. Brunet and R. Scopigno",
  keywords =     "Volume rendering, scientific visualization, finite
                 element methods, depth ordering, volume visualization,
                 visibility ordering.",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-382,
  pages =        "377--384",
  year =         "1999",
  title =        "Fast Lines: a Span by Span Method",
  author =       "V. Boyer and J. J. Bourdin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-382",
  abstract =     "Straight line's scan conversion and drawing is a major
                 field in computer graphics. Algorithm's time
                 computation is very important. Nowadays, most of
                 research papers suggest improvements of the DDA method
                 that was first presented by J. Bresenham. But other
                 approaches exist as well like combinatory analysis and
                 linguistic methods. Both of them use multiple string
                 copies that slow down the efficiency of the algorithms.
                 This paper proposes a new algorithm based on a careful
                 analysis of the line segments' properties some of them
                 previously unused. Our algorithm is proved
                 significantly faster than previously published ones.",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InProceedings{EVL-1999-383,
  pages =        "385--395",
  year =         "1999",
  title =        "An Efficient 2 1/2 {D} Rendering and Compositing
                 System",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-383",
  author =       "Max Froumentin and Philip Willis",
  abstract =     "We describe a method for doing image compositing using
                 either 2D geometric shapes or raster images as input
                 primitives. The resolution of the final image is
                 virtually unlimited but, as no frame buffer is used,
                 performance is much less dependant on resolution than
                 with standard painting programs, allowing rendering
                 very large images in reasonable time. Many standard
                 features found in compositing programs have been
                 implemented, like hierarchical data structures for
                 input primitives, lighting control for each layer and
                 filter operations (for antialiasing or defocus).",
  editor =       "P. Brunet and R. Scopigno",
  volume =       "18(3)",
  booktitle =    "Computer Graphics Forum (Eurographics '99)",
  publisher =    "The Eurographics Association and Blackwell
                 Publishers",
}

@InCollection{EVL-1999-384,
  pages =        "1--12",
  year =         "1999",
  title =        "Acquiring a Radiance Distribution to Superimpose
                 Virtual Objects onto a Real Scene",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-384",
  author =       "Imari Sato and Yoichi Sato and Katsushi Ikeuchi",
  abstract =     "This paper describes a new method for superimposing
                 virtual objects with correct shadings onto an image of
                 a real scene. Unlike the previously proposed methods,
                 our method can measure a radiance distribution of a
                 real scene automatically and use it for superimposing
                 virtual objects appropriately onto a real scene. First,
                 a geometric model of the scene is constructed from a
                 pair of omnidirectional images by using an
                 omnidirectional stereo algorithm. Then, radiance of the
                 scene is computed from a sequence of omnidirectional
                 images taken with different shutter speeds and mapped
                 onto the constructed geometric model. The radiance
                 distribution mapped onto the geometric model is used
                 for rendering virtual objects superimposed onto the
                 scene image. As a result, even for a complex radiance
                 distribution, our method can superimpose virtual
                 objects with convincing shadings and shadows cast onto
                 the real scene. We successfully tested the proposed
                 method by using real images to show its
                 effectiveness.",
  editor =       "Hans Hagen",
  keywords =     "Computer graphics, computer vision, augmented reality,
                 illumination distribution measurement, omnidirectional
                 stereo algorithm.",
  volume =       "5 (1)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-1999-385,
  pages =        "13--29",
  year =         "1999",
  title =        "Dynamic Scene Occlusion Culling",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-385",
  author =       "Oded Sudarsky and Craig Gotsman",
  abstract =     "Large, complex 3D scenes are best rendered in an
                 output-sensitive way, i.e., in time largely independent
                 of the entire scene model's complexity. Occlusion
                 culling is one of the key techniques for
                 output-sensitive rendering. We generalize existing
                 occlusion culling algorithms, intended for static
                 scenes, to handle dynamic scenes having numerous moving
                 objects. The data structure used by an occlusion
                 culling method is updated to reflect the objects'
                 possible positions. To avoid updating the structure for
                 every dynamic object at each frame, a temporal bounding
                 volume (TBV) is created for each occluded dynamic
                 object, using some known constraints on the object's
                 motion. The TBV is inserted into the structure instead
                 of the object. Subsequently, the object is ignored as
                 long as the TBV is occluded and guaranteed to contain
                 the object. The generalized algorithms' rendering time
                 is linearly affected only by the scene's visible parts,
                 not by hidden parts or by occluded dynamic objects. Our
                 techniques also save communications in distributed
                 graphic systems, e.g., multiuser virtual environments,
                 by eliminating update messages for hidden dynamic
                 objects. We demonstrate the adaptation of two occlusion
                 culling algorithms to dynamic scenes: hierarchical
                 Z-buffering and BSP tree projection.",
  editor =       "Hans Hagen",
  keywords =     "Occlusion culling, dynamic scenes, distributed
                 multiuser virtual environments, hierarchical Z-buffer,
                 octrees, BSP tree.",
  volume =       "5 (1)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-1999-386,
  pages =        "30--46",
  year =         "1999",
  title =        "On a Construction of a Hierarchy of Best Linear Spline
                 Approximations Using Repeated Bisection",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-386",
  author =       "Bernd Hamann and Benjamin W. Jordan and David F.
                 Wiley",
  abstract =     "We present a method for the construction of
                 hierarchies of single-valued functions in one, two, and
                 three variables. The input to our method is a coarse
                 decomposition of the compact domain of a function in
                 the form of an interval (univariate case), triangles
                 (bivariate case), or tetrahedra (trivariate case). We
                 compute best linear spline approximations, understood
                 in an integral least squares sense, for functions
                 defined over such triangulations and refine
                 triangulations using repeated bisection. This requires
                 the identification of the interval (triangle,
                 tetrahedron) with largest error and splitting it into
                 two intervals (triangles, tetrahedra). Each bisection
                 step requires the recomputation of all spline
                 coefficients due to the global nature of the best
                 approximation problem. Nevertheless, this can be done
                 efficiently by bisecting multiple intervals (triangles,
                 tetrahedra) in one step and by reducing the bandwidths
                 of the matrices resulting from the normal equations.",
  editor =       "Hans Hagen",
  keywords =     "Approximation, bisection, best approximation, grid
                 generation, hierarchical representation, linear spline,
                 multiresolution method, scattered data, spline,
                 triangulation, unstructured grid, visualization.",
  volume =       "5 (1)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-1999-387,
  pages =        "47--61",
  year =         "1999",
  title =        "Edgebreaker: Connectivity Compression for Triangle
                 Meshes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-387",
  author =       "Jarek Rossignac",
  abstract =     "Edgebreaker is a simple scheme for compressing the
                 triangle/vertex incidence graphs (sometimes called
                 connectivity or topology) of three-dimensional triangle
                 meshes. Edgebreaker improves upon the storage required
                 by previously reported schemes, most of which can
                 guarantee only an O(t log(t)) storage cost for the
                 incidence graph of a mesh of t triangles. Edgebreaker
                 requires at most 2t bits for any mesh homeomorphic to a
                 sphere and supports fully general meshes by using
                 additional storage per handle and hole. For large
                 meshes, entropy coding yields less than 1.5 bits per
                 triangle. Edgebreaker's compression and decompression
                 processes perform identical traversals of the mesh from
                 one triangle to an adjacent one. At each stage,
                 compression produces an op-code describing the
                 topological relation between the current triangle and
                 the boundary of the remaining part of the mesh.
                 Decompression uses these op-codes to reconstruct the
                 entire incidence graph. Because Edgebreaker's
                 compression and decompression are independent of the
                 vertex locations, they may be combined with a variety
                 of vertex-compressing techniques that exploit
                 topological information about the mesh to better
                 estimate vertex locations. Edgebreaker may be used to
                 compress the connectivity of an entire mesh bounding a
                 3D polyhedron or the connectivity of a triangulated
                 surface patch whose boundary need not be encoded. The
                 paper also offers a comparative survey of the rapidly
                 growing field of geometric compression.",
  editor =       "Hans Hagen",
  keywords =     "Compression, 3D models, geometric representation,
                 computer graphics, solid modeling",
  volume =       "5 (1)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-1999-388,
  year =         "1999",
  title =        "Adaptive Projection Operators in Multiresolution
                 Scientific Visualization",
  author =       "Mario Ohlberger and Martin Rumpf",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-388",
  abstract =     "Recently, multiresolution visualization methods have
                 become an indispensable ingredient of real-time
                 interactive postprocessing. The enormous databases,
                 typically coming along with some hierarchical
                 structure, are locally resolved on different levels of
                 detail to achieve a significant savings of CPU and
                 rendering time. Here, the method of adaptive projection
                 and the corresponding operators on data functions,
                 respectively, are introduced. They are defined and
                 discussed as mathematically rigorous foundations for
                 multiresolution data analysis. Keeping in mind data
                 from efficient numerical multigrid methods, this
                 approach applies to hierarchical nested grids
                 consisting of elements which are any tensor product of
                 simplices, generated recursively by an arbitrary,
                 finite set of refinement rules from some coarse grid.
                 The corresponding visualization algorithms, e.g., color
                 shading on slices or isosurface rendering, are confined
                 to an appropriate depth-first traversal of the grid
                 hierarchy. A continuous projection of the data onto an
                 adaptive, extracted subgrid is thereby calculated
                 recursively. The presented concept covers different
                 methods of local error measurement, time-dependent data
                 which have to be interpolated from a sequence of key
                 frames, and a tool for local data focusing.
                 Furthermore, it allows for a continuous level of
                 detail",
  editor =       "Hans Hagen",
  volume =       "5 (1)",
  keywords =     "Adaptive projection operators, multiresolution,
                 efficient data analysis, error indicators, hierarchical
                 grids, visualization of large data sets.",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-1999-389,
  pages =        "62--73",
  year =         "1999",
  title =        "Real-Time Elastic Deformations of Soft Tissues for
                 Surgery Simulation St{\'{e}}phane Cotin, Herv{\'{e}}
                 Delingette, and Nicholas Ayache",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-389",
  author =       "St{\'{e}}phane Cotin and Herv{\'{e}} Delingette and
                 Nicholas Ayache",
  abstract =     "In this paper, we describe a new method for surgery
                 simulation including a volumetric model built from
                 medical images and an elastic modeling of the
                 deformations. The physical model is based on elasticity
                 theory which suitably links the shape of deformable
                 bodies and the forces associated with the deformation.
                 A real-time computation of the deformation is possible
                 thanks to a preprocessing of elementary deformations
                 derived from a finite element method. This method has
                 been implemented in a system including a force feedback
                 device and a collision detection algorithm. The
                 simulator works in real-time with a high resolution
                 liver model.",
  editor =       "Hans Hagen",
  keywords =     "Surgery simulation, deformable models, real-time,
                 force feedback, soft tissue modeling, finite element.",
  volume =       "5 (1)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@Article{EVL-1999-39,
  pages =        "49--64",
  year =         "1999",
  title =        "Multiresolution banded refinement to accelerate
                 surface reconstruction from polygons",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-39",
  author =       "James D. Fix and Richard E. Ladner",
  abstract =     "We propose a method for constructing a tiling between
                 a pair of planar polygons. Our technique uses
                 multiresolution: tilings of lower resolution polygons
                 are used to construct a tiling for the full resolution
                 polygons. The tilings are constructed using banded
                 refinement, by restricted dynamic programming, in
                 roughly linear time and space. By contrast, the optimal
                 dynamic programming method requires quadratic time and
                 space. In our empirical study of surface reconstruction
                 of brain contours our algorithm exhibited significant
                 speedup over the optimal dynamic program, yet nearly
                 always found an optimal reconstruction. Our approach
                 appears to be generalizable to other geometric problems
                 solvable by dynamic programming, and flexible enough to
                 be tuned for varying data set characteristics.",
  month =        may,
  volume =       "12",
  keywords =     "Polygon tiling; Multiresolution; Surface
                 reconstruction; Dynamic programming; Shape
                 correspondence",
  number =       "1",
  journal =      "Computational Geometry",
}

@InCollection{EVL-1999-390,
  pages =        "196--209",
  year =         "1999",
  title =        "Efficient Skeletonization of Volumetric Objects",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-390",
  author =       "Yong Zho and Arthur W. Toga",
  abstract =     "Skeletonization promises to become a powerful tool for
                 compact shape description, path planning, and other
                 applications. However, current techniques can seldom
                 efficiently process real, complicated 3D data sets,
                 such as MRI and CT data of human organs. In this paper,
                 we present an efficient voxel-coding based algorithm
                 for skeletonization of 3D voxelized objects. The
                 skeletons are interpreted as connected centerlines,
                 consisting of sequences of medial points of consecutive
                 clusters. These centerlines are initially extracted as
                 paths of voxels, followed by medial point replacement,
                 refinement, smoothness, and connection operations. The
                 voxel-coding techniques have been proposed for each of
                 these operations in a uniform and systematic fashion.
                 In addition to preserving basic connectivity and
                 centeredness, the algorithm is characterized by
                 straightforward computation, no sensitivity to object
                 boundary complexity, explicit extraction of
                 ready-to-parameterize and branch-controlled skeletons,
                 and efficient object hole detection. These issues are
                 rarely discussed in traditional methods. A range of 3D
                 medical MRI and CT data sets were used for testing the
                 algorithm, demonstrating its utility.",
  editor =       "Hans Hagen",
  keywords =     "3D skeleton and centerline, medial axis, volume
                 subdivision, region growing, hole detection, distance
                 transformation, voxel-coding.",
  volume =       "5 (3)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-1999-391,
  pages =        "210--223",
  year =         "1999",
  title =        "Ray Casting Architectures for Volume Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-391",
  author =       "Harvey Ray and Hanspeter Pfister and Deborah Silver
                 and Todd A. Cook",
  abstract =     "Real-time visualization of large volume datasets
                 demands high performance computation, pushing the
                 storage, processing, and data communication
                 requirements to the limits of current technology.
                 General purpose parallel processors have been used to
                 visualize moderate size datasets at interactive frame
                 ratReal-time visualization of large volume datasets
                 demands high performance computation, pushing the
                 storage, processing, and data communication
                 requirements to the limits of current technology.
                 General purpose parallel processors have been used to
                 visualize moderate size datasets at interactive frame
                 rates; however, the cost and size of these
                 supercomputers inhibits the widespread use for
                 real-time visualization. This paper surveys several
                 special purpose architectures that seek to render
                 volumes at interactive rates. These specialized
                 visualization accelerators have cost, performance, and
                 size advantages over parallel processors. All
                 architectures implement ray casting using parallel and
                 pipelined hardware. We introduce a new metric that
                 normalizes performance to compare these architectures.
                 The architectures included in this survey are VOGUE,
                 VIRIM, Array Based Ray Casting, EM-Cube, and VIZARD II.
                 We also discuss future applications of special purpose
                 acceleratorses; however, the cost and size of these
                 supercomputers inhibits the widespread use for
                 real-time visualization. This paper surveys several
                 special purpose architectures that seek to render
                 volumes at interactive rates. These specialized
                 visualization accelerators have cost, performance, and
                 size advantages over parallel processors. All
                 architectures implement ray casting using parallel and
                 pipelined hardware. We introduce a new metric that
                 normalizes performance to compare these architectures.
                 The architectures included in this survey are VOGUE,
                 VIRIM, Array Based Ray Casting, EM-Cube, and VIZARD II.
                 We also discuss future applications of special purpose
                 accelerators",
  editor =       "Hans Hagen",
  keywords =     "Volume rendering, interactive, ray casting, parallel
                 architecture, VOGUE, VIRIM, array based ray casting,
                 EM-Cube, VIZARD II.",
  volume =       "5 (3)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-1999-392,
  pages =        "224--237",
  year =         "1999",
  title =        "Simplification of Tetrahedral Meshes with Error
                 Bounds",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-392",
  author =       "Issac J. Trotts and Bernd Hamann and Kenneth I. Joy",
  abstract =     "We present a method for the construction of multiple
                 levels of tetrahedral meshes approximating a trivariate
                 scalar-valued function at different levels of detail.
                 Starting with an initial, high-resolution triangulation
                 of a three-dimensional region, we construct coarser
                 representation levels by collapsing edges of the mesh.
                 Each triangulation defines a linear spline function,
                 where the function values associated with the vertices
                 are the spline coefficients. Error bounds are stored
                 for individual tetrahedra and are updated as the mesh
                 is simplified. Two algorithms are presented that
                 simplify the mesh within prescribed error bounds. Each
                 algorithm treats simplification on the mesh boundary.
                 The result is a hierarchical data description suited
                 for efficient visualization of large data sets at
                 varying levels of detail.",
  editor =       "Hans Hagen",
  keywords =     "Mesh simplification, hierarchical representation,
                 multiresolution method, scattered data, spline,
                 tetrahedral mesh",
  volume =       "5 (3)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-1999-393,
  pages =        "238--250",
  year =         "1999",
  title =        "Interactive Ray Tracing for Volume Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-393",
  author =       "Steven Parker and Michael Parker and Yarden Livnat and
                 Peter-Pike Sloan and Charles Hansen and Peter Shirley",
  abstract =     "We present a brute-force ray tracing system for
                 interactive volume visualization. The system runs on a
                 conventional (distributed) shared-memory multiprocessor
                 machine. For each pixel we trace a ray through a volume
                 to compute the color for that pixel. Although this
                 method has high intrinsic computational cost, its
                 simplicity and scalability make it ideal for large
                 datasets on current high-end parallel systems. To gain
                 efficiency several optimizations are used including a
                 volume bricking scheme and a shallow data hierarchy.
                 These optimizations are used in three separate
                 visualization algorithms: isosurfacing of rectilinear
                 data, isosurfacing of unstructured data, and
                 maximum-intensity projection on rectilinear data. The
                 system runs interactively (i.e., several frames per
                 second) on an SGI Reality Monster. The graphics
                 capabilities of the Reality Monster are used only for
                 display of the final color image.",
  editor =       "Hans Hagen",
  keywords =     "Ray tracing, visualization, isosurface,
                 maximum-intensity projection.",
  volume =       "5 (3)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-1999-394,
  pages =        "251--267",
  year =         "1999",
  title =        "Alias-Free Voxelization of Geometric Objects",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-394",
  author =       "Milos Sramek and Arie E. Kaufman",
  abstract =     "We introduce a new concept for alias-free voxelization
                 of geometric objects based on a Voxelization model
                 (V-model). The V-model of an object is its
                 representation in three-dimensional continuous space by
                 a trivariate density function. This function is sampled
                 during the voxelization and the resulting values are
                 stored in a volume buffer. This concept enables us to
                 study general issues of sampling and rendering
                 separately from object specific design issues. It
                 provides us with a possibility to design such V-models,
                 which are correct from the point of view of both the
                 sampling and rendering, thus leading to both alias-free
                 volumetric representation and alias-free rendered
                 images. We performed numerous experiments with
                 different combinations of V-models and reconstruction
                 techniques. We have shown that the V-model with a
                 Gaussian surface density profile combined with tricubic
                 interpolation and Gabor derivative reconstruction
                 outperforms the previously published technique with a
                 linear density profile. This enables higher fidelity of
                 images rendered from volume data due to increased
                 sharpness of edges and thinner surface patches.",
  editor =       "Hans Hagen",
  keywords =     "Volume graphics, volume rendering, filter-based
                 voxelization, normal estimation, error estimation.",
  volume =       "5 (3)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InCollection{EVL-1999-395,
  pages =        "268--276",
  year =         "1999",
  title =        "Fast Iterative Refinement of Articulated Solid
                 Dynamics",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-395",
  author =       "Fran{\c{c}}ois Faure",
  abstract =     "A new dynamics algorithm for articulated solid
                 animation is presented. It provides enhancements of
                 computational efficiency and accuracy control with
                 respect to previous solutions. Iterative refinement
                 allows us to perform interactive animations which could
                 be only computed off-line using previous methods. The
                 efficiency results from managing two sets of
                 constraints associated with the kinematic graph, and
                 proceeding in two steps. First, the acyclic constraints
                 are solved in linear time. An iterative process then
                 reduces the closed loop errors while maintaining the
                 acyclic constraints. This allows the user to
                 efficiently trade off accuracy for computation time. We
                 analyze the complexity and investigate practical
                 efficiency compared with other approaches. In contrast
                 with previous research, we present a single method
                 which is computationally efficient for acyclic bodies
                 as well as for mesh-like bodies. The accuracy control
                 is provided by the iterative improvement performed by
                 the algorithm and also from the existence of two
                 constraint priority levels induced by the method. Used
                 in conjunction with a robust integration scheme, this
                 new algorithm allows the interactive animation of
                 scenes containing a few thousand geometric constraints,
                 including closed loops. It has been successfully
                 applied to real- time simulations.",
  editor =       "Hans Hagen",
  keywords =     "Dynamics, articulated bodies.",
  volume =       "5 (3)",
  booktitle =    "IEEE Transactions on Visualization and Computer
                 Graphics",
  publisher =    "IEEE Computer Society",
}

@InProceedings{EVL-1999-396,
  pages =        "1--34",
  year =         "1999",
  title =        "Reflectance and texture of real-world surfaces",
  author =       "Kristin J. Dana and Bram van Ginneken and Shree K.
                 Nayar and Jan J. Koenderink",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-396",
  abstract =     "In this work, we investigate the visual appearance of
                 real-world surfaces and the dependence of appearance on
                 the geometry of imaging conditions. We discuss a new
                 texture representation called the BTF (bidirectional
                 texture function) which captures the variation in
                 texture with illumination and viewing direction. We
                 present a BTF database with image textures from over 60
                 different samples, each observed with over 200
                 different combinations of viewing and illumination
                 directions. We describe the methods involved in
                 collecting the database as well as the importqance and
                 uniqueness of this database for computer graphics. A
                 related quantity to the BTF is the familiar BRDF
                 (bidirectional reflectance distribution function). The
                 measurement methods involved in the BTF database are
                 conducive to simultaneous measurement of the BRDF.
                 Accordingly, we also present a BRDF database with
                 reflectance measurements for over 60 different samples,
                 each observed with over 200 different combinations of
                 viewing and illumination directions. Both of these
                 unique databases are publicly available and have
                 important implications for computer graphics.",
  volume =       "18 (1)",
  keywords =     "Intensity, color, photometry, thresholding, texture,
                 physically based modeling, imaging geometry,
                 radiometry,",
  booktitle =    "ACM Transactions on Graphics",
}

@InProceedings{EVL-1999-397,
  pages =        "35--55",
  year =         "1999",
  title =        "Combining constructive and equational geometric
                 constraint-solving techniques",
  author =       "R. Joan-Arinyo and A. Soto-Riera",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-397",
  abstract =     "In the past few years, there has been a strong trend
                 towards developing parametric, computer-aided design
                 systems based on geometric constraint solving. An
                 effective way to capture the design intent in these
                 systems is to define relationships between geometric
                 and technological variables. In general, geometric
                 constraint solving including functional relationships
                 requires a general approach and appropriate techniques
                 to achieve the expected functional capabilities. This
                 work reports on a hybrid method that combines two
                 geometric constraint solving techniques: constructive
                 and equational. The hybrid solver has the capability of
                 managing functional relationships between dimension
                 variables and variables representing conditions
                 external to the geometric problem. The hybrid solver is
                 described as a rewriting system and is shown to be
                 correct.",
  volume =       "18 (1)",
  keywords =     "Canonical forms, constructive techniques, equational
                 techniques, geometric constraint solving, rewriting
                 systems",
  booktitle =    "ACM Transactions on Graphics",
}

@InProceedings{EVL-1999-398,
  pages =        "56--94",
  year =         "1999",
  title =        "Two methods for display of high contrast images",
  author =       "Jack Tumblin and Jessica K. Hodgins and Brian K.
                 Guenter",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-398",
  abstract =     "High contrast images are common in night scenes and
                 other scenes that include dark shadows and bright light
                 sources. These scenes are difficult to display because
                 their contrasts greatly exceed the range of most
                 display devices for images. As a result, the image
                 constrasts are compressed or truncated, obscuring
                 subtle textures and details. Humans view and understand
                 high contrast scenes easily, {"}adapting{"} their
                 visual response to avoid compression or truncation with
                 no apparent loss of detail. By imitating some of these
                 visual adaptation processes, we developed methods for
                 the improved display of high-contrast images. The first
                 builds a display image from several layers of lighting
                 and surface properties. Only the lighting layers are
                 compressed, drastically reducing contrast while
                 preserving much of the image detail. This method is
                 practical only for synthetic images where the layers
                 can be retained from the rendering process. The second
                 method interactively adjusts the displayed image to
                 preserve local contrasts in a small {"}foveal{"}
                 neighborhood. Unlike the first method, this technique
                 is usable on any image and includes a new tone
                 reproduction operator. Both methods use a sigmoid
                 function for contrast compression. This function has no
                 effect when applied to small signals but compresses
                 large signals to fit within an asymptotic limit. We
                 demonstrate the effectiveness of these approaches by
                 comparing processed and unprocessed images.",
  keywords =     "Adaptation, adaption, tone reproduction, visual
                 appearance",
  volume =       "18 (1)",
  booktitle =    "ACM Transactions on Graphics",
}

@InProceedings{EVL-1999-399,
  pages =        "96--127",
  year =         "1999",
  title =        "Model and representation: the effect of visual
                 feedback on human performance in a color picker
                 interface",
  author =       "Sarah A. Douglas and Arthur E. Kirkpatrick",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-399",
  abstract =     "User interfaces for color selection consist of a
                 visible screen representation, an input method, and the
                 underlying conceptual organization of the color model.
                 We report a two-way factorial, between-subjects
                 variable experiment that tested the effect of high and
                 low visual feedback interfaces on speed and accuracy of
                 color matching for RGB and HSV color models. The only
                 significant effect was improved accuracy due to
                 increased visual feedback. Using color groups as a
                 within-subjects variable, we found differences in
                 performance of both speed and accuracy. We recommend
                 that experimental tests adopt a color test set that
                 does not show bias toward a particular model, but is
                 based instead on a range of colors that would be most
                 likely matched in practice by people using color
                 selection software. We recomment the Macbeth Color
                 Checker naturals, primaries, and grays. As a follow-up
                 study, a qualitative case analysis of the way users
                 navigated through the color space indicates that
                 feedback helps users with limited knowledge of the
                 model, allowing them to refine their match to a higher
                 degree of accuracy. Users with very little or a lot of
                 knowledge of the color model do not appear to be aided
                 by increased feedback. In conclusion, we suggest that
                 visual feedback and design of the interface may be a
                 more important factor in improving the usability of a
                 color selection interface than the particular color
                 model used.",
  volume =       "18 (2)",
  keywords =     "HSV, RGB, color model, color selection, feedback,
                 mental model, user interface",
  booktitle =    "ACM Transactions on Graphics",
}

@Article{EVL-1999-4,
  year =         "1999",
  title =        "Active Vision for Complete Scene Reconstruction and
                 Exploration",
  author =       "{\'{E}}ric Marchand and Fran{\c{c}}ois Chaumette",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-4",
  language =     "en",
  abstract =     "This paper deals with the 3D structure estimation and
                 exploration of static scenes using active vision. Our
                 method is based on the structure from controlled motion
                 approach that constrains camera motions to obtain an
                 optimal estimation of the 3D structure of a geometrical
                 primitive. Since this approach involves to gaze on the
                 considered primitive, we have developed perceptual
                 strategies able to perform a succession of robust
                 estimations. This leads to a gaze planning strategy
                 that mainly uses a representation of known and unknown
                 areas as a basis for selecting viewpoints. This
                 approach ensures a reconstruction as complete as
                 possible of the scene.",
  month =        jan,
  volume =       "21",
  keywords =     "3D reconstruction, scene exploration, purposive and
                 active vision, perception strategies",
  number =       "1",
  journal =      "IEEE Transactions on Pattern Analysis and Machine
                 Intelligence",
}

@Article{EVL-1999-40,
  pages =        "65--90",
  year =         "1999",
  title =        "Using generic programming for designing a data
                 structure for polyhedral surfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-40",
  author =       "Lutz Kettner",
  abstract =     "Software design solutions are presented for
                 combinatorial data structures, such as polyhedral
                 surfaces and planar maps, tailored for program
                 libraries in computational geometry. Design issues
                 considered are flexibility, time and space efficiency,
                 and ease-of-use. We focus on topological aspects of
                 polyhedral surfaces and evaluate edge-based
                 representations with respect to our design goals. A
                 design for polyhedral surfaces in a halfedge data
                 structure is developed following the generic
                 programming paradigm known from the Standard Template
                 Library STL for C++. Connections are shown to planar
                 maps and face-based structures.",
  month =        may,
  volume =       "13",
  keywords =     "Library design; Generic programming; Combinatorial
                 data structure; Polyhedral surface; Halfedge data
                 structure",
  number =       "1",
  journal =      "Computational Geometry",
}

@InProceedings{EVL-1999-400,
  pages =        "128--170",
  year =         "1999",
  title =        "Fast and accurate hierarchical radiosity using global
                 visibility",
  author =       "Fr{\'{e}}do Durand and George Drettakis and Claude
                 Puech",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-400",
  abstract =     "Recent hierarchical global illumination algorithms
                 permit the generation of images with a high degree of
                 realism. Nonetheless, appropriate refinement of light
                 transfers, high quality meshing, and accurate
                 visibility calculation can be challenging tasks. This
                 is particularly true for scenes containing multiple
                 light sources and scenes lit mainly by indirect light.
                 We present solutions to these problems by extending a
                 global visibility data structure, the Visibility
                 Skeleton. This extension allows us to calculate exact
                 point-to-polygon form-factors at vertices created by
                 subdivision. The structures also provides visibility
                 information for all light interactions, allowing
                 intelligent refinement strategies. High-quality meshing
                 is effected based on a perceptualy based ranking
                 strategy which results in appropriate insertions of
                 discontinuity curves into the meshes representing
                 illumination. We introduce a hierarchy of
                 triangulations that allows the generation of a
                 hierarchical radiosity solution using accurate
                 visibility and meshing. Results of our implementation
                 show that our new algorithm produces high quality
                 view-independent lighting solutions for direct
                 illumination, for scenes with multiple lights and also
                 scenes lit mainly by indirect illumination.",
  volume =       "18 (2)",
  keywords =     "Discontinuity meshing, form factor calculation, global
                 illumination, global visibility, hierarchical
                 radiosity, hierarchical triangulation, perception",
  booktitle =    "ACM Transactions on Graphics",
}

@InProceedings{EVL-1999-401,
  pages =        "195--212",
  year =         "1999",
  title =        "Techniques for interactive design using the {PDE}
                 method",
  author =       "Hassan Ugail and Malcolm I. G. Bloor and Michael J.
                 Wilson",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-401",
  abstract =     "Interactive design of practical surfaces using the
                 partial differential equation (PDE) method is
                 considered. The PDE method treats surface design as a
                 boundary value problem (ensuring that surfaces can be
                 defined using a small set of design parameters). Owing
                 to the elliptic nature of the PDE operator, the
                 boundary conditions imposed around the edges of the
                 surface control the internal shape of the surface.
                 Moreover, surfaces obtained in this manner tend to be
                 smooth and fair. The PDE chosen has a closed form
                 solution allowing the interactive manipulation of the
                 surfaces in real time. Thus we present efficient
                 techniques by which we show how surfaces of practical
                 significance can be constructed interactively in real
                 time.",
  volume =       "18 (2)",
  keywords =     "CAD, PDE method, interactive design, partial
                 differential equations",
  booktitle =    "ACM Transactions on Graphics",
}

@InProceedings{EVL-1999-402,
  pages =        "171--194",
  year =         "1999",
  title =        "Anisotropic diffusion for Monte Carlo noise
                 reduction",
  author =       "Michael D. McCool",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-402",
  abstract =     "Monte Carlo sampling can be used to estimate solutions
                 to global light transport and other rendering problems.
                 However, a large number of observations may be needed
                 to reduce the variance to acceptable levels. Rather
                 than computing more observations within each pixel, if
                 spatial coherence exists in image space it can be used
                 to reduce visual error by averaging estimators in
                 adjacent pixels. Anisotropic diffusion is a
                 space-variant noise reduction technique that can
                 selectively preserve texture, edges, and other details
                 using a map of image coherence. The coherence map can
                 be estimated from depth and normal information as well
                 as interpixel color distance. Incremental estimation of
                 the reduction in variance, in conjunction with
                 statistical normalization of interpixel color
                 distances, yields an energy-preserving algorithm that
                 converges to a spatially nonconstant steady state.",
  volume =       "18 (2)",
  keywords =     "Monte Carlo methods, anisotropic diffusion, global
                 illumination, image processing, image synthesis, light
                 transport, noise reduction, space-variant filtering",
  booktitle =    "ACM Transactions on Graphics",
}

@InProceedings{EVL-1999-403,
  pages =        "213--256",
  year =         "1999",
  title =        "Radiance interpolants for accelerated bounded-error
                 ray tracing",
  author =       "Kavita Bala and Julie Dorsey and Seth Teller",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-403",
  abstract =     "Ray tracers, which sample radiance, are usually
                 regarded as offline rendering algorithms that are too
                 slow for interactive use. In this article we present a
                 system that exploits object-space, ray-space,
                 image-space, and temporal coherence to accelerate ray
                 tracing. Our system uses per-surface interpolants to
                 approximate radiance both interactive and batch ray
                 tracers. Our approach explicity decouples the two
                 primary operations of a ray tracer--shading and
                 visibility determination--and accelerates each of them
                 independently. Shading is accelerated by
                 quadrilinearily interpolating lazily acquired radiance
                 samples. Interpolation error does not exceed a
                 user-specified bound, allowing the user to control
                 performance/quality tradeoffs. Error is bounded by
                 adaptive sampling at discontinuities and radiance
                 nonlinearities. Visibility determination at pixels is
                 accelerated by reprojecting interpolants as the user's
                 viewpoint changes. A fast scan-line alogoithm then
                 achieves high performance without sacrificing image
                 quality. For a smoothly varying viewpoint, the
                 combination of lazy interpolants and projection
                 substantially accelerates the ray tracer. Additionally,
                 an efficient cache management algorithm keeps the
                 memory footprint of the system small with negilgible
                 overhead.",
  volume =       "18 (3)",
  keywords =     "4D interpolation, approximation, data structures,
                 error bounds, interactive, interval arithmetic,
                 radiance, rendering, rendering systems, visibility",
  booktitle =    "ACM Transactions on Graphics",
}

@InProceedings{EVL-1999-404,
  pages =        "257--277",
  year =         "1999",
  title =        "Analyzing bounding boxes for object intersection",
  author =       "Subhash Suri and Philip M. Hubbard and John F.
                 Hughes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-404",
  abstract =     "Heuristics that exploit bouning boxes are common in
                 algorithms for rendering, modeling, and animation.
                 While experience has shown that bounding boxes improve
                 the performance of these algorithms in practice, the
                 previous theoretical analysis has concluded that
                 bounding boxes perform poorly in the worst case. This
                 paper reconciles this discrepancy by analyzing
                 intersections among n geometric objects in terms of two
                 parameters: an upper bound on the aspect ratio or
                 elongatedness of each object; and an upper bound on the
                 scale factor or size disparity between the largest and
                 smallest objects. Letting Ko and Kb be the number of
                 intersecting object pairs and bounding box pairs,
                 respectively, we analyze a ratio measure of the
                 bounding boxes' efficiency, r=Kb/n+Ko . The analysis
                 proves that r=Oas log2s and r=Was .",
  volume =       "18 (3)",
  keywords =     "Aspect ratio, bounding boxes, collison detection",
  booktitle =    "ACM Transactions on Graphics",
}

@InProceedings{EVL-1999-405,
  pages =        "278--292",
  year =         "1999",
  title =        "Searchlight and Doppler effects in the visualization
                 of special relativity: a corrected derivation of the
                 transformation of radiance",
  author =       "Daniel Weiskopf and Ute Kraus and Hanns Ruder",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-405",
  abstract =     "We demonstrate that a photo-realistic image of a
                 rapidly moving object is dominated by the searchlight
                 and Doppler effects. Using a photon-counting technique,
                 we derive expressions for the relativistic
                 transformation of radiance. We show how to incorportate
                 the Doppler and searchlight effects in the two common
                 techniques of special relativistic visualization,
                 namely ray tracing and polygon rendering. Most authors
                 consider geometrical appearance only and neglect
                 relativistic effects on the lighting model. Chang et
                 al. [1996] present an incorrect derivation of the
                 searchlight effect, which we compare to our results.
                 Some examples are given to show the results of image
                 synthesis with relativistic effects taken into
                 account.",
  volume =       "18 (3)",
  keywords =     "Doppler effect, Lorentz transformation, aberration of
                 light, illumination, searchlight effect, special
                 relativity",
  booktitle =    "ACM Transactions on Graphics",
}

@InProceedings{EVL-1999-406,
  pages =        "293--315",
  year =         "1999",
  title =        "Modeling generalized cylinders via Fourier morphing",
  author =       "Alberto S. Aguado and Eugenia Montiel and Ed Zaluska",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-406",
  abstract =     "Generalized cylinders provide a compact representation
                 for modeling many components of natural objects as well
                 as a great variety of human-made industrial parts. This
                 paper presents a new approach to modeling generalized
                 cylinders based on cross-sectional curves defined using
                 Fourier descriptors. This modeling is based on contour
                 interpolation and is implemented using a subdivision
                 technique. The definition of generalized cylinders uses
                 a three-dimensional trajectory which provides an
                 adequate control for the smoothness of bend with a
                 small number of parameters and includes the orientation
                 of each cross-section (i.e, the local coordinate
                 system) in the interpolation framework. Fourier
                 representations of cross-sectional curves are obtained
                 from contours in digital images, and corresponding
                 points are identified by considering angular and
                 arc-length parametrizations. Changes in cross-section
                 shape through the trajectory are performed using
                 Fourier morphing. The technique proposed provides a
                 comprehensive definition that allows the modeling of a
                 wide variety of shapes, while maintaining a compact
                 characterization to facilitate the description of
                 shapes and displays.",
  volume =       "18 (4)",
  keywords =     "Fourier expansion, contour interpolation, generalized
                 cylinders, morphing, parametric surfaces, solid
                 modeling, subdivision methods",
  booktitle =    "ACM Transactions on Graphics",
}

@InProceedings{EVL-1999-407,
  pages =        "316--328",
  year =         "1999",
  title =        "A simple method for drawing a rational curve as two
                 {B}{\'{e}}zier segments",
  author =       "Jean Gallier",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-407",
  abstract =     "In this paper we give a simple method for drawing a
                 closed rational curve specified in terms of control
                 points as two B{\'{e}}zier segments. The main result is
                 the following: For every affine frame (r,s) (where rs),
                 for every rational curve F(t) specified over [r,s] by
                 some control polygon (0, ..., m) (where the zero are
                 weighted control points or control vectors), the
                 control points (0,... ,m (w.r.t.[r,s]) of the rational
                 curve G(t) = F4t are given by qi=-1 ibi, where 4:RP1RP1
                 is the projectivity mapping [r,s] onto RP1]r,s]. Thus,
                 in order to draw the entire trace of the curve F over
                 -,+ , we simply draw the curve segments F[(r,s]) and
                 G([r,s]). The correctness of the method is established
                 using a simple geometric argument about ways of
                 partitioning the real projective line into two disjoint
                 segments. Other known methods for drawing rational
                 curves can be justified using similar geometric
                 arguments.",
  volume =       "18 (4)",
  keywords =     "B{\'{e}}zier curves, control points, de Casteljau
                 algorithm, rational curves, subdivision, weights",
  booktitle =    "ACM Transactions on Graphics",
}

@InProceedings{EVL-1999-408,
  pages =        "329--360",
  year =         "1999",
  title =        "A unified approach for hierarchical adaptive
                 tesselation of surfaces",
  author =       "Luiz Velho and Luiz Henrique de Figueiredo and Jonas
                 Gomes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-408",
  abstract =     "This paper introduces a unified and general
                 tesselation algorithm for parametric and implicit
                 surfaces. The algorithm produces a hierarchial mesh
                 that is adapted to the surface geometry and has a
                 multiresolution and progressive structure. The
                 representation can be exploited with advantages in
                 several applications.",
  volume =       "18 (4)",
  keywords =     "Adapted meshes, geometric modeling, implicit surfaces,
                 multiresolution representations, parametric surfaces,
                 polygonization, surface approximation",
  booktitle =    "ACM Transactions on Graphics",
}

@InProceedings{EVL-1999-409,
  pages =        "361--368",
  year =         "1999",
  title =        "The holodeck ray cache: an interactive rendering
                 system for global illumination in nondiffuse
                 environments",
  author =       "Gregory Ward and Maryann Simmons",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-409",
  abstract =     "We present a new method for rendering complex
                 environments using interactive, progressive,
                 view-independent, parallel ray tracing. A
                 four-dimensional holodeck data structure serves as a
                 rendering target and caching mechanism for interactive
                 walk-throughs of nondiffuse environments with full
                 global illumination. Ray sample density varies locally
                 according to need, and on-demand ray computation is
                 supported in a parallel implementation. The holodeck
                 file is stored on disk and cached in memory by a server
                 using a least-recently-used (LRU) beam-replacement
                 strategy. The holodeck server coordinates separate ray
                 evaluation and display processes, optimizing disk and
                 memory usage. Different display systems are supported
                 by specialized drivers, which handle display rendering,
                 user interaction, and input. The display driver creates
                 an image from ray samples sent by the server and
                 permits the manipulation of local objects, which are
                 rendered dynamically using approximate lighting
                 computed from holodeck samples. The overall method
                 overcomes many of the conventionl limits of interactive
                 rendering in scenes with complex surface geometry and
                 reflectance properties, through an effective
                 combination of ray tracing, caching, and hardware
                 rendering.",
  volume =       "18 (4)",
  keywords =     "Illumination, image reconstruction, mesh generation,
                 ray tracing, rendering system, virtual reality",
  booktitle =    "ACM Transactions on Graphics",
}

@Book{EVL-1999-41,
  year =         "1999",
  title =        "Computational Neuroscience: Trends in Research 1999",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-41",
  abstract =     "This volume includes papers originally presented at
                 the 7th annual Computational Neuroscience Meeting
                 (CNS'98) held in July of 1998 at the Fess Parker
                 Doubletree Inn in Santa Barbara, California. The CNS
                 meetings bring together computational neuroscientists
                 representing many different fields and backgrounds as
                 well as many different experimental preparations and
                 theoretical approaches. The papers published here range
                 from pure experimental neurobiology, to neuro-ethology,
                 mathematics, physics, and engineering. In all cases the
                 research described is focused on understanding how
                 nervous systems compute. The actual subjects of the
                 research include a highly diverse number of
                 preparations, modeling approaches, and analysis
                 techniques. Accordingly, this volume reflects the
                 breadth and depth of current research in computational
                 neuroscience taking place throughout the world.",
  editor =       "J. M. Bower",
  publisher =    "Elsevier Science",
}

@InProceedings{EVL-1999-410,
  year =         "1999",
  title =        "A Graph Theoretic Approach to {CAD} for the Analysis
                 of Planar Mechanisms",
  author =       "K. T. Hong and Y. T. Lang",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-410",
  abstract =     "One of the main goals of multibody dynamics research
                 is to develop formulations that automatically derive
                 the equations of motions for complex mechanical
                 systems. Equations of motion are formulated
                 systematically and then solved numerically. This paper
                 studies analytic methods for formulating the equation
                 of motions, with emphasis on the graph theoretic
                 modelling approach. The graph theoretic approach is
                 proposed for modelling and analysis of planar
                 mechanical systems. The development of the CAD system
                 incorporating graph theoretic models would be used for
                 mechanical analysis. A graphical user interface will be
                 used to capture the design and to display the solutions
                 of the dynamic and kinematic computations. Throughout
                 the analysis, the CAD tool ( PMech) would be useful for
                 relieving the engineer's task of doing the calculations
                 by hand, the result of kinematic and dynamic analysis
                 of the different prototypes constitutes the graphical
                 display of the mechanism, enhancing the engineer's
                 understanding of the mechanical behaviour under
                 consideration.",
  editor =       "V. Skala",
  keywords =     "CAD, computer graphics, constraint motion, graph
                 theory, simulation.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-411,
  year =         "1999",
  title =        "On a Fundamental Physical Principle Underlying the
                 Point Location Algorithm in Computer Graphics",
  author =       "Sumit Ghosh",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-411",
  abstract =     "The issue of point location is an important problem in
                 computer graphics and the study of efficient data
                 structures and fast algorithms is an important research
                 area for both computer graphics and computational
                 geometry disciplines. When filling the interior region
                 of a planar polygon in computer graphics, it is
                 necessary to identify all points that lie within the
                 interior region and those that are outside. Sutherland
                 and Hodgman are credited for designing the first
                 algorithm to solve the problem. Their approach utilizes
                 vector construction and vector cross products, and
                 forms the basis of the {"}odd parity{"} rule. To verify
                 whether a test point is within or outside a given
                 planar polygon, a ray from the test point is drawn
                 extending to infinity in any direction without
                 intersecting a vertex. If the ray intersects the
                 polygon outline an odd number of times, the region is
                 considered interior. Otherwise, the point is outside
                 the region. In 3 dimensional space, Lee and Preparata
                 propose an algorithm but their approach is limited to
                 point location relative to convex polyhedrons with
                 vertices in 3D-space. Although it is rich on optimal
                 data structures to reduce the storage requirement and
                 efficient algorithms for fast execution, a proof of
                 correctness of the algorithm, applied to the general
                 problem of point location relative to any arbitrary
                 surface in 3D-space, is absent in the literature. This
                 paper argues that the electromagnetic field theory and
                 Gauss's Law constitute a fundamental basis for the
                 {"}odd parity{"} rule and shows that the {"}odd
                 parity{"} rule may be correctly extended to point
                 location relative to any arbitrary closed surface in
                 3D-space.",
  editor =       "V. Skala",
  keywords =     "Point location, computer graphics, odd parity rule,
                 Gauss' Law, electromagnetic field theory.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-412,
  year =         "1999",
  title =        "Three Dimensional Computer Animation for Presenting
                 Weather Forecast",
  author =       "Alphan Es and Veysi Isler",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-412",
  abstract =     "Three-dimensional computer animation can be very
                 useful in presenting weather forecast on TV channels.
                 However, generation of the animation for weather
                 forecast using general-purpose tools might be difficult
                 and time-consuming since this kind of animation
                 requires special visual effects and various animatable
                 parameters. Furthermore, the animation is to be
                 generated by ordinary users with no training in
                 animation. In this paper, we present a
                 three-dimensional computer animation system
                 specifically designed for presenting weather forecast.
                 The system is designed and implemented using
                 object-oriented techniques that are very helpful for
                 computer animation and motion abstraction. The
                 animation system also incorporates multiresolution
                 modeling and paging of huge terrains for fast
                 rendering.",
  editor =       "V. Skala",
  keywords =     "Three dimensional computer animation, object oriented
                 programming, key frame animation, multiresolution
                 modeling.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-413,
  year =         "1999",
  title =        "Geometric Reasoning in Support of Assembly-Oriented
                 {CAD}",
  author =       "S. J. Tate and G. E. M. Jared and K. G. Swift",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-413",
  abstract =     "Assembly-oriented CAD has long been accepted as a
                 necessary development from the current component
                 focused solid modelling systems. It is proposed that
                 such an environment should incorporate assembly
                 sequence generation and Design for Assembly (DFA)
                 analyses to assist the designer, including some
                 automatic inference to facilitate ease of use. The key
                 to enabling the various assembly analyses lies in
                 interrogation of the CAD model and this poses some
                 interesting challenges in the field of geometric
                 reasoning. Statistics from case studies show that, in
                 particular, the identification of part symmetry and
                 principal axes is fundamental to many of the required
                 algorithms which have been identified. This paper
                 reviews methods for symmetry detection. However, no
                 pre-existing method suitable for this application is
                 found and so a new technique is proposed which exploits
                 the existence of loops within the CAD model. This
                 entails the comparison of loop areas to discover exact
                 symmetry, partial symmetry and repeated features.
                 Implementation of this technique is described and in
                 conclusion the benefits and problems associated with it
                 are discussed.",
  editor =       "V. Skala",
  keywords =     "DFA, Symmetry, Assembly-Oriented CAD, Geometric
                 Reasoning, Design Methodology.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-414,
  year =         "1999",
  title =        "Gesture Controlled Object Interaction: {A} Virtual
                 Table Case-Study",
  author =       "Oliver Bimber",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-414",
  abstract =     "The metaphor introduced in this paper describes a more
                 natural, human-like way to interact with objects in
                 virtual environments. The interaction is controlled by
                 motion-based gestures, which are recognized using the
                 method described in [Bimbe98]. In contrast to the
                 application of artificial and unnatural tools, natural
                 input devices (e.g. the human hand) and human-like
                 expression methods (e.g. gestures) can be used to
                 interact with the objects; but on cost of precision.
                 Pre-selection and artificial assistance (like displayed
                 handles, etc.) become unnecessary or might be used in
                 addition. Possible application scenarios are imaginable
                 wherever an exact alternation of the objects is
                 secondary (e.g. presentation, brain-storming,
                 tactical/strategic planning, etc.), or if more precise
                 methods are used in combination. Three problems have to
                 be addressed : First, the definition of natural,
                 human-like gestures; second, the recognition of these
                 gestures; and, finally, the association with
                 appropriate interactions. We focus on finding
                 human-like, natural gestures (classified as act
                 gestures by [Humme97]) which, in contrast to predefined
                 symbolic commands (e.g. emblems, used in GIVEN
                 [Bo{"}hm92]), don't have to be learned by the user, and
                 combine them with appropriate actions to interact with
                 the objects. This augments ideas expressed by
                 [Bolt80,Latos97].",
  editor =       "V. Skala",
  keywords =     "Gesture recognition, object interaction, virtual
                 environments, natural, human-like.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-415,
  year =         "1999",
  title =        "Optimization Strategies Applied to Global Similarity
                 Based Image Registration Methods",
  author =       "K. Capek",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-415",
  abstract =     "This paper investigates the best possible optimisation
                 strategy for determining global extremes of objective
                 functions in the framework of image registration. We
                 register serial optical slices captured by a confocal
                 laser-scanning microscope (CLSM). We experiment with
                 following objective functions: the Sum of Absolute
                 Valued Differences (SAVD), Normalised Correlation
                 Coefficient (NCC) and Mutual Information (MI). Full
                 search of the parametric space for the global extreme
                 is tedious, therefore, we study following optimisation
                 strategies: the genetic algorithm, downhill simplex
                 method, Powell's method, and simulated annealing, all
                 with the use of multiresolution approach to decrease
                 the computational load.",
  editor =       "V. Skala",
  keywords =     "Image registration, optimisation strategy, global
                 similarity evaluation method, serial optical slices.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-416,
  year =         "1999",
  title =        "Haptic Visualization of Molecular Data",
  author =       "Ale^s K^renek and Martin ^Cernohorsk 'y and Zden^ek
                 Kabel 'a^c",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-416",
  abstract =     "A haptic display allows human operators to manipulate
                 and feel virtual environments. Visual and auditory
                 feedback alone cannot enable a person to interact with
                 the computers as naturally as he would interact with a
                 real environment. In this paper we present our current
                 results on creating virtual models of results of
                 quantum chemistry computation, particularly electron
                 density field utilizing a haptic device. We mainly
                 focus on mapping quantities with chemical meaning to
                 force field. And we show how they are used together
                 with the point interface haptic display PHANToMTM .",
  editor =       "V. Skala",
  keywords =     "Hptic, PHANToM, chemistry, molecule, visualization",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-417,
  year =         "1999",
  title =        "Muli-Sensory Rendering: Combining Graphics and
                 Acoustics",
  author =       "Jackson Pope and Alan Chalmers",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-417",
  abstract =     "Human perception of the geometry and spatial layout of
                 an environment is a multi-sensory process. In addition
                 to sight, the human brain is also particularly adept at
                 subconsciously processing echoes and using these
                 reflected sounds to provide some indication of the
                 dimensions of an environment. This auditory impression
                 of the size of an environment will incorporate surfaces
                 not only to the front, but also to the sides and rear
                 of the person and thus currently hidden from his/her
                 view. So while computer graphics can provide an image
                 of what a person can currently see, the level of
                 perceptual realism may be significantlyimproved by
                 incorporating auditory effects as well. This paper
                 describes a method for combining the computation of
                 lighting and acoustics to provide enhanced rendering of
                 virtual environments.",
  editor =       "V. Skala",
  keywords =     "Computer Graphics, Acoustic Rendering, Virtual
                 Reality, Particle Tracing, Perception.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-418,
  year =         "1999",
  title =        "Learning in Real-time Environment Based on Classifier
                 System",
  author =       "Ce'dric Sanza and Paul Sabatier",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-418",
  abstract =     "This paper presents a new architecture of a classifier
                 system for learning in virtual environments. The model
                 will be integrated in our multi-user platform to
                 provide interaction between intelligent agents and user
                 clones. An agent is an autonomous entity equipped with
                 sensors and effectors. Its behavior is guided by
                 rewards coming from the environment that produce rules
                 called classifiers. The knowledge is shared between
                 agents by using the {"}sending-message{"} protocol to
                 increase the global efficiency of the group. The
                 classifier system is specially adapted to a multi-task
                 environment and incorporates a short-term memory to
                 record the recent events of the simulation. These ideas
                 have been implemented and used to develop a virtual
                 soccer where the user plays with autonomous agents that
                 combine communication and evolution.",
  editor =       "V. Skala",
  keywords =     "Agent, Evolution, Adaptation, Classifier Systems,
                 Virtual Environments.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-419,
  year =         "1999",
  title =        "Geometrical Reconstruction from Single Line Drawings
                 Using Optimization-Based Approaches",
  author =       "P. Company and J. M. Gomis and M. Contero",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-419",
  abstract =     "Optimization is one of the most promising geometrical
                 reconstruction approaches. In this approach, the 2D
                 vertices of the given figure maintain their plane
                 coordinates (X,Y), while a set of Z coordinates
                 (orthogonal to the plane) is computed to obtain a 3D
                 configuration that matches the {"}implicit spatial
                 information{"} contained in the departure drawing. In
                 other words, Z coordinates are the variables, and image
                 regularities are used to define both the Objective
                 Function and the Constraints. Some authors have
                 introduced and tested the approach. Nevertheless,
                 further improvements are needed. Mainly because in this
                 problem only global optimum is acceptable in order to
                 ensure the {"}psychologically plausible{"} model is
                 always the one to be obtained. In this paper, some key
                 aspects of the strategy proposed by the authors to
                 convey the optimization process towards the
                 psychologically plausible solution are discussed.",
  editor =       "V. Skala",
  keywords =     "Geometrical reconstruction, optimization.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@Conference{EVL-1999-42,
  pages =        "321--328",
  year =         "1999",
  title =        "Shape memory for closed active contours",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-42",
  author =       "F. Abram and P. T. Sander",
  abstract =     "In many cases, the general overall shape of the
                 contours of specific objects in images is known, e.g.,
                 bones and organs in biomedical images. This information
                 can be used to initialize an active contour and to
                 constrain its global evolution while fitting itself by
                 local deformations to the object in the image. The
                 model thus not only extracts the object in the image,
                 but also provides an evaluation of the deformation of
                 the extracted contour with respect to the given
                 original shape. This paper describes an active contour
                 segmentation technique based on such knowledge of
                 contours in images. Following a rough initial placement
                 of the model, the deformation process consists in three
                 stages, a free deformation stage under the effect of
                 image attraction or other external forces, a stage of
                 regularization to constrain the previous deformation
                 with respect to the a priori known shape of the
                 contour, and finally a stage of re-positioning the
                 contour using the theory of material systems.",
  organization = "University of West Bohemia",
  month =        feb,
  address =      "Plzen (Czech Republic)",
  editor =       "Vaclac Skala",
  keywords =     "active contours, edge detection, segmentation,
                 registration, regularization, material systems, free
                 form deformation, differential geometry",
  volume =       "2",
  booktitle =    "WSCG'99 Conference Presentation",
}

@InProceedings{EVL-1999-420,
  year =         "1999",
  title =        "An Automatic Estimation of the Axis of Rotation of
                 Fragments of Archaeological Pottery: {A} Multi-Step
                 Model-Based Approach",
  author =       "Radim Hal'i^r",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-420",
  abstract =     "This paper presents a method for an estimation of the
                 axis of rotation of radially symmetric objects from
                 their fragments. In particular, fragments of
                 archaeological pottery are considered. The method
                 exploits basic geometrical properties of fragments
                 which are described by an appropriate model. The
                 estimation is performed in several steps which
                 continuously improve accuracy and robustness of an
                 initial estimate. Because only coordinates of surface
                 points of a fragment are required for the estimation,
                 the proposed method can be implemented as a fully
                 automatic process. The approach has been evaluated on
                 both synthetic objects and real pottery fragments.",
  editor =       "V. Skala",
  keywords =     "Radial symmetry, axis of rotation, least squares
                 optimization, M-estimators, robust fitting.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-421,
  year =         "1999",
  title =        "Word Location Techniques for Text/Graphic Mixed Images
                 Using 3-Dimensional Graph Model",
  author =       "Se-Young Ock and Hwan-Chul Park and Hwan-Gue Cho",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-421",
  abstract =     "his paper presents a new extracting method for several
                 types of texts from a text/graphic mixed document
                 image. We also propose a new word grouping method when
                 intersected words are placed on a circular arc or any
                 line segment with an arbitrary orientation. The basic
                 strategy of our algorithm is based on the analysis of
                 the run-length of the document image. The average and
                 variance of the number of runs in a run-length encoding
                 provide a nice structural property for symbols and
                 texts. We propose 3-dimensional neighborhood graph for
                 grouping word from a set of isolated characters, which
                 are obtained from the first character-isolating phase.
                 This graph maps each letter to a vertex in
                 3-dimensional space according to the {"}volume{"} of
                 the character. Experimental results show that more than
                 97% of words were successfully extracted from a
                 text/graphic mixed document.",
  editor =       "V. Skala",
  keywords =     "Document analysis, pattern recognition, text
                 extraction, image processing.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-422,
  year =         "1999",
  title =        "Visualization of Geometrical and non-geometrical
                 Data",
  author =       "M. B. Carmo and J. D. Cunha and A. P. Claudio",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-422",
  abstract =     "A prototype for the visualization of both geometrical
                 and non-geometrical data is described. Special emphasis
                 is given to flat structured or tabular data. Main
                 features of this prototype are: a data independent
                 architecture; filtering of information using degree of
                 interest functions and establishing limits for the
                 variation of one or more data attributes; interactive
                 specification of attribute restrictions combining DQG
                 and RU operators; choice of representations using
                 current scale factor and the result of the degree of
                 interest function. The concept of information classes
                 is introduced to associate a set of attributes with a
                 spatial reference and a graphical representation.
                 Derived information classes are used to combine
                 attribute restrictions with DQG and RU operators.",
  editor =       "V. Skala",
  keywords =     "Visualization, degree of interest function, filtering,
                 graphical representation.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-423,
  year =         "1999",
  title =        "A General-Purpose Logic-Based Visualization
                 Frameworkose Logic-Based Visualization Framework",
  author =       "Camil Demetrescuy and Irene Finocchiz",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-423",
  abstract =     "ABSTRACT This paper describes a general-purpose
                 visualization framework for producing images through
                 reasoning processes. The approach is significantly
                 different from the usual imperative one: indeed, the
                 visualizer retrieves visual information from a
                 knowledge base that may depend on an external source of
                 data. This knowledge base is written by the user in a
                 logic-based language and contains declarations about
                 visual objects, their interrelations and their
                 geometric and retinal features. Different algorithms
                 for querying the knowledge base are presented,
                 discussing both their temporal complexity and their
                 robustness.",
  editor =       "V. Skala",
  keywords =     "Visualization, logic languages.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-424,
  year =         "1999",
  title =        "Vega--{A} User-centered Approach to the Distributed
                 Visualization of Geometric Algorithms",
  author =       "Christoph A. Hipke and Sven Schuierer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-424",
  abstract =     "We present a new approach to the distributed
                 visualization of geometric algorithms that emphasizes
                 the position of the end user. Concepts are introduced
                 that enable a more flexible usage of visualized
                 geometric algorithms, while keeping the task of
                 adapting existing algorithms to the new scheme as
                 simple as possible. A main proposition is that
                 interactivity should not be built into the visualized
                 algorithms, but into the visualizing system. With this
                 in mind, we devise a visualization model for geometric
                 algorithms that incorporates strong algorithm execution
                 control, flexible manipulation of geometric
                 input/output data and adjustable view attributes. The
                 new visualization model is implemented in the Vega1
                 system. Vega offers distributed visualization of
                 geometric algorithms based on source code annotation
                 and supports the standard libraries LEDA and CGAL.",
  editor =       "V. Skala",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-427,
  year =         "1999",
  title =        "Comparison of Two Methods for Reconstruction of 3{D}
                 Objects from Countours",
  author =       "Petr Felkel and Ji^r'i Jan'a^cek",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-427",
  abstract =     "Two approaches for reconstruction of 3D objects from
                 contours in serial sections (cross-sections) are
                 compared. The first method is based on thresholding and
                 3D volume reconstruction, the second on direct
                 reconstruction from parallel contours. The accuracy of
                 surface and volume measurement of reconstructed
                 structures was estimated. Both methods construct a
                 triangulated surface of the object and its inner
                 structures.",
  editor =       "V. Skala",
  keywords =     "3D reconstruction from contours, surface measurement,
                 volume measurement.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-429,
  year =         "1999",
  title =        "An Integrated and Efficient Voxel-Based Rendering
                 System: From Discrete Skeleton Shape Descriptor to
                 Implicit Surface Visualization",
  author =       "Stefanie Prevost and Laurent Lucas",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-429",
  abstract =     "This paper presents a new algorithm of volume
                 rendering based on a dual representation of data in
                 order to improve at the same time the image generation
                 speed and its quality. Two shapes descriptors are used
                 then to characterize the object : an Euclidean Distance
                 Transform (EDT) and a digital Euclidean skeleton. The
                 EDT is used at a time to accelerate the image
                 generation and compute the skeleton of this shape,
                 whereas skeleton is used as an initialization to the
                 processing of the implicit surface. The goal consists
                 in the obtaining of an interactive system of
                 visualization for the analysis of volumetric data,
                 either in the setting of a qualitative exploration, or
                 like a guide in the qualitative measures. The speed of
                 treatments associated with a good visualization would
                 give the feasibility to achieve in an interactive
                 manner, a 3D survey of a natural object. The method has
                 been successfully applied to both synthetic and real
                 data.",
  editor =       "V. Skala",
  keywords =     "Volume rendering, implicit surface, EDT, morphologic
                 skeleton.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@Conference{EVL-1999-43,
  pages =        "369--374",
  year =         "1999",
  title =        "Optimization Strategies applied to Similarity based
                 Image Registration Methods",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-43",
  author =       "Martin Capek",
  abstract =     "This paper investigates the best possible optimization
                 strategy for determining global extremes of objective
                 functions in the framework of image registration. We
                 register serial optical slices captured by a confocal
                 laser-scanning microscope (CLSM). We experiment with
                 following objective functions: The Sum of Absolute
                 Valued Differences (SAVD), Normalized Correlation
                 Coefficient (NCC) and Mutual Information (MI). Full
                 search of the parametric space for the global extreme
                 is tedious, therefore, we study following optimization
                 strategies: the genetic algorithm, downhill simplex
                 method, Powell's method, and simulated annealing, all
                 with the use of multiresolution approach to decrease
                 the computational load.",
  organization = "University of West BohemiaI",
  month =        feb,
  address =      "Plzen, Czech Republic",
  keywords =     "image registration, optimization strategy, global
                 similarity evaluation method, serial optical slices",
  volume =       "2",
  booktitle =    "WSCG'99 Conference Presentation",
}

@InProceedings{EVL-1999-430,
  year =         "1999",
  title =        "A 3{D} Fluid-Flow Visualizer for Entry Level
                 Computers",
  author =       "A. Sannay and B. Montrucchioy and R. Arinaz andL.
                 Massassoz",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-430",
  abstract =     "In computational fluid dynamics (CFD), visualization
                 is a frequently used tool for data evaluation and
                 qualitative comparison to flow visualizations
                 originating from experiments; moreover, the
                 visualization of three-dimensional fluid flows can be a
                 very important teaching instrument. In particular, it
                 has been proved that streak lines and time lines are
                 effective visualization techniques for depicting time
                 dependent phenomena such as vortical flows (vortex
                 shedding, vortex breakdown, an so on), moving shock
                 waves and turbulent flows. Often, the visualization of
                 unsteady flows requires a high computational power
                 available only by using high-end workstations. In this
                 paper we present a platform-independent software for
                 particle tracing by path lines, streak lines, and time
                 lines which can be used in an interactive way on entry
                 level computers; moreover, we propose an innovative
                 texture-based visualization approach we called TOSL
                 (Thick Oriented Streamline).",
  editor =       "V. Skala",
  keywords =     "Scientific visualization, flow visualization, vector
                 field visualization.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-431,
  year =         "1999",
  title =        "3d Object Database Simplification Using {A} Vertex
                 Clustering Algorithm",
  author =       "Tolga Bayik and Mehmet B. Akhan",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-431",
  abstract =     "3D graphics rendering is computationally very
                 intensive particularly for interactive applications.
                 The complexity of the 3D object databases becomes a
                 major problem in a rendering pipeline as the number of
                 polygons is increased with model complexity.
                 Simplifying large object databases and eliminating some
                 of the polygons in the database with minimal impact on
                 visual quality is desirable for faster rendering. This
                 paper discusses the necessity for simplifying 3D object
                 databases in accelerating 3D graphics and introduces a
                 new method based on vertex clustering technique.",
  editor =       "V. Skala",
  keywords =     "3D Graphics, Rendering, Levels of Details,
                 Simplification.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-433,
  year =         "1999",
  title =        "Web Clustering: {A} New Approach to Space
                 Partitioning",
  author =       "A. James and A. M. Day",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-433",
  abstract =     "The Binary Space Partitioning Tree has provided a
                 sound structure on which to build rendering algorithms
                 since the early eighties. In this paper, we manipulate
                 the BSP tree in a new way, and subsequently morph the
                 algorithm and data structure resulting in a new, very
                 different, technique. Our algorithm is restricted to
                 two-dimensions in this paper and thus our input is line
                 segments; our aim is to decrease the line segment
                 splitting associated with BSP trees. We present the Web
                 Clustering algorithm with a run-time competitive to
                 Binary Space Partitioning in random scenes and better
                 with structures that incorporate empty passage ways or
                 `void' regions. We show how buildings, for example, can
                 be divided by their corridors with very few splits. Our
                 method changes a fundamental part of BSP generation,
                 leading to the abolition of the binary restriction.
                 This allows us to branch our partitioning of the
                 environment where a BSP partition would slice through
                 the scene, regardless of obstacles. In terms of the
                 overall rendering times, the algorithm reduces line
                 segment splitting compared to the BSP technique.
                 Preprocessing is also attractive since it exploits the
                 characteristics of structured scenes and is thus a
                 faster divide and conquer technique.",
  editor =       "V. Skala",
  keywords =     "Binary-space partitioning, hidden-surface removal,
                 portals, scene clustering.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-434,
  year =         "1999",
  title =        "A Geometric Compression Algorithm for Massive Terrain
                 Data Using Delauny Triangulation",
  author =       "Sung-Soo Kim and Yang-Soo Kim and Mi-Gyung CHwan-Gue
                 Choho",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-434",
  abstract =     "In this paper we introduce a new compression technique
                 for a large triangulated terrain using Delaunay
                 triangulation. Our compression technique decomposes a
                 triangulated mesh into two parts. One is a point set
                 whose connecting structure is defined implicitly by
                 Delaunay edges. The other is the set of edges which
                 cannot be recovered by the implicit Delaunay
                 triangulation rule. Thus we only need to keep the whole
                 vertex coordinates and a few edges which is not
                 included in the Delaunay edges. For the vertex
                 coordinate, we apply {"}entropy coding{"} given by
                 [Costa98], and we store only the edges not included in
                 Delaunay triangulation. In experiments, we prepared
                 several TIN data set with various resolutions, which
                 were generated by five typical algorithms for terrain
                 simplification. Those algorithms include progressive
                 meshing, vertex decimation and incremental greedy
                 insertion etc. We found that most of terrain
                 triangulations are quite similar( = nearly 93%) to the
                 plane-projected Delaunay triangular mesh. This
                 experimental work shows that more than 93% edges of a
                 common terrain data is included in Delaunay
                 triangulation. By exploiting this result, we can
                 compress the common terrain data by 1.2 bits per
                 vertex. Another advantage of our Delaunay compression
                 approach is that we can reconstruct the original
                 terrain structure locally, since we can easily
                 determine if an edge is included in Delaunay edges by
                 observing only a few local surrounding vertices.",
  editor =       "V. Skala",
  keywords =     "Data Compression, Delaunay Triangulation, Terrain
                 Modeling",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-435,
  year =         "1999",
  title =        "Mesh Surface Simplification Based on Constrained
                 Energy",
  author =       "Hidenori Sato and Akira Onozawa and Hitoshi Kitazawa",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-435",
  abstract =     "A triangle-mesh-surface-simplification algorithm based
                 on a constrained energy function is pro-posed. In the
                 algorithm, vertices are deleted iteratively according
                 to the dimension-less energy function, which represents
                 the cost of surface modification by vertex deletion and
                 subsequent hole re-triangulation. The energy function
                 is used both for selecting a vertex with the minimum
                 energy for deletion and for constraining the target
                 curvature of the surface to be simplified. During
                 simplification, the target curvature of the surface to
                 be simplified is modified gradually by increasing upper
                 limit constraints. The experimental results show that
                 the proposed algorithm has the advantage that a set of
                 parameters can be applied to various-scale data
                 composed of a variety of sizes and shapes with boundary
                 edges. Keywords: computer graphics, triangle meshes,
                 mesh simplification, level of detail",
  editor =       "V. Skala",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-436,
  year =         "1999",
  title =        "An Exact Weaving Rasterization Algorithm",
  author =       "C. Lincke and C. A. W{\"u}thrich",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-436",
  abstract =     "In this paper a new 3D plane rasterization algorithm
                 based on weaving techniques is presented. This
                 algorithm uses the coherence present in 3D parallel
                 lines within the plane. Parallel lines have the same
                 cyclic Freeman chain code but shifted by a parameter.
                 This article provides the theory to determine this
                 shift parameter.",
  editor =       "V. Skala",
  keywords =     "Rasterization, digital planes, weaving.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-437,
  year =         "1999",
  title =        "The Morphing Space",
  author =       "Marc Alexa and Wolfgang M{\"u}ller",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-437",
  abstract =     "Morphing is a well known technique to generate smooth
                 transitions between two objects. We propose a more
                 general understanding of morphing: First, we use
                 morphing to describe objects as a composite of other
                 objects. Second, we allow this description to
                 incorporate more than two base objects. Given this
                 concept a set of objects produced by morphing among
                 multiple objects forms a mathematical space, which we
                 call morphing space. We present a mathematical
                 framework to discuss the properties of such spaces and
                 their relation to the applied morphing algorithms for
                 synthesizing the elements. We show that not only
                 synthesis of objects but also analysis of objects is
                 possible in a morphing space. Analysis results in the
                 definition of an object in terms of the base objects of
                 which it is comprised. Algorithms are proposed for both
                 the synthesis and the analysis process.",
  editor =       "V. Skala",
  keywords =     "Morphing, warping, vector space, synthesis,
                 analysis.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-438,
  year =         "1999",
  title =        "A Warping-based Refinement of Lumigraphs",
  author =       "Wolfgang Heidrich and Hartmut Schirmacher and Hendrik
                 K{\"u}ck and Hans-Peter Seidel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-438",
  abstract =     "Light fields and Lumigraphs have recently received a
                 lot of attention in the computer graphics community,
                 since they allow to render scenes from a database of
                 images very efficiently. While warping based approaches
                 are still too slow to achieve truly interactive frame
                 rates, Lumigraph rendering can easily achieve tens of
                 frames per second in full screen on a contemporary
                 graphics workstation. On the down side, high resolution
                 Lumigraphs require large amounts of storage and are
                 expensive to acquire, while low resolutions yield low
                 quality images with lots of blurring. In this paper, we
                 propose to refine an initial sparse Lumigraph through
                 the use of a specialized warping algorithm. This
                 algorithm increases the resolution of the Lumigraph by
                 inserting new views, which can then be used to generate
                 higher quality images through normal quadri-linear
                 interpolation. This way, storage requirements and
                 acquisition costs remain low, but image quality is
                 increased drastically.",
  editor =       "V. Skala",
  keywords =     "Image based rendering, image warping, light field,
                 Lumigraph, view morphing.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-439,
  year =         "1999",
  title =        "A New Method for Image-based Representation and Fast
                 Rendering of the 3{D} Object or Scene",
  author =       "Li Xinxiao and Zheng Nanning",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-439",
  abstract =     "An efficient image-based modeling and rendering
                 approach is presented. This method, which is an
                 extension of layered depth image (LDI), uses a
                 panorama-like data structure to describe the model of
                 an object or scene. The representation has the
                 conventional advantages resting with 2D image-space,
                 such as regularity of pixels' position and epipolar
                 line relationship among multiple views. It is fitful to
                 compress N sample images efficiently, and generate new
                 views in interactive rate.",
  editor =       "V. Skala",
  keywords =     "Image-based rendering, panoramic image, image warping,
                 layered depth image.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@Conference{EVL-1999-44,
  pages =        "448--455",
  year =         "1999",
  title =        "Mesh Surface Simplification Based on Constrained
                 Energy",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-44",
  author =       "Hidenori Sato and Akira Onozawa and Hitoshi Kitazawa",
  abstract =     "A triangle-mesh-surface-simplification algorithm based
                 on a constrained energy function is proposed. In the
                 algorithm, vertices are deleted iteratively according
                 to the dimension-less energy function, which represents
                 the cost of surface modification by vertex deletion and
                 subsequent hole re-triangulation. The energy function
                 is used both for selecting a vertex with the minimum
                 energy for deletion and for constraining the target
                 curvature of the surface to be simplified. During
                 simplification, the target curvature of the surface to
                 be simplified is modified gradually by increasing upper
                 limit constraints. The experimental results show that
                 the proposed algorithm has the advantage that a set of
                 parameters can be applied to various-scale data
                 composed of a variety of sizes and shapes with boundary
                 edges.",
  organization = "University of West Bohemia",
  month =        feb,
  address =      "Plzen, Czech Republic",
  editor =       "Vaclav skala",
  keywords =     "computer graphics, triangle meshes, mesh
                 simplification, level of detail",
  volume =       "2",
  booktitle =    "WSCG'99 Conference Presentation",
}

@InProceedings{EVL-1999-440,
  year =         "1999",
  title =        "Fast Maximum Intensity Projection using Binary
                 Shear-Warp Factorization",
  author =       "Balazs Csebfalvi and Andreas K{\"o}nig andEduard
                 Gr{\"o}ller",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-440",
  abstract =     "This paper presents a fast maximum intensity
                 projection technique based on binary shear-warp
                 factorization. The proposed method divides the density
                 domain into a small number of intervals, and to each
                 interval a binary code representation is assigned. In a
                 preprocessing step, an additional volume is created
                 which contains for each voxel the code of the interval
                 enclosing the given voxel density. We present an
                 appropriate data structure for storing this volume and
                 an efficient lookup table technique which can be used
                 to rapidly access a voxel of a certain density code.
                 The volume is efficiently resampled along viewing rays
                 only in voxels where the densities reside in the
                 interval which contains the appropriate maximum
                 value.",
  editor =       "V. Skala",
  keywords =     "Maximum Intensity Projection, Volume Rendering,
                 Shear-Warp Factorization.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-441,
  year =         "1999",
  title =        "Fast Geometrical Wrinkles on Animated Surfaces",
  author =       "Pascal Volino and Nadia Magenat Thalmann",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-441",
  abstract =     "Fast and interactive animation of polygon based
                 surface models such as cloth and skin usually require
                 to performthe animation on a rough surface description
                 containing as few polygons as possible.. Fine wrinkle
                 patterns usually disappear in the process, as their
                 deformation scale becomes incompatible with the size of
                 the meshelements. We propose a fast geometrical
                 wrinkling algorithm which can be implemented on top of
                 any rough surface deformation model, and which
                 modulates the amplitude of a predefined wrinkle
                 heightmap in order tosimulate metric surface
                 conservation. A wrinkle pattern is initially applied on
                 the animated surface mesh. The native edge length of
                 the mesh is used tocompute dynamically the amplitude of
                 the wrinkles as the mesh is deformed using a fast and
                 robust geometric model. Several wrinkle patterns can be
                 combined to simulate complex deformations. The
                 presentedimplementation deforms an interpolated mesh
                 with adaptive refinement to display the wrinkles.",
  editor =       "V. Skala",
  keywords =     "Wrinkles, surface animation, cloth simulation, skin
                 deformation, smoothing, interactive display.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-442,
  year =         "1999",
  title =        "Local Approach to Dynamic Visibility in the Plane",
  author =       "Karel Nechvile and Petr Tobola",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-442",
  abstract =     "This paper proposes the new method for maintaining of
                 the view around a moving point in a static scene in the
                 plane. The scene is composed of n line segments. To
                 maintain the view efficiently, present algorithms avoid
                 construction of the full arrangement of discontinuity
                 lines and use an implicit description instead. On the
                 contrary to previously published papers, we use the
                 structure containing information only about
                 discontinuity lines that are in the proximity of the
                 current viewpoint position. We show how to create this
                 local structure and we give two ways how to apply it in
                 the context of previous work.",
  editor =       "V. Skala",
  keywords =     "Visibility, discontinuity line, incremental update,
                 duality, dynamic convex hull.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-443,
  year =         "1999",
  title =        "Boolean Representation of General Planar Polygons",
  author =       "F. R. Feito and M. Rivero and A. Rueda",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-443",
  abstract =     "There are many applications of the geometric modelling
                 in which it is necessary to accomplish decompositions
                 of the geometric objects that intervene in the model,
                 to facilitate the analysis and design of the algorithms
                 implicated in the resolution of particular problems.
                 Triangulations, trapezoidations, convex decompositions
                 and BSP (binary space partitioning trees) are some of
                 the methods used for the polygon representation. In
                 this work we present a new type of decomposition, based
                 on triangles, valid for closed polygons, manifold and
                 non-manifold, with or without holes. The structure as,
                 well as the necessary algorithms for the calculation of
                 the representation, is easy to implement. The resulting
                 formula can be seen as a particular case of CSG
                 representation of polygons, in which the primitives are
                 triangles.",
  editor =       "V. Skala",
  keywords =     "Polygons, Boolean expressions, decomposition of
                 polygons, triangles, CSG",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-444,
  year =         "1999",
  title =        "Heuristic Reasoning for 2{D} Containment Problems",
  author =       "Nuno Marques and Joao Bernardo and Pedro Capela",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-444",
  abstract =     "Industry nowadays seeks for automation in processes
                 that, until a few years ago, were mainly dependent on
                 human work and intelligence. Containment problems
                 belong to that family of automation processes. We can
                 define a containment problem as a way of placing a set
                 of shapes into another shape without overlapping. In
                 this paper we apply A. I. concepts such as A* informed
                 search algorithms to solve 2D translational containment
                 problems. A containment problem can be described
                 through a search problem and solved by using a
                 heuristic that makes A* search complete and optimal.
                 Morphological operators define the geometric
                 description of the state space. To implement
                 morphological operations, discrete mathematics is
                 applied, allowing us to change the complexity analysis
                 from the polygonal vertex/edge approach to the discrete
                 unit/area approach.",
  editor =       "V. Skala",
  keywords =     "Containment Problems, Geometric Algorithms, Heuristic
                 Search, Morphological Operators, Admissible
                 Heuristics.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-445,
  year =         "1999",
  title =        "Linear {BSP} tree in the Plane for Set of Segments
                 with Low Directional Density",
  author =       "Petr Tobola and Karel Nechvile",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-445",
  abstract =     "We introduce a new BSP tree construction method for
                 set of segments in the plane. Our algorithm is able to
                 create BSP tree of linear size in the time O(n log3 n)
                 for set of segments with low directional density (i.e.
                 it holds for arbitrary segment si from such set, that a
                 line created as extension of this segment doesn't
                 intersect too many other segments from the set in a
                 near neighbourhood of si) and a directional constant
                 ffi belonging to this set.",
  editor =       "V. Skala",
  keywords =     "BSP, segment, tree, partitioning.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-446,
  year =         "1999",
  title =        "Radiosity Calculations for Rotational Surfaces",
  author =       "Helmut Mastal and Robert F. Tobler and Werner
                 Purgathofer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-446",
  abstract =     "Rotational Surfaces are very important for
                 photorealistic representations of plants and other
                 natural phenomena. In this paper radiosity calculations
                 for rotational surfaces are presented that handle
                 surfaces as whole objects rather than approximating
                 them by large numbers of plane patches. Mathematical
                 expressions are given for the radiance of cylinders,
                 cones and spheres that are ideal Lambertian reflectors.
                 Although there exists no complete radiosity algorithm
                 which uses the formulae for cylinders, cones and
                 spheres at the moment, it can be estimated that the
                 number of objects of a global illumination scene as
                 well as the number of interactions could be reduced
                 dramatically. Therefore it should be possible to render
                 more complex scenes with plants, like forests, in
                 reasonable time.",
  editor =       "V. Skala",
  keywords =     "Radiosity, rotational surfaces, natural phenomena.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-447,
  year =         "1999",
  title =        "Adaptive Wavelet Densities for Monte Carlo Ray
                 Tracing",
  author =       "Georg Pietrek and Ingmar Peter",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-447",
  abstract =     "Monte Carlo integration is a well established
                 technique to solve the rendering equation. The
                 efficiency of Monte Carlo integration strongly depends
                 on the probability density functions (pdfs) used to
                 control the stochastic process. We will introduce a new
                 method for representation and adaption of pdfs for
                 Monte Carlo importance sampling based on a new
                 mathematical approach for adaptive pdfs in basis
                 representation. During the normal Monte Carlo
                 integration process an approximation of the integrand
                 is obtained that can be used to construct refined pdfs
                 that tend to achieve better results. Based on this
                 strategy we present a multi pass Monte Carlo algorithm
                 using hierarchical function bases as known from wavelet
                 applications. This approach is used to optimise the
                 calculation of indirect illumination in a backward ray
                 tracing application. The results show that the use of
                 adaptive pdfs improves the image quality as well as the
                 computational efficiency of the calculations.",
  editor =       "V. Skala",
  keywords =     "Monte Carlo Ray Tracing, Importance Sampling, Adaptive
                 PDFs, Wavelets.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-448,
  year =         "1999",
  title =        "Combining Biderectional Path Tracing and Multipass
                 Rendering",
  author =       "Frank Suykens and Yves D. Willems",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-448",
  abstract =     "A flexible multipass framework for global illumination
                 is presented in which bidirectional path tracing is
                 used as a final pass. This final pass is constructed so
                 that it is ensured that a complete solution for the
                 global illumination problem is obtained, no matter what
                 kind of preprocessing steps are used. Results show
                 significant improvement compared to both other
                 multipass methods and standard bidirectional path
                 tracing.",
  editor =       "V. Skala",
  keywords =     "Global illumination, Monte Carlo, multipass rendering,
                 bidirectional path tracing.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-449,
  year =         "1999",
  title =        "On The Start-Up Bisas Problem of Metropolis",
  author =       "Laszlo Szirmay-Kalos and Peter Dornbach and Werner
                 Purgathofer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-449",
  abstract =     "The paper presents an analysis of the start-up bias
                 problem of Metropolis sampling. The analysis is carried
                 out both theoretically and using simulations. In order
                 to allow theoretical treatment, a simplified model is
                 established and the convergence of the Metropolis
                 sampling is examined by Fourier analysis. It is
                 concluded that the start-up bias can be quite
                 significant if the integrand is relatively uniform,
                 thus Metropolis sampling becomes really efficient only
                 for difficult integrands, that is for difficult
                 lighting situations. The theoretical results are then
                 validated for two different integral formulations of
                 the rendering equation. The first is based on
                 bi-directional path tracing, and the second is on
                 ray-bundle tracing.",
  editor =       "V. Skala",
  keywords =     "Metropolis sampling, importance sampling, Monte-Carlo
                 integration, rendering equation.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@Conference{EVL-1999-45,
  pages =        "SP69--SP73",
  year =         "1999",
  title =        "Registration of image medical data",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-45",
  author =       "B. Jezek and K. Antos and V. Chrobok and E.
                 Sim{\'{a}}kov{\'{a}}",
  abstract =     "The paper deals with registration algorithms. Suitable
                 procedures for the registration of image medical data
                 are presented. During our work we used manual
                 registration and point-based registration methods to
                 register digitized slices of the temporal bone. To
                 create referential points we included a different type
                 of biological material to the sample of the explored
                 object and the preparation was embedded in paraffin
                 wax. The manual registration was found to be more
                 precise than the point-based registration. Three
                 dimensional reconstructions of the inner ear, ossicles
                 of the middle ear and the facial nerve were made from
                 the set of registered data.",
  organization = "University of West Bohemia",
  month =        feb,
  address =      "Plzen, Czech Republic",
  editor =       "Vaclav Skala",
  keywords =     "registration, medical imaging, reconstruction,
                 temporal bone",
  booktitle =    "WSCG'99 Conference Presentation",
}

@InProceedings{EVL-1999-450,
  year =         "1999",
  title =        "Continuous 6{DOF} Gesture Recognition: {A} Fussy Logic
                 Approach",
  author =       "Oliver Bimber",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-450",
  abstract =     "The method described in this paper recognizes
                 previously learned gestures which the system was taught
                 by performing them. Any kind of 6DOF input device (e.g.
                 data glove, visual systems, space mouse etc.) can be
                 used to gather the motion data that carries out the
                 gesture. The reliability of the recognition process can
                 be improved by repeating the same gestures several
                 times and correcting wrong recognition results. This
                 extends the system's knowledge. Once learned, the
                 system translates the recognized gestures into (for a
                 computer) identifiable objects (numbers, strings,
                 events, etc.) that can be further processed. The
                 system's advantages are its usability (e.g. with 2D or
                 3D input devices or in combination with finger status
                 information, etc.), the minimum of information needed
                 to recognize a gesture, and, consequently, the high
                 speed of its scanning and comparison process.",
  editor =       "V. Skala",
  keywords =     "Continuous gesture recognition, fuzzy logic, knowledge
                 based, training, motion.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-451,
  year =         "1999",
  title =        "Emotional Characters in Virtual Environments",
  author =       "Tiago Sepu'lveda and Carlos Martinho and Jose' Proenc
                 and Ma'rio Rui Gomes",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-451",
  abstract =     "The design of believable emotional characters is the
                 goal of the architecture described in this paper. The
                 novelty of our approach relies on the distribution of
                 the synthetic characters' architecture in two
                 complementary world models: semantic and physical. The
                 characters emotional state is considered in the
                 semantic world model, implemented as stand-alone
                 Artificial Intelligence module, where general behaviour
                 directives are generated. The agent physical
                 architecture, part of the physical world model, carries
                 out those directives taking advantage of its layered
                 structure. The proposed architecture was used to build
                 the interactive application S3A, a story of dolphins ,
                 exhibited in Lisbon at EXPO 98 and featuring two
                 synthetic dolphins.",
  editor =       "V. Skala",
  keywords =     "Virtual environments, computer graphics, synthetic
                 actors, behaviour simulation.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-452,
  year =         "1999",
  title =        "Adding Realtime Shadow Feedback to Globally
                 Illuminated Virtual Environments",
  author =       "Frank Sch{\"o}ffel",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-452",
  abstract =     "In this paper, we introduce a new method for providing
                 realtime shadow feedback in interactive environments
                 with global illumination simulated by a radiosity
                 process. Several approaches for updating global
                 illumination in dynamic environments have been
                 presented in recent years, but none of these can
                 produce true realtime feedback for moderately complex
                 scenes. The realtime shadows presented here are created
                 by manipulating the vertex colors resulting from a
                 previous radiosity calculation. The notion of
                 multipoints is introduced, that allow significant
                 speed-up of the shading process. This method works on
                 the same scene representation as the radiosity
                 simulation does. We show how the multipoint shading
                 method can be integrated into a Virtual Reality system,
                 in combination with a radiosity update process. Thus,
                 our system provides realtime shadows during user
                 interaction and still enables correct soft radiosity
                 shadows to be updated after the interaction has been
                 finished. The visual appearance of the realtime shadows
                 is similar to the radiosity shadows, and its visual
                 quality can easily be controlled by the user, which
                 makes the method adjustable and thus applicable to
                 complex environments",
  editor =       "V. Skala",
  keywords =     "Shadow update, realtime feedback, interaction,
                 radiosity, multipoints.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-453,
  year =         "1999",
  title =        "Lowering the Cost of Virtual Human RenderingWith
                 Structured Animated Impostors",
  author =       "Amaury Aubel and Ronan Boulic and Daniel Thalmann",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-453",
  abstract =     "We present in this paper an image-based rendering
                 technique to represent virtual humans in real-time.
                 Though we deal only with virtual humans, the concept
                 can be applied to any articulated character. Our
                 technique is termed animated impostors because it
                 extends the notion of dynamic impostors to take into
                 account changes in the character's appearance. We also
                 show how impostors can be embedded into large-scale
                 simulations/applications.",
  editor =       "V. Skala",
  keywords =     "Virtual humans, impostors, LOD, sprites, image-based
                 rendering.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-454,
  year =         "1999",
  title =        "Animating Finger Movements: {A} Feedforward Neural
                 Network Applied to Artificial Tendons",
  author =       "A. Kuchar and J. N. Scrimger",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-454",
  abstract =     "In our research, we are focusing on the preliminary
                 developments of creating an artificially intelligent
                 virtual human (AIVH) who is capable of moving and
                 interacting in its environment with little intervention
                 from the animator. Before an AIVH can move completely,
                 an AIVH needs to learn how to move its body. Thus, the
                 body needs to be divided into separate areas and the
                 AIVH needs to learn about movements in these areas. In
                 this paper, we report the use of a feedforward
                 artificial neural network (ANN) to animate a
                 computerized human finger. The underlying model of the
                 computerized human finger is based on a biological
                 model involving tendons. The ANN triggers any tendons
                 that need to be manipulated to achieve an animated
                 goal. The prescribed animated goals involve different
                 flexion and extension movements. There are several
                 drawbacks associated with using a multilayer
                 feedforward network. Firstly, since the architecture is
                 designed by trial-and-error, it is very difficult to
                 create an effective a priori architecture. Secondly,
                 ANNs are not trained on an entire input space, but
                 there is a supposition that the ANN will work correctly
                 when presented with any possible input within this
                 problem space. In this ongoing project, the ability for
                 an AIVH to move its finger using an ANN has been
                 developed and tested, producing positive results.",
  editor =       "V. Skala",
  keywords =     "Biological-based animation, artificial neural
                 networks, backpropagation training algorithm, motion
                 control, human figure movements.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-455,
  year =         "1999",
  title =        "Motion Blending Using a Classifier System",
  author =       "Tony Polichroniadis and Neil Dodgson",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-455",
  abstract =     "Motion blending is commonly thought of as creating the
                 transition of an animated figure to the first frame of
                 a piece of motion. We describe a new way of thinking
                 about motion blending by removing the assumption that
                 each piece of motion must start at the first frame. We
                 describe a classifier system that finds the nearest
                 match in a set of animation frames to a given state of
                 an animated figure and show how this classifier can be
                 used to create better motion blends. We also describe
                 how the parameters for this system were optimised using
                 a genetic algorithm. The classifier given was found to
                 be efficient enough to work in real time with many
                 articulated figures.",
  editor =       "V. Skala",
  keywords =     "Computer animation, motion control, human figure
                 animation, motion blending.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-456,
  year =         "1999",
  title =        "Dynamic Animation of Spline Like Objects",
  author =       "Yannick Re'mion and Jean-Michel Nourrit and Didier
                 Gillard",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-456",
  abstract =     "This paper presents an accurate and efficient method
                 for dynamic animation of one-dimensional objects
                 modelled as successions of spline segments. At each
                 time step the object shape conforms to its splines
                 definitions, thus insuring that every property implied
                 by the nature of the chosen splines is verified. This
                 fact is achieved by the animation of the control points
                 of the splines. However, these control points are not
                 considered as weighty points but moreover as the
                 degrees of freedom of the continuous object. The chosen
                 dynamic equations ( Lagrangian formalism) reflect this
                 modelling idea and yield to an accurate and very
                 efficient linear system. Suitable handling of forces
                 and constraints, and numerical resolution are discussed
                 in this scheme too.",
  editor =       "V. Skala",
  keywords =     "Simulation, Dynamic animation , Lagrangian equations,
                 spline animation.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-457,
  year =         "1999",
  title =        "Cloth Deformation Modelling Using a Plate Bending
                 Model",
  author =       "Lihua You and Jian J. Zhang and Peter Comninos",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-457",
  abstract =     "Modelling and simulation of cloth deformation are of
                 great importance in computer graphics and engineering
                 applications. Although many models have been developed,
                 problems still exist. No model so far can accurately
                 describe both motion and deformation of cloth in a
                 unified equation. And the equations for the in-plane
                 deformation of cloth have yet to be developed. In this
                 paper, a model for both motion and deformation of cloth
                 will be developed which will account for the in-plane
                 deformation of cloth. In this model, the cloth
                 deformations under out-plane and in-plane loads were
                 uncoupled into out-plane and in-pane deformations. The
                 theory of small deflection bending of plates with
                 elastic behaviours was applied to the out-plane
                 deformation and the theory of plane stress of elastic
                 objects was applied to the in-plane deformation. Both
                 static and dynamic problems of cloth were taken into
                 account. The basic governing equations were given and
                 the corresponding finite difference formulae were
                 developed. Two examples were given to demonstrate the
                 applications of the proposed approach.",
  editor =       "V. Skala",
  keywords =     "Cloth deformation, plate bending, computer graphics,
                 computer simulation and animation.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-458,
  year =         "1999",
  title =        "Graph Grammar Algebraic Approach For Generating
                 Fractal Pattern",
  author =       "H. K. Hussein and A. E. Hassanien",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-458",
  abstract =     "This paper presents a new approach to fractal image
                 generation by translating the evolution of graph
                 grammar into graphics output. It uses the algebraic
                 approach of graph grammar as a powerful abstract
                 modeling tool for the generation of self- similar
                 fractal images via its graph production, derivations
                 and double-pushout construction. We focus on three
                 proposed methods for image generation which exhibit a
                 certain analogy with fractals. Validation of our
                 approach is given in terms of experimental results. An
                 investigation of the relationships between the
                 generated images and their corresponding graph grammar
                 is also discussed.",
  editor =       "V. Skala",
  keywords =     "Graph Grammars, Fractal Geometry, Escape-time
                 algorithm, Strange attractors, Graph productions,
                 Double-pushout construction, Pattern Generation.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-459,
  year =         "1999",
  title =        "{SMURF} - {A} Smart Surface Model for Advanced
                 Visualization Techniques",
  author =       "Helwig L{\"o}ffelmann and Thomas Theussl and Andreas
                 K{\"o}nig and Eduard Gr{\"o}ller",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-459",
  abstract =     "Highly elaborated visualization techniques that are
                 based on surfaces often are independent from the origin
                 of the surface data. For re-using advanced
                 visualization methods for surfaces of various kind, we
                 developed an abstract surface interrogation layer
                 called Smurf. In this paper we discuss the steps
                 necessary to unify multiple types of surfaces under a
                 shared general purpose interface.",
  editor =       "V. Skala",
  keywords =     "Visualization, surfaces.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@Conference{EVL-1999-46,
  pages =        "SP139--SP146",
  year =         "1999",
  title =        "On possible Extensions of Volume Reconstruction",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-46",
  author =       "Marek Zim{\'{a}}nyi and Andrej Ferko and Silvester
                 Czanner",
  abstract =     "The volume reconstruction methods give a powerful tool
                 for providing appropriate visualization of a measured
                 spatial data. However, the input data are often
                 restricted to a set of slices (CT, MRI, or others). In
                 the paper we examine the more general input sets and we
                 introduce three ways how to reconstruct the volume from
                 one possible not sliced input. We introduce a
                 slice-oriented reconstruction method based on Markov
                 random fields, simulated annealing, and genetic
                 algorithms and discuss using the method in broader
                 context.",
  organization = "University of West Bohemia",
  month =        feb,
  address =      "Plzen, Czech Republic",
  editor =       "Vaclav Skala",
  keywords =     "Object reconstruction, volume rendering,
                 visualization",
  booktitle =    "WSCG'99 Conference Presentation",
}

@InProceedings{EVL-1999-460,
  year =         "1999",
  title =        "Modelling and Rendering Liquids in Motion",
  author =       "Alan Murta and James Miller",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-460",
  abstract =     "An experimental method for modelling and rendering
                 liquids in motion is presented. A particle system is
                 used to model dynamic liquid behaviour, while implicit
                 surfaces serve as a wrapper to support realistic
                 rendering methods. Aspects of particle-environment
                 interaction and the use of dynamically sized particle
                 primitives are discussed. Techniques used in the
                 generation of implicit surfaces are also described,
                 including environmental distortions of surface profiles
                 and the management of surface extents to maintain
                 constant volume. Preliminary work on multi-fluid
                 interactions is also outlined. A custom ray tracer is
                 used for final image generation, and several examples
                 of the liquid modelling system are provided as an
                 illustration of the method.",
  editor =       "V. Skala",
  keywords =     "Natural phenomena, ray tracing, particle systems,
                 implicit surfaces.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-461,
  year =         "1999",
  title =        "New Implicit Primitives Used in Reconstruction by
                 Skeletons",
  author =       "Dominique Rigaudie`re and Gilles GDominique
                 Faudotesquie`re",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-461",
  abstract =     "Implicit surfaces defined by density functions allow
                 an easy modelling for different objects with a property
                 of soft blending. So they can be used in reconstruction
                 of objects which are defined by their skeletons. The
                 discrete skeleton of an object can be break down in
                 several primitives. In this article we present new
                 implicit primitives which allow us to build a
                 corresponding element for each skeleton's type.",
  editor =       "V. Skala",
  keywords =     "Implicit surface, anisotropic blob, skeleton,
                 reconstruction.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-462,
  year =         "1999",
  title =        "Object Oriented Constraint Satisfaction for
                 Hierarchical Declarative Scene Modeling",
  author =       "Pierre-Franc and ois Bonnefoi and Dimitri Plemenos",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-462",
  abstract =     "Declarative modeling transforms deeply the task of
                 scene designing by adding to it flexibility, and more
                 creativity and by being more user-friendly. However
                 it's difficult to generalize its use, because it
                 requires too much processing time and memory. The way
                 to improve this is to use the most suited and efficient
                 method from constraint logic programming, to deeply
                 study the declarative modeler in order to exploit all
                 useful informations that it can provide. The defined
                 framework must be implemented with extensibility,
                 scalability and robustness in order to use it as base
                 for fast implementation of new techniques, to study the
                 influences of these techniques on the modeler process
                 and their combination in order to achieve the best
                 reduction of needed work for obtaining a scene.",
  editor =       "V. Skala",
  keywords =     "Declarative Modeling, Constraint Logic Programming,
                 Object Oriented Programming.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-463,
  year =         "1999",
  title =        "Comparison of 2{D} and 3{D} Objects for Parts Reuse",
  author =       "Jean-Pierre Jung and Sandrine Leinen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-463",
  abstract =     "The design of new parts with a CAD software is a task
                 which requires much reflection of the designer. We
                 propose to assist him for this task by integrating the
                 concept of re-use. Indeed, we will associate with the
                 system of CAD a functionality allowing to detect
                 automatically equalities or inclusions between the part
                 in design and the library's data. The method employed
                 in 2D, is based on a method of objects constraints
                 graph normalization. Then, it adapts the graph
                 formalism of the concept of CSP (Constraint
                 Satisfaction Problem) for the comparison of normalized
                 representations. In 3D, the method employed is based on
                 a representation of the geometrical volumical model
                 called DSG (Destructive Solid Geometry) and defined a
                 rules system making possible the comparison of
                 volumical representations.",
  editor =       "V. Skala",
  keywords =     "Computer Aided Design, constraints, modelization,
                 Constrainst Satisfaction Problem, Destructive Solid
                 Geometry.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-464,
  year =         "1999",
  title =        "Correction of a 2{D} Sketch for 3{D} Reconstruction",
  author =       "Phil Kuzo and Pierre Mace'",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-464",
  abstract =     "Among the different methods of 3D modeling, 3D
                 reconstruction from sketches may be quoted. These
                 methods have all to deal with the hard problem of
                 errors on the sketch, leading to a minimization
                 problem. We propose to distinguish, on the one hand,
                 the violation of projective constraints, and on the
                 other hand, the metrics errors, and to first correct
                 the sketch in the way to respect only the projective
                 and affine constraints. The methods was tested on
                 sketches involving projective constraints.",
  editor =       "V. Skala",
  keywords =     "3D reconstruction, sketch correction, projective
                 space, projective geometry, known state propagation.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-465,
  year =         "1999",
  title =        "Analysis of the Quasi-Monte Carlo Integration of the
                 Rendering Equation",
  author =       "L'aszl'o Szirmay-Kalos and Werner Purgathofer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-465",
  abstract =     "Quasi-Monte Carlo integration is said to be better
                 than Monte-Carlo integration since its error bound can
                 be in the order of O(N \Gamma (1\Gamma ffl)) instead of
                 the O(N \Gamma 0:5) probabilistic bound of classical
                 Monte-Carlo integration if the integrand has finite
                 variation. However, since in computer graphics the
                 integrand of the rendering equation is usually
                 discontinuous and thus has infinite variation, the
                 superiority of quasi-Monte Carlo integration has not
                 been theoretically justified. This paper examines the
                 integration of discontinuous functions using both
                 theoretical arguments and simulations and explains what
                 kind of improvements can be expected from the
                 quasi-Monte Carlo techniques in computer graphics.",
  editor =       "V. Skala",
  keywords =     "Rendering equation, quasi-Monte Carlo quadrature,
                 Hardy-Krause variation.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-466,
  year =         "1999",
  title =        "Mathematical Free-form Solid Modelling Based on
                 Extended Simplicial Chains",
  author =       "J. Ruiz de Miras and F. R. Feito",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-466",
  abstract =     "A mathematical model for geometric modelling is
                 presented in this work. This model is based on the
                 concept of extended simplicial chain . Solids are
                 defined by means of simple algebraic operations
                 (additions and subtractions) performed on what we call
                 extended cells. In like manner, by using the operations
                 defined for extended simplicial chains, we obtain the
                 traditional Boolean operations in geometric modelling.
                 Thus, we define a new model which allows us to
                 represent and operate with free-form solids,
                 particularised for the case of solids whose boundaries
                 are free-form surfaces expressed as a set of low degree
                 algebraic patches. However, the concept of extended
                 simplicial chain is more general and wide both in the
                 dimension in which the solid is defined and in the type
                 of surface used for its representation.",
  editor =       "V. Skala",
  keywords =     "Formal methods in computer graphics, mathematical
                 models, free-form solid modelling, Boolean
                 operations.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-467,
  year =         "1999",
  title =        "Ray Tracing of Parametric Surfaces Based on Adaptive
                 Simplicial Complexes",
  author =       "Franz Duckstein and Reiner Kolla",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-467",
  abstract =     "Objects defined by parametric surfaces are a compact
                 representation delivering continuous defined accuracy.
                 For the sake of speed, a discretization towards
                 triangles is inevitable, which can be done adaptively
                 to the projective properties. Simplicial complexes suit
                 perfectly to store the adaptive discretization. Based
                 on adaptive simplicial complexes we propose a modified
                 ray tracing algorithm. The algorithm steers the scene
                 discretization in order to deliver the required
                 accuracy. Invisible object areas gain less accuracy,
                 which increases the time available for processing
                 visible parts. With an appropriate hardware support we
                 hope to get ray tracing like quality for dynamic
                 complex scenes in real time.",
  editor =       "V. Skala",
  keywords =     "Virtual Reality, Parametric Surfaces, Adaptive
                 Tessellation, Ray Tracing, Radiosity.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-468,
  year =         "1999",
  title =        "An Empirical Comparison of Monte Carlo Radiosity
                 Algorithms",
  author =       "Philippe Bekaert and Ronald Cools and Yves D.
                 Willems",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-468",
  abstract =     "Monte Carlo radiosity algorithms are radiosity
                 algorithms in which the radiosity integral equation or
                 system of linear equations is solved using Monte Carlo
                 random walk techniques. Since explicit form factor
                 computation and storage is completely avoided in Monte
                 Carlo radiosity algorithms, these algorithms are more
                 reliable and require significantly less storage than
                 other radiosity algorithms, making it feasible to
                 render much more complex scenes. This paper presents a
                 comparative study of four main aspects in which
                 proposed Monte Carlo radiosity algorithms differ:
                 whether the discrete or continuous equation is being
                 solved, the random walk state transition simulation
                 technique, sampling order and the sample number
                 generator.",
  editor =       "V. Skala",
  keywords =     "Global Illumination, Radiosity, Quasi Monte Carlo,
                 Random Walk.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-469,
  year =         "1999",
  title =        "Perceptually-Driven Termination for Stochastic
                 Radiosity",
  author =       "Jan P^rikryl and Werner Purgathofer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-469",
  abstract =     "In this paper we present a heuristic
                 perceptually-based termination criterion for a
                 stochastic radiosity algorithms. The proposed criterion
                 makes it possible to terminate the iterative radiosity
                 simulation as soon as any further changes of the
                 radiosity solution are predicted not to be noticed by
                 the human observer. We use tone mapping operators and
                 perceptually uniform CIE L \Lambda u\Lambda v\Lambda
                 colour space to arrive at the difference of displayed
                 values. In order to decide when to stop the iterations
                 of the radiosity method, we evaluate the least squares
                 fit of a power function to the error distribution
                 data.",
  editor =       "V. Skala",
  keywords =     "Human perception, radiosity, termination prediction.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@Conference{EVL-1999-47,
  pages =        "1--8",
  year =         "1999",
  title =        "3{D} Object Database Simplification using a vertex
                 clustering algorithm",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-47",
  author =       "Tolga Bayik and Mehmet B. Akhan",
  abstract =     "3D graphics rendering is computationally very
                 intensive particularly for interactive applications.
                 The complexity of the 3D object databases becomes a
                 major problem in a rendering pipeline as the number of
                 polygons is increased with model complexity.
                 Simplifying large object databases and eliminating some
                 of the polygons in the database with minimal impact on
                 visual quality is desirable for faster rendering. This
                 paper discusses the necessity for simplifying 3D object
                 databases in accelerating 3D graphics and introduces a
                 new method based on vertex clustering technique.",
  organization = "University of West Bohemia",
  month =        feb,
  address =      "Plzen, Czech Republic",
  editor =       "Vaclav Skala",
  keywords =     "3D Graphics, Rendering, Levels of Details,
                 Simplification",
  volume =       "1",
  booktitle =    "WSCG'99 Conference Presentation",
}

@InProceedings{EVL-1999-470,
  year =         "1999",
  title =        "Deformable Elastic 2-{D} Objects",
  author =       "F. Abram and P. T. Sander",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-470",
  abstract =     "In this paper, we present the first steps of a 4-D
                 oriented study of object deformation based on both
                 local geometric deformations and global topological
                 constraints of regularization, and shape preservation.
                 The model described handles the shape of the objects to
                 simulate the manipulation of elastic objects that
                 always return to their original shape when no
                 constraints are applied. Many kinds of forces can be
                 applied on the objects in a scene, e.g., some user
                 forces, object interactions or attraction/repulsion
                 fields. Following the placement of objects into a
                 scene, the deformation process consists in three
                 stages, a free deformation stage under the effect of
                 all the forces of the scene, a stage of regularization
                 to constrain the previous deformation with respect to
                 the original shape of the object, and finally a stage
                 of re-positioning the object in the scene using the
                 theory of material systems.",
  editor =       "V. Skala",
  keywords =     "Regularization, material systems, free form
                 deformation, differential geometry.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-471,
  year =         "1999",
  title =        "Thermal Convection in Turbulent Flow, Image Synthesis
                 and Heat Animation",
  author =       "Didier Arque`s and Eric Felgines and Sylvain
                 MichelKarine Zampieriin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-471",
  abstract =     "This paper focuses on realistic ray-tracing rendering
                 of scenes which integrate participating media. We work
                 on the temperature sensitive index of refraction, in
                 order to deflect the rays' path and thus visualize a
                 scene through heat. We can also visualize
                 three-dimensional convective motions, under the shape
                 of fumes. In this respect, we suggest to solve the NS
                 (Navier-Stokes) equations through a simple approach,
                 which offers better steadiness than classical methods,
                 and which is easy to implement. We are considering the
                 three-dimensional and turbulent case of this system,
                 combined with the energy equation, in order to obtain
                 incompressible fluid or gaseous realistic participating
                 media.",
  editor =       "V. Skala",
  keywords =     "Ray-tracing, numerical simulation, heat transfer
                 convection.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-472,
  year =         "1999",
  title =        "Dynamic Collision Detection in Virtual Reality
                 Applications",
  author =       "Jens Eckstein and Elmar Sch{\"o}mer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-472",
  abstract =     "We present data structures and algorithms for dynamic
                 collision detection in virtual reality (VR)
                 applications. The methods are applicable to all general
                 polygonal models. They combine the advantages of
                 collision detection using bounding volume (BV)
                 hierarchies with the ability to compute dynamic
                 collision detection results. The results are used as
                 input for further simulations, e.g. contact or dynamics
                 simulation. First we present new methods to compute BV
                 hierarchies using optimization goals which can also be
                 used to improve known computation methods. Second we
                 show how to integrate BV hierarchies into a process of
                 dynamic collision detection, so that the bounding
                 objects as well as the surface patches of the objects
                 are tested for overlap during their motions. The
                 performance of the techniques is shown by means of a
                 fitting simulation in the automotive industry.",
  editor =       "V. Skala",
  keywords =     "Dynamic collision detection, bounding volume
                 hierarchies, fitting simulation, virtual reality.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-473,
  year =         "1999",
  title =        "3{D} Point Location for Special Feature Preservation
                 During Surface Smoothing",
  author =       "M. M. Madi and D. J. Walton",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-473",
  abstract =     "The development of a smooth geometric model from a
                 crudely approximated object is often sought in CAD/CAM,
                 computer-aided geometric design and similar
                 applications. For example, many surface fitting
                 techniques may have as their input data that are
                 crudely sampled from free form objects to digitally
                 reproduce such objects to look smooth again. However,
                 regardless of the surface fitting technique used whose
                 choice is usually application dependent, there are
                 always particular features on the approximated models
                 that should not undergo the smoothing procedures to
                 maintain visual realism. A technique for interactively
                 marking such sampled points for preservation and
                 special treatment is described.",
  editor =       "V. Skala",
  keywords =     "3D Point Location, Surface Fitting and Refinement.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-475,
  year =         "1999",
  title =        "Puzzle Piece Topology: Detecting Arrangements in Smart
                 Object Interfaces",
  author =       "Lori L. Scarlatos",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-475",
  abstract =     "Smart object interfaces enable a computer to respond
                 to a group of users' manipulations of a physical
                 environment. This unobtrusive interface is especially
                 well suited for providing guidance as students attempt
                 to solve mathematical and scientific puzzles. This
                 paper introduces a formalism for describing
                 arrangements of smart objects on a 2D surface, and
                 suggests a strategy for efficiently representing such
                 arrangements in a computer application. It then shows
                 how these techniques are implemented in a Tangram with
                 a smart objects interface, which provides multimedia
                 feedback as children play with the puzzle pieces.",
  editor =       "V. Skala",
  keywords =     "Tangible user interface, spatial relationships,
                 computer vision, multimodal interface, graphical
                 interaction, multimedia, tracking, educational
                 application, Tangram.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@Misc{EVL-1999-476,
  year =         "1999",
  title =        "Mechanismen zur {"U}bertragung und zur platzsparenden,
                 interaktiven Darstellung grosser Bilddateien in mobilen
                 Umgebungen",
  author =       "Tino Weinkauf",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-476",
  month =        jul,
  howpublished = "Final Year Project thesis, University of Rostock,
                 Department of Computer Science, Institute of Computer
                 Graphics",
}

@InProceedings{EVL-1999-477,
  pages =        "3--15",
  year =         "1999",
  title =        "Wavelet-Based 3{D} Compression Scheme for Interactive
                 Visualization of Very Large Volume Data",
  author =       "Insung Ihm and Sanghun Park",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-477",
  abstract =     "Interactive visualization of very large volume data
                 has been recognized as a task requiring great effort in
                 a variety of science and engineering fields. In
                 particular, such data usually places considerable
                 demands on run-time memory space. In this paper, we
                 present an effective 3D compression scheme for
                 interactive visualization of very large volume data,
                 that exploits the power of wavelet theory. In designing
                 our method, we have compromised between two important
                 factors: high compression ratio and fast run-time
                 random access ability. Our experimental results on the
                 Visual Human data sets show that our method achieves
                 fairly good compression ratios. In addition, it
                 minimizes the overhead caused during run-time
                 reconstruction of voxel values. This 3D compression
                 scheme will be useful in developing many interactive
                 visualization systems for huge volume data, especially
                 when they are based on personal computers or
                 workstations with limited memory.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "18(1)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1999-478,
  pages =        "3--15",
  year =         "1999",
  title =        "Animating Sand, Mud, and Snow",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-478",
  author =       "Robert W. Sumner and O'Brien and Jessica K. Hodgins",
  abstract =     "Computer animations often lack subtle environmental
                 changes that should occur due to the actions of the
                 characters. Squealing car tires usually leave no skid
                 marks, airplanes rarely leave jet trails in the sky,
                 and most runners leave no footprints. In this paper, we
                 describe a simulation model of ground surfaces that can
                 be deformed by the impact of rigid body models of
                 animated characters. To demonstrate the algorithms, we
                 show footprints made by a simulated runner in sand,
                 mud, and snow as well as bicycle tire tracks, a bicycle
                 crash, and a falling runner. The shapes of the
                 footprints in the three surfaces are quite different,
                 but the effects were controlled through only six
                 essentially orthogonal parameters. To assess the
                 realism of the resulting motion, we compare the
                 simulated footprints to video footage of human
                 footprints.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  keywords =     "Animation, physical simulation, ground interaction,
                 terrain, sand, mud, snow.",
  volume =       "18(1)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1999-479,
  pages =        "27--39",
  year =         "1999",
  title =        "Nonphotorealistic Rendering by {Q}-mapping",
  author =       "P. Hall",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-479",
  abstract =     "We present Q-mapping which is a technique for
                 rendering three-dimensional objects using
                 nonphotorealistic cues, by applying Q-maps. Q-maps are
                 three-dimensional textures that make marks on objects,
                 and thus provide visual cues for shape, shade, and
                 texture. Q-maps adapt to light intensity, typically by
                 making more marks in darker areas. Q-maps can produce
                 images with a very wide range of visual styles (e.g.
                 half tone shading, and pen-and-ink colour wash). The
                 primary contribution is that these styles reside in a
                 single parametric space. Importantly this space
                 includes photorealism as a style, which is therefore
                 regarded as a special case of nonphotorealistic image
                 rendering in general. We illustrate our explanation of
                 Q-mapping using examples from scientific visualisation
                 and computer graphics - and provide a gallery of images
                 to show the versatility of the approach.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "18(1)",
  booktitle =    "Computer Graphics Forum",
}

@Conference{EVL-1999-48,
  pages =        "133--139",
  year =         "1999",
  title =        "Haptic Visualization of molecular model",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-48",
  author =       "Ales Krenek and Martin Cernohorsky and Zdenek
                 Kabel{\'{a}}c",
  abstract =     "A haptic display allows human operators to manipulate
                 and feel virtual environments. Visual and auditory
                 feedback alone cannot enable a person to interact with
                 the computers as naturally as he would interact with a
                 real environment. In this paper we present our current
                 results on creating virtual models of results of
                 quantum chemistry computation, particularly electron
                 density field utilizing a haptic device. We mainly
                 focus on mapping quantities with chemical meaning to
                 force field. And we show how they are used together
                 with the point interface haptic display PHANToM{^TM}.",
  organization = "University of West Bohemia",
  month =        feb,
  address =      "Plzen, Czech Republic",
  editor =       "Vaclav Skala",
  keywords =     "Haptic, PHANToM, chemistry, molecule, visualization",
  volume =       "1",
  booktitle =    "WSCG'99 Conference Presentation",
}

@InProceedings{EVL-1999-480,
  pages =        "41--56",
  year =         "1999",
  title =        "A Progressive Algorithm for Three Point Transport",
  author =       "Reynald Dumont and Kadi Bouatouch and Philippe
                 Gosselin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-480",
  abstract =     "When computing global illumination in environments
                 made up of surfaces with general Bidirectional
                 Reflection Distribution Functions, a three point
                 formulation of the rendering equation can be used.
                 Brute-force algorithms can lead to a linear system of
                 equations whose matrix is cubic, which is expensive in
                 time and space. The hierarchical approach is more
                 efficient. Aupperle et al. proposed a hierarchical
                 three point algorithm to compute global illumination in
                 the presence of glossy reflection. We present in this
                 paper some improvements we brought to this method:
                 shooting, `lazy' push-pull, photometric subdivision
                 criterion, etc. Then we will show how our new method
                 takes into account non-planar surfaces in the
                 hierarchical resolution process.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "18(1)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1999-481,
  pages =        "57--68",
  year =         "1999",
  title =        "Tetrahedra Based Adaptive Polygonization of Implicit
                 Surface Patches",
  author =       "K. C. Hui and Z. H. Jiang",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-481",
  abstract =     "This paper presents a tetrahedra based adaptive
                 polygonization technique for tessellating implicit
                 surface patches. An implicit surface patch is defined
                 as an implicit surface bounded by its intersections
                 with a set of clipping surfaces and which lies within
                 an enclosing tetrahedron. To obtain the polygonization
                 of an implicit surface patch, the tetrahedron
                 containing the patch is adaptively subdivided into
                 smaller tetrahedra according to the criteria introduced
                 in the paper. The result is a set of tetrahedra each
                 containing a facet approximating the surface. The
                 intersections between the facets and the clipping
                 surfaces are used to locate the surface patch boundary.
                 Ambiguous results in generating the facets for highly
                 curved surfaces or surfaces with singular points are
                 also addressed. The result of the polygonization is a
                 set of triangular facets that can be used for
                 visualization and numerical analysis. The proposed
                 method is also suitable for locating the intersection
                 of two implicit surfaces.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "18(1)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1999-482,
  pages =        "69--78",
  year =         "1999",
  title =        "Approximate Line Scan-Conversion and Antialiasing",
  author =       "Jim X. Chen and Xusheng Wang",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-482",
  abstract =     "This paper presents an approximate multiple segment
                 line scan-conversion method &mdash; the Slope Table
                 Method. The statistics show that the new method can
                 increase the percentage of multiple segment lines
                 (i.e., lines with more than one segment) in an N×N
                 raster area from about 39% to more than 99%. In
                 software implementation for scan-conversion and
                 antialiasing of randomly generated lines, this method
                 is on average more than 6 times faster than GuptaAEs
                 antialiasing line algorithm. Compared with other line
                 scan-conversion methods, the method may choose pixels
                 which are not the closest to the line (i.e., error
                 pixels). Here the paper demonstrates that the visual
                 effect is acceptable in most applications with the
                 error pixels.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "18(1)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1999-483,
  pages =        "97--119",
  year =         "1999",
  title =        "Multiresolution Curve and Surface Representation:
                 Reversing Subdivision Rules by Least-Squares Data
                 Fitting",
  author =       "Faramarz F. Samavati and Richard H. Bartels",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-483",
  abstract =     "This work explores how three techniques for defining
                 and representing curves and surfaces can be related
                 efficiently. The techniques are subdivision,
                 least-squares data fitting, and wavelets. We show how
                 least-squares data fitting can be used to `reverse' a
                 subdivision rule, how this reversal is related to
                 wavelets, how this relationship can provide a
                 multilevel representation, and how the
                 decomposition/reconstruction process can be carried out
                 in linear time and space through the use of a matrix
                 factorization. Some insights that this work brings
                 forth are that the inner product used in a
                 multiresolution analysis in uences the support of a
                 wavelet, that wavelets can be constructed by
                 straightforward matrix observations, and that matrix
                 partitioning and factorization can provide alternatives
                 to inverses or duals for building efficient
                 decomposition and reconstruction processes. We
                 illustrate our findings using an example curve,
                 grey-scale image, and tensor-product surface.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "18(2)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1999-484,
  pages =        "121--130",
  year =         "1999",
  title =        "Metis - An Object-Oriented Toolkit for Constructing
                 Virtual Reality Applications",
  author =       "Russell Turner and Song Li and Enrico Gobbetti",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-484",
  abstract =     "Virtual reality systems provide realistic look and
                 feel by seamlessly integrating three-dimensional input
                 and output devices. One software architecture approach
                 to constructing such systems is to distribute the
                 application between a computation-intensive simulator
                 back-end and a graphics-intensive viewer front-end
                 which implements user interaction. In this paper we
                 discuss Metis, a toolkit we have been developing based
                 on such a software architecture, which can be used for
                 building interactive immersive virtual reality systems
                 with computationally intensive components. The Metis
                 toolkit defines an application programming interface on
                 the simulator side, which communicates via a network
                 with a standalone viewer program that handles all
                 immersive display and interactivity. Network bandwidth
                 and interaction latency are minimized, by use of a
                 constraint network on the viewer side that
                 declaratively defines much of dynamic and interactive
                 behavior of the application.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "18(2)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1999-485,
  pages =        "131--137",
  year =         "1999",
  title =        "Generic Geometric Programming in the Computational
                 Geometry Algorithms Library",
  author =       "Remco C. Veltkamp",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-485",
  abstract =     "This paper describes a number of design issues and
                 programming paradigms that affect the development of
                 Cgal, the computational geometry algorithms library.
                 Genericity of the library is achieved by concepts such
                 as templates, iterators, and traits classes. This
                 allows the application programmer to plug in own types
                 of containers and point types, for example. The paper
                 gives an explanation of these concepts and examples of
                 how they are used.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "18(2)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1999-486,
  pages =        "139--147",
  year =         "1999",
  title =        "Fast Ray Tracing of Implicit Surfaces",
  author =       "Andrei Sherstyuk",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-486",
  abstract =     "A ray-tracing algorithm is described for rendering
                 implicit surfaces formed with C1-continuous bounded
                 functions f(x, y, z). This class of functions includes
                 such popular implicit models as blobby molecules,
                 metaballs, soft objects and convolution surfaces. The
                 algorithm employs analytical methods only, which makes
                 it fast, robust, and numerically stable. An earlier
                 version of this work was presented at the 3rd
                 International Workshop on Implicit Surfaces held in
                 Seattle in 1998.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "18(2)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1999-487,
  pages =        "149--158",
  year =         "1999",
  title =        "Extending the {CSG} Tree. Warping, Blending and
                 Boolean Operations in an Implicit Surface Modeling
                 System",
  author =       "Brian Wyvill and Andrew Guy and Eric Galin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-487",
  abstract =     "Automatic blending has characterized the major
                 advantage of implicit surface modeling systems.
                 Recently, the introduction of deformations based on
                 space warping and Boolean operations between primitives
                 has increased the usefulness of such systems. We
                 propose a further enhancement which will extend the
                 range of models that can be easily and intuitively
                 defined with a skeletal implicit surface system. We
                 describe a hierarchical method which allows arbitrary
                 compositions of models that make use of blending,
                 warping and Boolean operations. We call this structure
                 the BlobTree. Blending and space warping are treated in
                 the same way as union, difference and intersection,
                 i.e. as nodes in the BlobTree. The traversal of the
                 BlobTree is described along with two rendering
                 algorithms; a polygonizer and a ray tracer. We present
                 some examples of interesting models which can be made
                 easily using our approach that would be very difficult
                 to represent with conventional systems.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "18(2)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1999-488,
  pages =        "159--171",
  year =         "1999",
  title =        "Modelling and Rendering Graphics Scenes Composed of
                 Multiple Volumetric Datasets",
  author =       "Adrian Leu and Min Chen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-488",
  abstract =     "This paper presents a method for modelling graphics
                 scenes consisting of multiple volumetric objects. A
                 two-level hierarchical representation is employed,
                 which enables the reduction of the overall storage
                 consumption as well as rendering time. With this
                 approach, different objects can be derived from the
                 same volumetric dataset, and 2D images can be trivially
                 integrated into a scene. The paper also describes an
                 efficient algorithm for rendering such scenes on
                 ordinary workstations, and addresses issues concerning
                 memory requirements and disk swapping.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "18(2)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1999-489,
  pages =        "173--179",
  year =         "1999",
  title =        "Modeling and Rendering Escher-Like Impossible Scenes",
  author =       "Guillermo Savransky and Dan Dimerman and Craig
                 Gotsman",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-489",
  abstract =     "Inspired by the drawings of `impossible' objects by
                 artists such as M.C. Escher, we describe a mathematical
                 theory which captures some of the underlying principles
                 of their work. Using this theory, we show how
                 impossible three-dimensional scenes may be modeled and
                 rendered synthetically.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "18(2)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1999-49,
  pages =        "1--10",
  year =         "1999",
  title =        "Validity-Preserving Simplification of Very Complex
                 Polyhedral Solids",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-49",
  author =       "Carlos And{\'{u}}jar and Dolors Ayala and Pere
                 Brunet",
  abstract =     "In this paper we introduce the Discretized Polyhedra
                 Simplification (DPS), a framework for polyhedra
                 simplification using space decomposition models. The
                 DPS is based on a new error measurement and provides a
                 sound scheme for error-bounded, geometry and topology
                 simplification while preserving the validity of the
                 model. A method following this framework, Direct DPS,
                 is presented and discussed. Direct DPS uses an octree
                 for topology simplification and error control, and
                 generates valid solid representations. Our method is
                 also able to generate approximations which do not
                 interpenetrate the original model, either being
                 completely contained in the input solid or bounding it.
                 Unlike most of the current methods, restricted to
                 triangle meshes, our algorithm can deal and also
                 produces faces with arbitrary complexity.",
  organization = "Springer-Verlag Wien",
  editor =       "Michael Gervaut and Dieter Schmalstieg and Axel
                 Hildebrand",
  booktitle =    "Virtual Environments. Proceedings of the Eurographics
                 Workshop in Vienna, Austria",
}

@InProceedings{EVL-1999-490,
  pages =        "201--212",
  year =         "1999",
  title =        "Synchronisation and Load Balancing for Parallel
                 Hierarchical Radiosity of Complex Scenes on a
                 Heterogeneous Computer Network",
  author =       "Daniel Meneveaux and Kadi Bouatouch",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-490",
  abstract =     "In this paper we propose a SPMD parallel hierarchical
                 radiosity algorithm relying on a novel partitioning
                 method which may apply to any kind of architectural
                 scene. This algorithm is based on MPI (Message Passing
                 Interface), a communication library which allows the
                 use of either a heterogeneous set of concurrent
                 computers or a parallel computer or both. The database
                 is stored on a common directory and accessed by all the
                 processors (through NFS in case of a network of
                 computers). As the objective is to handle complex
                 scenes such as building interiors, to cope with the
                 problem of memory size, only a subset of the database
                 resides in memory of each processor. This subset is
                 determined with the help of a partitioning into 3D
                 cells, clustering and visibility calculations. A graph
                 expressing visibility between the resulting clusters is
                 determined, partitioned (with a new method based on
                 classification of K-means type) and distributed amongst
                 all the processors. Each processor is responsible for
                 gathering energy (using the Gauss-Seidel method) only
                 for its subset of clusters. In order to reduce the disk
                 transfers due to downloading these subsets of clusters,
                 we use an ordering strategy based on the traveling
                 salesman algorithm. Dynamic load balancing relies on a
                 task stealing approach while termination is detected by
                 configuring the processors into a ring and moving a
                 token around this ring. The parallel iterative
                 resolution is of group iterative type. Its mathematical
                 convergence is proven in the appendix.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "18(4)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1999-491,
  pages =        "213--222",
  year =         "1999",
  title =        "Adaptable Splatting for Irregular Volume Rendering",
  author =       "Wencheng Wang and Enhua Wu",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-491",
  abstract =     "By employment of a footprint table in conducting
                 intensity integration, splatting method has been very
                 successful in rendering regular data volumes. Recently,
                 the method has also been extended to render irregular
                 data volumes. However, since samples in irregular
                 volumes vary greatly in size and shape, the footprint
                 table is unable to be employed in an efficient manner.
                 This hinders the application of splatting approach from
                 being used in the irregular volume case. In this paper,
                 an adaptable splatting method is proposed, which
                 provides an efficient way to integrate color intensity
                 in terms of footprint table for the samples in various
                 sizes. Experiments show that the new method may be used
                 to produce better images without extra expense.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "18(4)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1999-492,
  pages =        "223--236",
  year =         "1999",
  title =        "{G}^1 Hierarchical {B}{\'{e}}zier Surface over
                 Arbitrary Meshes",
  author =       "M. Bercovier and O. Volpin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-492",
  abstract =     "This paper presents a method for constructing
                 composite surfaces based on a collection of
                 quadrilateral patches. A global parameterization using
                 bilinear quadrilateral meshes and FEM like minimization
                 procedure are introduced. Smoothing conditions such as
                 C^1 and G^1 are handled by constraint equation and a
                 related duality argument is implemented. The surfaces
                 that can be constructed in this way include conforming
                 and non-conforming connections (3 < n < 6, T-nodes)
                 between elementary patches. Non-manifold surfaces are
                 automatically treated also by this data structure. The
                 underlying quadratic programming with linear
                 constraints is solved by duality methods. Hierarchical
                 data structure with bordering matrices methods are
                 implemented to deal with local refinement
                 (subdivision). The present work details the actual
                 implementation for the case of B{\'{e}}zier patches.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "18(4)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1999-493,
  pages =        "237--248",
  year =         "1999",
  title =        "Hermitian {B}-Splines",
  author =       "Laurent Grisoni and Carole Blanc and Christophe
                 Schlick",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-493",
  abstract =     "This paper proposes to study a spline model, called
                 HB-splines, that is in fact a B-spline representation
                 of Hermite splines, combined with some restriction on
                 the differential values at segment boundaries. Although
                 this model does not appear able to offer something new
                 to the computer graphics community, we think that
                 HB-splines deserve to be considered for themselves
                 because they embed many interesting features. First,
                 they include all the classical properties required in a
                 geometric modeling environment (convex hull, local
                 control, arbitrary orders of parametric or geometric
                 continuity). Second, they have a nice aptitude for
                 direct manipulation (i.e. manipulation without using
                 control points). For this purpose, we propose a new
                 graphic widget, called control sails, that offers the
                 user an intuitive way to specify local properties
                 (position, tangent, curvature) of a curve or a surface.
                 Finally, they provide an elegant formulation of a
                 biorthogonal wavelet family, that permits
                 multiresolution manipulations of the resulting curves
                 or surfaces, in a very efficient way.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "18(4)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1999-494,
  pages =        "249--265",
  year =         "1999",
  title =        "Reflectance Models with Fast Importance Sampling",
  author =       "L{\'{a}}szl{\'{o}} Neumann and Attila Neumann and
                 L{\'{a}}szl{\'{o}} Szirmay-Kalos",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-494",
  abstract =     "We introduce a physically plausible mathematical model
                 for a large class of BRDFs. The new model is as simple
                 as the well-known Phong model, but eliminates its
                 disadvantages. It gives a good visual approximation for
                 many practical materials: coated metals, plastics,
                 ceramics, retro-reflective paints, anisotropic and
                 retro-reflective materials, etc. Because of its
                 illustrative properties it can be used easily in most
                 commercial software and because of its low
                 computational cost it is practical for virtual reality.
                 The model is based on a special basic BRDF definition,
                 which meets the requirements of reciprocity and of
                 energy conservation. Then a class of BRDFs is
                 constructed from this basic BRDF with different weight
                 functions. The definition of such weight functions
                 requires the user to specify the profile of the
                 highlights, from which the weight function is obtained
                 by derivation. It is also demonstrated how importance
                 sampling can be used with the new BRDFs.",
  organization = "Eurographics Association",
  editor =       "David Duke and Sabine Coquillart and Toby Howard",
  volume =       "18(4)",
  booktitle =    "Computer Graphics Forum",
}

@InProceedings{EVL-1999-495,
  pages =        "375--380",
  year =         "1999",
  title =        "{Comparison of Two methods for Reconstruction of 3{D}
                 Objects from Contours}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-495",
  author =       "P. Felkel and J. Jan{\'a}\v{c}ek",
  address =      "Pilsen",
  month =        feb,
  editor =       "V{\'{a}}clav Skala",
  volume =       "II",
  booktitle =    "WCCG'99, The Seventh International Conference in
                 Central Europe on Computer Graphics and Visualization",
  publisher =    "University of West Bohemia, Pilsen",
}

@InProceedings{EVL-1999-496,
  pages =        "254--263",
  year =         "1999",
  title =        "{Improvement of {O}liva's Algorithm for Surface
                 Reconstruction from Contours}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-496",
  author =       "P. Felkel and \v{S}. Obdr\v{z}{\'a}lek",
  address =      "Budmerice, Slovakia",
  month =        apr,
  editor =       "Ji\v{r}{\'\i} \v{Z}{\'a}ra",
  booktitle =    "Spring Conference on Computer Graphics",
}

@InProceedings{EVL-1999-497,
  pages =        "166",
  year =         "1999",
  title =        "{Improved Algorithm for 3D Surface Tiling}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-497",
  author =       "P. Felkel",
  month =        feb,
  editor =       "A. Strejc",
  booktitle =    "Proceedings of Workshop '99",
  publisher =    "Czech Technical University, Prague, Czech Republic",
}

@InProceedings{EVL-1999-498,
  pages =        "1--8",
  year =         "1999",
  title =        "Interactive Animation of Structured Deformable
                 Objects",
  author =       "Mathieu Desbrun and Peter Schr{\"{o}}der and Alan Barr
                 Abstract",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-498",
  abstract =     "In this paper, we propose a stable and efficient
                 algorithm for animating mass-spring systems. An
                 integration scheme derived from implicit integration
                 allows us to obtain interactive realistic animation of
                 any mass-spring network. We alleviate the need to solve
                 a linear system through the use of a
                 predictor-corrector approach: We first compute a rapid
                 approximation of the implicit integration, then we
                 correct this estimate in a post-step process to
                 preserve momentum. Combined with an inverse dynamics
                 process to implement collisions and other constraints,
                 this method provides a simple, stable and tunable model
                 for deformable objects suitable for virtual reality. An
                 implementation in a VR environment demonstrates this
                 approach.",
  editor =       "I. S. MacKenzie and J. Stewart",
  keywords =     "ht",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-499,
  pages =        "9--17",
  year =         "1999",
  title =        "Geometric Awareness for Interactive Object
                 Manipulation",
  author =       "Min-Hyung Choi and James Cremer",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-499",
  abstract =     "This paper describes formulation and management of
                 constraints that, combined with a nonlinear
                 optimization algorithm, enable interactive
                 geometrically aware manipulation of articulated
                 objects. Going beyond purely kinematic or dynamic
                 approaches, our solution method directly employs
                 geometric constraints to ensure non-interpenetration
                 during object manipulation. We present the formulation
                 of the inequality constraints used to ensure
                 nonpenetration, describe how to manage the set of
                 active inequality constraints as objects move, and show
                 how these results are combined with a nonlinear
                 optimization algorithm to achieve interactive
                 geometrically aware object manipulation. Our
                 optimization algorithm handles equality and inequality
                 constraints and does not restrict object topology. It
                 is an efficient iterative algorithm, quadratically
                 convergent, with each iteration bounded by O(nnz(L)),
                 where nnz(L) is the number of non-zeros in L , a
                 Cholesky factor of a sparse matrix.",
  editor =       "I. S. MacKenzie and J. Stewart",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@Article{EVL-1999-5,
  year =         "1999",
  title =        "Feature Extraction from Wavelet Coefficients for
                 Pattern Recognition Tasks",
  author =       "Stefan Pittner and Sagar V. Kamarthi",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-5",
  language =     "en",
  abstract =     "In this paper, a new efficient feature extraction
                 method based on the fast wavelet transform is
                 presented. This paper especially deals with the
                 assessment of process parameters or states in a given
                 application using the features extracted from the
                 wavelet coefficients of measured process signals. Since
                 the parameter assessment using all wavelet coefficients
                 will often turn out to be tedious or leads to
                 inaccurate results, a preprocessing routine that
                 computes robust features correlated to the process
                 parameters of interest is highly desirable. The method
                 presented divides the matrix of computed wavelet
                 coefficients into clusters equal to row vectors. The
                 rows that represent important frequency ranges (for
                 signal interpretation) have a larger number of clusters
                 than the rows that represent less important frequency
                 ranges. The features of a process signal are eventually
                 calculated by the euclidean norms of the clusters. The
                 effectiveness of this new method has been verified on a
                 flank wear estimation problem in turning processes and
                 on a problem of recognizing different kinds of lung
                 sounds for diagnosis of pulmonary diseases.",
  month =        jan,
  volume =       "21",
  keywords =     "Feature extraction, fast wavelet transform, signal
                 interpretation",
  number =       "1",
  journal =      "IEEE Transactions on Pattern Analysis and Machine
                 Intelligence",
}

@TechReport{EVL-1999-50,
  year =         "1999",
  title =        "New visibility partitions with applications in affine
                 pattern matching",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-50",
  author =       "M. Hagedoorn and M. H. Overmars and R. C. Veltkamp",
  abstract =     "Visibility partitions play an important role in
                 computer vision and pattern matching. A visibility
                 partition is formed by regions in which the
                 combinatorial structure of the visibility is constant.
                 This paper studies new types of visibility partitions
                 with applications in affine pattern matching. First, we
                 discuss the conventional visibility partition for
                 finite line segment collections. The machinery from
                 this analysis is then used to describe the two new
                 partitions induced by alternative forms of visibility,
                 namely trans-visibility and reflection-visibility. We
                 present algorithms that compute each of the partitions
                 in O((n + k) log(n) + v) randomised time, where k is
                 the number of visibility edges (at most $O(n^2)$), and
                 v is the number of vertices in the partition (at most
                 O(n2 + k2)). In addition, we show that each of the
                 partitions has worst-case combinatorial complexity
                 $\Omega (n^4)$. Our model of computation assumes that
                 the absolute value of a quotient of polynomials of
                 degree at most d can be integrated over a triangle in
                 $\Theta (d)$ time. We use reflection-visibility
                 partitions to compute the reflection metric in $O(r(n A
                 + n B))$ randomised time, for two line segment unions,
                 with nA and nB line segments, separately, where r is
                 the complexity of the overlay of two
                 reflection-visibility partitions (at most O(nA4 +
                 nB4)).",
  number =       "UU-CS-1999-21",
  institution =  "Department of Computer Science, Utrecht University",
}

@InProceedings{EVL-1999-500,
  pages =        "18--26",
  year =         "1999",
  title =        "A User Centered Task Analysis of Interface
                 Requirements for {MRI} Viewing",
  author =       "Johanna E. van der Heyden and Kori M. Inkpen and M.
                 Stella Atkins and M. Sheelagh T. Carpendale",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-500",
  abstract =     "This paper explores the viability of Magnetic
                 Resonance Image (MRI) presentation on a computer
                 screen. This includes investigating the feasibility of
                 presenting the information on a desktop computer in a
                 manner that facilitates MRI analysis and medical
                 diagnosis. Two key objectives are identified: 1)
                 understand the MRI analysis task and determine specific
                 presentation issues and requirements through
                 observations of radiologists; and 2) obtain user
                 feedback on design alternatives. Observations of the
                 MRI analysis task in the traditional light screen
                 environment reveal three requirement categories: user
                 control of films, easy navigation of images and
                 simultaneous availability of detail and context. Design
                 proposals, based on these requirements, include the use
                 of windowing techniques, workspace and overview design,
                 and detail-in-context concepts, as well as the adoption
                 of metaphor and structure from the traditional light
                 screen environment. The results from the preliminary
                 user feedback support the value and feasibility of
                 providing MRI analysis on a computer screen.",
  editor =       "I. S. MacKenzie and J. Stewart",
  keywords =     "User Interfaces, user-centerd task analysis, MRI
                 viewing, health care, medical images, screen real
                 estate problem, detail-in-context",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-501,
  pages =        "27--35",
  year =         "1999",
  title =        "A Desktop Design for Synchronous Collaboration",
  author =       "Bogdan Dorohonceanu and Ivan Marsic",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-501",
  abstract =     "This paper presents a novel graphics user interface
                 for desktop management of a synchronous groupware
                 client. The interface is part of the Rutgers University
                 DISCIPLE framework that enables sharing of
                 applications. The interface presents an individual view
                 of a collaboration space that contains collaboration
                 artifacts, collaborators, their groupings and
                 relationships. The conceptual model of the
                 collaboration process is described since it strongly
                 influences the design of the user interface. We
                 establish the requirements, describe the components of
                 the user interface and then discuss alternative
                 approaches. JavaBeans applications are shared by being
                 imported into the shared workspace, but additionally,
                 importing Beans allows user tailoring of the interface
                 and thus supports end-user programming. Interface
                 customization is demonstrated with multimodal
                 human/machine interfaces and the collaboration
                 components (such as group awareness widgets,
                 concurrency controllers, etc.). Another activity
                 supported is multi-user visual programming using the
                 JavaBeans technology. Users at geographically separate
                 locations can collaboratively build complex
                 applications using pre-existing components. This
                 interface has been implemented and tested on a variety
                 of Java applications.",
  editor =       "I. S. MacKenzie and J. Stewart",
  keywords =     "CSCW, synchronous collaboration, user interfaces,
                 end-user programming, JavaBeans",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-502,
  pages =        "42--49",
  year =         "1999",
  title =        "Stratified Wavelength Clusters for Efficient Spectral
                 Monte Carlo Rendering",
  author =       "Glenn F. Evans and Michael D. McCool",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-502",
  abstract =     "Wavelength dependent Monte Carlo rendering can
                 correctly and generally capture effects such as
                 spectral caustics (rainbows) and chromatic abberation.
                 It also improves the colour accuracy of reflectance
                 models and of illumination effects such as colour
                 bleeding and metamerism. The stratified wavelength
                 clustering (SWC) strategy carries several wavelength
                 stratified radiance samples along each light transport
                 path. The cluster is split into several paths or
                 degraded into a single path only if a specular
                 refraction at the surface of a dispersive material is
                 encountered along the path. The overall efficiency of
                 this strategy is high since the fraction of clusters
                 that need to be split or degraded in a typical scene is
                 low, and also because specular dispersion tends to
                 decrease the source colour variance, offseting the
                 increased amortized cost of generating each path.",
  editor =       "I. S. MacKenzie and J. Stewart",
  keywords =     "Monte Carlo methods, wavelength dependent (spectral)
                 rendering, caustics, rainbows, refraction, reflectance,
                 global illumination",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-503,
  pages =        "50--57",
  year =         "1999",
  title =        "Efficient Glossy Global Illumination with Interactive
                 Viewing",
  author =       "Marc Stamminger and Annette Scheel and Xavier Granier
                 and Frederic Perez-Cazorla and George Drettakis and
                 Francois X. Sillion",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-503",
  abstract =     "The ability to perform interactive walkthroughs of
                 global illumination solutions including glossy effects
                 is a challenging open problem. In this paper we
                 overcome certain limitations of previous approaches. We
                 first introduce a novel, memory- and compute-efficient
                 representation of incoming illumination, in the context
                 of a hierarchical radiance clustering algorithm. We
                 then represent outgoing radiance with an adaptive
                 hierarchical basis, in a manner suitable for
                 interactive display. Using appropriate refinement and
                 display strategies, we achieve walkthroughs of glossy
                 solutions at interactive rates for non-trivial scenes.
                 In addition, our implementation has been developed to
                 be portable and easily adaptable as an extension to
                 existing, diffuse-only, hierarchical radiosity systems.
                 We present results of the implementation of glossy
                 global illumination in two independent global
                 illumination systems.",
  editor =       "I. S. MacKenzie and J. Stewart",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-504,
  pages =        "58--65",
  year =         "1999",
  title =        "Controlling Memory Consumption of Hierarchical
                 Radiosity with Clustering",
  author =       "Xavier Granier and George Drettakis",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-504",
  abstract =     "Memory consumption is a major limitation of current
                 hierarchical radiosity algorithms, including those
                 using clustering. To overcome this drawback we present
                 a new algorithm which reduces the storage required for
                 both the hierarchy of subdivided elements and the links
                 representing light transfers. Our algorithm is based on
                 a link hierarchy, combined with a progressive shooting
                 algorithm. Links are thus stored only when they might
                 transfer energy at subsequent iterations. The push-pull
                 and refine/gather steps of hierarchical radiosity are
                 then combined, allowing the simplification of subtrees
                 of the element hierarchy during refinement. Subdivided
                 polygons replaced by textures and groups of input
                 objects contained in clusters may be deleted. A memory
                 control strategy is then used, forcing links to be
                 established higher in the link hierarchy, limiting the
                 overall memory used. Results of our implementation show
                 significant reduction in memory required for a
                 simulation, without much loss of accuracy or visual
                 quality.",
  editor =       "I. S. MacKenzie and J. Stewart",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-505,
  pages =        "66--75",
  year =         "1999",
  title =        "Approximating the Location of Integrand
                 Discontinuities for Penumbral Illumination with Linear
                 Light Sources",
  author =       "Marc J. Ouellette and Eugene Fiume",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-505",
  abstract =     "One of the benefits of shading with linear light
                 sources is also one of its major challenges: generating
                 soft shadows. The primary difficulty in this task is
                 determining the discontinuities in the linear light
                 source integrals that are caused by occluding objects.
                 We demonstrate in this paper that the computed location
                 of each discontinuity only needs to be moderately
                 accurate, provided that the expected value of this
                 location is a continuous function of the actual value
                 of the location. We introduce Random Seed Bisection
                 (RSB), an algorithm that has this property. We use this
                 algorithm to efficiently find the approximate location
                 of a discontinuity, in order to partition the domain of
                 integration into subintervals (panels) over which the
                 integrand is naturally smooth, and approximate the
                 integral efficiently over each panel using low-degree
                 numerical quadratures. We demonstrate the effectiveness
                 of this solution for shadowing problems with at most 1
                 discontinuity in the domain of integration. We also
                 provide efficient heuristics that take advantage of the
                 coherence in a scene to handle shadowing problems with
                 at most 2 discontinuities in the domain of integration.
                 This work is a first step toward a comprehensive
                 approach to efficiently solving numerical integration
                 problems for extended light sources",
  editor =       "I. S. MacKenzie and J. Stewart",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-506,
  pages =        "76--83",
  year =         "1999",
  title =        "Design of Virtual 3{D} Instruments for Musical
                 Interaction",
  author =       "Axel G. E. Mulder and S. Sidney Fels and Kenji Mase",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-506",
  abstract =     "An environment for designing virtual instruments with
                 3D geometry has been prototyped and applied to
                 real-time sound control and design. It was implemented
                 by extending a realtime, visual programming language
                 called Max/FTS, running on an SGI Onyx, with software
                 objects to interface CyberGloves and Polhemus sensors
                 and to compute human movement and virtual object
                 features. Virtual input devices with behaviours of a
                 rubber balloon and sheet were designed for the control
                 of sound spatialization and timbre parameters. Informal
                 evaluation showed that a sonification inspired by the
                 physical world appears natural and effective. More
                 research is required for a natural sonification of
                 virtual input device features such as shape, taking
                 into account possible co-articulation of these
                 features. While both hands can be used for
                 manipulation, left-hand-only interaction with a virtual
                 instrument may be a useful replacement for and
                 extension of the standard music synthesizer keyboard
                 modulation wheel. More research is needed to identify
                 and apply manipulation pragmatics and movement
                 features, and to investigate how they are
                 co-articulated, in the mapping of virtual object
                 parameters.",
  editor =       "I. S. MacKenzie and J. Stewart",
  keywords =     "Human-computer interface, mulidimensional control,
                 virtual sculpting, sound editing, multimedia mapping.
                 musical instrument design, gesture interface",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-507,
  pages =        "84--91",
  year =         "1999",
  title =        "A Handwriting-Based Equation Editor",
  author =       "Steve Smithies and Kevin Novins and James Arvo",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-507",
  abstract =     "Current equation editing systems rely on either
                 text-based equation description languages or on
                 interactive construction by means of structure
                 templates and menus. These systems are often tedious to
                 use, even for experts, because the user is forced to
                 ``parse'' the expressions mentally before they are
                 entered. This step is not normally part of the process
                 of writing equations on paper or on a whiteboard. We
                 describe a prototype equation editor that is based on
                 handwriting recognition and automatic equation parsing.
                 It is coupled with a user interface that incorporates a
                 set of simple procedures for correcting errors made by
                 the automatic interpretation. Although some correction
                 by the user is typically necessary before the formula
                 is recognized, we have found that the system is simpler
                 and more natural to use than systems based on
                 specialized languages or template-based interaction.",
  editor =       "I. S. MacKenzie and J. Stewart",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-508,
  pages =        "92--97",
  year =         "1999",
  title =        "A Psychophysical Comparison of Two Stylus-Driven Soft
                 Keyboards",
  author =       "Michael Bohan and Chad A. Phipps and Alex Chaparro and
                 Charles G. Halcomb",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-508",
  abstract =     "This study compared text entry performance of two
                 stylus-driven soft keyboards for use in hand-held
                 computing devices: the QWERTY and the T9. Participants
                 transcribed text presented on a computer screen into a
                 personal digital assistant (PDA) using a stylus and one
                 of these two keyboards. We introduced a new
                 psychophysical technique for measuring transcription
                 rate that provides a composite measure of speed and
                 accuracy. Using this technique, we calculated the
                 maximum transcription rate for each keyboard. The
                 results show that transcription rates were higher for
                 the QWERTY keyboard than for the T9, despite the T9
                 keyboard's apparent superior physical characteristics.
                 An ancillary experiment demonstrated that the poorer
                 performance of the T9 layout may have resulted from an
                 increase in visual scanning time due to perceptual
                 grouping of the letters on the keys. Together these
                 findings imply that the QWERTY keyboard layout remains
                 the most effective of the currently available designs
                 for stylus tapping on soft keyboards.",
  editor =       "I. S. MacKenzie and J. Stewart",
  keywords =     "Soft keyboards, stylus input, pen-based computing,
                 hand-held devices, transcription rate",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-509,
  pages =        "107--114",
  year =         "1999",
  title =        "Interactive Rendering of Wavelet Projected Light
                 Fields",
  author =       "Paul Lalonde and Alain Fournier",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-509",
  abstract =     "Light field techniques allow the rendering of objects
                 in time complexity unrelated to their geometric
                 complexity. The technique discretely samples the space
                 of light rays exiting the boundary around an object and
                 then reconstructs a requested view from these data. In
                 order to generate high quality images a dense sampling
                 of the space is required which leads to large data
                 sets. These data sets exhibit a high degree of
                 coherence and should be compressed in order to make
                 their size manageable. We present a wavelet-based
                 method for storing light fields over planar domains.
                 The parameterization is based on the Nusselt embedding,
                 which leads to simplifications in shading computations
                 when the light fields are used illumination sources.
                 The wavelet transform exploits the coherence in the
                 data to reduce the size of the data sets by factors of
                 20 times or more without objectionable deterioration in
                 the rendered images. The wavelet representation also
                 allows a hierarchical representation in which details
                 can be added incrementally, and in which each coarser
                 view is an appropriately filtered version of the finer
                 detail. By blending between the two seemless
                 transitions are possible. The wavelet coefficients are
                 compressed by thresholding the coefficients and storing
                 them in a sparse hexadecary tree. The tree encoding
                 allows random access over the compressed wavelet
                 coefficients which is essential for extracting slices
                 and point samples from the light field.",
  editor =       "I. S. MacKenzie and J. Stewart",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@TechReport{EVL-1999-51,
  year =         "1999",
  title =        "Efficient evaluation of triangular {B}-splines",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-51",
  author =       "M. Franssen and R. C. Veltkamp and W. Wesselink",
  keywords =     "triangular b-splines, evaluation graph",
  number =       "UU-CS-1999-18",
  institution =  "Utrecht University, Department of Computer Science",
}

@InProceedings{EVL-1999-510,
  pages =        "115--122",
  year =         "1999",
  title =        "Imaging all Visible Surfaces",
  author =       "Wolfgang Stuerzlinger",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-510",
  abstract =     "Today many systems exist to generate geometric models
                 of existing scenes and objects. However, very few
                 systems store accurate data about surface appearance
                 such as colors and textures. One way to capture surface
                 texture data is to record a series of images that,
                 collectively, captures all visible surfaces of the
                 object. Finding good viewpoints for this task is not
                 easy. This paper presents a new heuristic method to
                 find a good set of viewpoints for a given geometric
                 model. Taking images from the computed viewpoints will
                 show every visible part of every surface at least once.
                 The approach uses a hierarchical visibility algorithm
                 to preprocess the scene. A good set of viewing regions
                 is identified with simulated annealing and then good
                 viewpoints are derived. Results and visualizations of
                 the computed solutions are presented.",
  editor =       "I. S. MacKenzie and J. Stewart",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-511,
  pages =        "132--139",
  year =         "1999",
  title =        "Discrete Parametrization for Deforming Arbitrary
                 Meshes",
  author =       "Shigeru Kuriyama and Toyohisa Kaneko",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-511",
  abstract =     "Techniques for deforming polygonal meshes are
                 demonstrated by using two-dimensional lattices of
                 control points or functions for pasting features. The
                 deformations use a shape-preserving parametrization
                 that embeds the mesh's vertices in a normalized
                 two-dimensional space while preserving shape
                 consistency for non-flat surfaces. A discrete smoothing
                 used for the parametrization has inefficient iterative
                 calculations, which is unsuitable for manipulations of
                 dense meshes, and an initial approximation for the
                 smoothing is therefore proposed in order to reduce the
                 number of iterations. The approximation uses a
                 graph-searching algorithm and a discrete normalization
                 whose computational costs are negligible in comparison
                 with that of the iterative calculations.",
  editor =       "I. S. MacKenzie and J. Stewart",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-512,
  pages =        "140--147",
  year =         "1999",
  title =        "A General Model of Animated Shape Perturbation",
  author =       "Jean-Michel Dischler",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-512",
  abstract =     "Stochastic shape--perturbation, called shape-- or
                 hyper--texturing, represents an attractive way for
                 rendering many complex surface structures including
                 fur, fire, cotton or rocks. While current methods often
                 limit applications to static, isolated and rather
                 simple objects, this paper attempts to provide a more
                 general approach, based on projection schemes. The
                 presented technique not only permits to deal with
                 various shapes, but furthermore combines perturbation
                 with animation using 3D deformation models based on a
                 principle of smoothed particles. The parameters of the
                 deformation are controlled by the user through
                 {"}projection primitives{"}. Examples of animated
                 surface behaviors (applied to usual polyhedrons such as
                 the Utah teapot) including morphing, fluttering,
                 burning and waving illustrate the possibilities of the
                 approach.",
  editor =       "I. S. MacKenzie and J. Stewart",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-513,
  pages =        "148--156",
  year =         "1999",
  title =        "Interactive Mesh Fusion Based on Local 3{D}
                 Metamorphosis",
  author =       "Takashi Kanai and Hiromasa Suzuki and Jun Mitani and
                 Fumihiko Kimura",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-513",
  abstract =     "This paper proposes a new mesh modeling scheme, called
                 mesh fusion , based on three-dimensional (3D)
                 mesh-based metamorphosis. We establish the attachment
                 from a part of one mesh to a part of another with
                 smooth boundaries, employing the traditional cutting
                 and pasting operation in conjunction with a combination
                 of meshes, applying the idea of 3D metamorphosis. We
                 also offer an algorithm for adjusting two boundaries by
                 using the combination of three geometrical operations
                 rigid transformation, scaling and deformation . Our
                 schematic offers a computation time swift enough that
                 the user can create various shapes with interactive
                 speed.",
  editor =       "I. S. MacKenzie and J. Stewart",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-514,
  pages =        "157--166",
  year =         "1999",
  title =        "Observational Model of Blenders and Erasers in
                 Computer-Generated Pencil Rendering Observational Model
                 of Blenders and Erasers in Computer-Generated Pencil
                 Rendering",
  author =       "Mario Costa Sousa and John William Buchanan",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-514",
  abstract =     "In this paper we present a blender and eraser model
                 that extends our graphite pencil and paper model. This
                 blender and eraser model enhances the rendering results
                 producing realistic looking graphite pencil tones and
                 textures. Our model is based on observations on the
                 absorptive and dispersive properties of blenders and
                 erasers interacting with lead material deposited over
                 drawing paper. The parameters of our model are the
                 particle composition of the lead over the paper, the
                 texture of the paper, the position and shape of the
                 blender and eraser, and the pressure applied to them.
                 We demonstrate the capabilities of our approach with a
                 variety of pencil swatches and compare them to
                 digitized pencil drawings. We also present automatic
                 and interactive image-based rendering results
                 implementing traditional graphite pencil tone rendering
                 methods.",
  editor =       "I. S. MacKenzie and J. Stewart",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-515,
  pages =        "167--174",
  year =         "1999",
  title =        "Halftoning with Image-Based Dither Screens",
  author =       "Oleg Veryovka and John Buchanan",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-515",
  abstract =     "Continuous tone images must be halftoned to be
                 displayed on binary output devices such as printers.
                 The ordered dither algorithm is a popular approach to
                 halftoning. This algorithm uses a threshold matrix to
                 approximate gray scale values. The arrangement of
                 thresholds in the matrix determines texture artifacts
                 introduced into the halftoned image. Thus, the
                 challenge of research in ordered dithering is to find a
                 matrix that results in the least visible texture
                 artifacts. In this work we control the halftoning
                 texture by generating a threshold matrix from an
                 arbitrary image. We demonstrate that processing images
                 using adaptive histogram equalization results in pixel
                 distributions similar to traditional dither screens.
                 Ordered dithering with the resulting threshold matrix
                 enables us to define texture in the halftoned image. We
                 control the appearance of this texture by a combination
                 of the ordered dither algorithm with an error diffusion
                 process. We present applications of the image-based
                 dither screens to both photorealistic and artistic
                 rendering. In the case of photorealistic tone
                 reproduction this technique preserves textures and
                 edges of the original image. The ability to define an
                 arbitrary texture enables us to introduce a variety of
                 artistic effects. A halftoned image can be embossed
                 with another image, texture, or text. Also, halftoning
                 with textures clipped from the existing art works
                 approximates the look of traditional illustration
                 media.",
  editor =       "I. S. MacKenzie and J. Stewart",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-516,
  pages =        "175--182",
  year =         "1999",
  title =        "An illustration technique using hardware-based
                 intersections and skeletons",
  author =       "Oliver Deussen and J{\"{o}}rg Hamel and Andreas Raab
                 and Stefan Schlechtweg and Thomas Strothotte",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-516",
  abstract =     "We present a method for generating line drawings of
                 complex geometries in the style of crosshatched
                 illustrations. Hatching lines are generated by
                 intersecting the geometry with a set of planes.
                 Half-toning on the basis of the generated curves is
                 used to represent a given intensity distribution.
                 Computing a geometric skeleton allows us to determine
                 automatically the orientation of the intersection
                 planes for a wide variety of models. By using
                 predefined line styles different types of illustrations
                 can be generated. Applications of the method are
                 discussed, examples are given.",
  editor =       "I. S. MacKenzie and J. Stewart",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-517,
  pages =        "193--202",
  year =         "1999",
  title =        "A Visual Model For Blast Waves and Fracture",
  author =       "Michael Neff and Eugene Fiume",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-517",
  abstract =     "The expense, danger, planning and precision required
                 to create explosions suggests that the computational
                 visual modelling of explosions is worthwhile. However,
                 the short time scale at which explosions occur, and
                 their sheer complexity, poses a difficult modelling
                 challenge. After describing the basic phenomenology of
                 explosion events, we present an efficient computational
                 model of isotropic blast wave transport and an
                 algorithm for fracturing objects in their wake. Our
                 model is based on the notion of a blast curve that
                 gives the force-loading profile of an explosive
                 material on an object as a function of distance from
                 the explosion's centre. We also describe a technique
                 for fracturing materials loaded by a blast. Our
                 approach is based on the notion of rapid fracture :
                 that microfractures in a material together with loading
                 forces seed a fracturing process that quickly spreads
                 across the material and causes it to fragment.",
  editor =       "I. S. MacKenzie and J. Stewart",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-518,
  pages =        "203--210",
  year =         "1999",
  title =        "Animating Lava Flows",
  author =       "Dan Stora and Pierre-Olivier Agliati and Marie-Paule
                 Cani and Fabrice Neyret and Jean-Dominique Gascue",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-518",
  abstract =     "Animating lava flowing down the slopes of a volcano
                 brings several challenges: modeling the mechanical
                 features of lava and how they evolve over time
                 depending on temperature; computing, in a reasonable
                 time, the interactions inside the flow, and between the
                 flow and a complex terrain data-base; and lastly,
                 rendering the visual aspect of the flow. The methods
                 described rely on smoothed particles governed by a
                 state equation for animating the flow. We adapt this
                 model to the animation of lava by linking viscosity to
                 a temperature field and by simulating heat transfers.
                 We propose particular data-structures that lead to
                 linear computational time with respect to the number of
                 particles. Lastly, we study a model based on a
                 color-and-displacement procedural texture controlled by
                 the flow for the realistic rendering of lava.",
  editor =       "I. S. MacKenzie and J. Stewart",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-519,
  pages =        "211--217",
  year =         "1999",
  title =        "Animating Exploding Objects",
  author =       "Oleg Mazarak and Claude Martins and John Amanatides",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-519",
  abstract =     "The paper explores the physically-based modeling of a
                 blast wave impact on surrounding objects. We propose a
                 connected voxel representation of objects to model
                 explosions that result in realistic solid debris,
                 rather than flat polygons. The paper also presents
                 improved fracture algorithms capable of accounting for
                 the damage of multiple explosions. The important
                 implementation issues and the results of the simulation
                 are discussed.",
  editor =       "I. S. MacKenzie and J. Stewart",
  keywords =     "Explosions, blast waves, connected voxels, spring-mass
                 particle model, rigid bodies, solid modeling,
                 physically-based modeling",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@TechReport{EVL-1999-52,
  year =         "1999",
  title =        "Calculations on critical points under Gaussian
                 blurring",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-52",
  author =       "A. Kuijper and L. M. J. Florack",
  abstract =     "The behaviour of critical points of Gaussian
                 scale-space images is mainly described by their
                 creation and annihilation. In existing literature these
                 events are determined in so-called canonical
                 coordinates. A description in a userdefined Cartesian
                 coordinate system is stated, as well as the results of
                 a straightforward implementation. The location of a
                 catastrophe can be predicted with subpixel accuracy. An
                 example of an annihilation is given. Also an upper
                 bound is derived for the area where critical points can
                 be created. Experimental data of an MR, a CT, and an
                 artificial noise image satisfy this result.",
  number =       "UU-CS-1999-12",
  institution =  "Utrecht University, Department of Computer Science",
}

@InProceedings{EVL-1999-520,
  pages =        "183--192",
  year =         "1999",
  title =        "Computer Aided Serendipity: The Role of Autonomous
                 Assistants in Problem Solving",
  author =       "James Arvo",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-520",
  abstract =     "Pencil and paper are perhaps the most effective
                 problem-solving tools ever invented. Why is this so,
                 and what does this portend for computer-assisted
                 problem solving? In this paper we investigate why the
                 computer has not made more significant inroads into
                 many aspects of problem solving, even in domains
                 ostensibly concerned with purely formal methods. We
                 observe that for many problem-solving activities
                 computers are currently more constraining than
                 enabling, particularly during problem formulation. We
                 identify some of the obstacles that must be overcome in
                 making the computer a more attractive medium for
                 problem solving, and explore some of the tools that
                 will likely play a part in bringing this about.",
  editor =       "I. S. MacKenzie and J. Stewart",
  keywords =     "Artificial intelligence, autonomous assistants, belief
                 revision, human-computer interaction, numerical
                 experiments, symbolic computation",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@PhdThesis{EVL-1999-521,
  year =         "1999",
  title =        "Accuracy in Scientific Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-521",
  author =       "Adriano Lopes",
  keywords =     "accuracy, scientific visualization, contouring,
                 isosurfacing, marching-cubes, particle tracing",
  school =       "School of Computer Studies / University of Leeds",
}

@InProceedings{EVL-1999-522,
  pages =        "7--14",
  year =         "1999",
  title =        "Parallel lumigraph reconstruction",
  author =       "Peter-Pike Sloan and Charles Hansen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-522",
  abstract =     "This paper presents three techniques for
                 reconstructing Lumi-graphs/Lightfields on commercial
                 ccNUMA parallel distributed shared memory computers.
                 The first method is a parallel extension of the
                 software-based method proposed in the Lightfield paper.
                 This expands the ray/two-plane intersection test along
                 the film plane, which effectively becomes scan
                 conversion. The second method extends this idea by
                 using a shear/warp factorization that accelerates
                 rendering. The third technique runs on an SGI Reality
                 Monster using up to eight graphics pipes and texture
                 mapping hardware to reconstruct images. We characterize
                 the memory access patterns exhibited using the
                 hardware-based method and use this information to
                 reconstruct images from a tiled UV plane. We describe
                 amethod to use quad-cubic reconstruction kernels. We
                 analyze the memory access patterns that occur when
                 viewing Lumigraphs. This allows us to ascertain the
                 cost/benefit ratio of various tilings of the texture
                 plane.",
  editor =       "J. Ahrens and A. G. Chalmers and H-W. Shen",
  booktitle =    "Proceedings of the 1999 IEEE Symposium on Parallel
                 Visualization and Graphics",
}

@InProceedings{EVL-1999-523,
  pages =        "15--20",
  year =         "1999",
  title =        "Parallel Visualization of Large-Scale Aerodynamics
                 Calculations: {A} Case Study on the Cray {T3}",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-523",
  abstract =     "This paper reports the performance of a parallel
                 volume rendering algorithm for visualizing a
                 large-scale unstructured-grid dataset produced by a
                 three-dimensional aerodynamics simulation. This
                 dataset, containing over 18 million tetrahedra, allows
                 us to extend our performance results to a problem which
                 is more than 30 times larger than the one we examined
                 previously. This high resolution dataset also allows us
                 to see fine, three-dimensional features in the flow
                 field. All our tests were performed on the SGI/Cray T3E
                 operated by NASA's Goddard Space Flight Center. Using
                 511 processors, a rendering rate of almost 9 million
                 tetrahedra/second was achieved with a parallel overhead
                 of 26%.",
  editor =       "J. Ahrens and A. G. Chalmers and H-W. Shen",
  keywords =     "Parallel rendering, volume rendering, scientific
                 visualization, parallel algorithms, unstructured grids,
                 computational fluid dynamics, T3E",
  booktitle =    "Proceedings of the 1999 IEEE Symposium on Parallel
                 Visualization and Graphics",
}

@InProceedings{EVL-1999-524,
  pages =        "21--28",
  year =         "1999",
  title =        "Hybrid Scheduling for Parallel Rendering using
                 Coherent Ray Tasks",
  author =       "Erik Reinhard and Alan Chalmers and Frederik W.
                 Jansen",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-524",
  abstract =     "Parallelising ray tracing with a data parallel
                 approach allows rendering of arbitrarily large models,
                 but the inherent load imbalances may lead to severe
                 inefficiencies. To compensate for the uneven load
                 distribution, demand-driven tasks may be split off and
                 scheduled to processors that are less busy. We propose
                 a hybrid scheduling algorithm which brings tasks and
                 data together according to coherence between rays. The
                 amount of demand-driven versus data-parallel tasks is a
                 function of the coherence between rays and the amount
                 of imbalance in the basic data-parallel load.
                 Processing power, communication and memory are three
                 resources which should be evenly used. Our current
                 implementation is assessed against these requirements,
                 showing good scalability and very little communication
                 at the cost of a slightly larger memory overhead.",
  editor =       "J. Ahrens and A. G. Chalmers and H-W. Shen",
  keywords =     "Parallel computing, hybrid scheduling, ray tracing",
  booktitle =    "Proceedings of the 1999 IEEE Symposium on Parallel
                 Visualization and Graphics",
}

@InProceedings{EVL-1999-525,
  pages =        "39--46",
  year =         "1999",
  title =        "Transparent Distributed Processing For Rendering",
  author =       "Peter Kipfer and Philipp Slusallek",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-525",
  abstract =     "Rendering, in particular the computation of global
                 illumination, uses computationally very demanding
                 algorithms. As a consequence many researchers have
                 looked into speeding up the computation by distributing
                 it over a number of computational units. However, in
                 almost all cases did they completely redesign the
                 relevant algorithms in order to achieve high efficiency
                 for the particular distributed or parallel environment.
                 At the same time global illumination algorithms have
                 gotten more and more sophisticated and complex. Often
                 several basic algorithms are combined in multi-pass
                 arrangements to achieve the desired lighting effects.
                 As a result, it is becoming increasingly difficult to
                 analyze and adapt the algorithms for optimal parallel
                 execution at the lower levels. Furthermore, these
                 bottom-up approaches destroy the basic design of an
                 algorithm by polluting it with distribution logic and
                 thus easily make it unmaintainable. In this paper we
                 present a top-down approach for designing distributed
                 applications based on their existing object-oriented
                 decomposition. Distribution logic, in our case based on
                 the CORBA middleware standard, is introduced
                 transparently to the existing application logic. The
                 design approach is demonstrated using several examples
                 of multi-pass global illumination computation and
                 ray-tracing. The results show that a good speedup can
                 usually be obtained even with minimal intervention into
                 existing applications.",
  editor =       "J. Ahrens and A. G. Chalmers and H-W. Shen",
  keywords =     "Distributed Processing, Parallel Processing,
                 Object-oriented Design, Design Pattern, Global
                 Illumination, Lighting Networks",
  booktitle =    "Proceedings of the 1999 IEEE Symposium on Parallel
                 Visualization and Graphics",
}

@InProceedings{EVL-1999-526,
  pages =        "47--54",
  year =         "1999",
  title =        "Web based Collaborative Visualization of Distributed
                 and Parallel Simulation",
  author =       "C. Bajaj and S. Cutchin",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-526",
  abstract =     "This paper presents an interaction model to support
                 collaborative scientific visualization. Relevant prior
                 work is presented to contextualize the model and its
                 import. An implementation of the model is presented
                 within a collaborative system that supports flexible
                 collaborative coupling of multi-user applications. An
                 example application is presented to demonstrate the
                 capabilities of the model. The implementation is Web
                 based, fully supports multi-user interfaces, uses VRML
                 and compresed VRML for three dimensional graphic
                 display, and is implemented in Java with CORBA support
                 for external server access. An example experiment
                 involving multiple users is described.",
  editor =       "J. Ahrens and A. G. Chalmers and H-W. Shen",
  booktitle =    "Proceedings of the 1999 IEEE Symposium on Parallel
                 Visualization and Graphics",
}

@InProceedings{EVL-1999-527,
  pages =        "55--59",
  year =         "1999",
  title =        "Scalable Distributed Visualization Using Off-the-Shelf
                 Components",
  author =       "lan Heirich and Laurent Moll",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-527",
  abstract =     "This paper desribes a visualization architecture for
                 scalable computer systems. The architecture is
                 currently being prototyped for use in Beowulf-class
                 clustered systems. A set of OpenGL frame bufers are
                 driven in parallel by a set of CPUs. The visualization
                 architecture merges the contents of these frame buffers
                 by user-programmable associative and commutative
                 combining operations. The system hardware is built from
                 off-the-shelf components including OpenGL accelerators,
                 Field Programmable Gate Arrays (FPGAs), and gifabit
                 network interfaces and switches. A second-generation
                 prototype supports 60 Hz operation ay 1024 pixel
                 resolution with interactive latency up to 1000 nodes.",
  editor =       "J. Ahrens and A. G. Chalmers and H-W. Shen",
  keywords =     "FPGA, OpenGL, visualization, cluster. Beowulf,
                 gigabit, fat-tree",
  booktitle =    "Proceedings of the 1999 IEEE Symposium on Parallel
                 Visualization and Graphics",
}

@InProceedings{EVL-1999-528,
  pages =        "61--68",
  year =         "1999",
  title =        "Interactive Volume Segmentation with the {PAVLOV}
                 Architecture",
  author =       "Kevin Kreeger and Arie Kaufmany",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-528",
  abstract =     "With the recent development of interactive and
                 real-time volume rendering architectures, it is our
                 belief that users will quickly demand more
                 functionality from their visualization systems. We
                 propose a new paradigm for interactive volume
                 visualization which allows enhanced exploration of
                 datasets through the use of real-time volume
                 processing. Segmentation is a classical example of
                 volume processing. We show examples of how interactive
                 feedback from adjusting the segmentation parameters
                 gives the user a new way of examining a dataset. We
                 have developed a hardware co-processor, called PAVLOV.
                 Based on a SIMD mesh architecture, it can provide
                 volume processing, such as segmentation and feature
                 extraction, at over 15Hz while also performing 30Hz
                 rendering of the results. We present our method to
                 perform interactive segmentation with the PAVLOV
                 architecture. We further show how the PAVLOV systems
                 programmability is used for other volume processing
                 applications,such as feature extraction.",
  editor =       "J. Ahrens and A. G. Chalmers and H-W. Shen",
  keywords =     "Volume Processing, Segmentation, Volume Rendering,
                 SIMD, 2D Mesh Array",
  booktitle =    "Proceedings of the 1999 IEEE Symposium on Parallel
                 Visualization and Graphics",
}

@InProceedings{EVL-1999-529,
  pages =        "69--78",
  year =         "1999",
  author =       "Robert Garmann",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-529",
  abstract =     "The Hierarchical Radiosity Algorithm (HRA) is one of
                 the most efficient sequential algorithms for physically
                 based rendering. Unfortunately, it is hard to implement
                 in parallel. There exist fairly efficient shared-memory
                 implementations but things get worst in a distributed
                 memory (DM) environment. In this paper we examine the
                 structure of the HRA in a graph partitioning setting.
                 Various measurements performed on the task access graph
                 of the HRA indicate the existance of several
                 bottlenecks in a potential DM implementation. We
                 compare ``optimal'' partitioning results obtained by
                 the partitioning software Metis with a trivial and a
                 spatial partitioning algorithm, and show that the
                 spatial partitioning copes with most of the bottlenecks
                 well.",
  editor =       "J. Ahrens and A. G. Chalmers and H-W. Shen",
  booktitle =    "Proceedings of the 1999 IEEE Symposium on Parallel
                 Visualization and Graphics",
}

@InProceedings{EVL-1999-53,
  year =         "1999",
  title =        "View Dependent Topology Simplification",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-53",
  author =       "Jihad El-Sana and Amitabh Varshney",
  abstract =     "We propose a technique for performing view-dependent
                 simplifications for level-of-detail-based renderings of
                 complex models. Our method is based on exploiting
                 frame-to-frame coherence and is tolerant of various
                 commonly found degeneracies in real life polygonal
                 models. The algorithm proceeds by preprocessing the
                 input dataset into a binary tree of vertex collapses.
                 This tree is used at run time to generate the triangles
                 for display. Dependencies to avoid mesh foldovers in
                 manifold regions of the input object are stored in the
                 tree in an implicit fashion. This obviates the need for
                 any extra storage for dependency pointers and suggests
                 a potential for application to external memory
                 prefetching algorithms. We also propose a distance
                 metric that can be used to unify the geometry and genus
                 simplifications with the view-dependent parameters as
                 viewpoint, view-frustum, and local illumination.",
  organization = "Springer-Verlag Wien",
  editor =       "Axel Hildebrand {Michael Gervautz, Dieter
                 Schmalstieg}",
  booktitle =    "Virtual Environments '99. Proceedings of the
                 Eurographics Workshop",
}

@InProceedings{EVL-1999-530,
  pages =        "79--88",
  year =         "1999",
  title =        "Overlapping Multi-Processing and Graphics Hardware
                 Acceleration:Performance Evaluation",
  author =       "Xavier Cavin and Laurent Alonso and Jean-Claude Paul",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-530",
  abstract =     "Recently, multi-processing has been shown to deliver
                 good performance in rendering. However, in some
                 applications, processors spend too much time executing
                 tasks that could be more efficiently done through
                 intensive use of new graphics hardware. We present in
                 this paper a novel solution combining multi-processing
                 and advanced graphics hardware, where graphics
                 pipelines are used both for classical visualization
                 tasks and to advantageously perform geometric
                 calculations while remaining computations are handled
                 by multi-processors. The experiment is based on an
                 implementation of a new parallel wavelet radiosity
                 algorithm. The application is executed on the SGI
                 Origin 2000 connected to the SGI InfiniteReality 2
                 rendering pipeline. A performance evaluation is
                 presented. Keeping in mind that the approach can
                 benefit all available workstations and super-computers,
                 from small scale (2 processors and 1 graphics pipeline)
                 to large scale (\Theta processors and \Lambda graphics
                 pipelines), we highlight some important bottlenecks
                 that impede performance. However, our results show that
                 this approach could be a promising avenue for
                 scientific and engineering simulation and visualization
                 applications that need intensive geometric
                 calculations.",
  editor =       "J. Ahrens and A. G. Chalmers and H-W. Shen",
  keywords =     "Parallelism, Graphics Hardware, Hierarchical and
                 Multiresolution Algorithm, Wavelet, Realistic
                 Rendering, Radiosity",
  booktitle =    "Proceedings of the 1999 IEEE Symposium on Parallel
                 Visualization and Graphics",
}

@InProceedings{EVL-1999-531,
  pages =        "89--96",
  year =         "1999",
  title =        "Dynamic View-Dependent Partitioning of Structured
                 Grids with Complex Boundaries for Object-Order
                 Rendering Techniques",
  author =       "Lance C. Burton and Raghu Machiraju and Donna S.
                 Reese",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-531",
  abstract =     "Object-order rendering techniques present an
                 attractive approach to run-time visualization of
                 structured grid data, particularly when combined with a
                 parallel rendering paradigm such as image composition.
                 The ability of this combination to exploit hardware
                 exceeds that of parallel image order methods. However,
                 certain configurations of grid boundaries prevent
                 composition from being performed correctly. In
                 particular, when the boundary between two partitions
                 contains concave sections, the partitions may no longer
                 be depth sorted correctly, a requirement for some
                 visualization techniques such as direct volume
                 rendering. This occurs because the concave boundary
                 prevents even the simple ordering of two adjacent
                 partitions. If the data may be repartitioned such that
                 it can be depth sorted correctly, then an image
                 composition approach is a viable option. To facilitate
                 such an operation, we present an algorithm to analyze
                 the geometric structure of a grid boundary and extract
                 knowledge about how the boundary impacts depth sorting
                 and therefore image composition. We then show through
                 examples how this knowledge may be applied to create a
                 set of partitions that may be properly depth sorted.",
  editor =       "J. Ahrens and A. G. Chalmers and H-W. Shen",
  booktitle =    "Proceedings of the 1999 IEEE Symposium on Parallel
                 Visualization and Graphics",
}

@InProceedings{EVL-1999-532,
  pages =        "97--104",
  year =         "1999",
  title =        "Parallel and Out-of-core View-dependent Isocontour
                 Visualization Using Random Data Distribution",
  author =       "Xiaoyu Zhang and Chandrajit Bajaj and Vijaya
                 Ramachandran",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-532",
  abstract =     "In this paper we describe a parallel and out-of-core
                 view-dependent isocontour visualization algorithm that
                 efficiently extracts and renders the visible portions
                 of an isosurface from large datasets. The algorithm
                 first creates an occlusion map using ray-casting and
                 nearest neighbors. With the occlusion map constructed,
                 the visible portion of the isosurface is extracted and
                 rendered. All steps are in a single pass with minimal
                 communication overhead. The overall workload is well
                 balanced among parallel processors using random data
                 distribution. Volumetric datasets are statically
                 partitioned onto the local disks of each processor and
                 loaded only when necessary. This out-of-core feature
                 allows it to handle scalably large datasets. We
                 additionally demonstrate significant speedup of the
                 view-dependent isocontour visualization on a commodity
                 off-the-shelf PC cluster.",
  editor =       "J. Ahrens and A. G. Chalmers and H-W. Shen",
  booktitle =    "Proceedings of the 1999 IEEE Symposium on Parallel
                 Visualization and Graphics",
}

@InProceedings{EVL-1999-533,
  pages =        "3--14",
  year =         "1999",
  title =        "Computer-aided Staging",
  author =       "Patrizia Palamidese",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-533",
  abstract =     "This paper describes the genesis and main
                 functionalities of a real-time animation system for
                 designing multimedia scenes. Users can create a system
                 of synchronized events and give a structured shape to
                 the various parts of a performance. Much work has been
                 done on the production of realistic synthetic movies,
                 but little to support planning and pre-production. This
                 system addresses the requirements of those who have to
                 plan, preview and evaluate the spatial and temporal
                 arrangements of human figures with other media before
                 their practical realization in a real or virtual
                 environment. To define a user staging process which is
                 general and valid for different kinds of users and
                 different applications, we have analysed current
                 staging methodologies used in the theatre, cinema and
                 TV. Since these fields lack standard procedures,
                 obtaining a good degree of generality and completeness
                 entailed an iterative work of specification,
                 prototyping and testing with the help of professionals
                 from such fields. Unlike traditional systems which
                 treat media in separate software, e.g. human body
                 motion alone or speech alone, we consider the elements
                 in the scene altogether according to the theatrical
                 paradigm which is the basic framework for describing
                 the design of a performance.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  volume =       "10(1)",
  keywords =     "Staging, actor animation, multimedia scenes,
                 multimedia script",
  booktitle =    "The Journal of Visualization and Computer Animation",
}

@InProceedings{EVL-1999-534,
  pages =        "15--26",
  year =         "1999",
  title =        "Animation of Water Droplets Moving Down a Surface",
  author =       "Kazukumi Kaneda and Shinya Ikeda and Hideo Yamashita",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-534",
  abstract =     "Rendering a scene containing water is among the most
                 challenging aspects in the field of computer graphics.
                 Researchers have developed a wide variety of algorithms
                 for rendering water, but there still remain unaddressed
                 problems due to water's multiformity and complex
                 patterns of motion. This paper proposes a method for
                 generating realistic animation of water droplets on
                 transparent surfaces such as the windshield of a
                 vehicle. One of the method's applications can be
                 considered the animation of water droplets on a
                 windshield, a process vital for drive simulators. The
                 proposed method employs a particle system in a discrete
                 environment to calculate the movement of water droplets
                 on a surface, and environment mapping for indicating
                 the property of transparency to quickly render scenes
                 through transparent objects. Using the method,
                 windshield wipers moving on a windshield and other
                 obstacles moving against water droplets can also be
                 taken into account. Fully computer-generated animations
                 and video compositions of water droplets on a
                 windshield demonstrate the usefulness of the proposed
                 method.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  volume =       "10(1)",
  keywords =     "Computer animation, particle systems, discrete model,
                 environment mapping, video composition",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1999-535,
  pages =        "27--37",
  year =         "1999",
  title =        "Simple Cellular Automaton-based Simulation of Ink
                 Behaviour and its Application to Suibokuga-like 3{D}
                 Rendering of Trees",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-535",
  author =       "Qing Zhang and Youetsu Sato and Jun-ya Takahashi and
                 Kazunobu Muraoka and Norishige Chiba",
  abstract =     "Suibokuga is a style of monochrome painting
                 characterized by the use of Chinese black ink (sumi), a
                 complex interaction between brush, ink and paper, and
                 such visual features as Noutan (shade), Kasure
                 (scratchiness), and Nijimi (blur). In this paper we
                 present a simple behavioural model of water and ink
                 particles based on a 2D cellular automaton
                 computational model, and its application to a
                 Suibokuga-like rendering of 3D trees.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  keywords =     "Non-photorealistic rendering, Suibokuga-like
                 rendering, simulation of ink diffusion, cellular
                 automaton",
  volume =       "10(1)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1999-536,
  pages =        "39--54",
  year =         "1999",
  title =        "Computer Animation of Human Walking: a Survey",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-536",
  author =       "Franck Multon and Laure France and Marie-Paule
                 Cani-Gascuel and Giles Debunne",
  abstract =     "This paper surveys the set of techniques developed in
                 computer graphics for animating human walking. First we
                 focus on the evolution from purely kinematic
                 knowledge-based methods to approaches that incorporate
                 dynamic constraints or use dynamic simulations to
                 generate motion. Then we review the recent advances in
                 motion editing that enable the control of complex
                 animations by interactively blending and tuning
                 synthetic or captured motions.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  keywords =     "Computer animation, human walking, motion synthesis
                 and control, kinematics, dynamics, motion capture",
  volume =       "10(1)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1999-537,
  pages =        "57--78",
  year =         "1999",
  title =        "Calculation and Visualization of the Dynamic Ability
                 of the Human Body",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-537",
  author =       "Taku Komura and Yoshihisa Shinagawa and Tosiyasu L.
                 Kunii",
  abstract =     "There is a great demand for data on the mobility and
                 strength capability of the human body in many areas,
                 such as ergonomics, medical engineering, biomechanical
                 engineering, computer graphics (CG) and virtual reality
                 (VR). This paper proposes a new method that enables the
                 calculation of the maximal force exertable and
                 acceleration performable by a human body during
                 arbitrary motion. A musculoskeletal model of the legs
                 is used for the calculation. Using our algorithm, it is
                 possible to evaluate whether a given posture or motion
                 is a feasible one. A tool to visualize the calculated
                 maximal feasibility of each posture is developed. The
                 obtained results can be used as criteria of
                 manipulability or strength capability of the human
                 body, important in ergonomics and human animation.
                 Since our model is muscle-based, it is possible to
                 simulate and visualize biomechanical effects such as
                 fatigue and muscle training. The solution is based on
                 linear programming and the results can be obtained in
                 real time.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  keywords =     "Muscle-based model, human strength calculation, human
                 strength visualization",
  volume =       "10(2)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1999-538,
  pages =        "79--90",
  year =         "1999",
  title =        "High-quality Rendering of Smooth Isosurfaces",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-538",
  author =       "Eric LaMar and Bernd Hamann and Kenneth I. Joy",
  abstract =     "Animation and visualization of rectilinear data
                 require interpolation schemes for smooth image
                 generation. Piecewise trilinear interpolation, the de
                 facto standard for interpolating rectilinear data,
                 usually leads to significant visual artifacts in the
                 resulting imagery. These artifacts reduce the
                 confidence in the resulting visualization and may even
                 lead to false interpretations of the data. This paper
                 is concerned with the generation of smooth isosurface
                 image sequences, obtained by casting rays through the
                 image plane and computing their intersections with an
                 isosurface. We describe a novel solution to this
                 problem: we replace trilinear interpolation by tricubic
                 interpolation, smoothing out the artifacts in the
                 images; and we simplify the ray-isosurface intersection
                 calculations by rotating and resampling the original
                 rectilinear data in a second rectilinear grid - a grid
                 with one family of grid planes parallel to the image
                 plane. Our solution significantly reduces artifacts in
                 individual images and leads to smooth animations.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  keywords =     "Animation, approximation, trilinear splines, volume
                 visualization, ray casting, isosurfaces, resampling,
                 Catmull-Rom splines, Hardy's multiquadric scheme",
  volume =       "10(2)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1999-539,
  pages =        "91--107",
  year =         "1999",
  title =        "Efficient Normal Estimation Using Variable-size
                 Operator",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-539",
  author =       "Byeong-Seok Shin",
  abstract =     "Shading an object is to simulate the behaviour of
                 light incident on its surfaces. It is necessary to
                 calculate normal vectors on the surfaces of the object
                 for shading it. Since objects do not contain surface
                 inclination in voxel-based representation, a normal
                 vector for each voxel must be estimated from the
                 relative position of its neighbouring voxels that have
                 the same data value. The previously devised methods
                 that use fixed-size gradient operators can estimate
                 normal vectors accurately only in a limited area and
                 may cause some errors and artefacts. In this paper we
                 propose an efficient normal estimation method using an
                 extended central difference operator whose size can
                 vary according to the arrangement of surface-comprising
                 voxels. This method calculates normals more accurately
                 than the previous methods and its computation time is
                 shorter than that of methods that guarantee the same
                 image quality. In order to show the improvement, we
                 compare the quality of resulting images and processing
                 time by implementing the newly proposed method and the
                 previous methods and then applying them to some volume
                 data.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  keywords =     "Normal estimation, volume shading, voxel-based
                 representation, variable-size gradient operator",
  volume =       "10(2)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@TechReport{EVL-1999-54,
  year =         "1999",
  title =        "Visual Representations Embodying Spacetime Structure",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-54",
  author =       "Luc Florack",
  abstract =     "If one defines a visual front-end to be that part of a
                 visual system in which the structure of the optical
                 world is represented in some least committed form, then
                 it is plausible to assert that it reflects the
                 structural properties, notably symmetries, of the
                 optical environment. We propose a methodology for
                 constructing receptive field assemblies consistent with
                 these symmetries, which contributes to our
                 understanding of the visual front-end in a predictive
                 and retrodictive way.",
  keywords =     "Front-end vision, receptive fields, spacetime
                 symmetries",
  number =       "UU-CS-1999-07",
  institution =  "Utrecht University, Department of Computer Science",
}

@InProceedings{EVL-1999-540,
  pages =        "109--119",
  year =         "1999",
  title =        "3Desque: Interface Elements for a 3{D} Graphical User
                 Interface",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-540",
  author =       "Gavin Miller and Sally Grisedale and Kenneth T.
                 Anderson",
  abstract =     "This paper explores some design possibilities and
                 constraints for 3D enhancements of graphical user
                 interfaces (GUIs). This is done with the aim of
                 conveying more information in less screen space, while
                 avoiding visual clutter. Design elements include the
                 use of slapped-back windows obtained by projecting
                 windows and icons into three dimensions; trays, an
                 alternative to folders or piles; the use of transparent
                 beams for indicating hierarchy; and the use of periphs
                 for a form of fisheye projection on window borders. In
                 addition we employ shading and shadows to enhance the
                 interface, and automatic placement algorithms to
                 prevent visually confusing occlusion. By considering
                 these approaches together, rather than in isolation,
                 the interactions between the different modifications
                 are made explicit",
  organization = "10(2)",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  keywords =     "Graphical user interface, 3D, 2D, affordance,
                 projections, orthographic, perspective, piles,
                 progressive disclosure, trays, periphs, beams,
                 slapped-back windows, fisheye views",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1999-541,
  pages =        "123--131",
  year =         "1999",
  title =        "A Selective Rendering Method for Data Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-541",
  author =       "Wencheng Wang and Enhua Wu and Nelson Max",
  abstract =     "elective visualization is a solution for visualizing
                 data of large size and dimensionality. In this paper a
                 new method is proposed for effectively rendering
                 certain chosen parts among the full set of data in
                 terms of a colour buffer, referred to as the virtual
                 plane, for storing intermediate results. By this
                 method, scientists may concentrate their attention on
                 the contents of data in which they are interested.
                 Besides, the method could be easily integrated with all
                 the current direct volume rendering techniques,
                 especially progressive refinement methods and selective
                 methods",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  keywords =     "Refinement; volume rendering, scientific
                 visualization, interaction",
  volume =       "10(3)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1999-542,
  pages =        "133--142",
  year =         "1999",
  title =        "Object-order Template-Based Approach for Stereoscopic
                 Volume Rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-542",
  author =       "Yun-Mo Koo and Cheol-Hi Lee and Yeong-Gil Shin",
  abstract =     "Stereoscopic volume rendering provides powerful depth
                 information, but it takes a long time to render two-eye
                 images. Previous algorithms based on reprojection
                 methods project the result of one view of a stereo pair
                 into the other instead of rendering a new one
                 completely. Because of inaccurate mapping between the
                 two images, the quality of the reprojected image is not
                 satisfactory. This paper presents a new algorithm to
                 preserve the accuracy of both images with very little
                 increase in computation time. The efficiency of the new
                 algorithm comes from the use of ray templates and
                 object-order processing. This algorithm makes two
                 different templates for each eye and renders two images
                 simultaneously, tracing the volume only once in object
                 order. We also extend the algorithm to support
                 image-space supersampling by using more ray templates.
                 Experimental results show that the image quality of the
                 new algorithm is not only comparable with that of ray
                 casting but also the rendering speed is near that of
                 the interactive shear-warp algorithm employing
                 object-order processing and spatial data coherency.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  volume =       "10(3)",
  keywords =     "Stereoscopic volume rendering, template based,
                 supersampling, run-length encoding",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1999-543,
  pages =        "143--158",
  year =         "1999",
  title =        "3{D} Surface Cellular Automata and their
                 Applications",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-543",
  author =       "St{\'{e}}phane Gobron and Norishige Chiba",
  abstract =     "This paper describes the generation and rendering of
                 three-dimensional (3D) surface cellular automata (CA).
                 Our model's main advantage is that it gives direct
                 texturing simulation based on the actual shape of any
                 triangulated input object. We first introduce general
                 CA concepts and summarize works in the literature. We
                 then describe our 3D surface CA method, emphasizing how
                 it avoids potential problems in data structure and
                 rendering steps. We then detail, two examples of
                 specific 3D surface CA with their respective cell
                 structures and corresponding computer graphics
                 images.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  keywords =     "Computer graphics, cellular automata, surface
                 propagation, Voronoi diagram, patina, corrosion",
  volume =       "10(3)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1999-544,
  pages =        "159--178",
  year =         "1999",
  title =        "Metrics and Generation Specifications for Comparing
                 Volume-rendered Images",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-544",
  author =       "Peter L. Williams and Samuel P. Uselton",
  abstract =     "The goal of this paper is to lay a foundation for
                 objectively comparing volume-rendered images, leading
                 to objective evaluation of their accuracy and quality.
                 The key elements of the foundation are: (1) a rigorous
                 specification of all the input and parameters that need
                 to be specified to define the conditions under which a
                 volume-rendered image is generated; and (2) the basis
                 for a methodology for difference classification,
                 including a suite of functions or metrics to quantify
                 and classify the difference between two volume-rendered
                 images that will support an analysis of the relative
                 importance of particular differences. The results of
                 this method can be used to study the changes caused by
                 modifying particular parameter values, to compare and
                 quantify changes between images of similar data sets
                 rendered in the same way, and to detect errors in the
                 design, implementation or modification of a
                 volume-rendering system. If a benchmark image is
                 available, for example one created by a high-accuracy
                 volume-rendering system, the method can be used to
                 evaluate the accuracy of a given image. The key
                 contribution of this paper is the separation of the
                 difference into noise, bias and structured difference
                 components.",
  editor =       "Nadia Magnenat Thalmann and Daniel Thalmann",
  keywords =     "Volume rendering, image comparison metrics",
  volume =       "10(3)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1999-545,
  pages =        "181--192",
  year =         "1999",
  title =        "{RECODE}: An Image-based Collision Detection
                 Algorithm",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-545",
  author =       "George Baciu and Wingo Sai-Keung Wong and Hanqiu Sun",
  abstract =     "Object interactions are ubiquitous in interactive
                 computer graphics, 3D object motion simulations,
                 virtual reality and robotics applications. Most
                 collision detection algorithms are based on geometrical
                 object-space interference tests. Some algorithms have
                 employed an image-space approach to the collision
                 detection problem. In this paper we demonstrate an
                 image-space collision detection process that allows
                 substantial computational savings during the
                 image-space interference test. This approach makes
                 efficient use of the graphics rendering hardware for
                 real-time complex object interactions.",
  editor =       "Nadia Magnenat Thalmann and Yoshihisa Shinagawa and
                 Hung Chuan Teh",
  keywords =     "Image-based collision detection, object interference,
                 motion simulation",
  volume =       "10(4)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1999-546,
  pages =        "193--213",
  year =         "1999",
  title =        "Dust and Water Splashing Models for Hopping Figures",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-546",
  author =       "Golam Ashraf and Kok Cheong Wong",
  abstract =     "To enhance the realism of computer animation
                 sequences, the interaction between autonomous creatures
                 and their environment needs to be modelled. Two such
                 commonly occurring phenomena, dust and water splashing
                 on impact, are presented in this paper. These
                 pseudodynamic models, based on particle system
                 concepts, are designed for hoppers with approximately
                 elliptical bases. The proposed models can be extended
                 for use with multi-legged creatures. The dust model is
                 parameterized by creature dimensions, soil particle
                 dimensions and wind direction. The splash model is
                 parameterized by creature dimensions and water film
                 thickness. Explicit control of behavioural parameters
                 of both models is also provided to allow
                 application-specific usage.",
  editor =       "Nadia Magnenat Thalmann and Yoshihisa Shinagawa and
                 Hung Chuan Teh",
  keywords =     "Dynamics-based animation, particle systems, natural
                 phenomena, stochastic modelling, irregular field
                 modelling, fractal textures",
  volume =       "10(4)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1999-547,
  pages =        "215--224",
  year =         "1999",
  title =        "An Efficient Control over Human Running Animation with
                 Extension of Planar Hopper Model",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-547",
  author =       "Young-Min Kang and Hwan-Gue Cho and Ee-Taek Lee",
  abstract =     "The most important goal of character animation is to
                 efficiently control the motions of a character. Until
                 now, many techniques have been proposed for human gait
                 animation. Some techniques have been created to control
                 the emotions in gaits such as tired walking and brisk
                 walking by using parameter interpolation or motion data
                 mapping. Since it is very difficult to automate the
                 control over the emotion of a motion, the emotions of a
                 character model have been generated by creative
                 animators. This paper proposes a human running model
                 based on a one-legged planar hopper with a
                 self-balancing mechanism. The proposed technique
                 exploits genetic programming to optimize movement and
                 can be easily adapted to various character models. We
                 extend the energy minimization technique to generate
                 various motions in accordance with emotional
                 specifications.",
  editor =       "Nadia Magnenat Thalmann and Yoshihisa Shinagawa and
                 Hung Chuan Teh",
  keywords =     "Animation, human gait, genetic programming, energy
                 control",
  volume =       "10(4)",
  booktitle =    "he Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1999-548,
  pages =        "225--231",
  year =         "1999",
  title =        "A Non-Deterministic Reconstruction Approach for
                 Isotropic Reflectances and Transmittances",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-548",
  author =       "Gladimir V. G. Baranoski and Jon G. Rokne",
  abstract =     "Physically and biologically based reflectance and
                 transmittance models add realism to image synthesis
                 applications at the expense of a significant increase
                 in rendering time. Current research efforts in this
                 area focus on developing practical solutions to quickly
                 access a BDF (which represents a combination of BRDF
                 and BTDF) while preserving its original
                 characteristics. In this paper an approach to
                 reconstruct relatively complex isotropic BDFs is
                 presented. The spectral curves obtained using the
                 proposed approach are compared with measured spectral
                 curves, and some issues regarding its performance and
                 storage requirements are examined.",
  editor =       "Nadia Magnenat Thalmann and Yoshihisa Shinagawa and
                 Hung Chuan Teh",
  keywords =     "BDF, BRDF, BTDF, non-deterministic reconstruction
                 method",
  volume =       "10(4)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1999-549,
  pages =        "233--241",
  year =         "1999",
  title =        "Non-linear View Interpolation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-549",
  author =       "Hujun Bao and Li Chen and Jianguo Ying and Qunsheng
                 Peng",
  abstract =     "A new non-linear view interpolation algorithm is
                 presented in this paper. Unlike the linear
                 interpolation scheme, our method can exactly simulate
                 the perspective viewing transformation during
                 walkthrough. To accelerate the view interpolation, the
                 algorithm employs a binary subdivision scheme to
                 optimize the decomposition of the source image so that
                 the number of resultant blocks is greatly reduced.
                 Holes in the intermediate image are filled by two
                 steps, namely enlarging the transferred blocks at the
                 sides adjacent to holes and retrieving the local image
                 within holes from the destination images by
                 multidirectional interpolation. Experimental results
                 demonstrate that our algorithm is much more accurate
                 and efficient than the traditional one.",
  editor =       "Nadia Magnenat Thalmann and Yoshihisa Shinagawa and
                 Hung Chuan Teh",
  keywords =     "View interpolation, environment map, binary
                 subdivision, virtual reality",
  volume =       "10(4)",
  booktitle =    "The Journal of Visualization and Computer Animation",
  publisher =    "John Wiley & Sons, Ltd.",
}

@InProceedings{EVL-1999-55,
  pages =        "23--32",
  year =         "1999",
  title =        "Adaptive tessellation of connected primitives for
                 interactive walkthroughs in complex industrial virtual
                 environments",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-55",
  author =       "M. Krus and P. Bourdot and A. Osorio and F. Guisnel
                 and G. Thibault",
  abstract =     "Geometrical primitives used in virtual environments
                 are converted to an important amount triangles at
                 rendering time. The meshes of the resulting
                 simplifications usually introduce discontinuities
                 between neighboring object. We extend a simple adaptive
                 tessellation method which adapts the amount of
                 triangles to the viewing conditions with connection
                 information to ensure that the meshes of connected
                 primitives remain continuous. An ergonomical study has
                 validated this approach for applications using virtual
                 environments.",
  organization = "Springer-Verlag Wien",
  month =        may,
  editor =       "Michael Gervaut and Dieter Schmalstieg and Axel
                 Hildebrand",
  keywords =     "adaptive tessellation, virtual environments, user
                 evaluation",
  booktitle =    "Virtual Environments '99. Proceedings of the
                 Eurographics Workshop in Vienna, Austria",
}

@InProceedings{EVL-1999-550,
  pages =        "93--101",
  year =         "1999",
  title =        "On Modeling and Rendering Ocean Scenes Diffraction,
                 Surface Tracking and Illumination",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-550",
  author =       "Jean-Christophe Gonzato and Bertrand Le Sa{\"{e}}c",
  abstract =     "This paper is devoted to ocean images. First, we
                 propose an improvement of our model [Gonza97]
                 accounting for diffraction, transmission and
                 multi-waves trains. Then, we describe a specific
                 algorithm for the rendering of coastal scenes using a
                 particular illumination model for the scene and
                 displacement mapping texture techniques to deal with
                 multi-waves trains.",
  editor =       "V. Skala",
  keywords =     "Ocean, water waves, dynamic wave tracing, diffraction,
                 transmission, displacement mapping, illumination, ray
                 tracing.",
  booktitle =    "WSCG'99 Conference Proceedings",
}

@InProceedings{EVL-1999-551,
  pages =        "98--106",
  year =         "1999",
  title =        "Multi-layered image-based rendering",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-551",
  author =       "Sing Bing Kang and Huong Quynh Dinh",
  abstract =     "In this paper, we describe a multi-layered image-based
                 rendering system that can use different types of input
                 to produce and render new environments. Each separate
                 input that can be manipulated independently is called a
                 layer). In our implementation, the types of layers that
                 can be manipulated are the image-based) and (3-D-based)
                 layers. The computation required for rendering the
                 newly-crafted environment is reduced by using cached
                 composite snapshots of that environment at reference
                 poses. These cached snapshots are used to directly
                 generate novel views, and the original layers are used
                 only when necessary. Another key concept is the
                 identification of types of holes generated as a result
                 of pixel transfer from the composite snapshots to the
                 generated view. For optimal rendering quality, the
                 algorithm used in filling these holes is specific to
                 the hole type [either intralayer] or [interlayer]). The
                 ideas embodied in our multi-layered IBR system are
                 useful in augmenting the capabilities of applications
                 that require fast and geometrically consistent
                 rendering of 3-D scenes such as video editing.",
  editor =       "I. S. MacKenzie and J. Stewart",
  booktitle =    "Proceedings of Graphics Interface 99",
}

@InProceedings{EVL-1999-56,
  pages =        "33--42",
  year =         "1999",
  title =        "An Optical Tracking System for
                 {VR}/{AR}-Applications",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-56",
  author =       "Klaus Dorfm{"u}ller",
  abstract =     "In this paper, an optical tracking system is
                 introduced for the use within Virtual and Augmented
                 Reality applications. The system uses retroreflective
                 markers which are attached to a special designed
                 interaction device. The construction of the device
                 allows us to gather six degrees of freedom. In order to
                 achieve high tracking precision we introduce a
                 calibration algorithm which results in sub-pixel
                 accuray and is therefore well applicable within
                 Augmented Reality scenarios. Further the algorithm for
                 calculating the pose of a rigid body is described.
                 Finally, the optical tracking system is evaluated in
                 regard to its accuray.",
  organization = "Springer-Verlag Wien",
  editor =       "Michael Gervaut and Dieter Schmalstieg and Axel
                 Hildebrand",
  booktitle =    "Virtual Environments '99. Proceedings of the
                 Eurographics Workshop in Vienna, Austria",
}

@InProceedings{EVL-1999-57,
  pages =        "43--52",
  year =         "1999",
  title =        "The integration of optical and magnetic tracking for
                 multi-user augmented reality",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-57",
  author =       "Thomas Auer and Stefan Brantner and Axel Pinz",
  abstract =     "Multiuser augmented reality requires excellent
                 registration for all users, which cannot be achieved by
                 magnetic tracking alone. This paper presents a new
                 approach combining magnetic and optical tracking. The
                 magnetic tracker is used to coarsely predict the
                 positions of landmarks in the camera image. This
                 restricts the search area to a size which can be
                 managed close to real-time. This new hybrid tracking
                 system outperforms a calibrated magnetic tracker in
                 terms of position, orientation, and jitter.",
  organization = "Springer-Verlag Wien",
  editor =       "Michael Gervaut and Dieter Schmalstieg and Axel
                 Hildebrand",
  booktitle =    "Virtual Environments '99. Proceedings of the
                 Eurographics Workshop in Vienna, Austria",
}

@InProceedings{EVL-1999-58,
  pages =        "53--62",
  year =         "1999",
  title =        "An Optically Based Direct Manipulation Interface for
                 Human-Computer Interaction in an Augmented World",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-58",
  author =       "G. Klinker and D. Stricker and D. Reiners",
  abstract =     "Augmented Reality (AR) constitutes a very powerful
                 threedimensional user interface for many {"}hands-on{"}
                 application scenarios in which users cannot sit at a
                 conventional desktop computer. To fully exploit the AR
                 paradigm, the computer must not only augment the real
                 world, it also has to accept feedback from it. Such
                 feedback is typically collected via gesture languages,
                 3D pointers, or speech input - all tools which expect
                 users to communicate with the computer about their work
                 at a meta-level rather than just letting them pursue
                 their task. When the computer is capable of deducing
                 progress directly from changes in the real world, the
                 need for special abstract communication interfaces can
                 be reduced or even eliminated. In this paper, we
                 present an optical approach for analyzing and tracking
                 users and the objects they work with. In contrast to
                 emerging workbench and metaDESK approaches, our system
                 can be set up in any room after quickly placing a few
                 known optical targets in the scene. We present three
                 demonstration scenarios to illustrate the overall
                 concept and potential of our approach and then discuss
                 the research issues evolved.",
  organization = "Springer-Verlag Wien",
  editor =       "Michael Gervaut and Dieter Schmalstieg and Axel
                 Hildebrand",
  booktitle =    "Virtual Environments '99. Proceedings of the
                 Eurographics Workshop in Vienna, Austria",
}

@InProceedings{EVL-1999-59,
  pages =        "63--72",
  year =         "1999",
  title =        "Improving The Illumination Quality Of {VRML} 97
                 Walkthrough Via Intensive Texture Usage",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-59",
  author =       "Cyril Kardassevitch and Jean Pierre Jessel and Mathias
                 Paulin and Ren{\'{e}} Caubet",
  abstract =     "In this paper, we introduce a pipeline, dedicated to
                 global illumination and walkthrough of a VRML 97 scene.
                 This pipeline use intensively and exclusively textures
                 to represent light, and is entirely guided by them. We
                 will show how reversing the classic rendering pipeline,
                 allows to privilege high frequency information (direct
                 illumination), and to accelerate the complete lighting
                 process (global illumination). Finally, we will present
                 the filtering and reconstruction methods used to
                 improve the rendering visual quality.",
  organization = "Springer-Verlag Wien",
  editor =       "Michael Gervaut and Dieter Schmalstieg and Axel
                 Hildebrand",
  keywords =     "rendering, walkthrough, VRML 97, texture, light map,
                 global illumination, radiosity, filtering,
                 reconstruction",
  booktitle =    "Virtual Environments '99. Proceedings of the
                 Eurographics Workshop in Vienna, Austria",
}

@Article{EVL-1999-6,
  year =         "1999",
  title =        "Automated {B}-Spline Curve Representation
                 Incorporating {MDL} and Error-Minimizing Control Point
                 Insertion Strategies",
  author =       "Tat-Jen Cham and Roberto Cipolla",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-6",
  language =     "en",
  abstract =     "The main issues of developing an automatic and
                 reliable scheme for spline-fitting are discussed and
                 addressed in this paper, which are not fully covered in
                 previous papers or algorithms. The proposed method
                 incorporates B-spline active contours, the minimum
                 description length (MDL) principle, and a novel control
                 point insertion strategy based on maximizing the
                 Potential for Energy-Reduction Maximization (PERM). A
                 comparison of test results shows that it outperforms
                 one of the better existing methods.",
  month =        jan,
  volume =       "21",
  keywords =     "B-spline fitting, curve fitting, minimum description
                 length, collapse mechanism, active contour",
  number =       "1",
  journal =      "IEEE Transactions on Pattern Analysis and Machine
                 Intelligence",
}

@InProceedings{EVL-1999-60,
  pages =        "73--84",
  year =         "1999",
  title =        "Fast Walkthroughs with Image Caches and Ray Casting",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-60",
  author =       "Michael Wimmer and Markus Giegl and Dieter
                 Schmalstieg",
  abstract =     "We present an output-sensitive rendering algorithm for
                 accelerating walkthroughs of large, densely occluded
                 virtual environments using a multi-stage Image Based
                 Rendering Pipeline. In the first stage, objects within
                 a certain distance are rendered using the traditional
                 graphics pipeline, whereas the remaining scene is
                 rendered by a pixel-based approach using an Image
                 Cache, horizon estimation to avoid calculating sky
                 pixels, and finally, ray casting. The time complexity
                 of this approach does not depend on the total number of
                 primitives in the scene. We have measured speedups of
                 up to one oder of magnitude.",
  organization = "Springer-Verlag Wien",
  editor =       "Michael Gervaut and Dieter Schmalstieg and Axel
                 Hildebrand",
  booktitle =    "Virtual Environments '99. Proceedings of the
                 Eurographics Workshop in Vienna, Austria",
}

@InProceedings{EVL-1999-61,
  pages =        "85--94",
  year =         "1999",
  title =        "Using Virtual Environments to Enhance Visualization",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-61",
  author =       "D. R. S. Boyd and J. R. Gallop and K. E. V. Palmen amd
                 R. T. Platon and C. D. Seelig",
  abstract =     "Within the EU ESPRIT demonstrator project VIVRE, a
                 commercial virtual environment system has been used to
                 create a user-centred interaction environment for two
                 commercial general-purpose data visualization systems.
                 The project has exploited mechanisms provided by all
                 three systems to incorporate new user-developed
                 functionality. In addition to updating and navigating
                 the visualization scene, this includes user control of
                 the visualization application from within the virtual
                 environment. The project is assessing the degree to
                 which this extended interactive capability results in
                 greater benefits for commercial users.",
  organization = "Springer-Verlag Wien",
  editor =       "Michael Gervaut and Dieter Schmalstieg and Axel
                 Hildebrand",
  booktitle =    "Virtual Environments '99. Proceedings of the
                 Eurographics Workshop in Vienna, Austria",
}

@InProceedings{EVL-1999-62,
  pages =        "95--104",
  year =         "1999",
  title =        "Semantic Behaviours in Collaborative Virtual
                 Environments",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-62",
  author =       "Emmanuel Fr{\'{e}}con and Gareth Smith",
  abstract =     "Scripting facilities within a collaborative virtual
                 environment (CVE) allows animation and behaviour to be
                 added to otherwise static scenes. This paper describes
                 the integration of the Tcl scripting language into an
                 existing CVE, and describes the advantages gained by
                 such a marriage. Further, we describe how high-level
                 semantic behaviours can be readily introduced into
                 cooperative applications. They benefit from the
                 scripting language to provide an abstraction over
                 application development and be exploited to drastically
                 reduce network traffic.",
  organization = "Springer-Verlag Wien",
  editor =       "Michael Gervaut and Dieter Schmalstieg and Axel
                 Hildebrand",
  booktitle =    "Virtual Environments '99. Proceedings of the
                 Eurographics Workshop in Vienna, Austria",
}

@InProceedings{EVL-1999-63,
  pages =        "105--114",
  year =         "1999",
  title =        "A Distributed Device Diagnostics System Utilizing
                 Augmented Reality and 3{D} Audio",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-63",
  author =       "Reinhold Behringer and Steven Chen and Venkataraman
                 Sundareswaran and Kenneth Wang and Marius Vassiliou",
  abstract =     "Augmented Reality brings technology developed for
                 virtual environments into the real world. This approach
                 can be used to provide instructions for routine
                 maintenance and error diagnostics of technical devices.
                 The Rockwell Science Center is developing a system that
                 utilizes augmented Reality techniques to provide the
                 user with a form of {"}X-Ray Vision{"} into real
                 objects. The system can overlay 3D rendered objects,
                 animations, and text annotations onto the video image
                 of a known object, registered to the object during
                 camera motion. This allows the user to localize
                 problems of the device with the actual device in his
                 view. The user can query the status of device
                 components using a speech recognition system. The
                 response is given as an animation of the relevant
                 device module and/or as auditory cues using spatialized
                 3D audio. The diagnostic system also allows the user to
                 leave spoken annotations attached to device modules for
                 other users to retrieve. The position of the
                 user/camera relative to the device is tracked by a
                 computer-vision-based tracking system especially
                 developed for this purpose. The system is implemented
                 on a distributed network of PCs, utilizing standard
                 commercial off-the-shelf components (COTS).",
  organization = "Springer-Verlag Wien",
  editor =       "Michael Gervaut and Dieter Schmalstieg and Axel
                 Hildebrand",
  booktitle =    "Virtual Environments '99. Proceedings of the
                 Eurographics Workshop in Vienna, Austria",
}

@InProceedings{EVL-1999-64,
  pages =        "115--124",
  year =         "1999",
  title =        "Texture-based Volume Visualization for Multiple Users
                 on the World Wide Web",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-64",
  author =       "Klaus Engel and Thomas Ertl",
  abstract =     "We present a texture-based volume visualization tool,
                 which permits remote access to radiological data and
                 supports multi-user environments. The application uses
                 JAVA and the Virtual Reality Modeling Language (VRML),
                 thus it is platform-independent and able to use fast 3D
                 graphics acceleration hardware of client machines. The
                 application allows the shared viewing and manipulation
                 of three-dimensional medical volume datasets in a
                 heterogeneous network. Volume datasets are transferred
                 from a server to different client machines and locally
                 visualized using a JAVA-enabled web-browser. In order
                 to reduce network traffic, a data reduction and
                 compression scheme is proposed. The application allows
                 view dependent and orthogonal clipping planes, which
                 can be moved interactively. On the client side, the
                 users are able to join a visualization session and to
                 get the same view onto the volume dataset by
                 synchronizing the viewpoint and any other visualization
                 parameter. Interesting parts of the dataset are marked
                 for other users by placing a tag into the
                 visualization. In order to support collaborative work
                 users communicate with a chat applet, which we provide,
                 or by using any existing video conferencing tool.",
  organization = "Springer-Verlag Wien",
  editor =       "Michael Gervaut and Dieter Schmalstieg and Axel
                 Hildebrand",
  booktitle =    "Virtual Environments '99. Proceedings of the
                 Eurographics Workshop in Vienna, Austria",
}

@InProceedings{EVL-1999-65,
  pages =        "125--135",
  year =         "1999",
  title =        "{PVR} - An Architecture for Portable {VR}
                 Applications",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-65",
  author =       "Robert van Liere and Jurriaan D. Mulder",
  abstract =     "Virtual Reality shows great promise as a research tool
                 in computational science and engineering. However,
                 since VR involves new interface styles, a great deal of
                 implementation effort is required to develop VR
                 applications. In this paper we present PVR; an
                 event-based architecture for portable VR applications.
                 The goal of PVR is to provide a programming environment
                 which facilitates the development of VR applications.
                 PVR differentiates itself from other VR toolkits in two
                 ways: First, it decouples the coordination and
                 management of multiple data streams from actual data
                 processing. This simplifies the programmer's task of
                 managing and synchronizing the data streams. Second,
                 PVR strives for portability by shielding low-level
                 device specific details. Application programmers can
                 take full advantage of the underlying hardware while
                 maintaining a single code base spanning a variety of
                 input and output device configurations.",
  organization = "Springer-Verlag Wien",
  editor =       "Michael Gervaut and Dieter Schmalstieg and Axel
                 Hildebrand",
  booktitle =    "Virtual Environments '99. Proceedings of the
                 Eurographics Workshop in Vienna, Austria",
}

@InProceedings{EVL-1999-66,
  pages =        "137--146",
  year =         "1999",
  title =        "Rapid Development of {VRML} Content via Geometric
                 Programming",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-66",
  author =       "A. Paoluzzi and S. Francesi and S. Portuesi and M.
                 Vicentino",
  abstract =     "This paper aims to show that a functional design
                 language can be used as a general-purpose VRMl
                 generator. The PLaSM language, which allows for
                 algebraic computations with geometric shapes and maps,
                 has been recently extended with non geometric
                 attributes like colors, lights and textures. PLaSM is
                 used in the paper both to develop some general-purpose
                 tools, including B{\'{e}}zier manifolds of any
                 dimension and degree, the n-th derivative of any
                 parametric curve and {"}B{\'{e}}zier stripes{"} of
                 small width, as well as to quickly implement a quite
                 complex mountain landscape. Customized PLaSM
                 applications may generate fully parameterized virtual
                 worlds starting from small data files or streams.",
  organization = "Springer-Verlag Wien",
  editor =       "Michael Gervaut and Dieter Schmalstieg and Axel
                 Hildebrand",
  booktitle =    "Virtual Environments '99. Proceedings of the
                 Eurographics Workshop in Vienna, Austria",
}

@InProceedings{EVL-1999-67,
  pages =        "147--156",
  year =         "1999",
  title =        "Augmented Reality, the other way around",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-67",
  author =       "Didier Verna and Alain Grumbach",
  abstract =     "This paper aims at showing that the motion of
                 Augmented Reality has been developed in a biased way:
                 mostly in destination to the operator, and at the level
                 of his perceptions. This demonstration is achieved
                 through a model describing a situation of
                 tele-operation, on which we represent major cases of
                 Augmented Reality encountered in recent applications.
                 By taking advantage of the symmetry of the model, we
                 are able to show how Augmented Reality can be seen
                 {"}the other way around{"}, that is, in destination to
                 the environment, and at the level of the operator's
                 actions.",
  organization = "Springer-Verlag Wien",
  editor =       "Michael Gervaut and Dieter Schmalstieg and Axel
                 Hildebrand",
  booktitle =    "Virtual Environments '99. Proceedings of the
                 Eurographics Workshop in Vienna, Austria",
}

@InProceedings{EVL-1999-68,
  pages =        "157--168",
  year =         "1999",
  title =        "Interaction Techniques on the Virtual Workbench",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-68",
  author =       "Rogier van de Pol and William Ribarsky and Larry
                 Hodges and Frits Post",
  abstract =     "This paper evaluates interaction methods within the
                 general framework of navigation, selection, and
                 manipulation. It considers large display environments
                 and, in particular, the virtual workbench, comparing
                 this system to HMD and CAVE systems. The paper
                 addresses three issues: (a) identifying the
                 characteristics that set the workbench apart from other
                 virtual environments; (b) determining types and
                 examples of interaction techniques; (c) evaluating how
                 these techniques perform on the workbench to determine
                 which perform best. The evaluations are based on an
                 extensive set of user observations. Also discussed are
                 some problems that stereoscopic display coupled with
                 interaction bring out.",
  organization = "Springer-Verlag Wien",
  editor =       "Michael Gervaut and Dieter Schmalstieg and Axel
                 Hildebrand",
  booktitle =    "Virtual Environments '99. Proceedings of the
                 Eurographics Workshop in Vienna, Austria",
}

@InProceedings{EVL-1999-69,
  pages =        "169--178",
  year =         "1999",
  title =        "A General Framework for Cooperative Manipulation in
                 Virtual Environments",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-69",
  author =       "David Margery and Bruno Arnaldi and No{\"{e}}l
                 Plouzeau",
  abstract =     "Whereas cooperation and collaboration have become two
                 popular words in virtual reality, the problem of
                 cooperative manipulation has been mainly left aside due
                 to the great number of other challenges facing anyone
                 trying to setup multi-user worlds. We define
                 cooperative manipulation as a situation where two or
                 more users interact on the same object in a concurrent
                 but cooperative way. The focus of this paper is to
                 describe an experiment whose goal was to experiment
                 problems specific of cooperative manipulation setups.
                 Those problems include synchronizing user's input over
                 the network, mapping user's input into a meaningful 3-D
                 movement thanks to what we call a model of activity and
                 giving him relevant visual information. In this paper,
                 we present a general framework able to take into
                 account these problems. It is compatible with
                 physically simulated objects and has been implemented
                 using Java, VRML and a distributed approach.",
  organization = "Springer-Verlag Wien",
  editor =       "Michael Gervaut and Dieter Schmalstieg and Axel
                 Hildebrand",
  booktitle =    "Virtual Environments '99. Proceedings of the
                 Eurographics Workshop in Vienna, Austria",
}

@Article{EVL-1999-7,
  year =         "1999",
  title =        "Adaptive Nonlocal Filtering: {A} Fast Alternative to
                 Anisotropic Diffusion for Image Enhancement",
  author =       "Bruce Fischl and Eric L. Schwartz",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-7",
  language =     "en",
  abstract =     "Nonlinear anisotropic diffusion algorithms provide
                 significant improvement in image enhancement as
                 compared to linear filters. However, the excessive
                 computational cost of solving nonlinear PDEs precludes
                 their use in real-time vision applications. In the
                 present paper, we show that two orders of magnitude
                 speed improvement is provided by a new image filtering
                 paradigm in which an adaptively determined vector field
                 specifies nonlocal application points for an image
                 filter.",
  month =        jan,
  volume =       "21",
  keywords =     "Segmentation, diffusion, scale-space, anisotropic
                 diffusion, nonlinear diffusion, filtering, permutation
                 filter, nonlocal filter",
  number =       "1",
  journal =      "IEEE Transactions on Pattern Analysis and Machine
                 Intelligence",
}

@InProceedings{EVL-1999-70,
  pages =        "179--189",
  year =         "1999",
  title =        "Occlusion in Collaborative Augmented Environments",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-70",
  author =       "Anton Fuhrman and Gerd Hesina and Fron{\c{c}}ois Faure
                 and Michael Gervautz",
  abstract =     "Augmented environments superimpose computer graphics
                 on the real world. Such augmented environments are well
                 suited for collaboration of multiple users. To improve
                 the quality and consistency of the augmentation, the
                 occlusion of real objects by computer-generated objects
                 and vice versa has to be implemented. We present a
                 method how this can be done for a tracked user's body
                 and other real objects and how irritating artifacts due
                 to misalignments can be reduced. Our method is based on
                 simulating the occlusion of virtual objects by a
                 representation of the user modeled as kinematic chains
                 of articulated solids. Registration and modeling errors
                 of this model are being reduced by smoothing the border
                 between virtual world and occluding real object. An
                 implementation in our augmented environment and the
                 resulting improvements are presented.",
  organization = "Springer-Verlag Wien",
  editor =       "Michael Gervaut and Dieter Schmalstieg and Axel
                 Hildebrand",
  booktitle =    "Virtual Environments '99. Proceedings of the
                 Eurographics Workshop in Vienna, Austria",
}

@InProceedings{EVL-1999-71,
  year =         "1999",
  title =        "Sampling effects due to motion in diffusion-weighted
                 interleaved echo planar imaging",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-71",
  author =       "D. Atkinson and D. Porter and D. L. G. Hill and F.
                 Calamante and A. Connelly",
  abstract =     "Diffusion weighted imaging shows promise in the early
                 assessment of neurological ischemia following stroke or
                 birth asphyxiation. Compared with single-shot echo
                 planar images (EPI), multi-shot, interleaved EPI allows
                 improved resolution and reduced magnetic susceptibility
                 artefacts. However, patient motion causes individual
                 interleaves to experience different relative phase
                 shifts and k-space offsets. These can be measured with
                 navigator echos. Offsets in the ky (phase encode)
                 direction lead to an irregular sampling which can be
                 correctly re-gridded using a matrix inversion method.
                 The z component of motion leads to a phase shift across
                 the slice and a drop in the navigator signal.
                 Interleaves for which the navigator signal falls below
                 a threshold are rejected, further contributing to the
                 irregular sampling. Using six normal volunteers we show
                 that even with phase encode oversampling, 87% of data
                 sets contain someregions where the sampling in the ky
                 direction does not meet the Nyquist criterion. Taking
                 four consecutive measurements allows us to satisfy
                 Nyquist in all but 1% of images. Furthermore, the extra
                 measurements improve the condition number of the matrix
                 that must be inverted and improve the image signal to
                 noise ratio. This careful consideration of sampling and
                 reconstruction effects allows us to demonstrate high
                 quality diffusion weighted images from ungated scans of
                 six volunteers.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-72,
  year =         "1999",
  title =        "A fast model independent method for automatic
                 correction of intensity nonuniformity in {MRI} data",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-72",
  author =       "Elizabeth A Vokurka and Neil A Thacker and Alan
                 Jackson",
  abstract =     "A novel nonparametric approach for correcting
                 intensity nonuniformity in magnetic resonance (MR)
                 images is described. This model is based on smooth
                 shifts in intensity within homogeneous materials and
                 does not require radio frequency (RF) coil modelling,
                 tissue class models, or optimisation. The advantage of
                 this computationally fast method is that it can be
                 applied early in quantitative analysis while being
                 independent of pulse sequence and insensitive to
                 pathological processes. This algorithm has been tested
                 on both real and simulated data. Application to tissue
                 segmentation and functional MR imaging has shown a
                 marked improvement in quantitative analysis.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-73,
  year =         "1999",
  title =        "How to locate artifacts in parametric f{MRI}
                 analysis",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-73",
  author =       "A. J. Lacey and N. A. Thacker and E. Burton and A.
                 Jackson",
  abstract =     "In this paper we assess rigid body co-registration in
                 terms of residual motion artifacts for the different
                 correlation approaches used in fMRI. We summarise, from
                 a statistical perspective, the three main approaches to
                 parametric fMRI analysis and then present a new way of
                 visualising motion effects in correlation analysis.
                 This technique can be used both to select regions of
                 relatively unambiguous activation and to verify the
                 results of analysis. We demonstrate the usefulness of
                 this visualisation technique on fMRI data sets
                 suffering from motion correlated artifacts and use it
                 in our assesment of rigid body co-registration. Our
                 TINA software system is available as open source from
                 http://www.niac.man.ac.uk and includes the algorithms
                 discussed in this paper.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-74,
  year =         "1999",
  title =        "Non-planar reslicing for freehand 3{D} ultrasound",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-74",
  author =       "Andrew Gee and Richard Prager and Laurence Berman",
  abstract =     "In this paper we assess rigid body co-registration in
                 terms of residual motion artifacts for the different
                 correlation approaches used in fMRI. We summarise, from
                 a statistical perspective, the three main approaches to
                 parametric fMRI analysis and then present a new way of
                 visualising motion effects in correlation analysis.
                 This technique can be used both to select regions of
                 relatively unambiguous activation and to verify the
                 results of analysis. We demonstrate the usefulness of
                 this visualisation technique on fMRI data sets
                 suffering from motion correlated artifacts and use it
                 in our assesment of rigid body co-registration. Our
                 TINA software system is available as open source from
                 http://www.niac.man.ac.uk and includes the algorithms
                 discussed in this paper.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-75,
  year =         "1999",
  title =        "Surface interpolation for sparse cross sections using
                 region correspondence",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-75",
  author =       "G. M. Treece and R. W. Prager and A. H. Gee and L.
                 Berman",
  abstract =     "The ability to estimate a surface from a set of
                 cross-sections allows calculation of the enclosed
                 volume and display of the surface in three-dimensions
                 (3-D). However, extracting the cross-sections
                 (segmenting) can be very difficult, and automatic
                 segmentation methodsare not sufficiently robust to deal
                 with all situations. Hence, it is an advantage if the
                 surface reconstruction algorithm can work effectively
                 on a small number of cross-sections. An algorithm is
                 presented which can interpolate a surface through
                 sparse, complex cross-sections. This is an extension of
                 maximal disc guided interpolation, which is itself
                 based on shape based interpolation. The performance of
                 this algorithm is demonstrated on various types of
                 medical data (X-ray Computed Tomography, Magnetic
                 Resonance Imaging and three-dimensional ultrasound).
                 Although the correspondence problem in general remains
                 unsolved, it is demonstrated that correct surfaces can
                 be estimated from limited real data, through the use of
                 region rather than object correspondence.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-76,
  year =         "1999",
  title =        "Wavelet Compression of Active Appearance Models",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-76",
  author =       "C. B. H. Wolstenholme and C. J. Taylor",
  abstract =     "Active Appearance Models (AAMs) provide a method of
                 modelling the appearance of anatomical structures in
                 medical images and locating them automatically.
                 Although the AAM approach is computationally efficient,
                 the models used to search unseen images for the
                 structures of interest are large - typically the size
                 of 100 images. This is perfectly practical for most 2-D
                 images, but is currently impractical for 3-D images. We
                 present a method for compressing the model information
                 using a wavelet transform. The transform is applied to
                 a set of training images in a shape-normalised frame,
                 and coefficients of low variance across the training
                 set are removed to reduce the information stored. An
                 AAM is built from the training set using the wavelet
                 coefficients rather than the raw intensities. We show
                 that reliable image interpretation results can be
                 obtained at a compression ratio of 20:1, which is
                 sufficient to make 3-D AAMs a practical proposition.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-77,
  year =         "1999",
  title =        "Morphometric analysis of facial shape change with
                 active shape model assisted 3-{D} landmark detection",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-77",
  author =       "Z Mao and A J Naftel",
  abstract =     "This paper presents a new approach for analysing shape
                 changes in facial morphometry using Active Shape Model
                 (ASM)-assisted stereo landmark detection. We firstly
                 describe how to extend the 2-D ASM to 3-D space by
                 stereo correspondence and disparity map interpolation
                 for automatic facial landmark extraction. Morphometric
                 techniques such as generalized Procrustes analysis,
                 principal component analysis and thin plate spline
                 decomposition are then applied to the analysis of shape
                 changes in 2-D facial midline profiles and 3-D facial
                 landmarks. The proposed method is validated both
                 statistically and visually by characterizing bite-block
                 induced shape changes in a heterogeneous sample of
                 young orthodontic patients.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-78,
  year =         "1999",
  title =        "Atlas Matching using Active Appearance Models",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-78",
  author =       "T. F. Cootes and C. Beeston and G. J. Edwards and C.
                 J. Taylor",
  abstract =     "Statistical models of shape and texture can be used as
                 deformable anatomical atlases. By training on sets of
                 labelled examples these can represent both the mean
                 structure and appearance of anatomy in medical images,
                 and the allowable modes of deformation. Given enough
                 training examples such a model should be able to
                 synthesise any image of normal anatomy. By finding the
                 parameters which minimise the difference between a
                 synthesised model image and a target image we can
                 locate all the structures represented in the model.
                 This potentially time consuming step can be solved
                 rapidly using the Active Appearance Model (AAM). In
                 this paper we describe the models and the AAM algorithm
                 and demonstrate the approach on structures in MR brain
                 cross-sections.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-79,
  year =         "1999",
  title =        "The effect of non-linear image registration on
                 cerebral lesions",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-79",
  author =       "E. A. Stamatakis and J. T. L. Wilson and D. J. Wyper",
  abstract =     "Abstract. In the context of SPECT brain image
                 quantification we considered the effect that non-linear
                 alignment or warping (as offered by SPM96, the
                 software suite) can have on images that contain
                 lesions. We constructed images with spherical
                 artificial lesions in order to obtain images with known
                 and controllable characteristics. We examined the use
                 of basisfunctions for non-linear alignment in order to
                 achieve a better match between two images. We found
                 that as long as the lesion volume is relatively small
                 (< 8% of the total brain volume) and the lesion mean
                 intensity is not less that 50% of the normal mean
                 intensity then it is safe to use a small number (2x2x2)
                 of basis functions. With larger and deeper (darker)
                 lesions significant deformations occur if warping is
                 used and therefore 12 parameter linear affine
                 transformation should be used for alignment in those
                 cases.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@Article{EVL-1999-8,
  year =         "1999",
  title =        "Game-Theoretic Integration for Image Segmentation",
  author =       "Amit Chakraborty and James S. Duncan",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-8",
  language =     "en",
  abstract =     "Robust segmentation of structures from an image is
                 essential for a variety of image analysis problems.
                 However, the conventional methods of region-based
                 segmentation and gradient-based boundary finding are
                 often frustrated by poor image quality. Here we propose
                 a method to integrate the two approaches using game
                 theory in an effort to form a unified approach that is
                 robust to noise and poor initialization. This combines
                 the perceptual notions of complete boundary information
                 using edge data and shape priors with gray-level
                 homogeneity using two computational modules. The
                 novelty of the method is that this is a bidirectional
                 framework, whereby both computational modules improve
                 their results through mutual information sharing. A
                 number of experiments were performed both on synthetic
                 datasets and datasets of real images to evaluate the
                 new approach and it is shown that the integrated method
                 typically performs better than conventional
                 gradient-based boundary finding.",
  month =        jan,
  volume =       "21",
  keywords =     "Image segmentation, integration, game theory, boundary
                 finding, region-based segmentation, MRF",
  number =       "1",
  journal =      "IEEE Transactions on Pattern Analysis and Machine
                 Intelligence",
}

@InProceedings{EVL-1999-80,
  year =         "1999",
  title =        "The Use of Regional Fast Fluid Registration of Serial
                 {MRI} to Quantify Local Change in Neurodegenerative
                 Disease",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-80",
  author =       "William R Crum and Peter A. Freeborough and Nick C
                 Fox",
  abstract =     "Methods for estimating volume change using registered
                 serial Magnetic Resonance Imaging (eg the Brain
                 Boundary Shift Integral) can be used to quantify
                 atrophy over the whole brain. However regional change
                 is more difficult to measure. One potential method is
                 to use the result of a {"}fluid{"} (non-linear)
                 registration and sum the Jacobian (º point volume
                 change) measures over the structure of interest to
                 estimate the regional volume change. We applied an
                 algorithm for global matching of images by applying it
                 to a cerebral region of interest - the left temporal
                 lobe. An intensity weighting function was applied to
                 voxels in the border region to ensure that the image
                 intensity goes smoothly to zero at the edges to satisfy
                 the fluid boundary condition. By matching only over a
                 sub-volume, problems associated with matching neck and
                 scalp structures were largely eliminated. Regional
                 volume change was calculated by summing the Jacobians
                 over voxels within the regions. Image matching,
                 reproducibility of volume measures and discriminatory
                 power were examined using fluid registration of left
                 temporal lobe in normal controls and Alzheimer's
                 disease patients. Excellent image matching was achieved
                 in the subjects with good reproducibility of atrophy
                 measurement. Considerable overlap between the subject
                 groups limited the discriminatory power of the single
                 measure, however combinations of regional measures may
                 prove useful in diagnosis and tracking progression in
                 this important patient group.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-81,
  year =         "1999",
  title =        "Using Texture Mapping to Register Video Images to
                 Tomographic Images by Optimising Mutual Information",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-81",
  author =       "M. J. Clarkson and D. Rueckert and A. P. King and P.
                 J. Edwards and D. L. G. Hill and D. J. Hawkes",
  abstract =     "In this paper we propose a novel tracking method to
                 update the pose of two video cameras with respect to a
                 surface model derived from a 3D tomographic image. This
                 has a number of applications in image guided
                 interventions and therapy. Registration of 2D video
                 images to a pre-operative 3D image provides a mapping
                 between image and physical space and enables a
                 perspective projection of the pre-operative data to be
                 overlaid onto the video image. Assuming an initial
                 registration can be achieved, we propose a method for
                 updating the registration, which is based on image
                 intensity alone, and accomplished using texture
                 mapping. We performed four experiments on simulated and
                 volunteer data and validated the algorithm against an
                 accurate gold standard in all cases. We measured the
                 mean 3D error of our tracking algorithm to be 1.05 mm
                 for the simulation and 1.61 mm for the volunteer data.
                 Visually this corresponds to a good registration.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-82,
  year =         "1999",
  title =        "An Analysis of Calibration and Registration Errors in
                 an Augmented Reality System for Microscope-Assisted
                 Guided Interventions",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-82",
  author =       "A. P. King and P. J. Edwardsand M. R. Pike and D. L.
                 G. Hill and D. J. Hawkes",
  abstract =     "Assessing the accuracy of augmented reality systems
                 can be very difficult. We present a description of the
                 MAGI (Microscope-Assisted Guided Interventions)
                 augmented reality system, together with a comprehensive
                 analysis of the errors present in the system. A
                 simulation has been implemented which enables the
                 different error sources to be varied individually and
                 their relative impact assessed. Results are presented
                 from this simulation. Estimates of typical error values
                 are used to predict an overall system overlay error of
                 0.91mm. Further errors are introduced by the modelling
                 of zoom and focus in the microscope optics. The
                 magnitude of these errors is also investigated. The
                 predicted errors are consistent with measurements taken
                 from overlays produced by the system on phantoms,
                 volunteers and in clinical trials. The error analysis
                 has enabled the dominant error sources to be
                 identified.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-83,
  year =         "1999",
  title =        "A skin imaging method based on a colour formation
                 model and its application to the diagnosis of pigmented
                 skin lesions",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-83",
  author =       "Symon Cotton and Ela Claridge and Per Hall",
  abstract =     "Pigmented lesions with different histology can appear
                 similar, making clinical diagnosis of malignant
                 melanoma difficult. This paper describes a new
                 computer-based approach which uses an optical model of
                 the skin to interpret the colours occurring in a lesion
                 in terms of lesion histology. Through the development
                 of an optical image formation model of human skin it is
                 shown that all normal skin colours lie on a
                 two-dimensional surface patch within a
                 three-dimensional colour space. The colour coordinates
                 corresponding to atypical skin structures deviate in
                 characteristic ways from the predicted normal surface
                 patch and thus can be identified. In particular, the
                 method generates parametric images showing the presence
                 of melanin in the dermis. This information can help in
                 lesion discrimination.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-84,
  year =         "1999",
  title =        "Automatic Semantic Analysis for Histological Images",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-84",
  author =       "Lilian H Y Tang and Rudolf Hanka and Ringo Lam and
                 Grant Fuller and Horace H S Ip and Kent K T Cheung",
  abstract =     "This paper presents an approach for automatic analysis
                 of the semantic meaning of histological images. The
                 approach described in this paper is implemented as a
                 part of a larger system, I-Browse, which combines
                 iconic and semantic content for intelligent image
                 browsing. Our approach is based on partitioning input
                 image into a number of sub-images. A set of texture
                 features based on Gabor filter and colour histograms
                 that capture the visual characteristics of each of the
                 sub-images is computed. These image features then form
                 the input to several pattern classifiers that give an
                 initial coarse label assignment to sub-images, based on
                 an hierarchical clustering of the image features. To
                 facilitate supervised training of the classifiers, a
                 knowledge elicitation tool was developed which allows a
                 histopathologist to assign histological terms to a
                 sample of sub-images obtained from the digitised images
                 of the tissue. The initial labels and their spatial
                 distribution are then analysed by a semantic analyser
                 with the help of a knowledge base that contains prior
                 knowledge of the relative context of histological
                 images of a given part of anatomy. The labels assigned
                 to the sub-images are successively refined through a
                 process of appropriate feedback.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-85,
  year =         "1999",
  title =        "3{D} topology and morphology of branching networks
                 using computed tomography ({CT}) - Application to the
                 airway tree",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-85",
  author =       "V. Sauret and KA Goatman and JS Fleming and AG
                 Bailey",
  abstract =     "Detailed information on biological branching networks
                 (optical nerves, airways or blood vessels) is often
                 required to improve the analysis of 3D medical imaging
                 data or fluid dynamics modelling. This paper presents a
                 semi-automated algorithm for obtaining the full 3D
                 topology and dimensions (direction cosine, length,
                 diameter, branching and gravity angles) of branching
                 networks using their CT images. It has been tested
                 using CT images of a simple Perspex branching network
                 and applied to the CT images of a human cast of the
                 airway tree to investigate the bronchial morphometry.
                 Good agreement was found between the morphology and
                 topology of the computer derived network and its
                 manually measured dimensions. The airway dimensions
                 also compared well with previous values quoted in
                 literature. The data on angles are new and this
                 algorithm provides complete data set analysis much
                 quicker than manual measurements. Its use is limited by
                 the CT resolution which means that very small branches
                 are not measurable.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-86,
  year =         "1999",
  title =        "Scale-space analysis for the characterisation of
                 retinal blood vessels",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-86",
  author =       "M. Elena Martinez-Perez and Alun D. Hughes and Alice
                 V. Stanton and Simon A. Thom and Anil A. Bharath and
                 Kim H. Parker",
  abstract =     "We present a method for retinal blood vessel
                 segmentation based mupon the scale-space analysis of
                 the first and second derivative of the intensity image
                 which gives information about its topology and
                 overcomes the problem of variations in contrast
                 inherent in these images. We use the local maxima over
                 scales of the magnitude of the gradient and the maximum
                 principal curvature as the two features used in a
                 region growing procedure. In the first stage, the
                 growth is constrained to regions of low gradient
                 magnitude. In the final stage this constraint is
                 relaxed to allow borders between regions to be defined.
                 The algorithm is tested in both red-free and
                 fluorescein retinal images",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-87,
  year =         "1999",
  title =        "Statistical 3{D} vessel segmentation using a Rician
                 distribution",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-87",
  author =       "Albert C. S. Chung and J. Alison Noble",
  abstract =     "This paper presents an extended version of the
                 fully-automated 3D cerebral vessel segmentation
                 algorithm developed by Wilson and Noble which is
                 applicable to time-of-flight (TOF) and phase contrast
                 (PC) magnetic resonance angiography (MRA) images. We
                 introduce a Rician distribution for background-noise
                 modelling and use a modified EM
                 (Expectation-Maximization) algorithm for the parameter
                 estimation procedure. Experimental results show that
                 the estimated Rician distribution gives a better
                 quality-of-fit to the observed background noise
                 distribution than a Gaussian distribution. The
                 segmented 3D vasculature is shown to be qualitatively
                 comparable with the results obtained from higher
                 resolution TOF MRA images.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-88,
  year =         "1999",
  title =        "{CAMRA}: Parallel application for segmentation of left
                 ventricle ({LV}) from short cardiac axis {MR} images",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-88",
  author =       "A. Avedisijana and M. D. Westheada and M. J. Gravesb
                 and E. Berryc and P. Niemid {D.J. Beacockc, S. Kellyc,
                 R.T. Blackb}",
  abstract =     "Quantitative analysis of functional cardiac MR images
                 has been limited by the lack of effective
                 semi-automatic methods for the rapid analysis of the
                 large data sets that can be generated by
                 state-of-the-art MRI systems. We describe CAMRA
                 (CArdiac Magnetic Resonance Analysis), a
                 DICOM-compatible PC-based parallel processing tool,
                 which supports semi-automatic image mensuration using
                 an active contour model (ACM) based algorithm. The
                 CAMRA user-interface also provides facilities for rapid
                 manual modification of contours. The CAMRA software has
                 been used in a multi-centre clinical trial comparing
                 ACM-based contour detection with manual planimetry of
                 the left ventricular endocardium. The results show
                 excellent agreement between the manual and automatic
                 measurements of global left ventricular function, with
                 no significant difference between the manual and
                 semi-automatic determinations of ejection fraction.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-89,
  year =         "1999",
  title =        "Virtual 3{D} Cutting for Bone Segment Extraction in
                 Maxillofacial Surgery Planning",
  author =       "P Neumann and D Siebert and G Faulkner and M Krauss
                 and A Schulz and C Lwowsky and T Tolxdorff",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-89",
  abstract =     "An important step toward our main goal of a completely
                 computer-based maxillofacial surgical planning system
                 is the availability of tools fot the surgeon to define
                 bone segments from skull and jaw bones. We have
                 developed an easy-to-handle user interface that employs
                 visual and force-feedback devices to define subvolumes
                 of a patient's volume data set. This interface is a
                 main component of our maxillofacial surgical planning
                 tool MeVisTo-Jaw. The defined subvolumes together with
                 their spatial arrangements lead to an operation plan.",
  month =        jan,
  keywords =     "virtual reality, interactive segmentation, virtual
                 surgery planning",
  booktitle =    "7th International Conference Medicine Meets Virtual
                 Reality",
}

@Article{EVL-1999-9,
  pages =        "1--19",
  year =         "1999",
  title =        "Three-dimensional computer vision for tooth
                 restoration",
  author =       "D. Paulus and M. Wolf and S. Meller and H. Niemann",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-9",
  language =     "en",
  abstract =     "If a person with carious lesions needs or requests
                 crowns or inlays, these dental fillings have to be
                 manufactured for each tooth and each person
                 individually. We survey computer vision techniques
                 which can be used to automate this process. We
                 introduce three particular applications which are
                 concerned with the reconstruction of surface
                 information. The first one aims at building up a
                 database of normalized depth images of posterior teeth
                 and at extracting characteristic features from these
                 images. In the second applications, a given occlusal
                 surface of a posterior tooth with a prepared cavity is
                 digitally reconstructed using an intact model tooth
                 from a given database. The calculated surface data can
                 then be used for automatic milling of a dental
                 prosthesis, e.g. from a preshaped ceramic block. In the
                 third application a hand-made provisoric wax inlay or
                 crown can be digitally scanned by a laser sensor and
                 copied three dimensionally into a different material
                 such as ceramic. The results are converted to a format
                 required by the computer-integrated manufacturing (CIM)
                 system for automatic milling.",
  keywords =     "computer vision, dental images, image processing,
                 range images, segmentation, teeth, 3-D reconstruction",
  volume =       "3",
  number =       "1",
  journal =      "Medical Image Analysis",
}

@InProceedings{EVL-1999-90,
  year =         "1999",
  title =        "An Interaction Model for 3{D} Cutting in Maxillofacial
                 Surgery Planning",
  author =       "P Neumann and D Siebert and A Schulz and G Faulkner
                 and M Krauss and T Tolxdorff",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-90",
  abstract =     "Our main research work is the realization of a
                 completely computer-based maxillofacial surgery
                 planning system. An important step towards this goal is
                 the availability of virtual tools for the surgeon to
                 interactively define bone segments from skull and jaw
                 bones. The easy-to-handle user interface employs visual
                 and force-feedback devices to define subvolumes of a
                 patient's volume data set. The defined subvolumes
                 together with their spatial arrangements lead to an
                 operation plan. We have evaluated modern low-cost,
                 force-feedback devices with regard to their ability to
                 emulate the surgeon's working procedure.",
  month =        feb,
  keywords =     "surgery planning, maxillofacial surgery, volume
                 segmentation, virtual tools, volume growing, force
                 feedback, real-time visualization, input devices,
                 virtual reality, interactive segmentation",
  booktitle =    "SPIE Medical Imaging 1999",
}

@InProceedings{EVL-1999-91,
  year =         "1999",
  title =        "Automated {LV} Motion Analysis form 3{D}
                 Echocardiography.",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-91",
  author =       "G. I. Sanchez-Ortiz and J. A. Noble and G. J. T.
                 Wright and J. Feldmar and M. Mulet-Parada",
  abstract =     "3D-cardiac analysis methods based on MRI, CT and
                 nuclear imaging techniques are being developed which
                 begin to elucidate the relationship between 3D shape
                 deformation and heart malfunction. However, recent
                 advances in 3D+T cardiac ultrasound (US) acquisition
                 technology potentially offer now a near-real-time,
                 non-invasive alternative to these methods, at a
                 moderate cost. In this paper we describe an approach to
                 left ventricular (LV) reconstruction and motion
                 analysis, as a first step towards rapid quantification
                 of regional heart performance based on 3D-US. We
                 outline an automated approach that combines
                 state-of-the-art 2D-echogram feature detection with
                 3D-reconstruction and shape-modelling methods that are
                 being developed for tagged-MRI and SPECT cardiac
                 imagery. We evaluate our approach on 3D+T dense and
                 simulated free-hand sparse images. Results on real
                 cardiac and phantom data are presented in a qualitative
                 and quantitative manner.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-92,
  year =         "1999",
  title =        "Improving the Detection of Abnormal Masses in
                 Mammograms - Signature and Data Normalisation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-92",
  author =       "Reyer Zwiggelaar and Caroline M. E. Rubin",
  abstract =     "We describe a method for labelling image structure
                 based on non-linear scale-orientation signatures which
                 can be used as a basis for robust pixel classification.
                 The effect of normalisation of the signatures is
                 discussed as a means to improve classification
                 robustness with respect to grey-level variations. In
                 addition, model data selection, orientation and scale
                 normalisation are investigated as a means to improve
                 the robustness of detection with respect to the scale
                 of structures",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-93,
  year =         "1999",
  title =        "Accurate robust symmetry estimation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-93",
  author =       "Stephen Smith and Mark Jenkinson",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-94,
  year =         "1999",
  title =        "Segmentation of brain {MR} images using Markov random
                 field",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-94",
  author =       "Yongue Zhang and Stephen Smith and Mike Brady",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-95,
  year =         "1999",
  title =        "Classification of {SPECT} scans of Alzheimer's disease
                 and frontal lobe dementia based on intensity and
                 gradient information",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-95",
  author =       "Vassili Kovalev and Lennart Thurfjell and Roger
                 Lundqvist and Marco Pagani",
  abstract =     "This paper describes a method for classification of
                 SPECT perfusion scans of Alzheimer's disease (AD) and
                 Frontal lobe dementia (FLD) when compared to normal
                 controls. A brain atlas was used to define volumes of
                 interests corresponding to the brain lobes. For these,
                 intensity, gradient magnitude and orientation features
                 were computed. When applied to a SPECT material
                 containing 45 AD, 7 FLD and 34 normal scans, the
                 suggested method yielded an accuracy of 96.2%, 97.6%
                 and 94.2% in the separation of AD from normals, FLD
                 from normals, and AD from FLD scans, respectively.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-96,
  year =         "1999",
  title =        "Improving the Detection of Abnormal Masses in
                 Mammograms - Signature and Data Normalisation",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-96",
  author =       "Reyer Zwiggelaar and Caroline M. E. Rubin",
  abstract =     "We describe a method for labelling image structure
                 based on non-linear scale-orientation signatures which
                 can be used as a basis for robust pixel classification.
                 The effect of normalisation of the signatures is
                 discussed as a means to improve classification
                 robustness with respect to grey-level variations. In
                 addition, model data selection, orientation and scale
                 normalisation are investigated as a means to improve
                 the robustness of detection with respect to the scale
                 of structures.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-97,
  year =         "1999",
  title =        "Registration and matching of temporal mammograms for
                 detecting abnormalities",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-97",
  author =       "K. Marias and J. M. Brady and R. P. Highnam and S.
                 Parbhoo and A. M. Seifalian",
  abstract =     "Our aim is to establish accurate correspondences
                 between temporal mammograms in order to improve the
                 detection of tumours. We propose a method for detecting
                 abnormalities after aligning a pair of mammograms using
                 thin-plate spline interpolation based on corresponding
                 points on the breast boundary. We also suggest a method
                 for the automatic detection of landmarks within the
                 breast tissue. We show how using suitable boundary
                 points yields reasonable results whilst finding
                 internal landmarks is difficult and using points on the
                 pectoral muscle unreliable.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-98,
  year =         "1999",
  title =        "Evaluation of Non-rigid Registration using Free-Form
                 Deformations for Breast {MR} Images",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-98",
  author =       "D. Rueckert and L. I. Sonoda and E. Denton and S.
                 Rankin and C. Hayes and M. O. Leach and D. Hill and D.
                 J. Hawkes",
  abstract =     "In this paper we describe a novel non-rigid
                 registration algorithm for 3D contrast-enhanced breast
                 MR images. The registration algorithm uses a free-form
                 deformation (FFD) based on B-splines to model the
                 non-rigid motion of the breast. Normalised mutual
                 information is used as a voxel-based similarity measure
                 which is insensitive to intensity changes as the result
                 of the contrast enhancement. Registration is achieved
                 by minimising a cost function which represents a
                 combination of the cost associated with the smoothness
                 of the transformation and the cost associated with the
                 image similarity. We have compared the proposed
                 non-rigid registration algorithm with rigid and affine
                 registration algorithms. All three registration
                 techniques have been applied to the fully automated
                 registration of 54 contrast-enhanced breast MR data
                 sets of which 27 have been previously reported normal
                 and 27 contained histologically proven carcinoma. The
                 results have been assessed and ranked by two
                 radiologists independently. They clearly indicate that
                 the non-rigid registration algorithm is much better
                 able to recover the motion and deformation of the
                 breast than rigid or affine registration algorithms.",
  month =        jul,
  booktitle =    "Medical Image Understanding and Analysis 1999",
}

@InProceedings{EVL-1999-99,
  year =         "1999",
  title =        "Numerical Relativity in a Distributed Environment",
  author =       "W. Benger and I. Foster and J. Novotny and E. Seidel
                 and J. Shalf and W. Smith and P. Walker",
  URL =          "http://visinfo.zib.de/EVlib/Show?EVL-1999-99",
  abstract =     "The Cactus parallel simulation framework provides a
                 modular and extensible set of components for solving
                 relativity problems on parallel computers. In recent
                 work, we have investigated techniques that would enable
                 the execution of Cactus applications in wide area
                 {"}computational grid{"} environments. In a first
                 study, we investigated the feasibility of distributing
                 a single simulation across multiple supercomputers,
                 while in a second we studied techniques for reducing
                 communication costs associated with remote
                 visualization and steering. Distributed simluation was
                 achieved by using MPICH-G, an imlementation of the
                 Message Passing Interface standard that uses mechanisms
                 provided by the Globus grid toolkit to enable wide area
                 execution. Experiments were performed across SGI
                 Origins and Cray T3Es with geographical seperations
                 ranging from hundreds of thousands of kilometres. Total
                 execution time when distributed increased by between
                 18% ans 133%, depending on configuration. We view these
                 results as encouraging as they were obtained with
                 essentially no specialized algorithmic structures in
                 the Cactus application. Work on remote visualization
                 focused on the development of a Cactus module that
                 computes isosurfaces inline with numerical relativity
                 calculations. Experiments demonstrated that this
                 technique can reduce network bandwidth requirements by
                 a factor ranging from 2.5 to 114, depending on the
                 naturer of the problem.",
  month =        mar,
  booktitle =    "Proceedings of the Ninth SIAM Conference on Parallel
                 Processing for Scientific Computing",
}
