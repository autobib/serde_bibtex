@article{daume06megam,
  author =       {Hal {Daum\'e III} and Daniel Marcu},
  title =        {Domain Adaptation for Statistical Classifiers},
  journal =      {Journal of Artificial Intelligence Research (JAIR)},
  year =         {2006},
  volume =       {26},
  pages =        {101--126},
  abstract = {
    The most basic assumption used in statistical learning theory is that
    training data and test data are drawn from the same underlying
    distribution.  Unfortunately, in many applications, the ``in-domain''
    test data is drawn from a distribution that is related, but not
    identical, to the ``out-of-domain'' distribution of the training data.
    We consider the common case in which labeled out-of-domain data is
    plentiful, but labeled in-domain data is scarce.  We introduce a
    statistical formulation of this problem in terms of a simple mixture
    model and present an instantiation of this framework to maximum
    entropy classifiers and their linear chain counterparts.  We present
    efficient inference algorithms for this special case based on the
    technique of conditional expectation maximization.  Our experimental
    results show that our approach leads to improved performance on three
    real world tasks on four different data sets from the natural language
    processing domain.
  },
  keywords = {nlp ml da},
  tagline = {We address the problem of moving classifiers from one domain to another (compromising the second &quot;i&quot; in i.i.d.) by treating the data distribution as a mixture of three sources: in-domain, out-of-domain and general. Inference is based on conditional EM for conditional models (maximum entropy) and we get quite good performance on three natural language tasks. (NOTE: There are errors in some of the equation derivations; I will put out an official errata soon!)},
  url =          {http://pub.hal3.name/#daume06megam}
}

