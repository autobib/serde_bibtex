@article{Chen:2010:TTT:1837854.1735998,
 author = {Chen, Peter M.},
 title = {Transistors to toys: teaching systems to freshmen},
 abstract = {How should we introduce students to the art of system building, and when are students ready to start designing and building interesting systems? In this talk, I describe an experimental course at the University of Michigan that teaches systems to freshmen by having them conceive of, design, and build the hardware and software of a microprocessor-based educational toy. Students in this course build their own microprocessor on an FPGA using a hardware description language. They then write the complete software stack for their toy in assembly language, including device drivers for numerous I/O devices, a simple file system, a graphical user interface, digital audio processing, and application software. By building a substantial system involving hardware, system software, and application software, students gain an appreciation for the complexity and beauty of building computing systems.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {7},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {1--2},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1837854.1735998},
 doi = {http://doi.acm.org/10.1145/1837854.1735998},
 acmid = {1735998},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {education},
} 

@inproceedings{Chen:2010:TTT:1735997.1735998,
 author = {Chen, Peter M.},
 title = {Transistors to toys: teaching systems to freshmen},
 abstract = {How should we introduce students to the art of system building, and when are students ready to start designing and building interesting systems? In this talk, I describe an experimental course at the University of Michigan that teaches systems to freshmen by having them conceive of, design, and build the hardware and software of a microprocessor-based educational toy. Students in this course build their own microprocessor on an FPGA using a hardware description language. They then write the complete software stack for their toy in assembly language, including device drivers for numerous I/O devices, a simple file system, a graphical user interface, digital audio processing, and application software. By building a substantial system involving hardware, system software, and application software, students gain an appreciation for the complexity and beauty of building computing systems.},
 booktitle = {Proceedings of the 6th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '10},
 year = {2010},
 isbn = {978-1-60558-910-7},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {1--2},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1735997.1735998},
 doi = {http://doi.acm.org/10.1145/1735997.1735998},
 acmid = {1735998},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {education},
} 

@article{Pohle:2010:CWM:1837854.1736001,
 author = {Pohle, Aaron and D\"{o}bel, Bj\"{o}rn and Roitzsch, Michael and H\"{a}rtig, Hermann},
 title = {Capability wrangling made easy: debugging on a microkernel with valgrind},
 abstract = {Not all operating systems are created equal. Contrasting traditional monolithic kernels, there is a class of systems called microkernels more prevalent in embedded systems like cellphones, chip cards or real-time controllers. These kernels offer an abstraction very different from the classical POSIX interface. The resulting unfamiliarity for programmers complicates development and debugging. Valgrind is a well-known debugging tool that virtualizes execution to perform dynamic binary analysis. However, it assumes to run on a POSIX-like kernel and closely interacts with the system to control execution. In this paper we analyze how to adapt Valgrind to a non-POSIX environment and describe our port to the Fiasco.OC microkernel. Additionally, we analyze bug classes that are indigenous to capability systems and show how Valgrind's flexibility can be leveraged to create custom debugging tools detecting these errors.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {7},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {3--12},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1837854.1736001},
 doi = {http://doi.acm.org/10.1145/1837854.1736001},
 acmid = {1736001},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {capability, l4, microkernel, valgrind},
} 

@inproceedings{Pohle:2010:CWM:1735997.1736001,
 author = {Pohle, Aaron and D\"{o}bel, Bj\"{o}rn and Roitzsch, Michael and H\"{a}rtig, Hermann},
 title = {Capability wrangling made easy: debugging on a microkernel with valgrind},
 abstract = {Not all operating systems are created equal. Contrasting traditional monolithic kernels, there is a class of systems called microkernels more prevalent in embedded systems like cellphones, chip cards or real-time controllers. These kernels offer an abstraction very different from the classical POSIX interface. The resulting unfamiliarity for programmers complicates development and debugging. Valgrind is a well-known debugging tool that virtualizes execution to perform dynamic binary analysis. However, it assumes to run on a POSIX-like kernel and closely interacts with the system to control execution. In this paper we analyze how to adapt Valgrind to a non-POSIX environment and describe our port to the Fiasco.OC microkernel. Additionally, we analyze bug classes that are indigenous to capability systems and show how Valgrind's flexibility can be leveraged to create custom debugging tools detecting these errors.},
 booktitle = {Proceedings of the 6th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '10},
 year = {2010},
 isbn = {978-1-60558-910-7},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {3--12},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1735997.1736001},
 doi = {http://doi.acm.org/10.1145/1735997.1736001},
 acmid = {1736001},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {capability, l4, microkernel, valgrind},
} 

@article{Chow:2010:MRC:1837854.1736002,
 author = {Chow, Jim and Lucchetti, Dominic and Garfinkel, Tal and Lefebvre, Geoffrey and Gardner, Ryan and Mason, Joshua and Small, Sam and Chen, Peter M.},
 title = {Multi-stage replay with crosscut},
 abstract = {Deterministic record-replay has many useful applications, ranging from fault tolerance and forensics to reproducing and diagnosing bugs. When choosing a record-replay solution, the system administrator must choose a priori how comprehensively to record the execution and at what abstraction level to record it. Unfortunately, these choices may not match well with how the recording is eventually used. A recording may contain too little information to support the end use of replay, or it may contain more sensitive information than is allowed to be shown to the end user of replay. Similarly, fixing the abstraction level at the time of recording often leads to a semantic mismatch with the end use of replay. This paper describes how to remedy these problems by adding customizable replay stages to create special-purpose logs for the end users of replay. Our system, called Crosscut, allows replay logs to be "sliced" along time and abstraction boundaries. Using this approach, users can create slices that include only the processes, applications, or components of interest, excluding parts that handle sensitive data. Users can also retarget the abstraction level of the replay log to higher-level platforms, such as Perl or Valgrind. Execution can then be augmented with additional analysis code at replay time, without disturbing the replayed components in the slice. Crosscut thus uses replay itself to transform logs into a more efficient, secure, and usable form for replay-based applications. Our current Crosscut prototype builds on VMware Workstation's record-replay capabilities, and supports a variety of different replay environments. We show how Crosscut can create slices of only the parts of the computation of interest and thereby avoid leaking sensitive information, and we show how to retarget the abstraction level of the log to enable more convenient use during replay debugging.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {7},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1837854.1736002},
 doi = {http://doi.acm.org/10.1145/1837854.1736002},
 acmid = {1736002},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {design, experimentation, performance, replay, security, virtual machines},
} 

@inproceedings{Chow:2010:MRC:1735997.1736002,
 author = {Chow, Jim and Lucchetti, Dominic and Garfinkel, Tal and Lefebvre, Geoffrey and Gardner, Ryan and Mason, Joshua and Small, Sam and Chen, Peter M.},
 title = {Multi-stage replay with crosscut},
 abstract = {Deterministic record-replay has many useful applications, ranging from fault tolerance and forensics to reproducing and diagnosing bugs. When choosing a record-replay solution, the system administrator must choose a priori how comprehensively to record the execution and at what abstraction level to record it. Unfortunately, these choices may not match well with how the recording is eventually used. A recording may contain too little information to support the end use of replay, or it may contain more sensitive information than is allowed to be shown to the end user of replay. Similarly, fixing the abstraction level at the time of recording often leads to a semantic mismatch with the end use of replay. This paper describes how to remedy these problems by adding customizable replay stages to create special-purpose logs for the end users of replay. Our system, called Crosscut, allows replay logs to be "sliced" along time and abstraction boundaries. Using this approach, users can create slices that include only the processes, applications, or components of interest, excluding parts that handle sensitive data. Users can also retarget the abstraction level of the replay log to higher-level platforms, such as Perl or Valgrind. Execution can then be augmented with additional analysis code at replay time, without disturbing the replayed components in the slice. Crosscut thus uses replay itself to transform logs into a more efficient, secure, and usable form for replay-based applications. Our current Crosscut prototype builds on VMware Workstation's record-replay capabilities, and supports a variety of different replay environments. We show how Crosscut can create slices of only the parts of the computation of interest and thereby avoid leaking sensitive information, and we show how to retarget the abstraction level of the log to enable more convenient use during replay debugging.},
 booktitle = {Proceedings of the 6th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '10},
 year = {2010},
 isbn = {978-1-60558-910-7},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {13--24},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735997.1736002},
 doi = {http://doi.acm.org/10.1145/1735997.1736002},
 acmid = {1736002},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {design, experimentation, performance, replay, security, virtual machines},
} 

@article{Huang:2010:OCD:1837854.1736003,
 author = {Huang, Yijian and Chen, Haibo and Zang, Binyu},
 title = {Optimizing crash dump in virtualized environments},
 abstract = {Crash dump, or core dump is the typical way to save memory image on system crash for future offline debugging and analysis. However, for typical server machines with likely abundant memory, the time of core dump can significantly increase the mean time to repair (MTTR) by delaying the reboot-based recovery, while not dumping the failure context for analysis would risk recurring crashes on the same problems. In this paper, we propose several optimization techniques for core dump in virtualized environments, in order to shorten the MTTR of consolidated virtual machines during crashes. First, we parallelize the process of crash dump and the process of rebooting the crashed VM, by dynamically reclaiming and allocating memory between the crashed VM and the newly spawned VM. Second, we use the virtual machine management layer to introspect the critical data structures of the crashed VM to filter out the dump of unused memory. Finally, we implement disk I/O rate control between core dump and the newly spawned VM according to user-tuned rate control policy to balance the time of crash dump and quality of services in the recovery VM. We have implemented a working prototype, Vicover, that optimizes core dump on system crash of a virtual machine in Xen, to minimize the MTTR of core dump and recovery as a whole. In our experiment on a virtualized TPC-W server, Vicover shortens the downtime caused by crash dump by around 5X.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {7},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1837854.1736003},
 doi = {http://doi.acm.org/10.1145/1837854.1736003},
 acmid = {1736003},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {core dump, parallel core dump, virtual machines},
} 

@inproceedings{Huang:2010:OCD:1735997.1736003,
 author = {Huang, Yijian and Chen, Haibo and Zang, Binyu},
 title = {Optimizing crash dump in virtualized environments},
 abstract = {Crash dump, or core dump is the typical way to save memory image on system crash for future offline debugging and analysis. However, for typical server machines with likely abundant memory, the time of core dump can significantly increase the mean time to repair (MTTR) by delaying the reboot-based recovery, while not dumping the failure context for analysis would risk recurring crashes on the same problems. In this paper, we propose several optimization techniques for core dump in virtualized environments, in order to shorten the MTTR of consolidated virtual machines during crashes. First, we parallelize the process of crash dump and the process of rebooting the crashed VM, by dynamically reclaiming and allocating memory between the crashed VM and the newly spawned VM. Second, we use the virtual machine management layer to introspect the critical data structures of the crashed VM to filter out the dump of unused memory. Finally, we implement disk I/O rate control between core dump and the newly spawned VM according to user-tuned rate control policy to balance the time of crash dump and quality of services in the recovery VM. We have implemented a working prototype, Vicover, that optimizes core dump on system crash of a virtual machine in Xen, to minimize the MTTR of core dump and recovery as a whole. In our experiment on a virtualized TPC-W server, Vicover shortens the downtime caused by crash dump by around 5X.},
 booktitle = {Proceedings of the 6th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '10},
 year = {2010},
 isbn = {978-1-60558-910-7},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {25--36},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735997.1736003},
 doi = {http://doi.acm.org/10.1145/1735997.1736003},
 acmid = {1736003},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {core dump, parallel core dump, virtual machines},
} 

@inproceedings{Hunt:2010:LBS:1735997.1735999,
 author = {Hunt, Galen C.},
 title = {Looking beyond a singularity},
 abstract = {How does one build a truly dependable software system? Seven years ago, Microsoft Research started the Singularity project to answer this question. The premise was to start with the best known software development tools and to build a new kind of operating system from the ground up. The operating system was to be both an output artifact and a laboratory for the research. Portions of the code and ideas have been incorporated into three separate Microsoft operating systems so far. I will give a brief overview of Singularity planned and built, then describe what we learned, both positive and negative. I will speculate on OS futures including current research to build an operating system in which every last assembly instruction has been verified for type safety, a system for truly mobile computation, and new tools for automatically restructuring large software systems.},
 booktitle = {Proceedings of the 6th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '10},
 year = {2010},
 isbn = {978-1-60558-910-7},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {37--38},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1735997.1735999},
 doi = {http://doi.acm.org/10.1145/1735997.1735999},
 acmid = {1735999},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {sing\#, singularity, software-isolated processes (sips)},
} 

@article{Hunt:2010:LBS:1837854.1735999,
 author = {Hunt, Galen C.},
 title = {Looking beyond a singularity},
 abstract = {How does one build a truly dependable software system? Seven years ago, Microsoft Research started the Singularity project to answer this question. The premise was to start with the best known software development tools and to build a new kind of operating system from the ground up. The operating system was to be both an output artifact and a laboratory for the research. Portions of the code and ideas have been incorporated into three separate Microsoft operating systems so far. I will give a brief overview of Singularity planned and built, then describe what we learned, both positive and negative. I will speculate on OS futures including current research to build an operating system in which every last assembly instruction has been verified for type safety, a system for truly mobile computation, and new tools for automatically restructuring large software systems.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {7},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {37--38},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1837854.1735999},
 doi = {http://doi.acm.org/10.1145/1837854.1735999},
 acmid = {1735999},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {sing\#, singularity, software-isolated processes (sips)},
} 

@inproceedings{Titzer:2010:ICS:1735997.1736005,
 author = {Titzer, Ben L. and W\"{u}rthinger, Thomas and Simon, Doug and Cintra, Marcelo},
 title = {Improving compiler-runtime separation with XIR},
 abstract = {Intense research on virtual machines has highlighted the need for flexible software architectures that allow quick evaluation of new design and implementation techniques. The interface between the compiler and runtime system is a principal factor in the flexibility of both components and is critical to enabling rapid pursuit of new optimizations and features. Although many virtual machines have demonstrated modularity for many components, significant dependencies often remain between the compiler and the runtime system components such as the object model and memory management system. This paper addresses this challenge with a carefully designed strict compiler-runtime interface and the XIR language. Instead of the compiler backend lowering object operations to machine operations using hard-wired runtime-specific logic, XIR allows the runtime system to implement this logic, simultaneously simplifying and separating the backend from runtime-system details. In this paper we describe the design and implementation of this compiler-runtime interface and the XIR language in the C1X dynamic compiler, a port of the HotSpotTM Client compiler. Our results show a significant reduction in backend complexity with XIR and an overall reduction in the compiler-runtime interface complexity while still generating comparable quality code with only minor impact on compilation time.},
 booktitle = {Proceedings of the 6th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '10},
 year = {2010},
 isbn = {978-1-60558-910-7},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {39--50},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735997.1736005},
 doi = {http://doi.acm.org/10.1145/1735997.1736005},
 acmid = {1736005},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {JIT, compilers, intermediate representations, java, lowering, object model, register allocation, runtime interface, software architecture, virtual machines},
} 

@article{Titzer:2010:ICS:1837854.1736005,
 author = {Titzer, Ben L. and W\"{u}rthinger, Thomas and Simon, Doug and Cintra, Marcelo},
 title = {Improving compiler-runtime separation with XIR},
 abstract = {Intense research on virtual machines has highlighted the need for flexible software architectures that allow quick evaluation of new design and implementation techniques. The interface between the compiler and runtime system is a principal factor in the flexibility of both components and is critical to enabling rapid pursuit of new optimizations and features. Although many virtual machines have demonstrated modularity for many components, significant dependencies often remain between the compiler and the runtime system components such as the object model and memory management system. This paper addresses this challenge with a carefully designed strict compiler-runtime interface and the XIR language. Instead of the compiler backend lowering object operations to machine operations using hard-wired runtime-specific logic, XIR allows the runtime system to implement this logic, simultaneously simplifying and separating the backend from runtime-system details. In this paper we describe the design and implementation of this compiler-runtime interface and the XIR language in the C1X dynamic compiler, a port of the HotSpotTM Client compiler. Our results show a significant reduction in backend complexity with XIR and an overall reduction in the compiler-runtime interface complexity while still generating comparable quality code with only minor impact on compilation time.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {7},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {39--50},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1837854.1736005},
 doi = {http://doi.acm.org/10.1145/1837854.1736005},
 acmid = {1736005},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {JIT, compilers, intermediate representations, java, lowering, object model, register allocation, runtime interface, software architecture, virtual machines},
} 

@article{Geoffray:2010:VSM:1837854.1736006,
 author = {Geoffray, Nicolas and Thomas, Ga\"{e}l and Lawall, Julia and Muller, Gilles and Folliot, Bertil},
 title = {VMKit: a substrate for managed runtime environments},
 abstract = {Managed Runtime Environments (MREs), such as the JVM and the CLI, form an attractive environment for program execution, by providing portability and safety, via the use of a bytecode language and automatic memory management, as well as good performance, via just-in-time (JIT) compilation. Nevertheless, developing a fully featured MRE, including e.g. a garbage collector and JIT compiler, is a herculean task. As a result, new languages cannot easily take advantage of the benefits of MREs, and it is difficult to experiment with extensions of existing MRE based languages. This paper describes and evaluates VMKit, a first attempt to build a common substrate that eases the development of high-level MREs. We have successfully used VMKit to build two MREs: a Java Virtual Machine and a Common Language Runtime. We provide an extensive study of the lessons learned in developing this infrastructure, and assess the ease of implementing new MREs or MRE extensions and the resulting performance. In particular, it took one of the authors only one month to develop a Common Language Runtime using VMKit. VMKit furthermore has performance comparableto the well established open source MREs Cacao, Apache Harmony and Mono, and is 1.2 to 3 times slower than JikesRVM on most of the Dacapo benchmarks.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {7},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {51--62},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1837854.1736006},
 doi = {http://doi.acm.org/10.1145/1837854.1736006},
 acmid = {1736006},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {just in time compiler, virtual machine, vmkit},
} 

@inproceedings{Geoffray:2010:VSM:1735997.1736006,
 author = {Geoffray, Nicolas and Thomas, Ga\"{e}l and Lawall, Julia and Muller, Gilles and Folliot, Bertil},
 title = {VMKit: a substrate for managed runtime environments},
 abstract = {Managed Runtime Environments (MREs), such as the JVM and the CLI, form an attractive environment for program execution, by providing portability and safety, via the use of a bytecode language and automatic memory management, as well as good performance, via just-in-time (JIT) compilation. Nevertheless, developing a fully featured MRE, including e.g. a garbage collector and JIT compiler, is a herculean task. As a result, new languages cannot easily take advantage of the benefits of MREs, and it is difficult to experiment with extensions of existing MRE based languages. This paper describes and evaluates VMKit, a first attempt to build a common substrate that eases the development of high-level MREs. We have successfully used VMKit to build two MREs: a Java Virtual Machine and a Common Language Runtime. We provide an extensive study of the lessons learned in developing this infrastructure, and assess the ease of implementing new MREs or MRE extensions and the resulting performance. In particular, it took one of the authors only one month to develop a Common Language Runtime using VMKit. VMKit furthermore has performance comparableto the well established open source MREs Cacao, Apache Harmony and Mono, and is 1.2 to 3 times slower than JikesRVM on most of the Dacapo benchmarks.},
 booktitle = {Proceedings of the 6th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '10},
 year = {2010},
 isbn = {978-1-60558-910-7},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {51--62},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735997.1736006},
 doi = {http://doi.acm.org/10.1145/1735997.1736006},
 acmid = {1736006},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {just in time compiler, virtual machine, vmkit},
} 

@inproceedings{Zhang:2010:NSS:1735997.1736008,
 author = {Zhang, Qing and McCullough, John and Ma, Justin and Schear, Nabil and Vrable, Michael and Vahdat, Amin and Snoeren, Alex C. and Voelker, Geoffrey M. and Savage, Stefan},
 title = {Neon: system support for derived data management},
 abstract = {Modern organizations face increasingly complex information management requirements. A combination of commercial needs, legal liability and regulatory imperatives has created a patchwork of mandated policies. Among these, personally identifying customer records must be carefully access-controlled, sensitive files must be encrypted on mobile computers to guard against physical theft, and intellectual property must be protected from both exposure and "poisoning." However, enforcing such policies can be quite difficult in practice since users routinely share data over networks and derive new files from these inputs--incidentally laundering any policy restrictions. In this paper, we describe a virtual machine monitor system called Neon that transparently labels derived data using byte-level "tints" and tracks these labels end to end across commodity applications, operating systems and networks. Our goal with Neon is to explore the viability and utility of transparent information flow tracking within conventional networked systems when used in the manner in which they were intended. We demonstrate that this mechanism allows the enforcement of a variety of data management policies, including data-dependent confinement, mandatory I/O encryption, and intellectual property management.},
 booktitle = {Proceedings of the 6th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '10},
 year = {2010},
 isbn = {978-1-60558-910-7},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {63--74},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735997.1736008},
 doi = {http://doi.acm.org/10.1145/1735997.1736008},
 acmid = {1736008},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {difc, memory tainting, qemu, virtualization, xen},
} 

@article{Zhang:2010:NSS:1837854.1736008,
 author = {Zhang, Qing and McCullough, John and Ma, Justin and Schear, Nabil and Vrable, Michael and Vahdat, Amin and Snoeren, Alex C. and Voelker, Geoffrey M. and Savage, Stefan},
 title = {Neon: system support for derived data management},
 abstract = {Modern organizations face increasingly complex information management requirements. A combination of commercial needs, legal liability and regulatory imperatives has created a patchwork of mandated policies. Among these, personally identifying customer records must be carefully access-controlled, sensitive files must be encrypted on mobile computers to guard against physical theft, and intellectual property must be protected from both exposure and "poisoning." However, enforcing such policies can be quite difficult in practice since users routinely share data over networks and derive new files from these inputs--incidentally laundering any policy restrictions. In this paper, we describe a virtual machine monitor system called Neon that transparently labels derived data using byte-level "tints" and tracks these labels end to end across commodity applications, operating systems and networks. Our goal with Neon is to explore the viability and utility of transparent information flow tracking within conventional networked systems when used in the manner in which they were intended. We demonstrate that this mechanism allows the enforcement of a variety of data management policies, including data-dependent confinement, mandatory I/O encryption, and intellectual property management.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {7},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {63--74},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1837854.1736008},
 doi = {http://doi.acm.org/10.1145/1837854.1736008},
 acmid = {1736008},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {difc, memory tainting, qemu, virtualization, xen},
} 

@inproceedings{Ye:2010:ESV:1735997.1736009,
 author = {Ye, Lei and Lu, Gen and Kumar, Sushanth and Gniady, Chris and Hartman, John H.},
 title = {Energy-efficient storage in virtual machine environments},
 abstract = {Current trends in increasing storage capacity and virtualization of resources combined with the need for energy efficiency put a challenging task in front of system designers. Previous studies have suggested many approaches to reduce hard disk energy dissipation in native OS environments; however, those mechanisms do not perform well in virtual machine environments because a virtual machine (VM) and the virtual machine monitor (VMM) that runs it have different semantic contexts. This paper explores the disk I/O activities between VMM and VMs using trace driven simulation to understand the I/O behavior of the VM system. Subsequently, this paper proposes three mechanisms to address the isolation between VMM and VMs, and increase the burstiness of hard disk accesses to increase energy efficiency of a hard disk. Compared to standard shutdown mechanisms, with eight VMs the proposed mechanisms reduce disk spin-ups, increase the disk sleep time, and reduce energy consumption by 14.8\% with only 0.5\% increase in execution time. We implemented the proposed mechanisms in Xen and validated our simulation results.},
 booktitle = {Proceedings of the 6th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '10},
 year = {2010},
 isbn = {978-1-60558-910-7},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {75--84},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1735997.1736009},
 doi = {http://doi.acm.org/10.1145/1735997.1736009},
 acmid = {1736009},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy management, storage system, virtual machine},
} 

@article{Ye:2010:ESV:1837854.1736009,
 author = {Ye, Lei and Lu, Gen and Kumar, Sushanth and Gniady, Chris and Hartman, John H.},
 title = {Energy-efficient storage in virtual machine environments},
 abstract = {Current trends in increasing storage capacity and virtualization of resources combined with the need for energy efficiency put a challenging task in front of system designers. Previous studies have suggested many approaches to reduce hard disk energy dissipation in native OS environments; however, those mechanisms do not perform well in virtual machine environments because a virtual machine (VM) and the virtual machine monitor (VMM) that runs it have different semantic contexts. This paper explores the disk I/O activities between VMM and VMs using trace driven simulation to understand the I/O behavior of the VM system. Subsequently, this paper proposes three mechanisms to address the isolation between VMM and VMs, and increase the burstiness of hard disk accesses to increase energy efficiency of a hard disk. Compared to standard shutdown mechanisms, with eight VMs the proposed mechanisms reduce disk spin-ups, increase the disk sleep time, and reduce energy consumption by 14.8\% with only 0.5\% increase in execution time. We implemented the proposed mechanisms in Xen and validated our simulation results.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {7},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {75--84},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1837854.1736009},
 doi = {http://doi.acm.org/10.1145/1837854.1736009},
 acmid = {1736009},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy management, storage system, virtual machine},
} 

@inproceedings{Kazempour:2010:AAS:1735997.1736011,
 author = {Kazempour, Vahid and Kamali, Ali and Fedorova, Alexandra},
 title = {AASH: an asymmetry-aware scheduler for hypervisors},
 abstract = {Asymmetric multicore processors (AMP) consist of cores exposing the same instruction-set architecture (ISA) but varying in size, frequency, power consumption and performance. AMPs were shown to be more power efficient than conventional symmetric multicore processors, and it is therefore likely that future multicore systems will include cores of different types. AMPs derive their efficiency from core specialization: instruction streams can be assigned to run on the cores best suited to their demands for architectural resources. System efficiency is improved as a result. To perform effective matching of threads to cores, the thread scheduler must be asymmetry-aware; and while asymmetry-aware schedulers for operating systems are a well studied topic, asymmetry-awareness in hypervisors has not been addressed. A hypervisor must be asymmetry-aware to enable proper functioning of asymmetry-aware guest operating systems; otherwise they will be ineffective in virtual environments. Furthermore, a hypervisor must ensure that asymmetric cores are shared among multiple guests in a fair fashion or in accordance with their priorities. This work for the first time implements simple changes to the hypervisor scheduler, required to make it asymmetry-aware, and evaluates the benefits and overheads of these asymmetry-aware mechanisms. Our evaluation was performed using an open source hypervisor Xen on a real multicore system where asymmetry was emulated via CPU frequency scaling. We compared the asymmetry-aware hypervisor to default Xen. Our results indicate that asymmetry support can be implemented with low overheads, and resulting performance improvements can be significant, reaching up to 36\% in our experiments. Most performance improvements are derived from the fact that an asymmetry-aware hypervisor ensures that the fast cores do not go idle before slow cores and from the fact that it maps virtual cores to physical cores for asymmetry-aware guests according to the guest's expectations. Other benefits from asymmetry awareness are fairer sharing of computing resources among VMs and more stable execution times.},
 booktitle = {Proceedings of the 6th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '10},
 year = {2010},
 isbn = {978-1-60558-910-7},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735997.1736011},
 doi = {http://doi.acm.org/10.1145/1735997.1736011},
 acmid = {1736011},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asymmetric, heterogeneous, hypervisor, multicore processors, scheduling algorithms, virtual machine monitor},
} 

@article{Kazempour:2010:AAS:1837854.1736011,
 author = {Kazempour, Vahid and Kamali, Ali and Fedorova, Alexandra},
 title = {AASH: an asymmetry-aware scheduler for hypervisors},
 abstract = {Asymmetric multicore processors (AMP) consist of cores exposing the same instruction-set architecture (ISA) but varying in size, frequency, power consumption and performance. AMPs were shown to be more power efficient than conventional symmetric multicore processors, and it is therefore likely that future multicore systems will include cores of different types. AMPs derive their efficiency from core specialization: instruction streams can be assigned to run on the cores best suited to their demands for architectural resources. System efficiency is improved as a result. To perform effective matching of threads to cores, the thread scheduler must be asymmetry-aware; and while asymmetry-aware schedulers for operating systems are a well studied topic, asymmetry-awareness in hypervisors has not been addressed. A hypervisor must be asymmetry-aware to enable proper functioning of asymmetry-aware guest operating systems; otherwise they will be ineffective in virtual environments. Furthermore, a hypervisor must ensure that asymmetric cores are shared among multiple guests in a fair fashion or in accordance with their priorities. This work for the first time implements simple changes to the hypervisor scheduler, required to make it asymmetry-aware, and evaluates the benefits and overheads of these asymmetry-aware mechanisms. Our evaluation was performed using an open source hypervisor Xen on a real multicore system where asymmetry was emulated via CPU frequency scaling. We compared the asymmetry-aware hypervisor to default Xen. Our results indicate that asymmetry support can be implemented with low overheads, and resulting performance improvements can be significant, reaching up to 36\% in our experiments. Most performance improvements are derived from the fact that an asymmetry-aware hypervisor ensures that the fast cores do not go idle before slow cores and from the fact that it maps virtual cores to physical cores for asymmetry-aware guests according to the guest's expectations. Other benefits from asymmetry awareness are fairer sharing of computing resources among VMs and more stable execution times.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {7},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {85--96},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1837854.1736011},
 doi = {http://doi.acm.org/10.1145/1837854.1736011},
 acmid = {1736011},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {asymmetric, heterogeneous, hypervisor, multicore processors, scheduling algorithms, virtual machine monitor},
} 

@inproceedings{Lee:2010:SSR:1735997.1736012,
 author = {Lee, Min and Krishnakumar, A. S. and Krishnan, P. and Singh, Navjot and Yajnik, Shalini},
 title = {Supporting soft real-time tasks in the xen hypervisor},
 abstract = {Virtualization technology enables server consolidation and has given an impetus to low-cost green data centers. However, current hypervisors do not provide adequate support for real-time applications, and this has limited the adoption of virtualization in some domains. Soft real-time applications, such as media-based ones, are impeded by components of virtualization including low-performance virtualization I/O, increased scheduling latency, and shared-cache contention. The virtual machine scheduler is central to all these issues. The goal in this paper is to adapt the virtual machine scheduler to be more soft-real-time friendly. We improve two aspects of the VMM scheduler -- managing scheduling latency as a first-class resource and managing shared caches. We use enterprise IP telephony as an illustrative soft real-time workload and design a scheduler S that incorporates the knowledge of soft real-time applications in all</i> aspects of the scheduler to support responsiveness. For this we first define a laxity</i> value that can be interpreted as the target scheduling latency that the workload desires. The load balancer is also designed to minimize the latency for real-time tasks. For cache management, we take cache-affinity into account for real time tasks and load-balance accordingly to prevent cache thrashing. We measured cache misses and demonstrated that cache management is essential for soft real time tasks. Although our scheduler S employs a different design philosophy, interestingly enough it can be implemented with simple modifications to the Xen hypervisor's credit scheduler. Our experiments demonstrate that the Xen scheduler with our modifications can support soft real-time guests well, without penalizing non-real-time domains.},
 booktitle = {Proceedings of the 6th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '10},
 year = {2010},
 isbn = {978-1-60558-910-7},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {97--108},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735997.1736012},
 doi = {http://doi.acm.org/10.1145/1735997.1736012},
 acmid = {1736012},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {enterprise telephony workloads, laxity, server consolidation, virtualization, xen},
} 

@article{Lee:2010:SSR:1837854.1736012,
 author = {Lee, Min and Krishnakumar, A. S. and Krishnan, P. and Singh, Navjot and Yajnik, Shalini},
 title = {Supporting soft real-time tasks in the xen hypervisor},
 abstract = {Virtualization technology enables server consolidation and has given an impetus to low-cost green data centers. However, current hypervisors do not provide adequate support for real-time applications, and this has limited the adoption of virtualization in some domains. Soft real-time applications, such as media-based ones, are impeded by components of virtualization including low-performance virtualization I/O, increased scheduling latency, and shared-cache contention. The virtual machine scheduler is central to all these issues. The goal in this paper is to adapt the virtual machine scheduler to be more soft-real-time friendly. We improve two aspects of the VMM scheduler -- managing scheduling latency as a first-class resource and managing shared caches. We use enterprise IP telephony as an illustrative soft real-time workload and design a scheduler S that incorporates the knowledge of soft real-time applications in all</i> aspects of the scheduler to support responsiveness. For this we first define a laxity</i> value that can be interpreted as the target scheduling latency that the workload desires. The load balancer is also designed to minimize the latency for real-time tasks. For cache management, we take cache-affinity into account for real time tasks and load-balance accordingly to prevent cache thrashing. We measured cache misses and demonstrated that cache management is essential for soft real time tasks. Although our scheduler S employs a different design philosophy, interestingly enough it can be implemented with simple modifications to the Xen hypervisor's credit scheduler. Our experiments demonstrate that the Xen scheduler with our modifications can support soft real-time guests well, without penalizing non-real-time domains.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {7},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {97--108},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1837854.1736012},
 doi = {http://doi.acm.org/10.1145/1837854.1736012},
 acmid = {1736012},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {enterprise telephony workloads, laxity, server consolidation, virtualization, xen},
} 

@article{Odaira:2010:ERT:1837854.1736014,
 author = {Odaira, Rei and Ogata, Kazunori and Kawachiya, Kiyokuni and Onodera, Tamiya and Nakatani, Toshio},
 title = {Efficient runtime tracking of allocation sites in Java},
 abstract = {Tracking the allocation site of every object at runtime is useful for reliable, optimized Java. To be used in production environments, the tracking must be accurate with minimal speed loss. Previous approaches suffer from performance degradation due to the additional field added to each object or track the allocation sites only probabilistically. We propose two novel approaches to track the allocation sites of every object in Java with only a 1.0\% slow-down on average. Our first approach, the Allocation-Site-as-a-Hash-code (ASH) Tracker</i>, encodes the allocation site ID of an object into the hash code field of its header by regarding the ID as part of the hash code. ASH Tracker avoids an excessive increase in hash code collisions by dynamically shrinking the bit-length of the ID as more and more objects are allocated at that site. For those Java VMs without the hash code field, our second approach, the Allocation-Site-via-a-Class-pointer (ASC) Tracker</i>, makes the class pointer field in an object header refer to the allocation site structure of the object, which in turn points to the actual class structure. ASC Tracker mitigates the indirection overhead by constant-class-field duplication and allocation-site equality checks. While a previous approach of adding a 4-byte field caused up to 14.4\% and an average 5\% slowdown, both ASH and ASC Trackers incur at most a 2.0\% and an average 1.0\% loss. We demonstrate the usefulness of our low-overhead trackers by an allocation-site-aware memory leak detector and allocation-site-based pretenuring in generational GC. Our pretenuring achieved on average 1.8\% and up to 11.8\% speedups in SPECjvm2008.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {7},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {109--120},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1837854.1736014},
 doi = {http://doi.acm.org/10.1145/1837854.1736014},
 acmid = {1736014},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {allocation site, hash code, memory allocation},
} 

@inproceedings{Odaira:2010:ERT:1735997.1736014,
 author = {Odaira, Rei and Ogata, Kazunori and Kawachiya, Kiyokuni and Onodera, Tamiya and Nakatani, Toshio},
 title = {Efficient runtime tracking of allocation sites in Java},
 abstract = {Tracking the allocation site of every object at runtime is useful for reliable, optimized Java. To be used in production environments, the tracking must be accurate with minimal speed loss. Previous approaches suffer from performance degradation due to the additional field added to each object or track the allocation sites only probabilistically. We propose two novel approaches to track the allocation sites of every object in Java with only a 1.0\% slow-down on average. Our first approach, the Allocation-Site-as-a-Hash-code (ASH) Tracker</i>, encodes the allocation site ID of an object into the hash code field of its header by regarding the ID as part of the hash code. ASH Tracker avoids an excessive increase in hash code collisions by dynamically shrinking the bit-length of the ID as more and more objects are allocated at that site. For those Java VMs without the hash code field, our second approach, the Allocation-Site-via-a-Class-pointer (ASC) Tracker</i>, makes the class pointer field in an object header refer to the allocation site structure of the object, which in turn points to the actual class structure. ASC Tracker mitigates the indirection overhead by constant-class-field duplication and allocation-site equality checks. While a previous approach of adding a 4-byte field caused up to 14.4\% and an average 5\% slowdown, both ASH and ASC Trackers incur at most a 2.0\% and an average 1.0\% loss. We demonstrate the usefulness of our low-overhead trackers by an allocation-site-aware memory leak detector and allocation-site-based pretenuring in generational GC. Our pretenuring achieved on average 1.8\% and up to 11.8\% speedups in SPECjvm2008.},
 booktitle = {Proceedings of the 6th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '10},
 year = {2010},
 isbn = {978-1-60558-910-7},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {109--120},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735997.1736014},
 doi = {http://doi.acm.org/10.1145/1735997.1736014},
 acmid = {1736014},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {allocation site, hash code, memory allocation},
} 

@article{Tatsubori:2010:EJC:1837854.1736015,
 author = {Tatsubori, Michiaki and Tozawa, Akihiko and Suzumura, Toyotaro and Trent, Scott and Onodera, Tamiya},
 title = {Evaluation of a just-in-time compiler retrofitted for PHP},
 abstract = {Programmers who develop Web applications often use dynamic scripting languages such as Perl, PHP, Python, and Ruby. For general purpose scripting language usage, interpreter-based implementations are efficient and popular but the server-side usage for Web application development implies an opportunity to significantly enhance Web server throughput. This paper summarizes a study of the optimization of PHP script processing. We developed a PHP processor, P9, by adapting an existing production-quality just-in-time (JIT) compiler for a Java virtual machine, for which optimization technologies have been well-established, especially for server-side application. This paper describes and contrasts microbenchmarks and SPECweb2005 benchmark results for a well-tuned configuration of a traditional PHP interpreter and our JIT compiler-based implementation, P9. Experimental results with the microbenchmarks show 2.5-9.5x advantage with P9, and the SPECweb2005 measurements show about 20-30\% improvements. These results show that the acceleration of dynamic scripting language processing does matter in a realistic Web application server environment. CPU usage profiling shows our simple JIT compiler introduction reduces the PHP core runtime overhead from 45\% to 13\% for a SPECweb2005 scenario, implying that further improvements of dynamic compilers would provide little additional return unless other major overheads such as heavy memory copy between the language runtime and Web server frontend are reduced.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {7},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {121--132},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1837854.1736015},
 doi = {http://doi.acm.org/10.1145/1837854.1736015},
 acmid = {1736015},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic scripting languages, just-in-time compiler, php},
} 

@inproceedings{Tatsubori:2010:EJC:1735997.1736015,
 author = {Tatsubori, Michiaki and Tozawa, Akihiko and Suzumura, Toyotaro and Trent, Scott and Onodera, Tamiya},
 title = {Evaluation of a just-in-time compiler retrofitted for PHP},
 abstract = {Programmers who develop Web applications often use dynamic scripting languages such as Perl, PHP, Python, and Ruby. For general purpose scripting language usage, interpreter-based implementations are efficient and popular but the server-side usage for Web application development implies an opportunity to significantly enhance Web server throughput. This paper summarizes a study of the optimization of PHP script processing. We developed a PHP processor, P9, by adapting an existing production-quality just-in-time (JIT) compiler for a Java virtual machine, for which optimization technologies have been well-established, especially for server-side application. This paper describes and contrasts microbenchmarks and SPECweb2005 benchmark results for a well-tuned configuration of a traditional PHP interpreter and our JIT compiler-based implementation, P9. Experimental results with the microbenchmarks show 2.5-9.5x advantage with P9, and the SPECweb2005 measurements show about 20-30\% improvements. These results show that the acceleration of dynamic scripting language processing does matter in a realistic Web application server environment. CPU usage profiling shows our simple JIT compiler introduction reduces the PHP core runtime overhead from 45\% to 13\% for a SPECweb2005 scenario, implying that further improvements of dynamic compilers would provide little additional return unless other major overheads such as heavy memory copy between the language runtime and Web server frontend are reduced.},
 booktitle = {Proceedings of the 6th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '10},
 year = {2010},
 isbn = {978-1-60558-910-7},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {121--132},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735997.1736015},
 doi = {http://doi.acm.org/10.1145/1735997.1736015},
 acmid = {1736015},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic scripting languages, just-in-time compiler, php},
} 

@inproceedings{Namjoshi:2010:NOP:1735997.1736016,
 author = {Namjoshi, Manjiri A. and Kulkarni, Prasad A.},
 title = {Novel online profiling for virtual machines},
 abstract = {Application profiling</i> is a popular technique to improve program performance based on its behavior. Offline</i> profiling, although beneficial for several applications, fails in cases where prior program runs may not be feasible, or if changes in input cause the profile to not match the behavior of the actual program run. Managed languages, like Java and C\#, provide a unique opportunity to overcome the drawbacks of offline profiling by generating the profile information online during the current program run. Indeed, online profiling is extensively used in current VMs, especially during selective compilation to improve program startup</i> performance, as well as during other feedback-directed optimizations. In this paper we illustrate the drawbacks of the current reactive</i> mechanism of online profiling during selective compilation. Current VM profiling mechanisms are slow -- thereby delaying associated transformations, and estimate future behavior based on the program's immediate past -- leading to potential misspeculation that limit the benefits of compilation. We show that these drawbacks produce an average performance loss of over 14.5\% on our set of benchmark programs, over an ideal offline</i> approach that accurately compiles the hot methods early. We then propose and evaluate the potential of a novel strategy to achieve similar performance benefits with an online profiling approach. Our new online profiling strategy uses early determination of loop iteration bounds to predict future method hotness. We explore and present promising results on the potential, feasibility, and other issues involved for the successful implementation of this approach.},
 booktitle = {Proceedings of the 6th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '10},
 year = {2010},
 isbn = {978-1-60558-910-7},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {133--144},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735997.1736016},
 doi = {http://doi.acm.org/10.1145/1735997.1736016},
 acmid = {1736016},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {java, online profiling, virtual machines},
} 

@article{Namjoshi:2010:NOP:1837854.1736016,
 author = {Namjoshi, Manjiri A. and Kulkarni, Prasad A.},
 title = {Novel online profiling for virtual machines},
 abstract = {Application profiling</i> is a popular technique to improve program performance based on its behavior. Offline</i> profiling, although beneficial for several applications, fails in cases where prior program runs may not be feasible, or if changes in input cause the profile to not match the behavior of the actual program run. Managed languages, like Java and C\#, provide a unique opportunity to overcome the drawbacks of offline profiling by generating the profile information online during the current program run. Indeed, online profiling is extensively used in current VMs, especially during selective compilation to improve program startup</i> performance, as well as during other feedback-directed optimizations. In this paper we illustrate the drawbacks of the current reactive</i> mechanism of online profiling during selective compilation. Current VM profiling mechanisms are slow -- thereby delaying associated transformations, and estimate future behavior based on the program's immediate past -- leading to potential misspeculation that limit the benefits of compilation. We show that these drawbacks produce an average performance loss of over 14.5\% on our set of benchmark programs, over an ideal offline</i> approach that accurately compiles the hot methods early. We then propose and evaluate the potential of a novel strategy to achieve similar performance benefits with an online profiling approach. Our new online profiling strategy uses early determination of loop iteration bounds to predict future method hotness. We explore and present promising results on the potential, feasibility, and other issues involved for the successful implementation of this approach.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {7},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {133--144},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1837854.1736016},
 doi = {http://doi.acm.org/10.1145/1837854.1736016},
 acmid = {1736016},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {java, online profiling, virtual machines},
} 

@article{Guha:2010:DPS:1837854.1736018,
 author = {Guha, Apala and hazelwood, Kim and Soffa, Mary Lou},
 title = {DBT path selection for holistic memory efficiency and performance},
 abstract = {Dynamic binary translators(DBTs) provide powerful platforms for building dynamic program monitoring and adaptation tools. DBTs, however, have high memory demands because they cache translated code and auxiliary code to a software code cache and must also maintain data structures to support the code cache. The high memory demands make it difficult for memory-constrained embedded systems to take advantage of DBT-based tools. Previous research on DBT memory management focused on the translated code and auxiliary code only. However, we found that data structures are comparable to the code cache in size. We show that the translated code size, auxiliary code size and the data structure size interact in a complex manner, depending on the path selection (trace selection and link formation) strategy. Therefore, holistic memory efficiency (comprising translated code, auxiliary code and data structures) cannot be improved by focusing on the code cache only. In this paper, we use path selection for improving holistic memory efficiency which in turn impacts performance in memory-constrained environments. Although there has been previous research on path selection, such research only considered performance in memory-unconstrained environments. The challenge for holistic memory efficiency is that the path selection strategy results in complex interactions between the memory demand components. Also, individual aspects of path selection and the holistic memory efficiency may impact performance in complex ways. We explore these interactions to motivate path selection targeting holistic memory demand. We enumerate all the aspects involved in a path selection design and evaluate a comprehensive set of approaches for each aspect. Finally, we propose a path selection strategy that reduces memory demands by 20\% and at the same time improves performance by 5-20\% compared to an industrial-strength DBT.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {7},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1837854.1736018},
 doi = {http://doi.acm.org/10.1145/1837854.1736018},
 acmid = {1736018},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic binary translation, embedded systems, memory management, path selection, virtual machines},
} 

@inproceedings{Guha:2010:DPS:1735997.1736018,
 author = {Guha, Apala and hazelwood, Kim and Soffa, Mary Lou},
 title = {DBT path selection for holistic memory efficiency and performance},
 abstract = {Dynamic binary translators(DBTs) provide powerful platforms for building dynamic program monitoring and adaptation tools. DBTs, however, have high memory demands because they cache translated code and auxiliary code to a software code cache and must also maintain data structures to support the code cache. The high memory demands make it difficult for memory-constrained embedded systems to take advantage of DBT-based tools. Previous research on DBT memory management focused on the translated code and auxiliary code only. However, we found that data structures are comparable to the code cache in size. We show that the translated code size, auxiliary code size and the data structure size interact in a complex manner, depending on the path selection (trace selection and link formation) strategy. Therefore, holistic memory efficiency (comprising translated code, auxiliary code and data structures) cannot be improved by focusing on the code cache only. In this paper, we use path selection for improving holistic memory efficiency which in turn impacts performance in memory-constrained environments. Although there has been previous research on path selection, such research only considered performance in memory-unconstrained environments. The challenge for holistic memory efficiency is that the path selection strategy results in complex interactions between the memory demand components. Also, individual aspects of path selection and the holistic memory efficiency may impact performance in complex ways. We explore these interactions to motivate path selection targeting holistic memory demand. We enumerate all the aspects involved in a path selection design and evaluate a comprehensive set of approaches for each aspect. Finally, we propose a path selection strategy that reduces memory demands by 20\% and at the same time improves performance by 5-20\% compared to an industrial-strength DBT.},
 booktitle = {Proceedings of the 6th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '10},
 year = {2010},
 isbn = {978-1-60558-910-7},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1735997.1736018},
 doi = {http://doi.acm.org/10.1145/1735997.1736018},
 acmid = {1736018},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic binary translation, embedded systems, memory management, path selection, virtual machines},
} 

@article{Kondoh:2010:DBT:1837854.1736019,
 author = {Kondoh, Goh and Komatsu, Hideaki},
 title = {Dynamic binary translation specialized for embedded systems},
 abstract = {This paper describes the design and implementation of a novel dynamic binary translation technique specialized for embedded systems. Virtual platforms have been widely used to develop embedded software and dynamic binary translation is essential to boost their speed in simulations. However, unlike application simulation, the code generated for systems simulation is still slow because the simulator must replicate all of the functions of the target hardware. Embedded systems, which focus on providing one or a few functions, utilize only a small portion of the processor's features most of the time. For example, they may use a Memory Management Unit (MMU) in a processor to map physical memory to effective addresses, but they may not need paged memory support as in an OS. We can exploit this to specialize the dynamically translated code for more performance. We built a specialization framework on top of a functional simulator with a dynamic binary translator. Using the framework, we implemented three specializers for an MMU, bi-endianness, and register banks. Experiments with the EEMBC1.1 benchmark showed that the speed of the specialized code was up to 39\% faster than the unspecialized code.},
 journal = {SIGPLAN Not.},
 volume = {45},
 issue = {7},
 month = {March},
 year = {2010},
 issn = {0362-1340},
 pages = {157--166},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1837854.1736019},
 doi = {http://doi.acm.org/10.1145/1837854.1736019},
 acmid = {1736019},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic binary translation, embedded systems, partial evaluation, specialization},
} 

@inproceedings{Kondoh:2010:DBT:1735997.1736019,
 author = {Kondoh, Goh and Komatsu, Hideaki},
 title = {Dynamic binary translation specialized for embedded systems},
 abstract = {This paper describes the design and implementation of a novel dynamic binary translation technique specialized for embedded systems. Virtual platforms have been widely used to develop embedded software and dynamic binary translation is essential to boost their speed in simulations. However, unlike application simulation, the code generated for systems simulation is still slow because the simulator must replicate all of the functions of the target hardware. Embedded systems, which focus on providing one or a few functions, utilize only a small portion of the processor's features most of the time. For example, they may use a Memory Management Unit (MMU) in a processor to map physical memory to effective addresses, but they may not need paged memory support as in an OS. We can exploit this to specialize the dynamically translated code for more performance. We built a specialization framework on top of a functional simulator with a dynamic binary translator. Using the framework, we implemented three specializers for an MMU, bi-endianness, and register banks. Experiments with the EEMBC1.1 benchmark showed that the speed of the specialized code was up to 39\% faster than the unspecialized code.},
 booktitle = {Proceedings of the 6th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '10},
 year = {2010},
 isbn = {978-1-60558-910-7},
 location = {Pittsburgh, Pennsylvania, USA},
 pages = {157--166},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1735997.1736019},
 doi = {http://doi.acm.org/10.1145/1735997.1736019},
 acmid = {1736019},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic binary translation, embedded systems, partial evaluation, specialization},
} 

@inproceedings{Nagarajan:2009:ASS:1508293.1508295,
 author = {Nagarajan, Vijay and Gupta, Rajiv},
 title = {Architectural support for shadow memory in multiprocessors},
 abstract = {Runtime monitoring support serves as a foundation for the important tasks of providing security, performing debugging, and improving performance of applications. Often runtime monitoring requires the maintenance of information associated with each of the application's original memory location, which is held in corresponding shadow memory locations. Unfortunately, existing robust shadow memory implementations are inefficient. In this paper, we present a shadow memory implementation that is both efficient and robust. A combination of architectural support (in the form of ISA support and augmentations to the cache coherency protocol) and operating system support (in the form of coupled allocation of memory pages used by the application and associated shadow memory pages) is proposed. By coupling the coherency of shadow memory with the coherency of the main memory, we ensure that the shadow memory instructions execute atomically with their corresponding original memory instructions. Our page allocation policy enables fast translation of original addresses into corresponding shadow memory addresses; thus allowing implicit addressing of shadow memory. This approach obviates the need for page table entries for shadow pages. Our experiments show that the overheads of runtime monitoring tasks are significantly reduced in comparison to previous software implementations.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '09},
 year = {2009},
 isbn = {978-1-60558-375-4},
 location = {Washington, DC, USA},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1508293.1508295},
 doi = {http://doi.acm.org/10.1145/1508293.1508295},
 acmid = {1508295},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {coupled coherence, shadow memory},
} 

@inproceedings{Kliot:2009:LCI:1508293.1508296,
 author = {Kliot, Gabriel and Petrank, Erez and Steensgaard, Bjarne},
 title = {A lock-free, concurrent, and incremental stack scanning for garbage collectors},
 abstract = {Two major efficiency parameters for garbage collectors are the throughput overheads and the pause times that they introduce. Highly responsive systems need to use collectors with as short as possible pause times. Pause lengths have decreased significantly during the years, especially through the use of concurrent garbage collectors. For modern concurrent collectors, the longest pause is typically created by the need to atomically scan the runtime stack. All practical concurrent collectors that we are aware of must obtain a snapshot of the pointers on each thread's runtime stack, in order to reclaim objects correctly. To further reduce the length of the collector pauses, incremental stack scans were proposed. However, previous such methods employ locks to stop the mutator from accessing a stack frame while it is being scanned. Thus, these methods introduce a potential long and unpredictable pauses for a mutator thread. In this work we propose the first concurrent, incremental, and lock-free stack scanning for garbage collectors, allowing high responsiveness and support for programs that employ fine-synchronization to avoid locks. Our solution can be employed by all concurrent collectors that we are aware of, it is lock-free, it imposes a negligible overhead on the program execution, and it supports the special in-stack references existing in languages like C#.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '09},
 year = {2009},
 isbn = {978-1-60558-375-4},
 location = {Washington, DC, USA},
 pages = {11--20},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1508293.1508296},
 doi = {http://doi.acm.org/10.1145/1508293.1508296},
 acmid = {1508296},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {incremental and concurrent garbage collection, lock-free data structures, stack scanning},
} 

@inproceedings{Zhao:2009:DMB:1508293.1508297,
 author = {Zhao, Weiming and Wang, Zhenlin},
 title = {Dynamic memory balancing for virtual machines},
 abstract = {Virtualization essentially enables multiple operating systems and applications to run on one physical computer by multiplexing hardware resources. A key motivation for applying virtualization is to improve hardware resource utilization while maintaining reasonable quality of service. However, such a goal cannot be achieved without efficient resource management. Though most physical resources, such as processor cores and I/O devices, are shared among virtual machines using time slicing and can be scheduled flexibly based on priority, allocating an appropriate amount of main memory to virtual machines is more challenging. Different applications have different memory requirements. Even a single application shows varied working set sizes during its execution. An optimal memory management strategy under a virtualized environment thus needs to dynamically adjust memory allocation for each virtual machine, which further requires a prediction model that forecasts its host physical memory needs on the fly. This paper introduces MEmory Balancer (MEB) which dynamically monitors the memory usage of each virtual machine, accurately predicts its memory needs, and periodically reallocates host memory. MEB uses two effective memory predictors which, respectively, estimate the amount of memory available for reclaiming without a notable performance drop, and additional memory required for reducing the virtual machine paging penalty. Our experimental results show that our prediction schemes yield high accuracy and low overhead. Furthermore, the overall system throughput can be significantly improved with MEB.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '09},
 year = {2009},
 isbn = {978-1-60558-375-4},
 location = {Washington, DC, USA},
 pages = {21--30},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1508293.1508297},
 doi = {http://doi.acm.org/10.1145/1508293.1508297},
 acmid = {1508297},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {lru histogram, memory balancing, virtual machine},
} 

@inproceedings{Wood:2009:MBE:1508293.1508299,
 author = {Wood, Timothy and Tarasuk-Levin, Gabriel and Shenoy, Prashant and Desnoyers, Peter and Cecchet, Emmanuel and Corner, Mark D.},
 title = {Memory buddies: exploiting page sharing for smart colocation in virtualized data centers},
 abstract = {Many data center virtualization solutions, such as VMware ESX, employ content-based page sharing to consolidate the resources of multiple servers. Page sharing identifies virtual machine memory pages with identical content and consolidates them into a single shared page. This technique, implemented at the host level, applies only between VMs placed on a given physical host. In a multi-server data center, opportunities for sharing may be lost because the VMs holding identical pages are resident on different hosts. In order to obtain the full benefit of content-based page sharing it is necessary to place virtual machines such that VMs with similar memory content are located on the same hosts. In this paper we present Memory Buddies, a memory sharing-aware placement system for virtual machines. This system includes a memory fingerprinting system to efficiently determine the sharing potential among a set of VMs, and compute more efficient placements. In addition it makes use of live migration to optimize VM placement as workloads change. We have implemented a prototype Memory Buddies system with VMware ESX Server and present experimental results on our testbed, as well as an analysis of an extensive memory trace study. Evaluation of our prototype using a mix of enterprise and e-commerce applications demonstrates an increase of data center capacity (i.e. number of VMs supported) of 17\%, while imposing low overhead and scaling to as many as a thousand servers.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '09},
 year = {2009},
 isbn = {978-1-60558-375-4},
 location = {Washington, DC, USA},
 pages = {31--40},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1508293.1508299},
 doi = {http://doi.acm.org/10.1145/1508293.1508299},
 acmid = {1508299},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {consolidation, page sharing, virtualization},
} 

@inproceedings{Hermenier:2009:ECM:1508293.1508300,
 author = {Hermenier, Fabien and Lorca, Xavier and Menaud, Jean-Marc and Muller, Gilles and Lawall, Julia},
 title = {Entropy: a consolidation manager for clusters},
 abstract = {Clusters provide powerful computing environments, but in practice much of this power goes to waste, due to the static allocation of tasks to nodes, regardless of their changing computational requirements. Dynamic consolidation is an approach that migrates tasks within a cluster as their computational requirements change, both to reduce the number of nodes that need to be active and to eliminate temporary overload situations. Previous dynamic consolidation strategies have relied on task placement heuristics that use only local optimization and typically do not take migration overhead into account. However, heuristics based on only local optimization may miss the globally optimal solution, resulting in unnecessary resource usage, and the overhead for migration may nullify the benefits of consolidation. In this paper, we propose the Entropy resource manager for homogeneous clusters, which performs dynamic consolidation based on constraint programming and takes migration overhead into account. The use of constraint programming allows Entropy to find mappings of tasks to nodes that are better than those found by heuristics based on local optimizations, and that are frequently globally optimal in the number of nodes. Because migration overhead is taken into account, Entropy chooses migrations that can be implemented efficiently, incurring a low performance overhead.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '09},
 year = {2009},
 isbn = {978-1-60558-375-4},
 location = {Washington, DC, USA},
 pages = {41--50},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1508293.1508300},
 doi = {http://doi.acm.org/10.1145/1508293.1508300},
 acmid = {1508300},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cluster, dynamic consolidation, migration, reconfiguration, virtualization},
} 

@inproceedings{Hines:2009:PBL:1508293.1508301,
 author = {Hines, Michael R. and Gopalan, Kartik},
 title = {Post-copy based live virtual machine migration using adaptive pre-paging and dynamic self-ballooning},
 abstract = {We present the design, implementation, and evaluation of post-copy based live migration for virtual machines (VMs) across a Gigabit LAN. Live migration is an indispensable feature in today's virtualization technologies. Post-copy migration defers the transfer of a VM's memory contents until after its processor state has been sent to the target host. This deferral is in contrast to the traditional pre-copy approach, which first copies the memory state over multiple iterations followed by a final transfer of the processor state. The post-copy strategy can provide a "win-win" by reducing total migration time closer to its equivalent time achieved by non-live VM migration. This is done while maintaining the liveness benefits of the pre-copy approach. We compare post-copy extensively against the traditional pre-copy approach on top of the Xen Hypervisor. Using a range of VM workloads we show improvements in several migration metrics including pages transferred, total migration time and network overhead. We facilitate the use of post-copy with adaptive pre-paging in order to eliminate all duplicate page transmissions. Our implementation is able to reduce the number of network-bound page faults to within 21\% of the VM's working set for large workloads. Finally, we eliminate the transfer of free memory pages in both migration schemes through a dynamic self-ballooning (DSB) mechanism. DSB periodically releases free pages in a guest VM back to the hypervisor and significantly speeds up migration with negligible performance degradation.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '09},
 year = {2009},
 isbn = {978-1-60558-375-4},
 location = {Washington, DC, USA},
 pages = {51--60},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1508293.1508301},
 doi = {http://doi.acm.org/10.1145/1508293.1508301},
 acmid = {1508301},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {operating systems, post-copy, process migration, virtual machines, xen},
} 

@inproceedings{Ram:2009:AGU:1508293.1508303,
 author = {Ram, Kaushik Kumar and Santos, Jose Renato and Turner, Yoshio and Cox, Alan L. and Rixner, Scott},
 title = {Achieving 10 Gb/s using safe and transparent network interface virtualization},
 abstract = {This paper presents mechanisms and optimizations to reduce the overhead of network interface virtualization when using the driver domain I/O virtualization model. The driver domain model provides benefits such as support for legacy device drivers and fault isolation. However, the processing overheads incurred in the driver domain to achieve these benefits limit overall I/O performance. This paper demonstrates the effectiveness of two approaches to reduce driver domain overheads. First, Xen is modified to support multi-queue network interfaces to eliminate the software overheads of packet demultiplexing and copying. Second, a grant reuse mechanism is developed to reduce memory protection overheads. These mechanisms shift the bottleneck from the driver domain to the guest domains, improving scalability and enabling significantly higher data rates. This paper also presents and evaluates a series of optimizations that substantially reduce the I/O virtualization overheads in the guest domain. In combination, these mechanisms and optimizations increase the maximum throughput achieved by guest domains from 2.9Gb/s to full 10 Gigabit Ethernet link rates.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '09},
 year = {2009},
 isbn = {978-1-60558-375-4},
 location = {Washington, DC, USA},
 pages = {61--70},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1508293.1508303},
 doi = {http://doi.acm.org/10.1145/1508293.1508303},
 acmid = {1508303},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {device drivers, i/o, networking, performance analysis, virtual machine, virtualization},
} 

@inproceedings{Chang:2009:TWT:1508293.1508304,
 author = {Chang, Mason and Smith, Edwin and Reitmaier, Rick and Bebenita, Michael and Gal, Andreas and Wimmer, Christian and Eich, Brendan and Franz, Michael},
 title = {Tracing for web 3.0: trace compilation for the next generation web applications},
 abstract = {Today's web applications are pushing the limits of modern web browsers. The emergence of the browser as the platform of choice for rich client-side applications has shifted the use of in-browser JavaScript from small scripting programs to large computationally intensive application logic. For many web applications, JavaScript performance has become one of the bottlenecks preventing the development of even more interactive client side applications. While traditional just-in-time compilation is successful for statically typed virtual machine based languages like Java, compiling JavaScript turns out to be a challenging task. Many JavaScript programs and scripts are short-lived, and users expect a responsive browser during page loading. This leaves little time for compilation of JavaScript to generate machine code. We present a trace-based just-in-time compiler for JavaScript that uses run-time profiling to identify frequently executed code paths, which are compiled to executable machine code. Our approach increases execution performance by up to 116\% by decomposing complex JavaScript instructions into a simple Forth-based representation, and then recording the actually executed code path through this low-level IR. Giving developers more computational horsepower enables a new generation of innovative web applications.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '09},
 year = {2009},
 isbn = {978-1-60558-375-4},
 location = {Washington, DC, USA},
 pages = {71--80},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1508293.1508304},
 doi = {http://doi.acm.org/10.1145/1508293.1508304},
 acmid = {1508304},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic compilation, dynamically typed languages, forth, javascript, tamarin, trace trees, tracing, type specialization},
} 

@inproceedings{Frampton:2009:DMH:1508293.1508305,
 author = {Frampton, Daniel and Blackburn, Stephen M. and Cheng, Perry and Garner, Robin J. and Grove, David and Moss, J. Eliot B. and Salishev, Sergey I.},
 title = {Demystifying magic: high-level low-level programming},
 abstract = {The power of high-level languages lies in their abstraction over hardware and software complexity, leading to greater security, better reliability, and lower development costs. However, opaque abstractions are often show-stoppers for systems programmers, forcing them to either break the abstraction, or more often, simply give up and use a different language. This paper addresses the challenge of opening up a high-level language to allow practical low-level programming without forsaking integrity or performance. The contribution of this paper is three-fold: 1) we draw together common threads in a diverse literature, 2) we identify a framework for extending high-level languages for low-level programming, and 3) we show the power of this approach through concrete case studies. Our framework leverages just three core ideas: extending semantics via intrinsic methods, extending types via unboxing and architectural-width primitives, and controlling semantics via scoped semantic regimes. We develop these ideas through the context of a rich literature and substantial practical experience. We show that they provide the power necessary to implement substantial artifacts such as a high-performance virtual machine, while preserving the software engineering benefits of the host language. The time has come for high-level low-level programming to be taken more seriously: 1) more projects now use high-level languages for systems programming, 2) increasing architectural heterogeneity and parallelism heighten the need for abstraction, and 3) a new generation of high-level languages are under development and ripe to be influenced.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '09},
 year = {2009},
 isbn = {978-1-60558-375-4},
 location = {Washington, DC, USA},
 pages = {81--90},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1508293.1508305},
 doi = {http://doi.acm.org/10.1145/1508293.1508305},
 acmid = {1508305},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, intrinsics, jikes rvm, magic, mmtk, systems programming, virtualization, vmmagic},
} 

@inproceedings{Mao:2009:IPI:1508293.1508307,
 author = {Mao, Feng and Zhang, Eddy Z. and Shen, Xipeng},
 title = {Influence of program inputs on the selection of garbage collectors},
 abstract = {Many studies have shown that the best performer among a set of garbage collectors tends to be different for different applications. Researchers have proposed application-specific selection of garbage collectors. In this work, we concentrate on a second dimension of the problem: the influence of program inputs on the selection of garbage collectors. We collect tens to hundreds of inputs for a set of Java benchmarks, and measure their performance on Jikes RVM with different heap sizes and garbage collectors. A rigorous statistical analysis produces four-fold insights. First, inputs influence the relative performance of garbage collectors significantly, causing large variations to the top set of garbage collectors across inputs. Profiling one or few runs is thus inadequate for selecting the garbage collector that works well for most inputs. Second, when the heap size ratio is fixed, one or two types of garbage collectors are enough to stimulate the top performance of the program on all inputs. Third, for some programs, the heap size ratio significantly affects the relative performance of different types of garbage collectors. For the selection of garbage collectors on those programs, it is necessary to have a cross-input predictive model that predicts the minimum possible heap size of the execution on an arbitrary input. Finally, based on regression techniques, we demonstrate the predictability of the minimum possible heap size, indicating the potential feasibility of the input-specific selection of garbage collectors.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '09},
 year = {2009},
 isbn = {978-1-60558-375-4},
 location = {Washington, DC, USA},
 pages = {91--100},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1508293.1508307},
 doi = {http://doi.acm.org/10.1145/1508293.1508307},
 acmid = {1508307},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cross-input program analysis, input-specific selection, minimum possible heap size, profiling, selection of garbage collectors},
} 

@inproceedings{Kim:2009:TVM:1508293.1508308,
 author = {Kim, Hwanju and Lim, Hyeontaek and Jeong, Jinkyu and Jo, Heeseung and Lee, Joonwon},
 title = {Task-aware virtual machine scheduling for I/O performance.},
 abstract = {The use of virtualization is progressively accommodating diverse and unpredictable workloads as being adopted in virtual desktop and cloud computing environments. Since a virtual machine monitor lacks knowledge of each virtual machine, the unpredictableness of workloads makes resource allocation difficult. Particularly, virtual machine scheduling has a critical impact on I/O performance in cases where the virtual machine monitor is agnostic about the internal workloads of virtual machines. This paper presents a task-aware virtual machine scheduling mechanism based on inference techniques using gray-box knowledge. The proposed mechanism infers the I/O-boundness of guest-level tasks and correlates incoming events with I/O-bound tasks. With this information, we introduce partial boosting, which is a priority boosting mechanism with task-level granularity, so that an I/O-bound task is selectively scheduled to handle its incoming events promptly. Our technique focuses on improving the performance of I/O-bound tasks within heterogeneous workloads by lightweight mechanisms with complete CPU fairness among virtual machines. All implementation is confined to the virtualization layer based on the Xen virtual machine monitor and the credit scheduler. We evaluate our prototype in terms of I/O performance and CPU fairness over synthetic mixed workloads and realistic applications.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '09},
 year = {2009},
 isbn = {978-1-60558-375-4},
 location = {Washington, DC, USA},
 pages = {101--110},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1508293.1508308},
 doi = {http://doi.acm.org/10.1145/1508293.1508308},
 acmid = {1508308},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {inference, scheduling, virtual machine, virtualization, xen},
} 

@inproceedings{Weng:2009:HSF:1508293.1508309,
 author = {Weng, Chuliang and Wang, Zhigang and Li, Minglu and Lu, Xinda},
 title = {The hybrid scheduling framework for virtual machine systems},
 abstract = {The virtualization technology makes it feasible that multiple guest operating systems run on a single physical machine. It is the virtual machine monitor that dynamically maps the virtual CPU of virtual machines to physical CPUs according to the scheduling strategy. The scheduling strategy in Xen schedules virtual CPUs of a virtual machines asynchronously while guarantees the proportion of the CPU time corresponding to its weight, maximizing the throughput of the system. However, this scheduling strategy may deteriorate the performance when the virtual machine is used to execute the concurrent applications such as parallel programs or multithreaded programs. In this paper, we analyze the CPU scheduling problem in the virtual machine monitor theoretically, and the result is that the asynchronous CPU scheduling strategy will waste considerable physical CPU time when the system workload is the concurrent application. Then, we present a hybrid scheduling framework for the CPU scheduling in the virtual machine monitor. There are two types of virtual machines in the system: the high-throughput type and the concurrent type. The virtual machine can be set as the concurrent type when the majority of its workload is concurrent applications in order to reduce the cost of synchronization. Otherwise, it is set as the high-throughput type as the default. Moreover, we implement the hybrid scheduling framework based on Xen, and we will give a description of our implementation in details. At last, we test the performance of the presented scheduling framework and strategy based on the multi-core platform, and the experiment result indicates that the scheduling framework and strategy is feasible to improve the performance of the virtual machine system.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '09},
 year = {2009},
 isbn = {978-1-60558-375-4},
 location = {Washington, DC, USA},
 pages = {111--120},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1508293.1508309},
 doi = {http://doi.acm.org/10.1145/1508293.1508309},
 acmid = {1508309},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hybrid scheduling, scheduling strategy, virtualization},
} 

@inproceedings{Shinagawa:2009:BTH:1508293.1508311,
 author = {Shinagawa, Takahiro and Eiraku, Hideki and Tanimoto, Kouichi and Omote, Kazumasa and Hasegawa, Shoichi and Horie, Takashi and Hirano, Manabu and Kourai, Kenichi and Oyama, Yoshihiro and Kawai, Eiji and Kono, Kenji and Chiba, Shigeru and Shinjo, Yasushi and Kato, Kazuhiko},
 title = {BitVisor: a thin hypervisor for enforcing i/o device security},
 abstract = {Virtual machine monitors (VMMs), including hypervisors, are a popular platform for implementing various security functionalities. However, traditional VMMs require numerous components for providing virtual hardware devices and for sharing and protecting system resources among virtual machines (VMs), enlarging the code size of and reducing the reliability of the VMMs. This paper introduces a hypervisor architecture, called parapass-through, designed to minimize the code size of hypervisors by allowing most of the I/O access from the guest operating system (OS) to pass-through the hypervisor, while the minimum access necessary to implement security functionalities is completely mediated by the hypervisor. This architecture uses device drivers of the guest OS to handle devices, thereby reducing the size of components in the hypervisor to provide virtual devices. This architecture also allows to run only single VM on it, eliminating the components for sharing and protecting system resources among VMs. We implemented a hypervisor called BitVisor and a parapass-through driver for enforcing storage encryption of ATA devices based on the parapass-through architecture. The experimental result reveals that the hypervisor and ATA driver require approximately 20 kilo lines of code (KLOC) and 1.4 KLOC respectively.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '09},
 year = {2009},
 isbn = {978-1-60558-375-4},
 location = {Washington, DC, USA},
 pages = {121--130},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1508293.1508311},
 doi = {http://doi.acm.org/10.1145/1508293.1508311},
 acmid = {1508311},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hypervisors, parapass-through, shadow dma descriptor, trusted computing base, virtual machine monitors},
} 

@inproceedings{Chen:2009:CLC:1508293.1508312,
 author = {Chen, Huacai and Jin, Hai and Shao, Zhiyuan and Yu, Ke and Tian, Kun},
 title = {ClientVisor: leverage COTS OS functionalities for power management in virtualized desktop environment},
 abstract = {As an emerging trend, virtualization is more and more widely used in today's computing world. But, the introduc-tion of virtual machines bring trouble for the power man-agement (PM for short), since the operating system can not directly access and control the hardware as before. Solu-tions were proposed to manage the power in the server con-solidation case. However, such solutions are VMM-centric: the VMM gathers the PM decisions of the guests as hints, and makes the final decision to manipulate the hardware. These solutions do not fit well for the virtualized desktop environment, which is highly interactive with the users. In this paper, we propose a novel solution, called Cli-entVisor, to manage the power in the virtualized desktop environment. The key idea of our scheme is to leverage the functionalities of the Commercial-Off-The-Shelf (COTS) operating system, which actually interacts with the user, to manage the power of the processor and the peripheral de-vices in all possible cases. VMM coordinates the PM deci-sions of the guests only at the key points. By prototype implementation and experiments, we find our scheme re-sults in 22\% lower power consumption in the static power usage scenario, and about 8\% lower in the dynamic sce-nario than the corresponding cases of Xen. Moreover, the experimental data shows that the deployment of our scheme will not deteriorate the user experience.},
 booktitle = {Proceedings of the 2009 ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '09},
 year = {2009},
 isbn = {978-1-60558-375-4},
 location = {Washington, DC, USA},
 pages = {131--140},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1508293.1508312},
 doi = {http://doi.acm.org/10.1145/1508293.1508312},
 acmid = {1508312},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {client virtualization, power management, virtual machine},
} 

@inproceedings{Ongaro:2008:SIV:1346256.1346258,
 author = {Ongaro, Diego and Cox, Alan L. and Rixner, Scott},
 title = {Scheduling I/O in virtual machine monitors},
 abstract = {This paper explores the relationship between domain scheduling in avirtual machine monitor (VMM) and I/O performance. Traditionally, VMM schedulers have focused on fairly sharing the processor resources among domains while leaving the scheduling of I/O resources as asecondary concern. However, this can resultin poor and/or unpredictable application performance, making virtualization less desirable for applications that require efficient and consistent I/O behavior. This paper is the first to study the impact of the VMM scheduler on performance using multiple guest domains concurrently running different types of applications. In particular, different combinations of processor-intensive, bandwidth-intensive, andlatency-sensitive applications are run concurrently to quantify the impacts of different scheduler configurations on processor and I/O performance. These applications are evaluated on 11 different scheduler configurations within the Xen VMM. These configurations include a variety of scheduler extensions aimed at improving I/O performance. This cross product of scheduler configurations and application types offers insight into the key problems in VMM scheduling for I/O and motivates future innovation in this area.},
 booktitle = {Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '08},
 year = {2008},
 isbn = {978-1-59593-796-4},
 location = {Seattle, WA, USA},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346256.1346258},
 doi = {http://doi.acm.org/10.1145/1346256.1346258},
 acmid = {1346258},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {machine virtualization, network I/O, scheduling policy, server consolidation, xen},
} 

@inproceedings{Kim:2008:ISC:1346256.1346259,
 author = {Kim, Kangho and Kim, Cheiyol and Jung, Sung-In and Shin, Hyun-Sup and Kim, Jin-Soo},
 title = {Inter-domain socket communications supporting high performance and full binary compatibility on Xen},
 abstract = {Communication performance between two processes in their own domains on the same physical machine gets improved but it does not reach our expectation. This paper presents the design and implementation of high-performance inter-domain communication mechanism, called XWAY, that maintains binary compatibility for applications written in standard socket interface. As a result of our survey, we found that three overheads mainly contribute to the poor performance; those are TCP/IP processing cost in each domain, page flipping overhead, and long communication path between both sides of a socket. XWAY achieves high performance by bypassing TCP/IP stacks, avoiding page flipping overhead, and providing a direct, accelerated communication path between domains in the same machine. Moreover, we introduce the XWAY socket architecture to support full binary compatibility with as little effort as possible. We implemented our design on Xen 3.0.3-0 with Linux kernel 2.6.16.29, and evaluated basic performance, the speed of file transfer, DBT-1 benchmark, and binary compatibility using binary image of real socket applications. In our tests, we have proved that XWAY realizes the high performance that is comparable to UNIX domain socket and ensures full binary compatibility. The basic performance of XWAY, measured with netperf, shows minimum latency of 15.6 usec and peak bandwidth of 4.7Gbps, which is superior to that of native TCP socket. We have also examined whether several popular applications using TCP socket can be executed on XWAY with their own binary images. Those applications worked perfectly well.},
 booktitle = {Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '08},
 year = {2008},
 isbn = {978-1-59593-796-4},
 location = {Seattle, WA, USA},
 pages = {11--20},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346256.1346259},
 doi = {http://doi.acm.org/10.1145/1346256.1346259},
 acmid = {1346259},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Xen, high performance, socket binary compatibility, socket interface, virtual machine},
} 

@inproceedings{Apparao:2008:CAS:1346256.1346260,
 author = {Apparao, Padma and Iyer, Ravi and Zhang, Xiaomin and Newell, Don and Adelmeyer, Tom},
 title = {Characterization \& analysis of a server consolidation benchmark},
 abstract = {Virtualization is already becoming ubiquitous in data centers for the consolidation of multiple workloads on a single platform. However, there are very few performance studies of server consolidation workloads in the literature. In this paper, our goal is to analyze the performance characteristics of a representative server consolidation workload. To address this goal, we have carried out extensive measurement and profiling experiments of a newly proposed consolidation workload (vConsolidate). vConsolidate consists of a compute intensive workload, a web server, a mail server and a database application running simultaneously on a single platform. We start by studying the performance slowdown of each workload due to consolidation on a contemporary multi-core dual-processor Intel platform. We then look at architectural characteristics such as CPI (cycles per instruction) and L2 MP (L2 misses per instruction) I, and analyze the benefits of larger caches for such a consolidated workload. We estimate the virtualization overheads for events such as context switches, interrupts and page faults and show how these impact the performance of the workload in consolidation. Finally, we also present the execution profile of the server consolidation workload and illustrate the life of each VM in the consolidated environment. We conclude by presenting an approach to developing a preliminary performance model based on the performance.},
 booktitle = {Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '08},
 year = {2008},
 isbn = {978-1-59593-796-4},
 location = {Seattle, WA, USA},
 pages = {21--30},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346256.1346260},
 doi = {http://doi.acm.org/10.1145/1346256.1346260},
 acmid = {1346260},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache scaling, data center virtualization, vConsolidate, virtualization overheads, xen},
} 

@inproceedings{Kumar:2008:NVM:1346256.1346261,
 author = {Kumar, Sanjay and Schwan, Karsten},
 title = {Netchannel: a VMM-level mechanism for continuous, transparentdevice access during VM migration},
 abstract = {Efficient and seamless access to I/O devices during VM migration coupled with the ability to dynamically change the mappings of virtual to physical devices are required in multiple settings, including blade-servers, datacenters, and even in home-based personal computing environments. This paper develops a general solution for these problems, at a level of abstraction transparent to guest VMs and their device drivers. A key part of this solution is a novel VMM-level abstraction that transparently handles pending I/O transactions, termed Netchannel. Netchannel provides for (1) virtual device migration and device hot-swapping for networked as well as locally attached devices, and (2) remote access to devices not directly attached to networks via transparent device remoting, an example being a disk locally present on a bladeserver node. A Xen-based prototype of Netchannel demonstrates these capabilities for block and for USB devices, for both bulk and isochronous USB access methods. Within the same administrative domain, seamless access to these devices is maintained during live VM migration and during device hot-swapping. Experimental evaluations with micro-benchmarks and with representative server applications exhibit comparable performance for Netchannel-based remote vs. local devices.},
 booktitle = {Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '08},
 year = {2008},
 isbn = {978-1-59593-796-4},
 location = {Seattle, WA, USA},
 pages = {31--40},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346256.1346261},
 doi = {http://doi.acm.org/10.1145/1346256.1346261},
 acmid = {1346261},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {I/O devices, live VM migration, virtualization},
} 

@inproceedings{Merrill:2008:TFS:1346256.1346263,
 author = {Merrill, Duane and Hazelwood, Kim},
 title = {Trace fragment selection within method-based JVMs},
 abstract = {Java virtual machines have historically employed either a "wholemethod" or a "trace" methodology for selecting regions of code for optimization. Adaptive whole-method optimization primarily leverages intra-procedural optimizations derived from classic static compilation techniques whereas trace optimization utilizes an interpreter to select, manage, and dispatch inter-procedural fragments of frequently executed code. In this paper we present our hybrid approach for supplementing the comprehensive strengths of a whole-method JIT compiler with the inter-procedural refinement of trace fragment selection and show that that the two techniques would be mutually beneficial. Using the "interpreterless" Jikes RVM as a foundation, we use our trace profiling subsystem to identify an application's working set as a collection of hot traces and show that there is a significant margin for improvement in instruction ordering that can be addressed by trace execution. Our benchmark hot-trace profiles indicate that 20\% of transitions between machine-code basic blocks as laid out by the JIT compiler are non-contiguous, many of which are transfers of control flow to locations outside of the current virtual memory page. Additionally, the analyses performed by the adaptive whole-method JIT compiler allow for better identification of trace starting and stopping locations, an improvement over the popular next-executing-tail (NET) trace selection scheme. We show minimal overhead for trace selection indicating that inter-procedural trace execution provides an opportunity to improve both instruction locality as well as compiler-directed branch prediction without significant run-time cost.},
 booktitle = {Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '08},
 year = {2008},
 isbn = {978-1-59593-796-4},
 location = {Seattle, WA, USA},
 pages = {41--50},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346256.1346263},
 doi = {http://doi.acm.org/10.1145/1346256.1346263},
 acmid = {1346263},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {JIT compilation, JVM, region selection, traces},
} 

@inproceedings{Cheadle:2008:MSV:1346256.1346264,
 author = {Cheadle, A. M. and Field, A. J. and Nystrom-Persson, J.},
 title = {A method specialisation and virtualised execution environment for Java},
 abstract = {We present a virtualisation and method specialisation framework for Java that facilitates efficient, dynamic modification of the behaviour of object accesses at run time. The technique works by virtualising all method calls and field accesses associated with selected classes so that all corresponding object accesses are via the invocation of a virtual method. Different access behaviours are then supported by allowing arbitrary specialisations of those methods to be defined. The virtualisation overheads are partially recovered by allowing the JVM's optimisation subsystem to perform guarded inlining of specialised methods. We describe an implementation based on the Jikes RVM and show how the framework can be used to implement an 'implicit' readbarrier that supports incremental garbage collection. The performance overhead of full virtualisation, and the performance of the implicit read barrier compared with an existing conventional, explicit barrier, are evaluated using SPEC JVM98 and DaCapo benchmarks. The net virtualisation costs are shown to be remarkably low and the implicit barrier is shown to outperform the explicit barrier substantially in most cases. Other potential applications, including object proxying, caching, and relocation, and instrumentation are also discussed.},
 booktitle = {Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '08},
 year = {2008},
 isbn = {978-1-59593-796-4},
 location = {Seattle, WA, USA},
 pages = {51--60},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346256.1346264},
 doi = {http://doi.acm.org/10.1145/1346256.1346264},
 acmid = {1346264},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {incremental garbage collection, language implementation, memory management, method virtualization, read barrier},
} 

@inproceedings{Bruening:2008:PPC:1346256.1346265,
 author = {Bruening, Derek and Kiriansky, Vladimir},
 title = {Process-shared and persistent code caches},
 abstract = {Software code caches are increasingly being used to amortizethe runtime overhead of tools such as dynamic optimizers, simulators, and instrumentation engines. The additional memory consumed by these caches, along with the data structures used to manage them, limits the scalability of dynamic tool deployment. Inter-process sharing of code caches significantly improves the ability to efficiently apply code caching tools to many processes simultaneously. In this paper, we present a method of code cache sharing among processes for dynamic tools operating on native applications. Our design also supports code cache persistence for improved cold code execution in short-lived processes or long initialization sequences. Sharing raises security concerns, and we show how to achieve sharing without risk of privilege escalation and with read-only code caches and associated data structures. We evaluate process-shared and persisted code caches implemented in the DynamoRIO industrial-strength dynamic instrumentation engine, where we achieve a two-thirds reduction in both memory usage and startup time.},
 booktitle = {Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '08},
 year = {2008},
 isbn = {978-1-59593-796-4},
 location = {Seattle, WA, USA},
 pages = {61--70},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346256.1346265},
 doi = {http://doi.acm.org/10.1145/1346256.1346265},
 acmid = {1346265},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary translation, dynamic instrumentation, software code cache, tool scalability},
} 

@inproceedings{Yang:2008:UHP:1346256.1346267,
 author = {Yang, Jisoo and Shin, Kang G.},
 title = {Using hypervisor to provide data secrecy for user applications on a per-page basis},
 abstract = {Hypervisors are increasingly utilized in modern computer systems, ranging from PCs to web servers and data centers. Aside from server applications, hypervisors are also becoming a popular target for implementing many security systems, since they provide a small and easy-to-secure trusted computing base. This paper presents a novel way of using hypervisors to protect application data privacy even when the underlying operating system is not trustable. Each page in virtual address space is rendered to user applications according to the security context the application is running in. The hypervisor encrypts and decrypts each memory page requested depending on the application's access permission to the page. The main result of this system is the complete removal of the operating system from the trust base for user applications' data privacy. To reduce the runtime overhead of the system, two optimization techniques are employed. We use page-frame replication to reduce the number ofcryptographic operations by keeping decrypted versions of a page frame. We also employ lazy synchronization to minimize overhead due to an update to one of the replicated page frame. Our system is implemented and evaluated by modifying the Xen hypervisor, showing that it increases the application execution time only by 3\% for CPU and memory-intensive workloads.},
 booktitle = {Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '08},
 year = {2008},
 isbn = {978-1-59593-796-4},
 location = {Seattle, WA, USA},
 pages = {71--80},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346256.1346267},
 doi = {http://doi.acm.org/10.1145/1346256.1346267},
 acmid = {1346267},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {application protection, data privacy, hypervisor, virtualization},
} 

@inproceedings{Rosenblum:2008:VMC:1346256.1346268,
 author = {Rosenblum, Nathan E. and Cooksey, Gregory and Miller, Barton P.},
 title = {Virtual machine-provided context sensitive page mappings},
 abstract = {Context sensitive page mappings provide different mappings from virtual addresses to physical page frames depending on whether a memory reference occurs in a data or instruction context. Such differences can be used to modify the behavior of programs that reference their executable code in a data context. Previous work has demonstrated several applications of context sensitive page mappings, including protection against buffer-overrun attacks and circumvention of self-checksumming codes. We extend context sensitive page mappings to the virtual machine monitor, allowing operation independent of the guest operating system. Our technique takes advantage of the VMM's role in enforcing protection between guest operating systems to interpose on guest OS memory management operations and selectively introduce context sensitive page mappings. In this paper, we describe extensions to the Xen hypervisor that support context sensitive page mappings in unmodified guest operating systems. We demonstrate the utility of our technique in a case study by instrumenting and modifying self-checksumming tamper-resistant binaries. We further demonstrate that context sensitive page mappings can be provided by the VMM without incurring extensive overhead. Our measurements indicate only minor performance penalties stem from use of this technique. We suggest several further applications of VMM-provided context sensitive page mappings, including OS hardening and protection of processes from malicious applications.},
 booktitle = {Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '08},
 year = {2008},
 isbn = {978-1-59593-796-4},
 location = {Seattle, WA, USA},
 pages = {81--90},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346256.1346268},
 doi = {http://doi.acm.org/10.1145/1346256.1346268},
 acmid = {1346268},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {context sensitive memory, self checksumming code, virtual machine monitor, xen},
} 

@inproceedings{Jones:2008:VHP:1346256.1346269,
 author = {Jones, Stephen T. and Arpaci-Dusseau, Andrea C. and Arpaci-Dusseau, Remzi H.},
 title = {VMM-based hidden process detection and identification using Lycosid},
 abstract = {Use of stealth rootkit techniques to hide long-lived malicious processes is a current and alarming security issue. In this paper, we describe, implement, and evaluate a novel VMM-based hidden process detection and identification service called Lycosid that is based on the cross-view validation principle. Like previous VMM-based security services, Lycosid benefits from its protected location. In contrast top revious VMM-based hidden process detectors, Lycosid obtains guest process information implicitly. Using implicit information reduces its susceptibility to guest evasion attacks and decouples it from specific guest operating system versions and patch levels. The implicit information Lycosid depends on, however, can be noisy and unreliable. Statistical inference techniques like hypothesis testing and line arregression allow Lycosid to trade time for accuracy. Despite low quality inputs, Lycosid provides a robust, highly accurate service usable even insecurity environments where the consequences for wrong decisions can behig.},
 booktitle = {Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '08},
 year = {2008},
 isbn = {978-1-59593-796-4},
 location = {Seattle, WA, USA},
 pages = {91--100},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346256.1346269},
 doi = {http://doi.acm.org/10.1145/1346256.1346269},
 acmid = {1346269},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {inference, security, virtual machine},
} 

@inproceedings{Jansen:2008:PEC:1346256.1346271,
 author = {Jansen, Bernhard and Ramasamy, Hari-Govind V. and Schunter, Matthias},
 title = {Policy enforcement and compliance proofs for Xen virtual machines},
 abstract = {We address the problem of integrity management in a virtualized environment. We introduce a formal integrity model for managing the integrity of arbitrary aspects of a virtualized system. Based on the model, we describe an architecture called PEV, which stands for protection, enforcement, and verification. The architecture generalizes the integrity management functions of the Trusted Platform Module (TPM) to cover not just software binaries, but also VMs, virtual devices, and a wide range of security policies. The architecture enables the verification of security compliance and enforcement of security policies. We describe a prototype implementation of the architecture based on the Xen hypervisor. We demonstrate the policy enforcement and compliance checking capabilities of our prototype through multiple use cases.},
 booktitle = {Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '08},
 year = {2008},
 isbn = {978-1-59593-796-4},
 location = {Seattle, WA, USA},
 pages = {101--110},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346256.1346271},
 doi = {http://doi.acm.org/10.1145/1346256.1346271},
 acmid = {1346271},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {integrity management, trusted computing, virtual machines},
} 

@inproceedings{Reimer:2008:OBB:1346256.1346272,
 author = {Reimer, Darrell and Thomas, Arun and Ammons, Glenn and Mummert, Todd and Alpern, Bowen and Bala, Vasanth},
 title = {Opening black boxes: using semantic information to combat virtual machine image sprawl},
 abstract = {Virtual-machine images are currently distributed as disk-image files, which are files that mirror the content of physical disks. This format is convenient for the virtual machine monitors that execute these images. However, it is not well-suited for administering images because storing images as disk-image files forces administrators to maintain the software on images with the same tools that they use to maintain the software on physical machines. Already, these tools cannot cope with "physical server sprawl"; in the future, because images can be snapshotted and cloned easily, enterprises that migrate from physical machines to images will need tools that scale to cope with the larger problem of "virtual-machine image sprawl. To address this problem, this paper proposes the Mirage image format (MIF), a new storage format that exposes the rich semantic information currently buried in disk-image files. Disk-image files contain a mapping from file name to file content (and file metadata). MIF decouples this mapping into a manifest that maps file names to content descriptors (and file metadata) and a store that holds the content. Each image has its own manifest and a store may contain content for many images. As with disk-image files, images in MIF fully encapsulate application state including all software dependences. In addition, conversion between MIF and traditional disk-image formats is easy. This paper shows, through examples, that MIF makes some typical software management tasks--inventory control, customized deployment, and image update--faster and easier. The general technique is to operate on manifests instead of on content whenever possible. These tasks can be performed without starting images and, because manifests are simpler and orders of magnitude smaller than disk-image files, without accessing large amounts of data.},
 booktitle = {Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '08},
 year = {2008},
 isbn = {978-1-59593-796-4},
 location = {Seattle, WA, USA},
 pages = {111--120},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346256.1346272},
 doi = {http://doi.acm.org/10.1145/1346256.1346272},
 acmid = {1346272},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {deployment, installation, management, virtualization},
} 

@inproceedings{Dunlap:2008:ERM:1346256.1346273,
 author = {Dunlap, George W. and Lucchetti, Dominic G. and Fetterman, Michael A. and Chen, Peter M.},
 title = {Execution replay of multiprocessor virtual machines},
 abstract = {Execution replay of virtual machines is a technique which has many important applications, including debugging, fault-tolerance, and security. Execution replay for single processor virtual machines is well-understood, and available commercially. With the advancement of multi-core architectures, however, multiprocessor virtual machines are becoming more important. Our system, SMP-ReVirt, is the first system to log and replay a multiprocessor virtual machine on commodity hardware. We use hardware page protection to detect and accurately replay sharing between virtual cpus of a multi-cpu virtual machine, allowing us to replay the entire operating system and all applications. We have tested our system on a variety of workloads, and find that although sharing under SMP-ReVirt is expensive, for many workloads and applications, including debugging, the overhead is acceptable.},
 booktitle = {Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '08},
 year = {2008},
 isbn = {978-1-59593-796-4},
 location = {Seattle, WA, USA},
 pages = {121--130},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346256.1346273},
 doi = {http://doi.acm.org/10.1145/1346256.1346273},
 acmid = {1346273},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ReVirt, Xen, determinism, direct memory access, execution replay, hardware page protections, multiprocessors, multithreading, virtual machines},
} 

@inproceedings{Joisha:2008:PAN:1346256.1346275,
 author = {Joisha, Pramod G.},
 title = {A principled approach to nondeferred reference-counting garbage collection},
 abstract = {Nondeferred reference-counting (RC) garbage collection is among the oldest memory-management methods. Despite offering unique advantages, little attention has been paid on how to correctly implement it for modern programming languages. This paper revisits this collection method and describes how to implement it for a modern object-oriented language in an optimizing compiler. The main contribution is a general algorithm that realizes one form of nondeferred RC collection for an object-oriented language having features such as exceptions, interior pointers, and object pinning. The algorithm abstracts the pertinent characteristics of instructions using concepts from data-flow analysis, such as def/use information, so that instructions are handled in a uniform manner, instead of in an ad hoc or special-case way. The abstracted information is used to systematically compute what increments and decrements to do, even in the presence of subtle conditions such as exceptional control flow. These techniques enabled us to compile a large suite of programs to use nondeferred RC collection. The paper also discusses the modifications that were necessary in the compiler for supporting the inserted RC operations, and reports measurements from a reference implementation.},
 booktitle = {Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '08},
 year = {2008},
 isbn = {978-1-59593-796-4},
 location = {Seattle, WA, USA},
 pages = {131--140},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346256.1346275},
 doi = {http://doi.acm.org/10.1145/1346256.1346275},
 acmid = {1346275},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {instrumentation, reference counting, static analysis},
} 

@inproceedings{Cher:2008:CGU:1346256.1346276,
 author = {Cher, Chen-Yong and Gschwind, Michael},
 title = {Cell GC: using the cell synergistic processor as a garbage collection coprocessor},
 abstract = {In recent years, scaling of single-core superscalar processor performance has slowed due to complexity and power considerations. To improve program performance, designs are increasingly adopting chip multiprocessing with homogeneous or heterogeneous CMPs. By trading off features from a modern aggressive superscalar core, CMPs often offer better scaling characteristics in terms of aggregate performance, complexity and power, but often require additional software investment to rewrite, retune or recompile programs to take advantage of the new designs. The Cell Broadband Engine is a modern example of a heterogeneous CMP with coprocessors (accelerators) which can be found in supercomputers (Roadrunner), blade servers (IBM QS20/21), and video game consoles (SCEI PS3). A Cell BE processor has a host Power RISC processor (PPE) and eight Synergistic Processor Elements (SPE), each consisting of a Synergistic Processor Unit (SPU) and Memory Flow Controller (MFC). In this work, we explore the idea of offloading Automatic Dynamic Garbage Collection (GC) from the host processor onto accelerator processors using the coprocessor paradigm. Offloading part or all of GC to a coprocessor offers potential performance benefits, because while the coprocessor is running GC, the host processor can continue running other independent, more general computations. . We implement BDW garbage collection on a Cell system and offload the mark phase to the SPE co-processor. We show mark phase execution on the SPE accelerator to be competitive with execution on a full fledged PPE processor. We also explore object-based and block-based caching strategies for explicitly managed memory hierarchies, and explore to effectiveness of several prefetching schemes in the context of garbage collection. Finally, we implement Capitulative Loads using the DMA by extending software caches and quantify its performance impact on the coprocessor.},
 booktitle = {Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '08},
 year = {2008},
 isbn = {978-1-59593-796-4},
 location = {Seattle, WA, USA},
 pages = {141--150},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346256.1346276},
 doi = {http://doi.acm.org/10.1145/1346256.1346276},
 acmid = {1346276},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {BDW, SPE, SPU, accelerator, cell, coprocessor, explicitly managed memory hierarchies, garbage collection, local store, mark-sweep},
} 

@inproceedings{Murray:2008:IXS:1346256.1346278,
 author = {Murray, Derek Gordon and Milos, Grzegorz and Hand, Steven},
 title = {Improving Xen security through disaggregation},
 abstract = {Virtual machine monitors (VMMs) have been hailed as the basis for an increasing number of reliable or trusted computing systems. The Xen VMM is a relatively small piece of software -- a hypervisor -- that runs at a lower level than a conventional operating system in order to provide isolation between virtual machines: its size is offered as an argument for its trustworthiness. However, the management of a Xen-based system requires a privileged, full-blown operating system to be included in the trusted computing base (TCB). In this paper, we introduce our work to disaggregate the management virtual machine in a Xen-based system. We begin by analysing the Xen architecture and explaining why the status quo results in a large TCB. We then describe our implementation, which moves the domain builder, the most important privileged component, into a minimal trusted compartment. We illustrate how this approach may be used to implement "trusted virtualisation" and improve the security of virtual TPM implementations. Finally, we evaluate our approach in terms of the reduction in TCB size, and by performing a security analysis of the disaggregated system.},
 booktitle = {Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '08},
 year = {2008},
 isbn = {978-1-59593-796-4},
 location = {Seattle, WA, USA},
 pages = {151--160},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346256.1346278},
 doi = {http://doi.acm.org/10.1145/1346256.1346278},
 acmid = {1346278},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {disaggregation, trusted computing base, virtual machines},
} 

@inproceedings{Okumura:2008:RJV:1346256.1346279,
 author = {Okumura, Takashi and Childers, Bruce R. and Mosse, Daniel},
 title = {Running a Java VM inside an operating system kernel},
 abstract = {Operating system extensions have been shown to be beneficial to implement custom kernel functionality. In most implementations, the extensions are made by an administrator with kernel loadable modules. An alternative approach is to provide a run-time system within the operating system itself that can execute user kernel extensions. In this paper, we describe such an approach,where a lightweight Java virtual machine is embedded within the kernel for flexible extension of kernel network I/O. For this purpose, we first implemented a compact Java Virtual Machine with a Just-In-Time compiler on the Intel IA32 instruction set architecture at the user space. Then, the virtual machine was embedded onto the FreeBSDoperating system kernel. We evaluate the system to validate the model, with systematic benchmarking.},
 booktitle = {Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '08},
 year = {2008},
 isbn = {978-1-59593-796-4},
 location = {Seattle, WA, USA},
 pages = {161--170},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346256.1346279},
 doi = {http://doi.acm.org/10.1145/1346256.1346279},
 acmid = {1346279},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java virtual machine, just-in-time, kernel extensibility, network management},
} 

@inproceedings{Yu:2008:AFV:1346256.1346280,
 author = {Yu, Yang and Kolam, Hariharan and Lam, Lap-Chung and Chiueh, Tzi-cker},
 title = {Applications of a feather-weight virtual machine},
 abstract = {A Feather-weight Virtual Machine (FVM) is an OS-level virtualization technology that enables multiple isolated execution environments to exist on a single Windows kernel. The key design goal of FVM is efficient resource sharing among VMs so as to minimize VM startup/shutdown cost and scale to a larger number of concurrent VM instances. As a result, FVM provides an effective platform for fault-tolerant and intrusion-tolerant applications that require frequent invocation and termination of dispensable VMs. This paper presents three complete applications of the FVM technology: scalable web site testing; shared binary service for application deployment and distributed Display-Only File Server (DOFS). To identify malicious web sites that exploit browser vulnerabilities, we use a web crawler to access untrusted sites, render their pages in multiple browsers each running in a separate VM, and monitor their execution behaviors. To allow Windows-based end user machines to share binaries that are stored, managed and patched on a central location, we run shared binaries in a special VM on the end user machine whose runtime environment is imported from the central binary server. To protect confidential files in a file server against information theft by insiders, we ensure that file viewing/editing programs run in a VM, which grants file content display but prevents file content from being saved on the host machine. In this paper, we show how to customize the generic FVM framework to accommodate the needs of the three applications, and present experimental results that demonstrate their performance and effectiveness.},
 booktitle = {Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '08},
 year = {2008},
 isbn = {978-1-59593-796-4},
 location = {Seattle, WA, USA},
 pages = {171--180},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1346256.1346280},
 doi = {http://doi.acm.org/10.1145/1346256.1346280},
 acmid = {1346280},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary server, browser exploit, information theft, virtual machine, web crawler},
} 

@inproceedings{Bacon:2011:VAH:1952682.1952684,
 author = {Bacon, David F.},
 title = {Virtualization in the age of heterogeneous machines},
 abstract = {Since their invention over 40 years ago, virtual machines have been used to virtualize one or more von Neumann processors and their associated peripherals. System virtual machines provide the illusion that the user has their own instance of a physical machine with a given instruction set architecture (ISA). Process virtual machines provide the illusion of running on a synthetic architecture independent of the underlying ISA, generally for the purpose of supporting a high-level language. To continue the historical trend of exponential increase in computational power in the face of limits on clock frequency scaling, we must find ways to harness the inherent parallelism of billions of transistors. I contend that multi-core chips are a fatally flawed approach - instead, maximum performance will be achieved by using heterogeneous chips and systems that combine customized and customizable computational substrates that achieve very high performance by closely matching the computational and communications structures of the application at hand.  Such chips might look like a mashup of a conventional multicore, a GPU, an FPGA, some ASICs, and a DSP. But programming them with current technologies would be nightmarishly complex, portability would be lost, and innovation between chip generations would be severely limited. The answer (of course) is virtualization, and at both the device level and the language level.  In this talk I will illustrate some challenges and potential solutions in the context of IBM's Liquid Metal project, in which we are designing a new high-level language (Lime) and compiler/runtime technology to virtualize the underlying computational devices by providing a uniform semantic model.  I will also discuss problems (and opportunities) that this raises at the operating system and data center levels, particularly with computational elements like FPGAs for which "context switching" is currently either extremely expensive or simply impossible.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {1--2},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1952682.1952684},
 doi = {http://doi.acm.org/10.1145/1952682.1952684},
 acmid = {1952684},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {heterogeneous hardware, virtual machine},
} 

@inproceedings{Du:2011:PPV:1952682.1952686,
 author = {Du, Jiaqing and Sehrawat, Nipun and Zwaenepoel, Willy},
 title = {Performance profiling of virtual machines},
 abstract = {Profilers based on hardware performance counters are indispensable for performance debugging of complex software systems. All modern processors feature hardware performance counters, but current virtual machine monitors (VMMs) do not properly expose them to the guest operating systems. Existing profiling tools require privileged access to the VMM to profile the guest and are only available for VMMs based on paravirtualization. Diagnosing performance problems of software running in a virtualized environment is therefore quite difficult. This paper describes how to extend VMMs to support performance profiling. We present two types of profiling in a virtualized environment: guest-wide profiling and system-wide profiling. Guest-wide profiling shows the runtime behavior of a guest. The profiler runs in the guest and does not require privileged access to the VMM. System-wide profiling exposes the runtime behavior of both the VMM and any number of guests. It requires profilers both in the VMM and in those guests. Not every VMM has the right architecture to support both types of profiling. We determine the requirements for each of them, and explore the possibilities for their implementation in virtual machines using hardware assistance, paravirtualization, and binary translation. We implement both guest-wide and system-wide profiling for a VMM based on the x86 hardware virtualization extensions and system-wide profiling for a VMM based on binary translation. We demonstrate that these profilers provide good accuracy with only limited overhead.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {3--14},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1952682.1952686},
 doi = {http://doi.acm.org/10.1145/1952682.1952686},
 acmid = {1952686},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary translation, hardware-assisted virtualization, paravirtualization, performance profiling, virtual machine},
} 

@inproceedings{Nikolaev:2011:PFP:1952682.1952687,
 author = {Nikolaev, Ruslan and Back, Godmar},
 title = {Perfctr-Xen: a framework for performance counter virtualization},
 abstract = {Virtualization is a powerful technique used for variety of application domains, including emerging cloud environments that provide access to virtual machines as a service. Because of the interaction of virtual machines with multiple underlying software and hardware layers, the analysis of the performance of applications running in virtualized environments has been difficult. Moreover, performance analysis tools commonly used in native environments were not available in virtualized environments, a gap which our work closes. This paper discusses the challenges of performance monitoring inherent to virtualized environments and introduces a technique to virtualize access to low-level performance counters on a per-thread basis. The technique was implemented in perfctr-xen, a framework for the Xen hypervisor that provides an infrastructure for higher-level profilers. This framework supports both accumulative event counts and interrupt-driven event sampling. It is light-weight, providing direct user mode access to logical counter values. perfctr-xen supports multiple modes of virtualization, including paravirtualization and hardware-assisted virtualization. perfctr-xen applies guest kernel-hypervisor coordination techniques to reduce virtualization overhead. We present experimental results based on microbenchmarks and SPEC CPU2006 macrobenchmarks that show the accuracy and usability of the obtained measurements when compared to native execution.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {15--26},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1952682.1952687},
 doi = {http://doi.acm.org/10.1145/1952682.1952687},
 acmid = {1952687},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hpctoolkit, papi, perfctr, profilers, virtual machine monitors, xen},
} 

@inproceedings{Zhao:2011:DCC:1952682.1952688,
 author = {Zhao, Qin and Koh, David and Raza, Syed and Bruening, Derek and Wong, Weng-Fai and Amarasinghe, Saman},
 title = {Dynamic cache contention detection in multi-threaded applications},
 abstract = {In today's multi-core systems, cache contention due to true and false sharing can cause unexpected and significant performance degradation. A detailed understanding of a given multi-threaded application's behavior is required to precisely identify such performance bottlenecks. Traditionally, however, such diagnostic information can only be obtained after lengthy simulation of the memory hierarchy. In this paper, we present a novel approach that efficiently analyzes interactions between threads to determine thread correlation and detect true and false sharing. It is based on the following key insight: although the slowdown caused by cache contention depends on factors including the thread-to-core binding and parameters of the memory hierarchy, the amount of data sharing is primarily a function of the cache line size and application behavior. Using memory shadowing and dynamic instrumentation, we implemented a tool that obtains detailed sharing information between threads without simulating the full complexity of the memory hierarchy. The runtime overhead of our approach --- a 5x slowdown on average relative to native execution --- is significantly less than that of detailed cache simulation. The information collected allows programmers to identify the degree of cache contention in an application, the correlation among its threads, and the sources of significant false sharing. Using our approach, we were able to improve the performance of some applications up to a factor of 12x. For other contention-intensive applications, we were able to shed light on the obstacles that prevent their performance from scaling to many cores.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {27--38},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1952682.1952688},
 doi = {http://doi.acm.org/10.1145/1952682.1952688},
 acmid = {1952688},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache contention, dynamic instrumentation, false sharing, shadow memory},
} 

@inproceedings{Wang:2011:RVM:1952682.1952690,
 author = {Wang, Kun and Rao, Jia and Xu, Cheng-Zhong},
 title = {Rethink the virtual machine template},
 abstract = {Server virtualization technology facilitates the creation of an elastic computing infrastructure on demand. There are cloud applications like server-based computing and virtual desktop that concern startup latency and require impromptu requests for VM creation in a real-time manner. Conventional template-based VM creation is a time consuming process and lacks flexibility for the deployment of stateful VMs. In this paper, we present an abstraction of VM substrate to represent generic VM instances in miniature. Unlike templates that are stored as an image file in disk, VM substrates are docked in memory in a designated VM pool. They can be activated into stateful VMs without machine booting and application initialization. The abstraction leverages an arrange of techniques, including VM miniaturization, generalization, clone and migration, storage copy-on-write, and on-the-fly resource configuration, for rapid deployment of VMs and VM clusters on demand. We implement a prototype on a Xen platform and show that a server with typical configuration of TB disk and GB memory can accommodate more substrates in memory than templates in disk and stateful VMs can be created from the same or different substrates and deployed on to the same or different physical hosts in a cluster without causing any configuration conflicts. Experimental results show that general purpose VMs or a VM cluster for parallel computing can be deployed in a few seconds. We demonstrate the usage of VM substrates in a mobile gaming application.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {39--50},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1952682.1952690},
 doi = {http://doi.acm.org/10.1145/1952682.1952690},
 acmid = {1952690},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cloud computing, data center, virtual machine deployment, virtual machine template},
} 

@inproceedings{Cecchet:2011:DVD:1952682.1952691,
 author = {Cecchet, Emmanuel and Singh, Rahul and Sharma, Upendra and Shenoy, Prashant},
 title = {Dolly: virtualization-driven database provisioning for the cloud},
 abstract = {Cloud computing platforms are becoming increasingly popular for e-commerce applications that can be scaled on-demand in a very cost effective way. Dynamic provisioning is used to autonomously add capacity in multi-tier cloud-based applications that see workload increases. While many solutions exist to provision tiers with little or no state in applications, the database tier remains problematic for dynamic provisioning due to the need to replicate its large disk state. In this paper, we explore virtual machine (VM) cloning techniques to spawn database replicas and address the challenges of provisioning shared-nothing replicated databases in the cloud. We argue that being able to determine state replication time is crucial for provisioning databases and show that VM cloning provides this property. We propose Dolly, a database provisioning system based on VM cloning and cost models to adapt the provisioning policy to the cloud infrastructure specifics and application requirements. We present an implementation of Dolly in a commercial-grade replication middleware and evaluate database provisioning strategies for a TPC-W workload on a private cloud and on Amazon EC2. By being aware of VM-based state replication cost, Dolly can solve the challenge of automated provisioning for replicated databases on cloud platforms.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {51--62},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1952682.1952691},
 doi = {http://doi.acm.org/10.1145/1952682.1952691},
 acmid = {1952691},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {autonomic provisioning, database, virtualization},
} 

@inproceedings{Le:2011:REV:1952682.1952692,
 author = {Le, Michael and Tamir, Yuval},
 title = {ReHype: enabling VM survival across hypervisor failures},
 abstract = {With existing virtualized systems, hypervisor failures lead to overall system failure and the loss of all the work in progress of virtual machines (VMs) running on the system. We introduce ReHype, a mechanism for recovery from hypervisor failures by booting a new instance of the hypervisor while preserving the state of running VMs. VMs are stalled during the hypervisor reboot and resume normal execution once the new hypervisor instance is running. Hypervisor failures can lead to arbitrary state corruption and inconsistencies throughout the system. ReHype deals with the challenge of protecting the recovered hypervisor instance from such corrupted state and resolving inconsistencies between different parts of hypervisor state as well as between the hypervisor and VMs and between the hypervisor and the hardware. We have implemented ReHype for the Xen hypervisor. The implementation was done incrementally, using results from fault injection experiments to identify the sources of dangerous state corruption and inconsistencies. The implementation of ReHype involved only 880 LOC added or modified in Xen. The memory space overhead of ReHype is only 2.1MB for a pristine copy of the hypervisor code and static data plus a small reserved memory area. The fault injection campaigns used to evaluate the effectiveness of ReHype involved a system with multiple VMs running I/O and hypercall-intensive benchmarks. Our experimental results show that the ReHype prototype can successfully recover from over 90\% of detected hypervisor failures.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {63--74},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1952682.1952692},
 doi = {http://doi.acm.org/10.1145/1952682.1952692},
 acmid = {1952692},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {microreboot, recovery, reliability, virtualization, vmm, xen},
} 

@inproceedings{Park:2011:FSV:1952682.1952694,
 author = {Park, Eunbyung and Egger, Bernhard and Lee, Jaejin},
 title = {Fast and space-efficient virtual machine checkpointing},
 abstract = {Checkpointing, i.e., recording the volatile state of a virtual machine (VM) running as a guest in a virtual machine monitor (VMM) for later restoration, includes storing the memory available to the VM. Typically, a full image of the VM's memory along with processor and device states are recorded. With guest memory sizes of up to several gigabytes, the size of the checkpoint images becomes more and more of a concern. In this work we present a technique for fast and space-efficient checkpointing of virtual machines. In contrast to existing methods, our technique eliminates redundant data and stores only a subset of the VM's memory pages. Our technique transparently tracks I/O operations of the guest to external storage and maintains a list of memory pages whose contents are duplicated on non-volatile storage. At a checkpoint, these pages are excluded from the checkpoint image. We have implemented the proposed technique for paravirtualized as well as fully-virtualized guests in the Xen VMM. Our experiments with a paravirtualized guest (Linux) and two fullyvirtualized guests (Linux, Windows) show a significant reduction in the size of the checkpoint image as well as the time required to complete the checkpoint. Compared to the current Xen implementation, we achieve, on average, an 81\% reduction in the stored data and a 74\% reduction in the time required to take a checkpoint for the paravirtualized Linux guest. In a fully-virtualized environment runningWindows and Linux guests, we achieve a 64\% reduction of the image size along with a 62\% reduction in checkpointing time.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {75--86},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1952682.1952694},
 doi = {http://doi.acm.org/10.1145/1952682.1952694},
 acmid = {1952694},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {checkpointing, virtual machine},
} 

@inproceedings{Zhang:2011:FRC:1952682.1952695,
 author = {Zhang, Irene and Garthwaite, Alex and Baskakov, Yury and Barr, Kenneth C.},
 title = {Fast restore of checkpointed memory using working set estimation},
 abstract = {In order to make save and restore features practical, saved virtual machines (VMs) must be able to quickly restore to normal operation. Unfortunately, fetching a saved memory image from persistent storage can be slow, especially as VMs grow in memory size. One possible solution for reducing this time is to lazily restore memory after the VM starts. However, accesses to unrestored memory after the VM starts can degrade performance, sometimes rendering the VM unusable for even longer. Existing performance metrics do not account for performance degradation after the VM starts, making it difficult to compare lazily restoring memory against other approaches. In this paper, we propose both a better metric for evaluating the performance of different restore techniques and a better scheme for restoring saved VMs. Existing performance metrics do not reflect what is really important to the user -- the time until the VM returns to normal operation. We introduce the time-to-responsiveness metric, which better characterizes user experience while restoring a saved VM by measuring the time until there is no longer a noticeable performance impact on the restoring VM. We propose a new lazy restore technique, called working set restore, that minimizes performance degradation after the VM starts by prefetching the working set. We also introduce a novel working set estimator based on memory tracing that we use to test working set restore, along with an estimator that uses access-bit scanning. We show that working set restore can improve the performance of restoring a saved VM by more than 89\% for some workloads.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {87--98},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1952682.1952695},
 doi = {http://doi.acm.org/10.1145/1952682.1952695},
 acmid = {1952695},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {checkpoint/restore, performance},
} 

@inproceedings{Kourai:2011:FCP:1952682.1952696,
 author = {Kourai, Kenichi},
 title = {Fast and correct performance recovery of operating systems using a virtual machine monitor},
 abstract = {Rebooting an operating system is a final but effective recovery technique. However, the system performance largely degrades just after the reboot due to the page cache being lost in the main memory. For fast performance recovery, we propose a new reboot mechanism called the warm-cache reboot. The warm-cache reboot preserves the page cache during the reboot and enables an operating system to restore it after the reboot, with the help of a virtual machine monitor (VMM). To perform correct recovery, the VMM guarantees that the reused page cache is consistent with the corresponding files on disks. We have implemented the warm-cache reboot mechanism in the Xen VMM and the Linux operating system. Our experimental results showed that the warm-cache reboot decreased performance degradation just after the reboot. In addition, we confirmed that the file cache corrupted by faults was not reused. The overheads for maintaining cache consistency were not usually large.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {99--110},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1952682.1952696},
 doi = {http://doi.acm.org/10.1145/1952682.1952696},
 acmid = {1952696},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache consistency, page cache, performance degradation, reboot},
} 

@inproceedings{Svard:2011:EDC:1952682.1952698,
 author = {Sv\"{a}rd, Petter and Hudzia, Benoit and Tordsson, Johan and Elmroth, Erik},
 title = {Evaluation of delta compression techniques for efficient live migration of large virtual machines},
 abstract = {Despite the widespread support for live migration of Virtual Machines (VMs) in current hypervisors, these have significant shortcomings when it comes to migration of certain types of VMs. More specifically, with existing algorithms, there is a high risk of service interruption when migrating VMs with high workloads and/or over low-bandwidth networks. In these cases, VM memory pages are dirtied faster than they can be transferred over the network, which leads to extended migration downtime. In this contribution, we study the application of delta compression during the transfer of memory pages in order to increase migration throughput and thus reduce downtime. The delta compression live migration algorithm is implemented as a modification to the KVM hypervisor. Its performance is evaluated by migrating VMs running different type of workloads and the evaluation demonstrates a significant decrease in migration downtime in all test cases. In a benchmark scenario the downtime is reduced by a factor of 100. In another scenario a streaming video server is live migrated with no perceivable downtime to the clients while the picture is frozen for eight seconds using standard approaches. Finally, in an enterprise application scenario, the delta compression algorithm successfully live migrates a very large system that fails after migration using the standard algorithm. Finally, we discuss some general effects of delta compression on live migration and analyze when it is beneficial to use this technique.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {111--120},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1952682.1952698},
 doi = {http://doi.acm.org/10.1145/1952682.1952698},
 acmid = {1952698},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compression, live migration, performance evaluation, virtualization},
} 

@inproceedings{Wood:2011:CDP:1952682.1952699,
 author = {Wood, Timothy and Ramakrishnan, K. K. and Shenoy, Prashant and van der Merwe, Jacobus},
 title = {CloudNet: dynamic pooling of cloud resources by live WAN migration of virtual machines},
 abstract = {Virtual machine technology and the ease with which VMs can be migrated within the LAN, has changed the scope of resource management from allocating resources on a single server to manipulating pools of resources within a data center. We expect WAN migration of virtual machines to likewise transform the scope of provisioning compute resources from a single data center to multiple data centers spread across the country or around the world. In this paper we present the CloudNet architecure as a cloud framework consisting of cloud computing platforms linked with a VPN based network infrastructure to provide seamless and secure connectivity between enterprise and cloud data center sites. To realize our vision of efficiently pooling geographically distributed data center resources, CloudNet provides optimized support for live WAN migration of virtual machines. Specifically, we present a set of optimizations that minimize the cost of transferring storage and virtual machine memory during migrations over low bandwidth and high latency Internet links. We evaluate our system on an operational cloud platform distributed across the continental US. During simultaneous migrations of four VMs between data centers in Texas and Illinois, CloudNet's optimizations reduce memory migration time by 65\% and lower bandwidth consumption for the storage and memory transfer by 19GB, a 50\% reduction.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {121--132},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1952682.1952699},
 doi = {http://doi.acm.org/10.1145/1952682.1952699},
 acmid = {1952699},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cloud computing, virtualization, wan migration},
} 

@inproceedings{Zheng:2011:WLS:1952682.1952700,
 author = {Zheng, Jie and Ng, Tze Sing Eugene and Sripanidkulchai, Kunwadee},
 title = {Workload-aware live storage migration for clouds},
 abstract = {The emerging open cloud computing model will provide users with great freedom to dynamically migrate virtualized computing services to, from, and between clouds over the wide-area. While this freedom leads to many potential benefits, the running services must be minimally disrupted by the migration. Unfortunately, current solutions for wide-area migration incur too much disruption as they will significantly slow down storage I/O operations during migration. The resulting increase in service latency could be very costly to a business. This paper presents a novel storage migration scheduling algorithm that can greatly improve storage I/O performance during wide-area migration. Our algorithm is unique in that it considers individual virtual machine's storage I/O workload such as temporal locality, spatial locality and popularity characteristics to compute an efficient data transfer schedule. Using a fully implemented system on KVM and a trace-driven framework, we show that our algorithm provides large performance benefits across a wide range of popular virtual machine workloads.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {133--144},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1952682.1952700},
 doi = {http://doi.acm.org/10.1145/1952682.1952700},
 acmid = {1952700},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cloud computing, live storage migration, scheduling, virtual machine, workload-aware},
} 

@inproceedings{Litty:2011:PAI:1952682.1952702,
 author = {Litty, Lionel and Lie, David},
 title = {Patch auditing in infrastructure as a service clouds},
 abstract = {A basic requirement of a secure computer system is that it be up to date with regard to software security patches. Unfortunately, Infrastructure as a Service (IaaS) clouds make this difficult. They leverage virtualization, which provides functionality that causes traditional security patch update systems to fail. In addition, the diversity of operating systems and the distributed nature of administration in the cloud compound the problem of identifying unpatched machines. In this work, we propose P2, a hypervisor-based patch audit solution. P2 audits VMs and detects the execution of unpatched binary and non-binary files in an accurate, continuous and OSagnostic manner. Two key innovations make P2 possible. First, P2 uses efficient information flow tracking to identify the use of unpatched non-binary files in a vulnerable way.We performed a patch survey and discover that 64\% of files modified by security updates do not contain binary code, making the audit of non-binary files crucial. Second, P2 implements a novel algorithm that identifies binaries in mid-execution to allow handling of VMs resumed from a checkpoint or migrated into the cloud. We have implemented a prototype of P2 and and our experiments show that it accurately reports the execution of unpatched code while imposing performance overhead of 4\%.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1952682.1952702},
 doi = {http://doi.acm.org/10.1145/1952682.1952702},
 acmid = {1952702},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {application discovery, cloud computing, infrastructure as a service, patch management, virtualization},
} 

@inproceedings{Payer:2011:FUS:1952682.1952703,
 author = {Payer, Mathias and Gross, Thomas R.},
 title = {Fine-grained user-space security through virtualization},
 abstract = {This paper presents an approach to the safe execution of applications based on software-based fault isolation</i> and policy-based system call authorization</i>. A running application is encapsulated in an additional layer of protection using dynamic binary translation in user-space. This virtualization layer dynamically recompiles the machine code and adds multiple dynamic security guards that verify the running code to protect and contain the application. The binary translation system redirects all system calls to a policy-based system call authorization framework. This interposition framework validates every system call based on the given arguments and the location of the system call. Depending on the user-loadable policy and an extensible handler mechanism the framework decides whether a system call is allowed, rejected, or redirect to a specific user-space handler in the virtualization layer. This paper offers an in-depth analysis of the different security guarantees and a performance analysis of libdetox, a prototype of the full protection platform. The combination of software-based fault isolation</i> and policy-based system call authorization</i> imposes only low overhead and is therefore an attractive option to encapsulate and sandbox applications to improve host security.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {157--168},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1952682.1952703},
 doi = {http://doi.acm.org/10.1145/1952682.1952703},
 acmid = {1952703},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic binary translation, dynamic instrumentation, optimization, policy-based system call authorization, process sandboxing, security, user-space software virtualization, virtualization},
} 

@inproceedings{Lange:2011:MVL:1952682.1952705,
 author = {Lange, John R. and Pedretti, Kevin and Dinda, Peter and Bridges, Patrick G. and Bae, Chang and Soltero, Philip and Merritt, Alexander},
 title = {Minimal-overhead virtualization of a large scale supercomputer},
 abstract = {Virtualization has the potential to dramatically increase the usability and reliability of high performance computing (HPC) systems. However, this potential will remain unrealized unless overheads can be minimized. This is particularly challenging on large scale machines that run carefully crafted HPC OSes supporting tightly-coupled, parallel applications. In this paper, we show how careful use of hardware and VMM features enables the virtualization of a large-scale HPC system, specifically a Cray XT4 machine, with < = 5\% overhead on key HPC applications, microbenchmarks, and guests at scales of up to 4096 nodes. We describe three techniques essential for achieving such low overhead: passthrough I/O, workload-sensitive selection of paging mechanisms, and carefully controlled preemption. These techniques are forms of symbiotic virtualization, an approach on which we elaborate.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {169--180},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1952682.1952705},
 doi = {http://doi.acm.org/10.1145/1952682.1952705},
 acmid = {1952705},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {high performance computing, parallel computing, virtual machine monitors},
} 

@inproceedings{Xia:2011:VWB:1952682.1952706,
 author = {Xia, Lei and Kumar, Sanjay and Yang, Xue and Gopalakrishnan, Praveen and Liu, York and Schoenberg, Sebastian and Guo, Xingang},
 title = {Virtual WiFi: bring virtualization from wired to wireless},
 abstract = {As virtualization trend is moving towards "client virtualization", wireless virtualization remains to be one of the technology gaps that haven't been addressed satisfactorily. Today's approaches are mainly developed for wired network, and are not suitable for virtualizing wireless network interface due to the fundamental differences between wireless and wired LAN devices that we will elaborate in this paper. We propose a wireless LAN virtualization approach named virtual WiFi that addresses the technology gap. With our proposed solution, the full wireless LAN functionalities are supported inside virtual machines; each virtual machine can establish its own connection with self-supplied credentials; and multiple separate wireless LAN connections are supported through one physical wireless LAN network interface. We designed and implemented a prototype for our proposed virtual WiFi approach, and conducted detailed performance study. Our results show that with conventional virtualization overhead mitigation mechanisms, our proposed approach can support fully functional wireless functions inside VM, and achieve close to native performance of Wireless LAN with moderately increased CPU utilization.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {181--192},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1952682.1952706},
 doi = {http://doi.acm.org/10.1145/1952682.1952706},
 acmid = {1952706},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hypervisor, performance, virtualization, wifi, wireless},
} 

@inproceedings{Lange:2011:SSV:1952682.1952707,
 author = {Lange, John R. and Dinda, Peter},
 title = {SymCall: symbiotic virtualization through VMM-to-guest upcalls},
 abstract = {Symbiotic virtualization is a new approach to system virtualization in which a guest OS targets the native hardware interface as in full system virtualization, but also optionally exposes a software interface that can be used by a VMM, if present, to increase performance and functionality. Neither the VMM nor the OS needs to support the symbiotic virtualization interface to function together, but if both do, both benefit. We describe the design and implementation of the SymCall symbiotic virtualization interface in our publicly available Palacios VMM for modern x86 machines. SymCall makes it possible for Palacios to make clean synchronous upcalls into a symbiotic guest, much like system calls. One use of symcalls is to allow synchronous collection of semantically rich guest data during exit handling in order to enable new VMM features. We describe the implementation of SwapBypass, a VMM service based on SymCall that reconsiders swap decisions made by a symbiotic Linux guest. Finally, we present a detailed performance evaluation of both SwapBypass and SymCall.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {193--204},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1952682.1952707},
 doi = {http://doi.acm.org/10.1145/1952682.1952707},
 acmid = {1952707},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {operating systems, virtual machine monitors},
} 

@inproceedings{Williams:2011:OHM:1952682.1952709,
 author = {Williams, Dan and Jamjoom, Hani and Liu, Yew-Huey and Weatherspoon, Hakim},
 title = {Overdriver: handling memory overload in an oversubscribed cloud},
 abstract = {With the intense competition between cloud providers, oversubscription is increasingly important to maintain profitability. Oversubscribing physical resources is not without consequences: it increases the likelihood of overload. Memory overload is particularly damaging. Contrary to traditional views, we analyze current data center logs and realistic Web workloads to show that overload is largely transient: up to 88.1\% of overloads last for less than 2 minutes. Regarding overload as a continuum that includes both transient and sustained overloads of various durations points us to consider mitigation approaches also as a continuum, complete with tradeoffs with respect to application performance and data center overhead. In particular, heavyweight techniques, like VM migration, are better suited to sustained overloads, whereas lightweight approaches, like network memory, are better suited to transient overloads. We present Overdriver, a system that adaptively takes advantage of these tradeoffs, mitigating all overloads within 8\% of well-provisioned performance. Furthermore, under reasonable oversubscription ratios, where transient overload constitutes the vast majority of overloads, Overdriver requires 15\% of the excess space and generates a factor of four less network traffic than a migration-only approach.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {205--216},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1952682.1952709},
 doi = {http://doi.acm.org/10.1145/1952682.1952709},
 acmid = {1952709},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cloud computing, network memory, resource oversubscription, virtualization, vm migration},
} 

@inproceedings{Wang:2011:SHM:1952682.1952710,
 author = {Wang, Xiaolin and Zang, Jiarui and Wang, Zhenlin and Luo, Yingwei and Li, Xiaoming},
 title = {Selective hardware/software memory virtualization},
 abstract = {As virtualization becomes a key technique for supporting cloud computing, much effort has been made to reduce virtualization overhead, so a virtualized system can match its native performance. One major overhead is due to memory or page table virtualization. Conventional virtual machines rely on a shadow mechanism to manage page tables, where a shadow page table maintained by the VMM (Virtual Machine Monitor) maps virtual addresses to machine addresses while a guest maintains its own virtual to physical page table. This shadow mechanism will result in expensive VM exits whenever there is a page fault that requires synchronization between the two page tables. To avoid this cost, both Intel and AMD provide hardware assists, EPT (extended page table) and NPT (nested page table), to facilitate address translation. With the hardware assists, the MMU (Memory Management Unit) maintains an ordinary guest page table that translates virtual addresses to guest physical addresses. In addition, the extended page table as provided by EPT translates from guest physical addresses to host physical or machine addresses. NPT works in a similar style. With EPT or NPT, a guest page fault can be handled by the guest itself without triggering VM exits. However, the hardware assists do have their disadvantage compared to the conventional shadow mechanism -- the page walk yields more memory accesses and thus longer latency. Our experimental results show that neither hardware-assisted paging (HAP) nor shadow paging (SP) can be a definite winner. Despite the fact that in over half of the cases, there is no noticeable gap between the two mechanisms, an up to 34\% performance gap exists for a few benchmarks. We propose a dynamic switching mechanism that monitors TLB misses and guest page faults on the fly, and dynam-ically switches between the two paging modes. Our experiments show that this new mechanism can match and, sometimes, even beat the better performance of HAP and SP.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {217--226},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1952682.1952710},
 doi = {http://doi.acm.org/10.1145/1952682.1952710},
 acmid = {1952710},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic switching, hardware assisted paging, hardware-assisted virtualization, shadow paging, virtual machine},
} 

@inproceedings{Roy:2011:HBR:1952682.1952711,
 author = {Roy, Amitabha and Hand, Steven and Harris, Tim},
 title = {Hybrid binary rewriting for memory access instrumentation},
 abstract = {Memory access instrumentation is fundamental to many applications such as software transactional memory systems, profiling tools and race detectors. We examine the problem of efficiently instrumenting memory accesses in x86 machine code to support software transactional memory and profiling. We aim to automatically instrument all shared memory accesses in critical sections of x86 binaries</i>, while achieving overhead close to that obtained when performing manual instrumentation at the source code level. The two primary options in building such an instrumentation system are static and dynamic binary rewriting: the former instruments binaries at link time before execution, while the latter binary rewriting instruments binaries at runtime. Static binary rewriting offers extremely low overhead but is hampered by the limits of static analysis. Dynamic binary rewriting is able to use runtime information but typically incurs higher overhead. This paper proposes an alternative: hybrid binary rewriting. Hybrid binary rewriting is built around the idea of a persistent instrumentation cache (PIC) that is associated with a binary and contains instrumented code from it. It supports two execution modes when using instrumentation: active and passive modes. In the active execution mode, a dynamic binary rewriting engine (PIN) is used to intercept execution, and generate instrumentation into the PIC, which is an on-disk file. This execution mode can take full advantage of runtime information. Later, passive execution can be used where instrumented code is executed out of the PIC. This allows us to attain overheads similar to those incurred with static binary rewriting. This instrumentation methodology enables a variety of static and dynamic techniques to be applied. For example, in passive mode, execution occurs directly from the original executable save for regions that require instrumentation. This has allowed us to build a low-overhead transactional memory profiler. We also demonstrate how we can use the combination of static and dynamic techniques to eliminate instrumentation for accesses to locations that are thread-private.},
 booktitle = {Proceedings of the 7th ACM SIGPLAN/SIGOPS international conference on Virtual execution environments},
 series = {VEE '11},
 year = {2011},
 isbn = {978-1-4503-0687-4},
 location = {Newport Beach, California, USA},
 pages = {227--238},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1952682.1952711},
 doi = {http://doi.acm.org/10.1145/1952682.1952711},
 acmid = {1952711},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary rewriting, transactional memory},
} 

@inproceedings{Kawachiya:2007:CJN:1254810.1254812,
 author = {Kawachiya, Kiyokuni and Ogata, Kazunori and Silva, Daniel and Onodera, Tamiya and Komatsu, Hideaki and Nakatani, Toshio},
 title = {Cloneable JVM: a new approach to start isolated java applications faster},
 abstract = {Java has been successful particularly for writing applications in the server environment. However, isolation</i> of multiple applications hasnot been efficiently achieved in Java. Many customers require that their applications are guarded by independent OS processes, but starting a Java application with a new process results in a long sequence of initializations being repeated each time. To date, there has been no way to quickly start a new Java application as an isolated OS process. In this paper, we propose a new isolation approach called Cloneable JVM</i> to eliminate this startup overhead in Java. The key idea is to createa new Java application by copying, or cloning, the already-initialized image of the primary JVM process. Since the clone is already initialized, it can begin actual operations immediately as a new isolated process. This cloning abstraction can support new scenarios for Java, such as user isolation and transaction isolation. We implemented a prototype of the Cloneable JVM by modifying a production JVM on Linux, which provides a new API for cloning constructed on the Isolate API defined in JSR 121. Using this cloning API, several Java applications, including a large production J2EE application server, we remodified to demonstrate the isolation scenarios. Evaluations using these prototypes showed that new ready-to-serve Java applications can start up as a new process in less than 5 seconds, which is 4 to 170 times faster than starting these applications from scratch.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {1--11},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1254810.1254812},
 doi = {http://doi.acm.org/10.1145/1254810.1254812},
 acmid = {1254812},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cloning, isolation, java, startup overhead},
} 

@inproceedings{Wimmer:2007:AFO:1254810.1254813,
 author = {Wimmer, Christian and M\"{o}ssenb\"{o}ck, Hanspeter},
 title = {Automatic feedback-directed object inlining in the java hotspot\&\#8482; virtual machine},
 abstract = {Object inlining</i> is an optimization that embeds certain referenced objects into their referencing object. It reduces the costs of field accesses by eliminating unnecessary field loads. The order of objects in the heap is changed in such a way that objects that are accessed together are placed next to each other in memory so that their offset is fixed, i.e. the objects are colocated</i>. This allows field loads to be replaced by address arithmetic. We implemented this optimization for Sun Microsystems' Java HotSpot\&#8482; VM. The analysis is performed automatically at run time, requires no actions on the part of the programmer and supports dynamic class loading. We use read barriers to detect the most frequently accessed fields that are worth being optimized. To safely eliminate a field load, the colocation of the object that holds the field and the object that is referenced by the field must be guaranteed. Two preconditions must be satisfied for a field before it is optimized: the objects must be allocated together, and the field must not be overwritten later. These preconditions are checked by the just-in-time compiler to avoid a global data flow analysis. The garbage collector ensures that groups of colocated objects are not split: it copies groups as a whole to their new locations. The evaluation shows that our dynamic approach successfully identifies and optimizes frequently accessed fields for several benchmarks. The improved peak performance of 9\% in average for SPECjvm98 (with a maximum speedup of 51\%) justifies the startup overhead of 3\% in average (with a maximum slowdown of 11\%) that is caused mainly by the read barriers and the additional compilation of methods.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {12--21},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1254810.1254813},
 doi = {http://doi.acm.org/10.1145/1254810.1254813},
 acmid = {1254813},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache, garbage collection, java, just-in-time compilation, object colocation, object inlining, optimization, performance},
} 

@inproceedings{Su:2007:SOU:1254810.1254814,
 author = {Su, Lixin and Lipasti, Mikko H.},
 title = {Speculative optimization using hardware-monitored guarded regions for java virtual machines},
 abstract = {Aggressive dynamic optimization in high-performance Java Virtual Machines can be hampered by language features like Java's exception model, which requires precise detection and handling of program-generated exceptions. Furthermore, the compile-time overhead of guaranteeing correctness of code transformations precludes many effective optimizations from consideration. This paper describes a novel approach for circumventing the optimization-crippling effects of exception semantics and streamlining the implementation of aggressive optimizations at run time. Under a hardware-software hybrid model, the runtime system delineates guarded regions of code and specifies a contract--in the simplest case, one that requires exception-free execution--that must be adhered to in order to ensure that the aggressively optimized code within that region will behave as the programmer expects. The contracted runtime condition is assumed to be true, and code within a guarded region is aggressively optimized based on this assumption. Hardware monitors for exceptions throughout the region execution, and undoes the effects of the guarded region if an exception occurs, re-executing the region with a conventionally optimized version. Since exceptions are very rare, code can be optimized as if optimization-crippling conditions did not exist, leading to compile time reduction, code quality improvement, and potential performance improvement up to 67.7\% and averaging 15.9\% in our limit study of a set of Java benchmarks.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {22--32},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1254810.1254814},
 doi = {http://doi.acm.org/10.1145/1254810.1254814},
 acmid = {1254814},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {java, precise exceptions, speculative processors, transactional memory, virtual machines},
} 

@inproceedings{Lagar-Cavilla:2007:VGA:1254810.1254816,
 author = {Lagar-Cavilla, H. Andres and Tolia, Niraj and Satyanarayanan, M. and de Lara, Eyal},
 title = {VMM-independent graphics acceleration},
 abstract = {We have designed and implemented VMGL, a virtual machine monitor (VMM) independent, graphics processing unit (GPU) independent, and cross-platform OpenGL virtualization solution. VMGL allows applications executing within virtual machines (VMs) to leverage hardware rendering acceleration, thus solving a problem that has limited virtualization of a growing class of graphics-intensive applications. VMGL also provides applications running within VMs with suspend and resume capabilities across GPUs from different vendors. Our experimental results from a number of graphics-intensive applications show that VMGL provides excellent rendering performance, within 14\% or better of that obtained with native graphics hardware acceleration. Further, VMGL's performance is two orders of magnitude better than that of software rendering, the commonly available alternative today for graphics-intensive applications running in virtualized environments. Our results confirm VMGL's portability across VMware Workstation and Xen (on VT and non-VT hardware), and across Linux (with and without paravirtualization), FreeBSD, and Solaris. Our results also show that the resource demands of VMGL align well with the emerging trend of multi-core processors.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {33--43},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1254810.1254816},
 doi = {http://doi.acm.org/10.1145/1254810.1254816},
 acmid = {1254816},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {VMM-independence, graphics, hardware acceleration, portability, virtualization},
} 

@inproceedings{Ammons:2007:LLO:1254810.1254817,
 author = {Ammons, Glenn and Appavoo, Jonathan and Butrico, Maria and Da Silva, Dilma and Grove, David and Kawachiya, Kiyokuni and Krieger, Orran and Rosenburg, Bryan and Van Hensbergen, Eric and Wisniewski, Robert W.},
 title = {Libra: a library operating system for a jvm in a virtualized execution environment},
 abstract = {If the operating system could be specialized for every application, many applications would run faster. For example, Java virtual machines (JVMs) provide their own threading model and memory protection, so general-purpose operating system implementations of these abstractions are redundant. However, traditional means of transforming existing systems into specialized systems are difficult to adopt because they require replacing the entire operating system. This paper describes Libra, an execution environment specialized for IBM's J9 JVM. Libra does not replace the entire operating system. Instead, Libra and J9 form a single statically-linked image that runs in a hypervisor partition. Libra provides the services necessary to achieve good performance for the Java workloads of interest but relies on an instance of Linux in another hypervisor partition to provide a networking stack, a filesystem, and other services. The expense of remote calls is offset by the fact that Libra's services can be customized for a particular workload; for example, on the Nutch search engine, we show that two simple customizations improve application throughput by a factor of 2.7.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {44--54},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1254810.1254817},
 doi = {http://doi.acm.org/10.1145/1254810.1254817},
 acmid = {1254817},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {JVM, exokernels, virtualization, xen},
} 

@inproceedings{Yamada:2007:FTO:1254810.1254818,
 author = {Yamada, Hiroshi and Kono, Kenji},
 title = {FoxyTechnique: tricking operating system policies with a virtual machine monitor},
 abstract = {Integrating new resource management policies into operating systems (OSes) is an ongoing process. Despite innovative policy proposals being developed, it is quite difficult to deploy a new one widely because it is difficult, costly and often impractical endeavor to modify existing OSes to integrate a new policy. To address this problem, we explore the possibility of using virtual machine technology to incorporate a new policy into an existing OS without the need to make any changes to it. This paper describes FoxyTechnique</i>, which virtualizes physical devices differently</i> from real ones and tricks a guest OS into producing a behavior similar to a desired policy. FoxyTechnique offers several advantages. First, it allows us to implement a new policy without the need to make any changes to OS kernels. Second, Foxy-based policies are expected to be portable across different operating systems because they are isolated from guest OSes by stable virtual hardware interfaces. Finally, Foxy-based policies sometimes outperform guest OS policies because they can measure performance indicators more accurately than guest OSes. To demonstrate the usefulness of FoxyTechnique, we conducted two case studies, FoxyVegas and FoxyIdle, on the Xen virtual machine monitor. FoxyVegas and FoxyIdle tricked the original Linux and successfully mimicked TCP Vegas and Idletime scheduling, respectively.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {55--64},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1254810.1254818},
 doi = {http://doi.acm.org/10.1145/1254810.1254818},
 acmid = {1254818},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {interference, resource management, virtual machine},
} 

@inproceedings{Nethercote:2007:SBM:1254810.1254820,
 author = {Nethercote, Nicholas and Seward, Julian},
 title = {How to shadow every byte of memory used by a program},
 abstract = {Several existing dynamic binary analysis tools use shadowmemory</i>-they shadow, in software, every byte of memory used by a program with another value that says something about it. Shadow memory is difficult to implement both efficiently and robustly. Nonetheless, existing shadow memory implementations have not been studied in detail. This is unfortunate, because shadow memory is powerful-for example, some of the existing tools that use it detect critical errors such as bad memory accesses, data races, and uses of uninitialised or untrusted data. In this paper we describe the implementation of shadow memory in Memcheck, a popular memory checker built with Valgrind, a dynamic binary instrumentation framework. This implementation has several novel features that make it efficient: carefully chosen data structures and operations result in a mean slow-down factor of only 22.2 and moderate memory usage. This may sound slow, but we show it is 8.9 times faster and 8.5 times smaller on average than a naive implementation, and shadow memory operations account for only about half of Memcheck's execution time. Equally importantly, unlike some tools, Memcheck's shadow memory implementation is robust: it is used on Linux by thousands of programmers on sizeable programs such as Mozilla and OpenOffice, and is suited to almost any memory configuration. This is the first detailed description of a robust shadow memory implementation, and the first detailed experimental evaluation of any shadow memory implementation. The ideas within are applicable to any shadow memory tool built with any instrumentation framework.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {65--74},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1254810.1254820},
 doi = {http://doi.acm.org/10.1145/1254810.1254820},
 acmid = {1254820},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic binary analysis, dynamic binary instrumentation, memcheck, shadow memory, valgrind},
} 

@inproceedings{Kagstrom:2007:CEL:1254810.1254821,
 author = {K{\aa}gstr\"{o}m, Simon and Grahn, H{\aa}kan and Lundberg, Lars},
 title = {Cibyl: an environment for language diversity on mobile devices},
 abstract = {With an estimated installation base of around 1 billion units, the Java J2ME platform is one of the largest development targets available. For mobile devices, J2ME is often the only available environment. For the very large body of software written in C other languages, this means difficult and costly porting to another language to support J2ME devices. This paper presents the Cibyl programming environment which allows existing code written in C and other languages supported by GCC to be recompiled into Java bytecode and run with close to native Java performance on J2ME devices. Cibyl translates compiled MIPS binaries into Java bytecode. In contrast to other approaches, Cibyl supports the full C language, is based on unmodified standard tools, and does not rely on source code conversion. To achieve good performance, Cibyl employs extensions to the MIPS architecture to support low-overhead calls to native Java functionality and use knowledge of the MIPS ABI to avoid computing unused values and transfer unnecessary registers. An evaluation on multiple virtual machines shows that Cibyl achieves performance similar to native Java, with results ranging from a slowdown of around 2 to a speedup of over 9 depending on the JVM and the benchmark.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {75--82},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1254810.1254821},
 doi = {http://doi.acm.org/10.1145/1254810.1254821},
 acmid = {1254821},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {J2ME, binary translation, portability, programming environment},
} 

@inproceedings{Zaleski:2007:YGE:1254810.1254823,
 author = {Zaleski, Mathew and Brown, Angela Demke and Stoodley, Kevin},
 title = {YETI: a graduallY extensible trace interpreter},
 abstract = {The design of new programming languages benefits from interpretation, which can provide a simple initial implementation, flexibility to explore new language features, and portability to many platforms. The only downside is speed of execution, as there remains a large performance gap between even efficient interpreters and mixed-mode systems that include a just-in-time compiler (or JIT</i> for short). Augmenting an interpreter with a JIT, however, is not a small task. Today, JITs used for Java\&#8482; are loosely-coupled with the interpreter, with callsites of methods being the only transition point between interpreted and native code. To compile whole methods, the JIT must duplicate a sizable amount of functionality already provided by the interpreter, leading to a "big bang" development effort before the JIT can be deployed. Instead, adding a JIT to an interpreter would be easier if it were possible to leverage the existing functionality. In earlier work we showed that packaging virtual instructions as lightweight callable routines is an efficient way to build an interpreter. In this paper we describe how callable bodies help our interpreter to efficiently identify and run traces. Our closely coupled dynamic compiler can fall back on the interpreter in various ways, permitting an incremental approach in which additional performance gains can be realized as it is extended in two dimensions: (i) generating code for more types of virtual instructions, and (ii) identifying larger compilation units. Currently, Yeti identifies straight line regions of code and traces, and generates non-optimized code for roughly 50 Java integer and object bytecodes. Yeti runs roughly twice as fast as a direct-threaded interpreter on SPECjvm98 benchmarks.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {83--93},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1254810.1254823},
 doi = {http://doi.acm.org/10.1145/1254810.1254823},
 acmid = {1254823},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {JIT compiler, interpreter, mixed-mode, trace},
} 

@inproceedings{Kulkarni:2007:DCB:1254810.1254824,
 author = {Kulkarni, Prasad and Arnold, Matthew and Hind, Michael},
 title = {Dynamic compilation: the benefits of early investing},
 abstract = {Dynamic compilation is typically performed in a separate thread, asynchronously with the remaining application threads. This compilation thread is often scheduled for execution in a simple round-robin fashion either by the operating system or by the virtual machine itself. Despite the popularity of this approach in production virtual machines, it has a number of shortcomings that can lead to suboptimal performance. This paper explores a number of issues surrounding asynchronous dynamic compilation in a virtual machine. We begin by describing the shortcomings of current approaches and demonstrate their potential to perform poorly under certain conditions. We describe the importance of enforcing a minimum level of utilization for the compilation thread, and evaluate the performance implications of varying the utilization that is enforced. We observed surprisingly large speedups by increasing the priority of the compilation thread, averaging 18.2\% improvement over a large benchmark suite. Finally, we discuss options for implementing these techniques in a VM and address relevant issues when moving from a single-processor to a multiprocessor machine.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {94--104},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1254810.1254824},
 doi = {http://doi.acm.org/10.1145/1254810.1254824},
 acmid = {1254824},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, dynamic compilation, virtual machines},
} 

@inproceedings{Seelam:2007:VIS:1254810.1254826,
 author = {Seelam, Seetharami R. and Teller, Patricia J.},
 title = {Virtual I/O scheduler: a scheduler of schedulers for performance virtualization},
 abstract = {Virtualized storage systems are required to service concurrently executing workloads, with potentially diverse data delivery requirements, that are running under multiple operating systems. Although a number of algorithms have been developed for I/O performance virtualization among operating system (OS) instances and their applications, none results in absolute performance virtualization. By absolute performance virtualization we mean that the performance experienced by applications of one operating system does not suffer due to variations in the I/O request stream characteristics of applications of other operating systems. Key requirements of I/O performance virtualization are fairness and performance isolation. In this paper, we present a novel virtual I/O scheduler (VIOS) that provides absolute performance virtualization by being fair in sharing I/O system resources among operating systems and their applications, and provides performance isolation in the face of variations in the characteristics of I/O streams. The VIOS controls the coarse grain allocation of disk time to the different operating system instances and is OS independent; optionally, a set of OS-dependent schedulers may determine the fine-grain interleaving of requests from the corresponding operating systems to the storage system.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {105--115},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1254810.1254826},
 doi = {http://doi.acm.org/10.1145/1254810.1254826},
 acmid = {1254826},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {I/O scheduler, fairness, performance isolation, predictable performance, virtual I/O},
} 

@inproceedings{Chadha:2007:IPV:1254810.1254827,
 author = {Chadha, Vineet and Illiikkal, Ramesh and Iyer, Ravi and Moses, Jaideep and Newell, Donald and Figueiredo, Renato J.},
 title = {I/O processing in a virtualized platform: a simulation-driven approach},
 abstract = {Virtualization provides levels of execution isolation and service partition that are desirable in many usage scenarios, but its associated overheads are a major impediment for wide deployment of virtualized environments. While the virtualization cost depends heavily on workloads, it has been demonstrated that the overhead is much higher with I/O intensive workloads compared to those which are compute-intensive. Unfortunately, the architectural reasons behind the I/O performance overheads are not well understood. Early research in characterizing these penalties has shown that cache misses and TLB related overheads contribute to most of I/O virtualization cost. While most of these evaluations are done using measurements, in this paper we present an execution-driven simulation based analysis methodology with symbol annotation as a means of evaluating the performance of virtualized workloads. This methodology provides detailed information at the architectural level (with a focus on cache and TLB) and allows designers to evaluate potential hardware enhancements to reduce virtualization overhead. We apply this methodology to study the network I/O performance of Xen (as a case study) in a full system simulation environment, using detailed cache and TLB models to profile and characterize software and hardware hotspots. By applying symbol annotation to the instruction flow reported by the execution driven simulator we derive function level call flow information. We follow the anatomy of I/O processing in a virtualized platform for network transmit and receive scenarios and demonstrate the impact of cache scaling and TLB size scaling on performance.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {116--125},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1254810.1254827},
 doi = {http://doi.acm.org/10.1145/1254810.1254827},
 acmid = {1254827},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {performance model, simulation, virtual machines, virtualization, xen},
} 

@inproceedings{Govindan:2007:XCC:1254810.1254828,
 author = {Govindan, Sriram and Nath, Arjun R. and Das, Amitayu and Urgaonkar, Bhuvan and Sivasubramaniam, Anand},
 title = {Xen and co.: communication-aware CPU scheduling for consolidated xen-based hosting platforms},
 abstract = {Recent advances in software and architectural support for server virtualization have created interest in using this technology in the design of consolidated hosting platforms. Since virtualization enables easier and faster application migration as well as secure co-location of antagonistic applications, higher degrees of server consolidation are likely to result in such virtualization-based hosting platforms (VHPs). We identify a key shortcoming in existing virtual machine monitors (VMMs) that proves to be an obstacle in operating hosting platforms, such as Internet data centers, under conditions of such high consolidation: CPU schedulers that are agnostic to the communication behavior of modern, multi-tier applications. We develop a new communication-aware CPU scheduling algorithm to alleviate this problem. We implement our algorithm in the Xen VMM and build a prototype VHP on a cluster of servers. Our experimental evaluation with realistic Internet server applications and benchmarks demonstrates the performance/cost benefits and the wide applicability of our algorithms. For example, the TPC-W benchmark exhibited improvements in average response times of up to 35\% for a variety of consolidation scenarios. A streaming media server hosted on our prototype VHP was able to satisfactorily service up to 3.5 times as many clients as one running on the default Xen.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {126--136},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1254810.1254828},
 doi = {http://doi.acm.org/10.1145/1254810.1254828},
 acmid = {1254828},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CPU scheduler, multi-tier application, virtual machine monitor, xen},
} 

@inproceedings{Bungale:2007:PPF:1254810.1254830,
 author = {Bungale, Prashanth P. and Luk, Chi-Keung},
 title = {PinOS: a programmable framework for whole-system dynamic instrumentation},
 abstract = {PinOS</i> is an extension of the Pin</i> dynamic instrumentation framework for whole-system instrumentation, i.e., to instrument both kernel and user-level code. It achieves this by interposing between the subject system and hardware using virtualization techniques. Specifically, PinOS is built on top of the Xen virtual machine monitor with Intel VT technology to allow instrumentation of unmodified OSes. PinOS is based on software dynamic translation and hence can perform pervasive fine-grain instrumentation. By inheriting the powerful instrumentation API from Pin, plus introducing some new API for system-level instrumentation, PinOS can be used to write system-wide instrumentation tools for tasks like program analysis and architectural studies. As of today, PinOS can boot Linux on IA-32 in uniprocessor mode, and can instrument complex applications such as database and web servers.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {137--147},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1254810.1254830},
 doi = {http://doi.acm.org/10.1145/1254810.1254830},
 acmid = {1254830},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary translation, dynamic instrumentation, program analysis tools, virtualization, whole-system},
} 

@inproceedings{Xu:2007:MDM:1254810.1254831,
 author = {Xu, Chaohao and Li, Jianhui and Bao, Tao and Wang, Yun and Huang, Bo},
 title = {Metadata driven memory optimizations in dynamic binary translator},
 abstract = {A dynamic binary translator offers solutions for translating and running source architecture binaries on target architecture at runtime. Regardless of its growing popularity, practical dynamic binary translators usually suffer from the limited optimizations performed when generating the translated code due to the lack of useful information available in the executable files and the requirement to conform to the binary-level compatibility. Trying to generate more efficient translated code, we propose in this paper a novel method of passing performance critical information to a dynamic binary translator through the metadata section generated during the static compilation phase. With the performance critical metadata, the dynamic binary translator is able to perform aggressive optimizations to generate higher quality code. We implemented a general and extensible framework in GCC 4.0 and IA-32\&#174; Execution Layer, and selected metadata related to memory optimizations as our target. The metadata enables IA-32 EL to perform memory optimizations such as registerization, memory ordering relaxation and address disambiguation of memory instructions. Experimental data shows an overall performance improvement of 15.03\% for SPECfp2000 and 1.21\% for SPECint2000. For some specific benchmarks, the performance improvement is even up to 37.09\%.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {148--157},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1254810.1254831},
 doi = {http://doi.acm.org/10.1145/1254810.1254831},
 acmid = {1254831},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic binary translator, memory optimizations, metadata},
} 

@inproceedings{Huang:2007:NMO:1254810.1254833,
 author = {Huang, Wei and Liu, Jiuxing and Koop, Matthew and Abali, Bulent and Panda, Dhabaleswar},
 title = {Nomad: migrating OS-bypass networks in virtual machines},
 abstract = {Virtual machine (VM) technology is experiencing a resurgence due to various benefits including ease of management, security and resource consolidation. Live migration of virtual machines allows transparent movement of OS instances and hosted applications across physical machines. It is one of the most useful features of VM technology because it provides a powerful tool for effective administration of modern cluster environments. Migrating network resources is one of the key problems that need to be addressed in the VM migration process. Existing studies of VM migration have focused on traditional I/O interfaces such as Ethernet. However, modern high-speed interconnects with intelligent NICs pose significantly more challenges as they have additional features including hardware level reliable services and direct I/O accesses. In this paper we present Nomad, a design for migrating modern interconnects with the aforementioned features, focusing on cluster environments running VMs. We introduce a thin namespace virtualization layer to efficiently address location dependent resource handles and a handshake protocol which transparently maintains reliable service semantics during migration. We demonstrate our design by implementing a prototype based on the Xen virtual machine monitor and InfiniBand. Our performance analysis shows that Nomad can achieve efficient migration of network resources, even in environments with stringent communication performance requirements.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {158--168},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1254810.1254833},
 doi = {http://doi.acm.org/10.1145/1254810.1254833},
 acmid = {1254833},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {InfiniBand, high speed interconnects, migration, virtual machines, xen},
} 

@inproceedings{Bradford:2007:LWM:1254810.1254834,
 author = {Bradford, Robert and Kotsovinos, Evangelos and Feldmann, Anja and Schi\"{o}berg, Harald},
 title = {Live wide-area migration of virtual machines including local persistent state},
 abstract = {So far virtual machine (VM) migration has focused on transferring the run-time memory state of the VMs in local area networks (LAN). However, for wide-area network (WAN) migration it is crucial to not just transfer the VMs image but also transfer its local persistent state (its file system) and its on-going network connections. In this paper we address both: by combining a block-level solution with pre-copying and write throttling we show that we can transfer an entire running web server, including its local persistent state, with minimal disruption --- three seconds in the LAN and 68 seconds in the WAN); by combining dynDNS with tunneling, existing connections can continue transparently while new ones are redirected to the new network location. Thus we show experimentally that by combining well-known techniques in a novel manner we can provide system support for migrating virtual execution environments in the wide area.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {169--179},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1254810.1254834},
 doi = {http://doi.acm.org/10.1145/1254810.1254834},
 acmid = {1254834},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {live virtual machine migration, network redirection, persistent state, storage, wide area},
} 

@inproceedings{Lee:2007:PEA:1254810.1254836,
 author = {Lee, Kyungwoo and Fang, Xing and Midkiff, Samuel P.},
 title = {Practical escape analyses: how good are they?},
 abstract = {A key analysis developed for the compilation of parallel programs is thread escape analysis</i> (hereafter referred to as escape analysis</i>), which determines what objects are accessed in more than one thread, and which references within a program are references to such objects. Escape analysis has several important client optimizations: identifying objects on which races may exist, identifying locks that can be removed, identifying heap allocated objects referenced within a single thread, and compiling for strict memory models. While the effectiveness of individual escape analyses has been measured for different client optimizations, there has been no effort to compare the effectiveness of the different escape analyses over all the different clients. Nor has therebeen any attempt to develop a perfect escape analysis and measure how far from it the different escape analyses are. This paper presents a perfect escape analysis for specific runs of Java programs, which tracks all possibly escaping objects at runtime, and determines precisely which ones escape. It uses a caching technique to reduce the time and space needed for collecting access information by 8 times and 48 times, respectively, relative to not using the caching technique. We compare the perfect escape analysis results with the results from the practical escape analyses, using the four clients above or metrics that are significant for those clients. From this comparison we conclude that the relative precision of different escape analyses changes with different clients and that the most precise analysis for each client is "close enough" to the perfect analysis for three out of the four clients.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {180--190},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1254810.1254836},
 doi = {http://doi.acm.org/10.1145/1254810.1254836},
 acmid = {1254836},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, analysis precision, dynamic compilation, escape analysis},
} 

@inproceedings{Spring:2007:RAH:1254810.1254837,
 author = {Spring, Jesper Honig and Pizlo, Filip and Guerraoui, Rachid and Vitek, Jan},
 title = {Reflexes: abstractions for highly responsive systems},
 abstract = {Commercial Java virtual machines are designed to maximize the performance of applications at the expense of predictability. High throughput garbage collection algorithms, for example, can introduce pauses of 100 milliseconds or more. We are interested in supporting applications with response times in the tens of microseconds and their integration with larger timing-oblivious applications in the same Java virtual machine. We propose Reflexes, a new abstraction for writing highly responsive systems in Java and investigate the virtual machine support needed to add Reflexes to a Java environment. Our implementation of Reflexes was evaluated on several programs including an audio-processing application. We were able to run a Reflex at 22.05KHz with less than 0.2\% missed deadlines over 10 million observations, a result that compares favorably to an implementation written in C.},
 booktitle = {Proceedings of the 3rd international conference on Virtual execution environments},
 series = {VEE '07},
 year = {2007},
 isbn = {978-1-59593-630-1},
 location = {San Diego, California, USA},
 pages = {191--201},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1254810.1254837},
 doi = {http://doi.acm.org/10.1145/1254810.1254837},
 acmid = {1254837},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java virtual machine, memory management, ownership types, real-time systems},
} 

@inproceedings{Larus:2006:ARS:1134760.1134761,
 author = {Larus, James R.},
 title = {Abolish runtime systems: operating systems should control the execution},
 abstract = {Singularity [1] is a research project in Microsoft Research that started with a question: what would a software platform look like if it was designed from scratch with the primary goal of dependability? Singularity is working to answer this question by building on advances in programming languages and tools to develop a new system architecture and operating system (named Singularity), with the aim of producing a more robust and dependable software platform.Singularity made some design decisions that distinguish it from other systems. First, Singularity is written, for the most part, in safe, managed code and it will only run verifiably safe programs. Second, the system is the runtime; there is no separate JVM or CLR. Third, each process's execution environment is independent, with its own, distinct runtime, garbage collector, and libraries. As a consequence, Singularity uses control of the execution environment as a mechanism to enforce system policy and enhance system dependability.This talk will describe Singularity and then explain why conventional runtime systems, such as the JVM and CLR, should go away, like punch cards, teletypes, time sharing, etc.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1134760.1134761},
 doi = {http://doi.acm.org/10.1145/1134760.1134761},
 acmid = {1134761},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Hu:2006:SPD:1134760.1134764,
 author = {Hu, Wei and Hiser, Jason and Williams, Dan and Filipi, Adrian and Davidson, Jack W. and Evans, David and Knight, John C. and Nguyen-Tuong, Anh and Rowanhill, Jonathan},
 title = {Secure and practical defense against code-injection attacks using software dynamic translation},
 abstract = {One of the most common forms of security attacks involves exploiting a vulnerability to inject malicious code into an executing application and then cause the injected code to be executed. A theoretically strong approach to defending against any type of code-injection attack is to create and use a process-specific instruction set that is created by a randomization algorithm. Code injected by an attacker who does not know the randomization key will be invalid for the randomized processor effectively thwarting the attack. This paper describes a secure and efficient implementation of instruction-set randomization (ISR) using software dynamic translation. The paper makes three contributions beyond previous work on ISR. First, we describe an implementation that uses a strong cipher algorithm--the Advanced Encryption Standard (AES), to perform randomization. AES is generally believed to be impervious to known attack methodologies. Second, we demonstrate that ISR using AES can be implemented practically and efficiently (considering both execution time and code size overheads) without requiring special hardware support. The third contribution is that our approach detects malicious code before it is executed. Previous approaches relied on probabilistic arguments that execution of non-randomized foreign code would eventually cause a fault or runtime exception.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {2--12},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1134760.1134764},
 doi = {http://doi.acm.org/10.1145/1134760.1134764},
 acmid = {1134764},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {software dynamic translation, virtual execution},
} 

@inproceedings{Asrigo:2006:UVS:1134760.1134765,
 author = {Asrigo, Kurniadi and Litty, Lionel and Lie, David},
 title = {Using VMM-based sensors to monitor honeypots},
 abstract = {Virtual Machine Monitors (VMMs) are a common tool for implementing honeypots. In this paper we examine the implementation of a VMM-based intrusion detection and monitoring system for collecting information about attacks on honeypots. We document and evaluate three designs we have implemented on two open-source virtualization platforms: User-Mode Linux and Xen. Our results show that our designs give the monitor good visibility into the system and thus, a small number of monitoring sensors can detect a large number of intrusions. In a three month period, we were able to detect five different attacks, as well as collect and try 46 more exploits on our honeypots. All attacks were detected with only two monitoring sensors. We found that the performance overhead for monitoring such intrusions is independent of which events are being monitored, but depends entirely on the number of monitoring events and the underlying monitoring implementation. The performance overhead can be significantly improved by implementing the monitor directly in the privileged code of the VMM, though at the cost of increasing the size of the trusted computing base of the system.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {13--23},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1134760.1134765},
 doi = {http://doi.acm.org/10.1145/1134760.1134765},
 acmid = {1134765},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {IDS, honeypot monitoring, intrusion detection, virtual machine monitor},
} 

@inproceedings{Yu:2006:FVM:1134760.1134766,
 author = {Yu, Yang and Guo, Fanglu and Nanda, Susanta and Lam, Lap-chung and Chiueh, Tzi-cker},
 title = {A feather-weight virtual machine for windows applications},
 abstract = {Many fault-tolerant and intrusion-tolerant systems require the ability to execute unsafe programs in a realistic environment without leaving permanent damages. Virtual machine technology meets this requirement perfectly because it provides an execution environment that is both realistic and isolated. In this paper, we introduce an OS level virtual machine architecture for Windows applications called Feather-weight Virtual Machine</i> (FVM), under which virtual machines share as many resources of the host machine as possible while still isolated from one another and from the host machine. The key technique behind FVM is namespace virtualization</i>, which isolates virtual machines by renaming resources at the OS system call interface. Through a copy-on-write scheme, FVM allows multiple virtual machines to physically share resources but logically isolate their resources from each other. A main technical challenge in FVM is how to achieve strong isolation among different virtual machines and the host machine, due to numerous namespaces and interprocess communication mechanisms on Windows. Experimental results demonstrate that FVM is more flexible and scalable, requires less system resource, incurs lower start-up and run-time performance overhead than existing hardware-level virtual machine technologies, and thus makes a compelling building block for security and fault-tolerant applications.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {24--34},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1134760.1134766},
 doi = {http://doi.acm.org/10.1145/1134760.1134766},
 acmid = {1134766},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {copy on write, mobile code security, namespace virtualization, system call interception, virtual machine},
} 

@inproceedings{Chen:2006:LUO:1134760.1134767,
 author = {Chen, Haibo and Chen, Rong and Zhang, Fengzhe and Zang, Binyu and Yew, Pen-Chung},
 title = {Live updating operating systems using virtualization},
 abstract = {Many critical IT infrastructures require non-disruptive operations. However, the operating systems thereon are far from perfect that patches and upgrades are frequently applied, in order to close vulnerabilities, add new features and enhance performance. To mitigate the loss of availability, such operating systems need to provide features such as live update through which patches and upgrades can be applied without having to stop and reboot the operating system. Unfortunately, most current live updating approaches cannot be easily applied to existing operating systems: some are tightly bound to specific design approaches (e.g. object-oriented); others can only be used under particular circumstances (e.g. quiescence states).In this paper, we propose using virtualization to provide the live update capability. The proposed approach allows a broad range of patches and upgrades to be applied at any time without the requirement of a quiescence state. Moreover, such approach shares good portability for its OS-transparency and is suitable for inclusion in general virtualization systems. We present a working prototype, LUCOS, which supports live update capability on Linux running on Xen virtual machine monitor. To demonstrate the applicability of our approach, we use real-life kernel patches from Linux kernel 2.6.10 to Linux kernel 2.6.11, and apply some of those kernel patches on the fly. Performance measurements show that our implementation incurs negligible performance overhead: a less than 1\% performance degradation compared to a Xen-Linux. The time to apply a patch is also very minimal.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {35--44},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1134760.1134767},
 doi = {http://doi.acm.org/10.1145/1134760.1134767},
 acmid = {1134767},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {availability, live update, operating system, virtualization},
} 

@inproceedings{vanDoorn:2006:HVT:1134760.1134762,
 author = {van Doorn, Leendert},
 title = {Hardware virtualization trends},
 abstract = {As Intel is rolling out its Vanderpool processor virtualization technology and AMD its Secure Virtual Machine technology, we are only seeing the first wave of processor virtualization assists. Over the next few years the x86 space will change dramatically. We will see the introduction of massive multi-core, 64-bit, 2 nd generation processor virtualization capabilities, I/O isolation capabilities, and hardware security assists.Both Intel and AMD are differentiating their processors by providing enhancements that enable you to run multiple virtual machines in such a way that the guest is unaware that it is being virtualized. Ironically, largely because these technologies have been unavailable for so long, Linux and Windows are going into a different direction: paravirtualization</i>. With paravirtualization the guest operating system collaborates closely with the virtual machine monitor through a set of well defined software interfaces. This approach does not require any new hardware features at all and has the potential of performing much better. So, this raises an interesting dilemma: Some of the new virtualization capabilities may already be obsolete before they are brought to market.In this talk I will discuss the new virtualization technologies that will be introduced over the next few years, how they help virtualization, what challenges they pose and how these virtualization technologies will likely consolidate.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {45--45},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1134760.1134762},
 doi = {http://doi.acm.org/10.1145/1134760.1134762},
 acmid = {1134762},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Bocchino:2006:VLV:1134760.1134769,
 author = {Bocchino,Jr., Robert L. and Adve, Vikram S.},
 title = {Vector LLVA: a virtual vector instruction set for media processing},
 abstract = {We present Vector LLVA, a virtual instruction set architecture (VISA) that exposes extensive static information about vector parallelism while avoiding the use of hardware-specific parameters. We provide both arbitrary-length vectors (for targets that allow vectors of arbitrary length, or where the target length is not known) and fixed-length vectors (for targets that have a fixed vector length, such as subword SIMD extensions), together with a rich set of operations on both vector types. We have implemented translators that compile (1) Vector LLVA written with arbitrary-length vectors to the Motorola RSVP architecture and (2) Vector LLVA written with fixed-length vectors to both AltiVec and Intel SSE2. Our translatorgenerated code achieves speedups competitive with handwritten native code versions of several benchmarks on all three architectures. These experiments show that our V-ISA design captures vector parallelism for two quite different classes of architectures and provides virtual object code portability within the class of subword SIMD architectures.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {46--56},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1134760.1134769},
 doi = {http://doi.acm.org/10.1145/1134760.1134769},
 acmid = {1134769},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {SIMD, multimedia, vector, virtual instruction sets},
} 

@inproceedings{Peschanski:2006:SRE:1134760.1134770,
 author = {Peschanski, Fr\'{e}d\'{e}ric and Hym, Samuel},
 title = {A stackless runtime environment for a Pi-calculus},
 abstract = {The Pi-calculus is a formalism to model and reason about highly concurrent and dynamic systems. Most of the expressive power of the language comes from the ability to pass communication channels among concurrent processes, as any other value. We present in this paper the CubeVM, an interpreter architecture for an applied variant of the Pi-calculus, focusing on its operational semantics. The main characteristic of the CubeVM comes from its stackless architecture. We show, in a formal way, that the resource management model inside the VM may be greatly simplified without the need for nested stack frames. This is particularly true for the garbage collection of processes and channels. The proposed GC, based on a reference counting scheme, is highly concurrent and, most interestingly, does automatically detect and reclaim cycles of disabled processes. We also address the main performance issues raised by the fine-grained concurrency model of the Pi-calculus. We introduce the reactive variant of the semantics that allows, when applicable, to increase the performance drastically by bypassing the scheduler. We define the language subset of processes in so called chain-reaction forms for which the sequential semantics can be proved statically. We illustrate the expressive power and performance gains of such chain-reactions with examples of functional, dataflow and object-oriented systems. Encodings for the pure Pi-calculus are also demonstrated.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {57--67},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1134760.1134770},
 doi = {http://doi.acm.org/10.1145/1134760.1134770},
 acmid = {1134770},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Pi-calculus, garbage collection, interpreter, operational semantics},
} 

@inproceedings{Cunei:2006:NAR:1134760.1134771,
 author = {Cunei, Antonio and Vitek, Jan},
 title = {A new approach to real-time checkpointing},
 abstract = {The progress towards programming methodologies that simplify the work of the programmer involves automating, whenever possible, activities that are secondary to the main task of designing algorithms and developing applications. Automatic memory management, using garbage collection, and automatic persistence, using checkpointing, are both examples of mechanisms that operate behind the scenes, simplifying the work of the programmer. Implementing such mechanisms in the presence of real-time constraints, however, is particularly difficult.In this paper we review the behavior of traditional copy-on-write implementations of checkpointing in the context of real-time systems, and we show how such implementations may, in pathological cases, seriously impair the ability of the user code to meet its deadlines. We discuss the source of the problem, supply benchmarks, and discuss possible remedies. We subsequently propose a novel approach that does not rely on copy-on-write and that, while more expensive in terms of CPU time overhead, is unaffected by pathological user code. We also describe our implementation of the proposed solution, based on the Ovm RTSJ Java Virtual Machine, and we discuss our experimental results.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {68--77},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1134760.1134771},
 doi = {http://doi.acm.org/10.1145/1134760.1134771},
 acmid = {1134771},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, checkpoint, real-time, virtual machine},
} 

@inproceedings{Simon:2006:JBM:1134760.1134773,
 author = {Simon, Doug and Cifuentes, Cristina and Cleal, Dave and Daniels, John and White, Derek},
 title = {Java\&\#8482; on the bare metal of wireless sensor devices: the squawk Java virtual machine},
 abstract = {The Squawk virtual machine is a small Java\&#8482; virtual machine (VM) written mostly in Java that runs without an operating system on a wireless sensor platform. Squawk translates standard class file into an internal pre-linked, position independent format that is compact and allows for efficient execution of bytecodes that have been placed into a read-only memory. In addition, Squawk implements an application isolation mechanism whereby applications are represented as object and are therefore treated as first class objects (i.e., they can be reified). Application isolation also enables Squawk to run multiple applications at once with all immutable state being shared between the applications. Mutable state is not shared. The combination of these features reduce the memory footprint of the VM, making it ideal for deployment on small devices.Squawk provides a wireless API that allows developers to write applications for wireless sensor networks (WSNs), this API is an extension of the generic connection framework (GCF). Authentication of deployed files on the wireless device and migration of applications between devices is also performed by the VM.This paper describes the design and implementation of the Squawk VM as applied to the Sun\&#8482; Small Programmable Object Technology (SPOT) wireless device; a device developed at Sun Microsystems Laboratories for experimentation with wireless sensor and actuator applications.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {78--88},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1134760.1134773},
 doi = {http://doi.acm.org/10.1145/1134760.1134773},
 acmid = {1134773},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {IEEE 802.15.4, Java virtual machine, Sun SPOT, embedded systems, wireless sensor networks},
} 

@inproceedings{Ocean:2006:SPV:1134760.1134774,
 author = {Ocean, Michael J. and Bestavros, Azer and Kfoury, Assaf J.},
 title = {snBench: programming and virtualization framework for distributed multitasking sensor networks},
 abstract = {We envision future Sensor Networks (SNs) that will be composed of a hybrid collection of a variety of sensing devices embedded into shared environments. In such environments it follows that the embedded SN infrastructure would also be shared by various users, occupants, or administrators of these shared spaces. As such a clear need emerges to virtualize the SN, sharing the resources of the SN across various tasks executing simultaneously. To achieve this goal, we present the <sc>sn</sc>B<sc>ench</sc> (SN Workbench). The <sc>sn</sc>B<sc>ench</sc> abstracts a collection of dissimilar and disjoint resources into a shared virtual SN. The <sc>sn</sc>B<sc>ench</sc> provides an accessible high-level programming language that enables users to write "macro-level" program for their own virtual SN (i.e.</i>, programs are written at the scope of the SN rather than its individual components and specific details of the components or deployment need not be specified by the developer). To this end <sc>sn</sc>B<sc>ench</sc> provides execution environments and a run-time support infrastructure to provide each user a Virtual Sensor Network characterized by efficient automated program deployment, resource management, and a truly extensible architecture. In this paper we present an overview of the <sc>sn</sc>B<sc>ench</sc>, detailing its salient functionalities that support the entire life-cycle of a SN application.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {89--99},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1134760.1134774},
 doi = {http://doi.acm.org/10.1145/1134760.1134774},
 acmid = {1134774},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed resource management, domain specific languages, programming environments, sensor networks},
} 

@inproceedings{Hu:2006:IVE:1134760.1134775,
 author = {Hu, Shiwen and John, Lizy K.},
 title = {Impact of virtual execution environments on processor energy consumption and hardware adaptation},
 abstract = {During recent years, microprocessor energy consumption has been surging and efforts to reduce power and energy have received a lot of attention. At the same time, virtual execution environments (VEEs), such as Java virtual machines, have grown in popularity. Hence, it is important to evaluate the impact of virtual execution environments on microprocessor energy consumption. This paper characterizes the energy and power impact of two important components of VEEs, Just-in-time(JIT) optimization and garbage collection. We find that by reducing instruction counts, JIT optimization significantly reduces energy consumption, while garbage collection incurs runtime overhead that consumes more energy. Importantly, both JIT optimization and garbage collection decrease the average power dissipated by a program. Detailed analysis reveals that both JIT optimizer and JIT optimized code dissipate less power than un-optimized code. On the other hand, being memory bound and with low ILP, the garbage collector dissipates less power than the application code, but rarely affects the average power of the latter.Adaptive microarchitectures are another recent trend for energy reduction where microarchitectural resources can be dynamically tuned to match program runtime requirements. This research reveals that both JIT optimization and garbage collection alter a program's behavior and runtime requirements, which considerably affects the adaptation of configurable hardware units, and influences the overall energy consumption. This work also demonstrates that the adaptation preferences of the two VEE services differ substantially from those of the application code. Both VEE services prefer a simple core for high energy reduction. On the other hand, the JIT optimizer usually requires larger data caches, while the garbage collector rarely benefits from large data caches. The insights gained in this paper point to novel techniques that can further reduce microprocessor energy consumption.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {100--110},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1134760.1134775},
 doi = {http://doi.acm.org/10.1145/1134760.1134775},
 acmid = {1134775},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {energy efficiency, hardware adaptation, power dissipation},
} 

@inproceedings{Gu:2006:RFP:1134760.1134776,
 author = {Gu, Dayong and Verbrugge, Clark and Gagnon, Etienne M.},
 title = {Relative factors in performance analysis of Java virtual machines},
 abstract = {Many new Java runtime optimizations report relatively small, single-digit performance improvements. On modern virtual and actual hardware, however, the performance impact of an optimization can be influenced by a variety of factors in the underlying systems. Using a case study of a new garbage collection optimization in two different Java virtual machines, we show the relative effects of issues that must be taken into consideration when claiming an improvement. We examine the specific and overall performance changes due to our optimization and show how unintended side-effects can contribute to, and distort the final assessment. Our experience shows that VM and hardware concerns can generate variances of up to 9.5\% in whole program execution time. Consideration of these confounding effects is critical to a good, objective understanding of Java performance and optimization.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {111--121},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1134760.1134776},
 doi = {http://doi.acm.org/10.1145/1134760.1134776},
 acmid = {1134776},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, caches, garbage collection, hardware counters, performance analysis},
} 

@inproceedings{Hiser:2006:EFC:1134760.1134778,
 author = {Hiser, Jason D. and Williams, Daniel and Filipi, Adrian and Davidson, Jack W. and Childers, Bruce R.},
 title = {Evaluating fragment construction policies for SDT systems},
 abstract = {Software Dynamic Translation (SDT) systems have been used for program instrumentation, dynamic optimization, security policy enforcement, intrusion detection, and many other uses. To be widely applicable, the overhead (runtime, memory usage, and power consumption) should be as low as possible. For instance, if an SDT system is protecting a web server against possible attacks, but causes 30\% slowdown, a company may need 30\% more machines to handle the web traffic they expect. Consequently, the causes of SDT overhead should be studied rigorously. This work evaluates many alternative policies for the creation of fragments within the Strata SDT framework. In particular, we examine the effects of ending translation at conditional branches; ending translation at unconditional branches; whether to use partial inlining for call instructions; whether to build the target of calls immediately or lazily; whether to align branch targets; and how to place code to transition back to the dynamic translator. We find that effective translation strategies are vital to program performance, improving performance from as much as 28\% overhead, to as little as 3\% overhead on average for the SPEC CPU2000 benchmark suite. We further demonstrate that these translation strategies are effective across several platforms, including Sun SPARC UltraSparc IIi, AMD Athlon Opteron, and Intel Pentium IV processors.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {122--132},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1134760.1134778},
 doi = {http://doi.acm.org/10.1145/1134760.1134778},
 acmid = {1134778},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic translation performance, low overhead, performance, software dynamic translator},
} 

@inproceedings{Huang:2006:DCM:1134760.1134779,
 author = {Huang, Xianglong and Lewis, Brian T and McKinley, Kathryn S},
 title = {Dynamic code management: improving whole program code locality in managed runtimes},
 abstract = {Poor code locality degrades application performance by increasing memory stalls due to instruction cache and TLB misses. This problem is particularly an issue for large server applications written in languages such as Java and C# that provide just-in-time (JIT) compilation, dynamic class loading, and dynamic recompilation. However, managed runtimes also offer an opportunity to dynamically profile applications and adapt them to improve their performance. This paper describes a Dynamic Code Management system (DCM) in a managed runtime that performs whole program code layout optimizations to improve instruction locality.We begin by implementing the widely used Pettis-Hansen algorithm for method layout to improve code locality. Unfortunately, this algorithm is too costly for a dynamic optimization system, O</i>(n</i><sup>3</sup>) in time in the call graph. For example, Pettis-Hansen requires a prohibitively expensive 35 minutes to lay out MiniBean which has 15,586 methods. We propose three new code placement algorithms that target ITLB misses, which typically have the greatest impact on performance. The best of these algorithms, Code Tiling</i>, groups methods into page sized tiles</i> by performing a depth-first traversal of the call graph based on call frequency. Excluding overhead, experimental results show that DCM with Code Tiling improves performance by 6\% on the large MiniBean benchmark over a baseline that orders methods based on invocation order, whereas Pettis-Hansen placement offers less improvement, 2\%, over the same base. Furthermore, Code Tiling lays out MiniBean in just 0.35 seconds for 15,586 methods (6000 times faster than Pettis-Hansen) which makes it suitable for high-performance managed runtimes.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {133--143},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1134760.1134779},
 doi = {http://doi.acm.org/10.1145/1134760.1134779},
 acmid = {1134779},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {code generation, code layout, dynamic optimization, locality, performance monitoring, virtual machines},
} 

@inproceedings{Gal:2006:HEJ:1134760.1134780,
 author = {Gal, Andreas and Probst, Christian W. and Franz, Michael},
 title = {HotpathVM: an effective JIT compiler for resource-constrained devices},
 abstract = {We present a just-in-time compiler for a Java VM that is small enough to fit on resource-constrained devices, yet is surprisingly effective. Our system dynamically identifies traces of frequently executed bytecode instructions (which may span several basic blocks across several methods) and compiles them via Static Single Assignment (SSA) construction. Our novel use of SSA form in this context allows to hoist instructions across trace side-exits without necessitating expensive compensation code in off-trace paths. The overall memory consumption (code and data) of our system is only 150 kBytes, yet benchmarks show a speedup that in some cases rivals heavy-weight just-in-time compilers.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {144--153},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1134760.1134780},
 doi = {http://doi.acm.org/10.1145/1134760.1134780},
 acmid = {1134780},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic compilation, embedded and resource-constrained systems, mixed-mode interpretive compiled systems, software trace scheduling, static single assignment form, virtual machines},
} 

@inproceedings{Bhansali:2006:FIT:1134760.1220164,
 author = {Bhansali, Sanjay and Chen, Wen-Ke and de Jong, Stuart and Edwards, Andrew and Murray, Ron and Drini\'{c}, Milenko and Miho\v{c}ka, Darek and Chau, Joe},
 title = {Framework for instruction-level tracing and analysis of program executions},
 abstract = {Program execution traces provide the most intimate details of a program's dynamic behavior. They can be used for program optimization, failure diagnosis, collecting software metrics like coverage, test prioritization, etc. Two major obstacles to exploiting the full potential of information they provide are: (i</i>) performance overhead while collecting traces, and (ii</i>) significant size of traces even for short execution scenarios. Reducing information output in an execution trace can reduce both performance overhead and the size of traces. However, the applicability of such traces is limited to a particular task. We present a runtime framework with a goal of collecting a complete, machine- and task-independent, user-mode trace of a program's execution that can be re-simulated deterministically with full fidelity down to the instruction level. The framework has reasonable runtime overhead and by using a novel compression scheme, we significantly reduce the size of traces. Our framework enables building a wide variety of tools for understanding program behavior. As examples of the applicability of our framework, we present a program analysis and a data locality profiling tool. Our program analysis tool is a time travel debugger that enables a developer to debug in both forward and backward direction over an execution trace with nearly all information available as in a regular debugging session. Our profiling tool has been used to improve data locality and reduce the dynamic working sets of real world applications.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {154--163},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1134760.1220164},
 doi = {http://doi.acm.org/10.1145/1134760.1220164},
 acmid = {1220164},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {callback, code emulation, code replay, time-travel debugging, tracing},
} 

@inproceedings{Yang:2006:DIT:1134760.1220165,
 author = {Yang, Jing and Zhou, Shukang and Soffa, Mary Lou},
 title = {Dimension: an instrumentation tool for virtual execution environments},
 abstract = {Translation-based virtual execution environments (VEEs) are becoming increasingly popular because of their usefulness. With dynamic translation, a program in a VEE has two binaries: an input source binary and a dynamically generated target binary. Program analysis is important for these binaries, and both the developers and users of VEEs need an instrumentation system to customize program analysis tools. However, existing instrumentation systems for use in VEEs have two drawbacks. First, they are tightly bound with a specific VEE and thus are difficult to reuse without a lot of effort. Second, most of them can not support instrumentation on both the source and target binaries.This paper presents Dimension, a flexible tool that provides instrumentation services for a variety of VEEs. To our knowledge, it is the first stand-alone instrumentation tool that is specially designed for use in VEEs. Given an instrumentation specification, Dimension can be used by a VEE to provide customized instrumentation, enabling analyses on both the source and target binaries.We present two case studies demonstrating that Dimension can be reused easily by different VEEs. We experiment with the two cases and show that the same instrumentation provided by Dimension does not lose efficiency compared to its manual implementation for that particular VEE (the average performance difference is within 2\%). We also illustrate that by interfacing with a special VEE that has the same source and target binary formats, Dimension can be used to build an efficient dynamic instrumentation system for traditional execution environments.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {164--174},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1134760.1220165},
 doi = {http://doi.acm.org/10.1145/1134760.1220165},
 acmid = {1220165},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic translation, instrumentation, program analysis tool, virtual execution environment},
} 

@inproceedings{Sridhar:2006:HOS:1134760.1220166,
 author = {Sridhar, Swaroop and Shapiro, Jonathan S. and Northup, Eric and Bungale, Prashanth P.},
 title = {HDTrans: an open source, low-level dynamic instrumentation system},
 abstract = {Dynamic translation is a general purpose tool used for instrumenting programs at run time. Performance of translated execution relies on balancing the cost of translation against the benefits of any optimizations achieved, and many current translators perform substantial rewriting during translation in an attempt to reduce execution time. Our results show that these optimizations offer no significant benefit even when the translated program has a small, hot working set. When used in a broader range of applications, such as ubiquitous policy enforcement or penetration detection, translator performance cannot rely on the presence of a hot working set to amortize the cost of translation. A simpler, more maintainable, adaptable, and smaller translator appears preferable to more complicated designs in most cases.HDTrans is a light-weight dynamic instrumentation system for the IA-32 architecture that uses some simple and effective translation techniques in combination with established trace linearization and code caching optimizations. We present an evaluation of translation overhead under both benchmark and less idealized conditions, showing that conventional benchmarks do not provide a good prediction of translation overhead when used pervasively.A further contribution of this paper is an analysis of the effectiveness of post-link static pre-translation techniques for overhead reduction. Our results indicate that static pre-translation is effective only when expensive instrumentation or optimization is performed.},
 booktitle = {Proceedings of the 2nd international conference on Virtual execution environments},
 series = {VEE '06},
 year = {2006},
 isbn = {1-59593-332-8},
 location = {Ottawa, Ontario, Canada},
 pages = {175--185},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1134760.1220166},
 doi = {http://doi.acm.org/10.1145/1134760.1220166},
 acmid = {1220166},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binary translation, dynamic instrumentation, dynamic translation},
} 

@inproceedings{Smith:2005:UVV:1064979.1064980,
 author = {Smith, James E.},
 title = {A unified view of virtualization},
 abstract = {Virtualization technologies have been developed by a number of computer science and engineering disciplines, sometimes independently, often by different groups and at different times. Not surprisingly, these groups each view virtualization as a sub-discipline, so it is studied in a fragmented way. In the future, however, virtualization will become an essential part of all computer systems by providing smart interconnection mechanisms for the three major system components - application software, system software, and hardware. Consequently, the study of virtualization technologies will become a discipline in its own right and will stand on equal footing with the other major areas of computer systems design.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {1--1},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1064979.1064980},
 doi = {http://doi.acm.org/10.1145/1064979.1064980},
 acmid = {1064980},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Zhang:2005:FVM:1064979.1064983,
 author = {Zhang, Yuting and Bestavros, Azer and Guirguis, Mina and Matta, Ibrahim and West, Richard},
 title = {Friendly virtual machines: leveraging a feedback-control model for application adaptation},
 abstract = {With the increased use of "Virtual Machines" (VMs) as vehicles that isolate applications running on the same host, it is necessary to devise techniques that enable multiple VMs to share underlying resources both fairly and efficiently. To that end, one common approach is to deploy complex resource management techniques in the hosting infrastructure. Alternately, in this paper, we advocate the use of self-adaptation in the VMs themselves based on feedback about resource usage and availability. Consequently, we define "Friendly" VM (FVM) to be a virtual machine that adjusts its demand for system resources, so that they are both efficiently and fairly allocated to competing FVMs. Such properties are ensured using one of many provably convergent control rules, such as Additive-Increase/Multiplicative-Decrease (AIMD). By adopting this distributed application-based approach to resource management, it is not necessary to make assumptions about the underlying resources nor about the requirements of FVMs competing for these resources. To demonstrate the elegance and simplicity of our approach, we present a prototype implementation of our FVM framework in User-Mode Linux (UML)---an implementation that consists of less than 500 lines of code changes to UML. We present an analytic, control-theoretic model of FVM adaptation, which establishes convergence and fairness properties. These properties are also backed up with experimental results using our prototype FVM implementation.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {2--12},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064979.1064983},
 doi = {http://doi.acm.org/10.1145/1064979.1064983},
 acmid = {1064983},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {feedback Control, friendy virtual machines, resource management},
} 

@inproceedings{Menon:2005:DPO:1064979.1064984,
 author = {Menon, Aravind and Santos, Jose Renato and Turner, Yoshio and Janakiraman, G. (John) and Zwaenepoel, Willy},
 title = {Diagnosing performance overheads in the xen virtual machine environment},
 abstract = {Virtual Machine (VM) environments (e.g., VMware and Xen) are experiencing a resurgence of interest for diverse uses including server consolidation and shared hosting. An application's performance in a virtual machine environment can differ markedly from its performance in a non-virtualized environment because of interactions with the underlying virtual machine monitor and other virtual machines. However, few tools are currently available to help debug performance problems in virtual machine environments.In this paper, we present Xenoprof, a system-wide statistical profiling toolkit implemented for the Xen virtual machine environment. The toolkit enables coordinated profiling of multiple VMs in a system to obtain the distribution of hardware events such as clock cycles and cache and TLB misses. The toolkit will facilitate a better understanding of performance characteristics of Xen's mechanisms allowing the community to optimize the Xen implementation.We use our toolkit to analyze performance overheads incurred by networking applications running in Xen VMs. We focus on networking applications since virtualizing network I/O devices is relatively expensive. Our experimental results quantify Xen's performance overheads for network I/O device virtualization in uni- and multi-processor systems. With certain Xen configurations, networking workloads in the Xen environment can suffer significant performance degradation. Our results identify the main sources of this overhead which should be the focus of Xen optimization efforts. We also show how our profiling toolkit was used to uncover and resolve performance bugs that we encountered in our experiments which caused unexpected application behavior.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {13--23},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064979.1064984},
 doi = {http://doi.acm.org/10.1145/1064979.1064984},
 acmid = {1064984},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {performance analysis, statistical profiling, virtual machine monitors},
} 

@inproceedings{Garthwaite:2005:SPL:1064979.1064985,
 author = {Garthwaite, Alex and Dice, David and White, Derek},
 title = {Supporting per-processor local-allocation buffers using lightweight user-level preemption notification},
 abstract = {One challenge for runtime systems like the Java\&#8482; platform that depend on garbage collection is the ability to scale performance with the number of allocating threads. As the number of such threads grows, allocation of memory in the heap becomes a point of contention. To relieve this contention, many collectors allow threads to preallocate blocks of memory from the shared heap. These per-thread local-allocation buffers (LABs) allow threads to allocate most objects without any need for further synchronization. As the number of threads exceeds the number of processors, however, the cost of committing memory to local-allocation buffers becomes a challenge and sophisticated LAB-sizing policies must be employed.To reduce this complexity, we implement support for local-allocation buffers associated with processors instead of threads using multiprocess restartable critical sections (MP-RCSs). MP-RCSs allow threads to manipulate processor-local data safely. To support processor-specific transactions in dynamically generated code, we have developed a novel mechanism for implementing these critical sections that is efficient, allows preemption-notification at known points in a given critical section, and does not require explicit registration of the critical sections. Finally, we analyze the performance of per-processor LABs and show that, for highly threaded applications, this approach performs better than per-thread LABs, and allows for simpler LAB-sizing policies.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {24--34},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064979.1064985},
 doi = {http://doi.acm.org/10.1145/1064979.1064985},
 acmid = {1064985},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {locality, memory allocation, restartable critical sections},
} 

@inproceedings{Kirsch:2005:PMR:1064979.1064986,
 author = {Kirsch, Christoph M. and Sanvido, Marco A. A. and Henzinger, Thomas A.},
 title = {A programmable microkernel for real-time systems},
 abstract = {We present a new software system architecture for the implementation of hard real-time applications. The core of the system is a microkernel whose reactivity (interrupt handling as in synchronous reactive programs) and proactivity (task scheduling as in traditional RTOSs) are fully programmable. The microkernel, which we implemented on a StrongARM processor, consists of two interacting domain-specific virtual machines, a reactive E (Embedded) machine and a proactive S (Scheduling) machine. The microkernel code (or microcode) that runs on the microkernel is partitioned into E and S code. E code manages the interaction of the system with the physical environment: the execution of E code is triggered by environment interrupts, which signal external events such as the arrival of a message or sensor value, and it releases application tasks to the S machine. S code manages the interaction of the system with the processor: the execution of S code is triggered by hardware interrupts, which signal internal events such as the completion of a task or time slice, and it dispatches application tasks to the CPU, possibly preempting a running task. This partition of the system orthogonalizes the two main concerns of real-time implementations: E code refers to environment time and thus defines the reactivity of the system in a hardware- and scheduler-independent fashion; S code refers to CPU time and defines a system scheduler. If both time lines can be reconciled, then the code is called time safe; violations of time safety are handled again in a programmable way, by run-time exceptions. The separation of E from S code permits the independent programming, verification, optimization, composition, dynamic adaptation, and reuse of both reaction and scheduling mechanisms. Our measurements show that the system overhead is very acceptable even for large sets of task, generally in the 0.2--0.3\% range.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {35--45},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064979.1064986},
 doi = {http://doi.acm.org/10.1145/1064979.1064986},
 acmid = {1064986},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {operating system, real time, virtual machine},
} 

@inproceedings{Click:2005:PGA:1064979.1064988,
 author = {Click, Cliff and Tene, Gil and Wolf, Michael},
 title = {The pauseless GC algorithm},
 abstract = {Modern transactional response-time sensitive applications have run into practical limits on the size of garbage collected heaps. The heap can only grow until GC pauses exceed the response-time limits. Sustainable, scalable concurrent collection has become a feature worth paying for.Azul Systems has built a custom system (CPU, chip, board, and OS) specifically to run garbage collected virtual machines. The custom CPU includes a read barrier instruction. The read barrier enables a highly concurrent (no stop-the-world phases), parallel and compacting GC algorithm. The Pauseless algorithm is designed for uninterrupted application execution and consistent mutator throughput in every GC phase.Beyond the basic requirement of collecting faster than the allocation rate, the Pauseless collector is never in a "rush" to complete any GC phase. No phase places an undue burden on the mutators nor do phases race to complete before the mutators produce more work. Portions of the Pauseless algorithm also feature a "self-healing" behavior which limits mutator overhead and reduces mutator sensitivity to the current GC state.We present the Pauseless GC algorithm, the supporting hardware features that enable it, and data on the overhead, efficiency, and pause times when running a sustained workload.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {46--56},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064979.1064988},
 doi = {http://doi.acm.org/10.1145/1064979.1064988},
 acmid = {1064988},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, concurrent GC, custom hardware, garbage collection, memory management, read barriers},
} 

@inproceedings{Spoonhower:2005:UPR:1064979.1064989,
 author = {Spoonhower, Daniel and Blelloch, Guy and Harper, Robert},
 title = {Using page residency to balance tradeoffs in tracing garbage collection},
 abstract = {We introduce an extension of mostly copying collection that uses page residency</i> to determine when to relocate objects. Our collector promotes pages with high residency in place, avoiding unnecessary work and wasted space. It predicts the residency of each page, but when its predictions prove to be inaccurate, our collector reclaims unoccupied space by using it to satisfy allocation requests.Using residency allows our collector to dynamically balance the tradeoffs of copying and non-copying collection. Our technique requires less space than a pure copying collector and supports object pinning without otherwise sacrificing the ability to relocate objects.Unlike other hybrids, our collector does not depend on application-specific configuration and can quickly respond to changing application behavior. Our measurements show that our hybrid performs well under a variety of conditions; it prefers copying collection when there is ample heap space but falls back on non-copying collection when space becomes limited.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {57--67},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064979.1064989},
 doi = {http://doi.acm.org/10.1145/1064979.1064989},
 acmid = {1064989},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compaction, fragmentation, object pinning, predicted residency},
} 

@inproceedings{Chen:2005:EFF:1064979.1064990,
 author = {Chen, Guangyu and Kandemir, Mahmut and Irwin, Mary J.},
 title = {Exploiting frequent field values in java objects for reducing heap memory requirements},
 abstract = {The capabilities of applications executing on embedded and mobile devices are strongly influenced by memory size limitations. In fact, memory limitations are one of the main reasons that applications run slowly or even crash in embedded/mobile devices. While improvements in technology enable the integration of more memory into embedded devices, the amount memory that can be included is also limited by cost, power consumption, and form factor considerations. Consequently, addressing memory limitations will continue to be of importance.Focusing on embedded Java environments, this paper shows how object compression can improve memory space utilization. The main idea is to make use of the observation that a small set of values tend to appear in some fields of the heap-allocated objects much more frequently than other values. Our analysis shows the existence of such frequent field values in the SpecJVM98 benchmark suite. We then propose two object compression schemes that eliminate/reduce the space occupied by the frequent field values. Our extensive experimental evaluation using a set of eight Java benchmarks shows that these schemes can reduce the minimum heap size allowing Java applications to execute without out-of-memory exceptions by up to 24\% (14\% on an average).},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {68--78},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064979.1064990},
 doi = {http://doi.acm.org/10.1145/1064979.1064990},
 acmid = {1064990},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java virtual machine, frequent field value, garbage collection, heap},
} 

@inproceedings{Koju:2005:EGR:1064979.1064992,
 author = {Koju, Toshihiko and Takada, Shingo and Doi, Norihisa},
 title = {An efficient and generic reversible debugger using the virtual machine based approach},
 abstract = {The reverse execution of programs is a function where programs are executed backward in time. A reversible debugger is a debugger that provides such a functionality. In this paper, we propose a novel reversible debugger that enables reverse execution of programs written in the C language. Our approach takes the virtual machine based approach</i>. In this approach, the target program is executed on a special virtual machine. Our contribution in this paper is two-fold. First, we propose an approach that can address problems of (1) compatibility and (2) efficiency that exist in previous works. By compatibility, we mean that previous debuggers are not generic, i.e., they support only a special language or special intermediate code. Second, our approach provides two execution modes: the native mode, where the debuggee is directly executed on a real CPU, and the virtual machine mode, where the debuggee is executed on a virtual machine. Currently, our debugger provides four types of trade-off settings (designated by unit and optimization) to consider trade-offs between granularity, accuracy, overhead and memory requirement. The user can choose the appropriate setting flexibly during debugging without finishing and restarting the debuggee.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {79--88},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1064979.1064992},
 doi = {http://doi.acm.org/10.1145/1064979.1064992},
 acmid = {1064992},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugger, reverse execution, virtual machine},
} 

@inproceedings{Li:2005:MTR:1064979.1064993,
 author = {Li, Jianhui and Zhang, Peng and Etzion, Orna},
 title = {Module-aware translation for real-life desktop applications},
 abstract = {A dynamic binary translator is a just-in-time compiler that translates source architecture binaries into target architecture binaries on the fly. It enables the fast running of the source architecture binaries on the target architecture. Traditional dynamic binary translators invalidate their translations when a module is unloaded, so later re-loading of the same module will lead to a full retranslation. Moreover, most of the loading and unloading are performed on a few "hot" modules, which causes the dynamic binary translator to spend a significant amount of time on repeatedly translating these "hot" modules. Furthermore, the retranslation may lead to excessive memory consumption if the code pages containing the translated codes that have been invalidated are not timely recycled. In addition, we observed that the overhead for translating real-life desktop applications is a big challenge to the overall performance of the applications, and our detailed analysis proved that real-life desktop applications dynamically load and unload modules much more frequently as compared to popular benchmarks, such as SPEC CPU2000. To address these issues, we propose a translation reuse engine that uses a novel verification method and a module-aware memory management mechanism. The proposed approach was fully implemented in IA-32 Execution Layer (IA-32 EL) [1], a commercial dynamic binary translator that enables the execution of IA-32 applications on Intel\&#174; Itanium\&#174; processor family. Collected results show that the module-aware translation improves the performance of Adobe* Illustrator by 14.09\% and Microsoft* Publisher by 9.73\%. The overhead brought by the translation reuse engine accounts for no more than 0.2\% of execution time.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {89--99},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064979.1064993},
 doi = {http://doi.acm.org/10.1145/1064979.1064993},
 acmid = {1064993},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dynamic binary translation, dynamic loaded module, memory management, translation reuse},
} 

@inproceedings{Zhou:2005:PCB:1064979.1064994,
 author = {Zhou, Shukang and Childers, Bruce R. and Soffa, Mary Lou},
 title = {Planning for code buffer management in distributed virtual execution environments},
 abstract = {Virtual execution environments have become increasingly useful in system implementation, with dynamic translation techniques being an important component for performance-critical systems. Many devices have exceptionally tight performance and memory constraints (e.g., smart cards and sensors in distributed systems), which require effective resource management. One approach to manage code memory is to download code partitions on-demand from a server and to cache the partitions in the resource-constrained device (client). However, due to the high cost of downloading code and re-translation, it is critical to intelligently manage the code buffer to minimize the overhead of code buffer misses. Yet, intelligent buffer management on the tightly constrained client can be too expensive. In this paper, we propose to move code buffer management to the server, where sophisticated schemes can be employed. We describe two schemes that use profiling information to direct the client in caching code partitions. One scheme is designed for workloads with stable run-time behavior, while the other scheme adapts its decisions for workloads with unstable behaviors. We evaluate and compare our schemes and show they perform well, compared to other approaches, with the adaptive scheme having the best performance overall.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {100--109},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1064979.1064994},
 doi = {http://doi.acm.org/10.1145/1064979.1064994},
 acmid = {1064994},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptive code cache, code buffer, distributed environments, dynamic translation, generational cache, program partitioning},
} 

@inproceedings{Nally:2005:ASV:1064979.1064981,
 author = {Nally, Martin},
 title = {Application servers: virtualizing location, resources, memory, users and threads for business applications and web applications},
 abstract = {Application servers provide an environment for running business and web applications. By virtualizing threads, data and processing resources, memory and users, they provide the simplifying illusion for the programmer that the application is interacting with a single user, is running alone on the server, and is the sole user of resources, while allowing an efficient realization that scales with the number of users, and available hardware. They also provide a virtual environment where security enforcement and demarcation of transaction boundaries are automatic. This talk will describe some of the major features of modern application servers and show how concepts of virtualization are fundamental to their design and realization.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {110--110},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/1064979.1064981},
 doi = {http://doi.acm.org/10.1145/1064979.1064981},
 acmid = {1064981},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{Kotzmann:2005:EAC:1064979.1064996,
 author = {Kotzmann, Thomas and M\"{o}ssenb\"{o}ck, Hanspeter},
 title = {Escape analysis in the context of dynamic compilation and deoptimization},
 abstract = {In object-oriented programming languages, an object is said to escape</i> the method or thread in which it was created if it can also be accessed by other methods or threads. Knowing which objects do not escape allows a compiler to perform aggressive optimizations.This paper presents a new intraprocedural and interprocedural algorithm for escape analysis in the context of dynamic compilation where the compiler has to cope with dynamic class loading and deoptimization. It was implemented for Sun Microsystems' Java HotSpot\&#8482; client compiler and operates on an intermediate representation in SSA form. We introduce equi-escape sets for the efficient propagation of escape information between related objects. The analysis is used for scalar replacement</i> of fields and synchronization removal</i>, as well as for stack allocation</i> of objects and fixed-sized arrays. The results of the interprocedural analysis support the compiler in inlining decisions and allow actual parameters to be allocated on the caller stack.Under certain circumstances, the Java HotSpot\&#8482; VM is forced to stop executing a method's machine code and transfer control to the interpreter. This is called deoptimization</i>. Since the interpreter does not know about the scalar replacement and synchronization removal performed by the compiler, the deoptimization framework was extended to reallocate and relock objects on demand.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {111--120},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1064979.1064996},
 doi = {http://doi.acm.org/10.1145/1064979.1064996},
 acmid = {1064996},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, deoptimization, escape analysis, just-in-time compilation, optimization, scalar replacement, stack allocation, synchronization removal},
} 

@inproceedings{Stepanian:2005:IJN:1064979.1064997,
 author = {Stepanian, Levon and Brown, Angela Demke and Kielstra, Allan and Koblents, Gita and Stoodley, Kevin},
 title = {Inlining java native calls at runtime},
 abstract = {We introduce a strategy for inlining native functions into Java\&#8482; applications using a JIT compiler. We perform further optimizations to transform inlined callbacks</i> into semantically equivalent lightweight operations. We show that this strategy can substantially reduce the overhead of performing JNI calls, while preserving the key safety and portability properties of the JNI. Our work leverages the ability to store statically-generated IL alongside native binaries, to facilitate native inlining at Java callsites at JIT compilation time. Preliminary results with our prototype implementation show speedups of up to 93X when inlining and callback transformation are combined.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {121--131},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064979.1064997},
 doi = {http://doi.acm.org/10.1145/1064979.1064997},
 acmid = {1064997},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {JIT compilation, JNI, Java, inlining, native code},
} 

@inproceedings{Wimmer:2005:OIS:1064979.1064998,
 author = {Wimmer, Christian and M\"{o}ssenb\"{o}ck, Hanspeter},
 title = {Optimized interval splitting in a linear scan register allocator},
 abstract = {We present an optimized implementation of the linear scan register allocation algorithm for Sun Microsystems' Java HotSpot\&#8482; client compiler. Linear scan register allocation is especially suitable for just-in-time compilers because it is faster than the common graph-coloring approach and yields results of nearly the same quality.Our allocator improves the basic linear scan algorithm by adding more advanced optimizations: It makes use of lifetime holes, splits intervals if the register pressure is too high, and models register constraints of the target architecture with fixed intervals. Three additional optimizations move split positions out of loops, remove register-to-register moves and eliminate unnecessary spill stores. Interval splitting is based on use positions, which also capture the kind of use and whether an operand is needed in a register or not. This avoids the reservation of a scratch register.Benchmark results prove the efficiency of the linear scan algorithm: While the compilation speed is equal to the old local register allocator that is part of the Sun JDK 5.0, integer benchmarks execute about 15\% faster. Floating-point benchmarks show the high impact of the Intel SSE2 extensions on the speed of numeric Java applications: With the new SSE2 support enabled, SPECjvm98 executes 25\% faster compared with the current Sun JDK 5.0.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {132--141},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1064979.1064998},
 doi = {http://doi.acm.org/10.1145/1064979.1064998},
 acmid = {1064998},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compilers, graph-coloring, java, just-in-time compilation, linear scan, optimization, register allocation},
} 

@inproceedings{Haupt:2005:ELA:1064979.1065000,
 author = {Haupt, Michael and Mezini, Mira and Bockisch, Christoph and Dinkelaker, Tom and Eichberg, Michael and Krebs, Michael},
 title = {An execution layer for aspect-oriented programming languages},
 abstract = {Language mechanisms deserve language implementation effort. While this maxim has led to sophisticated support for language features specific to object-oriented, functional and logic programming languages, aspect-oriented programming languages are still mostly implemented using postprocessors. The Steamloom virtual machine, based on IBM's Jikes RVM, provides support for aspect-oriented programming at virtual machine level. A bytecode framework called BAT was integrated with the Jikes RVM to replace its bytecode management logic. While preserving the functionality needed by the VM, BAT also allows for querying application code for join point shadows, avoiding redundancy in bytecode representation. Performance measurements show that an AOP-enabled virtual machine like Steamloom does not inflict unnecessary performance penalties on a running application; when it comes to executing AOP-related operations, there even are significant performance gains compared to other approaches.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {142--152},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064979.1065000},
 doi = {http://doi.acm.org/10.1145/1064979.1065000},
 acmid = {1065000},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {aspect-oriented programming, virtual machine support},
} 

@inproceedings{Shi:2005:VMS:1064979.1065001,
 author = {Shi, Yunhe and Gregg, David and Beatty, Andrew and Ertl, M. Anton},
 title = {Virtual machine showdown: stack versus registers},
 abstract = {Virtual machines (VMs) are commonly used to distribute programs in an architecture-neutral format, which can easily be interpreted or compiled. A long-running question in the design of VMs is whether stack architecture or register architecture can be implemented more efficiently with an interpreter. We extend existing work on comparing virtual stack and virtual register architectures in two ways. Firstly, our translation from stack to register code is much more sophisticated. The result is that we eliminate an average of more than 47\% of executed VM instructions, with the register machine bytecode size only 25\% larger than that of the corresponding stack bytecode. Secondly we present an implementation of a register machine in a fully standard-compliant implementation of the Java VM. We find that, on the Pentium 4, the register architecture requires an average of 32.3\% less time to execute standard benchmarks if dispatch is performed using a C switch statement. Even if more efficient threaded dispatch is available (which requires labels as first class values), the reduction in running time is still approximately 26.5\% for the register architecture.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {153--163},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064979.1065001},
 doi = {http://doi.acm.org/10.1145/1064979.1065001},
 acmid = {1065001},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {interpreter, register architecture, stack architecture, virtual machine},
} 

@inproceedings{Biberstein:2005:IAP:1064979.1065002,
 author = {Biberstein, Marina and Sreedhar, Vugranam C. and Mendelson, Bilha and Citron, Daniel and Giammaria, Alberto},
 title = {Instrumenting annotated programs},
 abstract = {Instrumentation is commonly used to track application behavior: to collect program profiles; to monitor component health and performance; to aid in component testing; and more. Program annotation enables developers and tools to pass extra information to later stages of software development and execution. For example, the .NET runtime relies on annotations for a significant chunk of the services it provides. Both mechanisms are evolving into important parts of software development \%, in the context of modern platforms such as Java and .NET.Instrumentation tools are generally not aware of the semantics of information passed via the annotation mechanism. This is especially true for post-compiler, e.g., run-time, instrumentation. The problem is that instrumentation may affect the correctness of annotations, rendering them invalid or misleading, and producing unforeseen side-effects during program execution. This problem has not been addressed so far.In this paper, we show the subtle interaction that takes place between annotations and instrumentation using several real-life examples. Many annotations are intended to provide information for the runtime; the virtual environment is a prominent annotation consumer, and must be aware of this conflict. It may also be required to provide runtime support to other annotation consumers. We propose an annotation taxonomy and show how instrumentation affects various annotations that were used in research and in industrial applications. We show how the annotations can expose enough information about themselves to prevent the instrumentation from accidentally corrupting the annotations. We demonstrate this approach on our annotations benchmark.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {164--174},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064979.1065002},
 doi = {http://doi.acm.org/10.1145/1064979.1065002},
 acmid = {1065002},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {annotation, custom attributes, meta-annotation, program instrumentation},
} 

@inproceedings{Alpern:2005:PVE:1064979.1065004,
 author = {Alpern, Bowen and Auerbach, Joshua and Bala, Vasanth and Frauenhofer, Thomas and Mummert, Todd and Pigott, Michael},
 title = {PDS: a virtual execution environment for software deployment},
 abstract = {The Progressive Deployment System (PDS) is a virtual execution environment and infrastructure designed specifically for deploying software, or "assets", on demand while enabling management from a central location. PDS intercepts a select subset of system calls on the target machine to provide a partial virtualization at the operating system level. This enables an asset's install-time environment to be reproduced virtually while otherwise not isolating the asset from peer applications on the target machine. Asset components, or "shards", are fetched as they are needed (or they may be pre-fetched), enabling the asset to be progressively deployed by overlapping deployment with execution. Cryptographic digests are used to eliminate redundant shards within and among assets, which enables more efficient deployment. A framework is provided for intercepting interfaces above the operating system (e.g., Java class loading), enabling optimizations requiring semantic awareness not present at the OS level. The paper presents the design of PDS, motivates its "porous isolation model" with respect to the challenges of software deployment, and presents measurements of PDS's execution characteristics.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {175--185},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064979.1065004},
 doi = {http://doi.acm.org/10.1145/1064979.1065004},
 acmid = {1065004},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {deployment, installation, management, streaming, virtualization},
} 

@inproceedings{Calder:2005:EVM:1064979.1065005,
 author = {Calder, Brad and Chien, Andrew A. and Wang, Ju and Yang, Don},
 title = {The entropia virtual machine for desktop grids},
 abstract = {Desktop distributed computing allows companies to exploit the idle cycles on pervasive desktop PC systems to increase the available computing power by orders of magnitude (10x - 1000x). Applications are submitted, distributed, and run on a grid of desktop PCs. Since the applications may be malformed, or malicious, the key challenges for a desktop grid are how to 1) prevent the distributed computing application from unwarranted access or modification of data and files on the desktop PC, 2) control the distributed computing application's resource usage and behavior as it runs on the desktop PC, and 3) provide protection for the distributed application's program and its data. In this paper we describe the Entropia Virtual Machine, and the solutions it embodies for each of these challenges.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {186--196},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064979.1065005},
 doi = {http://doi.acm.org/10.1145/1064979.1065005},
 acmid = {1065005},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {desktop grids, grid computing, virtual machine},
} 

@inproceedings{Kourai:2005:HVD:1064979.1065006,
 author = {Kourai, Kenichi and Chiba, Shigeru},
 title = {HyperSpector: virtual distributed monitoring environments for secure intrusion detection},
 abstract = {In this paper, a virtual distributed monitoring environment called HyperSpector</i> is described that achieves secure intrusion detection in distributed computer systems. While multiple intrusion detection systems (IDSes) can protect a distributed system from attackers, they can increase the number of insecure points in the protected system. HyperSpector overcomes this problem without any additional hardware by using virtualization to isolate each IDS from the servers it monitors. The IDSes are located in a virtual machine called an IDS VM and the servers are located in a server VM. The IDS VMs among different hosts are connected using a virtual network. To enable legacy IDSes running in the IDS VM to monitor the server VM, HyperSpector provides three inter-VM monitoring mechanisms: software port mirroring</i>, inter-VM disk mounting</i>, and inter-VM process mapping</i>. Consequently, active attacks, which directly attack the IDSes, are prevented. The impact of passive attacks, which wait until data including malicious code is read by an IDS and the IDS becomes compromised, is confined to within an affected HyperSpector environment.},
 booktitle = {Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments},
 series = {VEE '05},
 year = {2005},
 isbn = {1-59593-047-7},
 location = {Chicago, IL, USA},
 pages = {197--207},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1064979.1065006},
 doi = {http://doi.acm.org/10.1145/1064979.1065006},
 acmid = {1065006},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {distributed IDS, inter-VM monitoring, virtual machine, virtual network},
} 

